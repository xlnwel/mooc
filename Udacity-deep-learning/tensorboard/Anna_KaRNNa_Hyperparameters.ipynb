{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33, 78, 61, 76, 30, 17, 42, 25, 64,  2,  2,  2, 54, 61, 76, 76, 82,\n",
       "       25,  1, 61, 12, 26, 65, 26, 17, 19, 25, 61, 42, 17, 25, 61, 65, 65,\n",
       "       25, 61, 65, 26, 69, 17, 43, 25, 17, 48, 17, 42, 82, 25, 16, 10, 78,\n",
       "       61, 76, 76, 82, 25,  1, 61, 12, 26, 65, 82, 25, 26, 19, 25, 16, 10,\n",
       "       78, 61, 76, 76, 82, 25, 26, 10, 25, 26, 30, 19, 25, 13, 27, 10,  2,\n",
       "       27, 61, 82,  6,  2,  2, 49, 48, 17, 42, 82, 30, 78, 26, 10],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the number of batches. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33, 78, 61, 76, 30, 17, 42, 25, 64,  2],\n",
       "       [ 3, 10, 22, 25, 78, 17, 25, 12, 13, 48],\n",
       "       [25, 24, 61, 30, 24, 78, 26, 10, 35, 25],\n",
       "       [13, 30, 78, 17, 42, 25, 27, 13, 16, 65],\n",
       "       [25, 30, 78, 17, 25, 65, 61, 10, 22, 18],\n",
       "       [25, 11, 78, 42, 13, 16, 35, 78, 25, 65],\n",
       "       [30, 25, 30, 13,  2, 22, 13,  6,  2,  2],\n",
       "       [13, 25, 78, 17, 42, 19, 17, 65,  1, 41],\n",
       "       [78, 61, 30, 25, 26, 19, 25, 30, 78, 17],\n",
       "       [17, 42, 19, 17, 65,  1, 25, 61, 10, 22]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll write another function to grab batches out of the arrays made by split data. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "        \n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "        x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "    \n",
    "    with tf.name_scope('targets'):\n",
    "        targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "        y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "        y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # Build the RNN layers\n",
    "    with tf.name_scope(\"RNN_cells\"):\n",
    "        def make_cell():\n",
    "            lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "            drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "            return drop\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([make_cell() for _ in range(num_layers)])\n",
    "    \n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    \n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one row for each cell output\n",
    "    with tf.name_scope('sequence_reshape'):\n",
    "        seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "        output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer and calculate the cost\n",
    "    with tf.name_scope('logits'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                               name='softmax_w')\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        tf.summary.histogram('softmax_w', softmax_w)\n",
    "        tf.summary.histogram('softmax_b', softmax_b)\n",
    "\n",
    "    with tf.name_scope('predictions'):\n",
    "        preds = tf.nn.softmax(logits, name='predictions')\n",
    "        tf.summary.histogram('predictions', preds)\n",
    "    \n",
    "    with tf.name_scope('cost'):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_reshaped, name='loss')\n",
    "        cost = tf.reduce_mean(loss, name='cost')\n",
    "        tf.summary.scalar('cost', cost)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    with tf.name_scope('train'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "        optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer', 'merged']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. The two you probably haven't seen before are `lstm_size` and `num_layers`. These set the number of hidden units in the LSTM layers and the number of LSTM layers, respectively. Of course, making these bigger will improve the network's performance but you'll have to watch out for overfitting. If your validation loss is much larger than the training loss, you're probably overfitting. Decrease the size of the network or decrease the dropout keep probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p checkpoints/anna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, file_writer):\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Use the line below to load a checkpoint and resume training\n",
    "        #saver.restore(sess, 'checkpoints/anna20.ckpt')\n",
    "\n",
    "        n_batches = int(train_x.shape[1]/num_steps)\n",
    "        iterations = n_batches * epochs\n",
    "        for e in range(epochs):\n",
    "\n",
    "            # Train network\n",
    "            new_state = sess.run(model.initial_state)\n",
    "            loss = 0\n",
    "            for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "                iteration = e*n_batches + b\n",
    "                start = time.time()\n",
    "                feed = {model.inputs: x,\n",
    "                        model.targets: y,\n",
    "                        model.keep_prob: 0.5,\n",
    "                        model.initial_state: new_state}\n",
    "                summary, batch_loss, new_state, _ = sess.run([model.merged, model.cost, \n",
    "                                                              model.final_state, model.optimizer], \n",
    "                                                              feed_dict=feed)\n",
    "                loss += batch_loss\n",
    "                end = time.time()\n",
    "                print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                      'Iteration {}/{}'.format(iteration, iterations),\n",
    "                      'Training loss: {:.4f}'.format(loss/b),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "\n",
    "                file_writer.add_summary(summary, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-3ad34c58f1c2>:57: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Epoch 1/20  Iteration 1/3560 Training loss: 4.4115 0.3018 sec/batch\n",
      "Epoch 1/20  Iteration 2/3560 Training loss: 4.3983 0.2493 sec/batch\n",
      "Epoch 1/20  Iteration 3/3560 Training loss: 4.3830 0.2709 sec/batch\n",
      "Epoch 1/20  Iteration 4/3560 Training loss: 4.3606 0.2037 sec/batch\n",
      "Epoch 1/20  Iteration 5/3560 Training loss: 4.3165 0.2074 sec/batch\n",
      "Epoch 1/20  Iteration 6/3560 Training loss: 4.2289 0.2081 sec/batch\n",
      "Epoch 1/20  Iteration 7/3560 Training loss: 4.1369 0.1856 sec/batch\n",
      "Epoch 1/20  Iteration 8/3560 Training loss: 4.0583 0.2239 sec/batch\n",
      "Epoch 1/20  Iteration 9/3560 Training loss: 3.9904 0.1963 sec/batch\n",
      "Epoch 1/20  Iteration 10/3560 Training loss: 3.9320 0.1859 sec/batch\n",
      "Epoch 1/20  Iteration 11/3560 Training loss: 3.8779 0.1732 sec/batch\n",
      "Epoch 1/20  Iteration 12/3560 Training loss: 3.8321 0.2024 sec/batch\n",
      "Epoch 1/20  Iteration 13/3560 Training loss: 3.7917 0.1861 sec/batch\n",
      "Epoch 1/20  Iteration 14/3560 Training loss: 3.7576 0.1866 sec/batch\n",
      "Epoch 1/20  Iteration 15/3560 Training loss: 3.7271 0.1761 sec/batch\n",
      "Epoch 1/20  Iteration 16/3560 Training loss: 3.6993 0.2023 sec/batch\n",
      "Epoch 1/20  Iteration 17/3560 Training loss: 3.6732 0.1941 sec/batch\n",
      "Epoch 1/20  Iteration 18/3560 Training loss: 3.6517 0.1905 sec/batch\n",
      "Epoch 1/20  Iteration 19/3560 Training loss: 3.6309 0.1767 sec/batch\n",
      "Epoch 1/20  Iteration 20/3560 Training loss: 3.6104 0.1892 sec/batch\n",
      "Epoch 1/20  Iteration 21/3560 Training loss: 3.5923 0.1781 sec/batch\n",
      "Epoch 1/20  Iteration 22/3560 Training loss: 3.5753 0.1800 sec/batch\n",
      "Epoch 1/20  Iteration 23/3560 Training loss: 3.5594 0.1740 sec/batch\n",
      "Epoch 1/20  Iteration 24/3560 Training loss: 3.5451 0.1941 sec/batch\n",
      "Epoch 1/20  Iteration 25/3560 Training loss: 3.5311 0.1914 sec/batch\n",
      "Epoch 1/20  Iteration 26/3560 Training loss: 3.5186 0.1872 sec/batch\n",
      "Epoch 1/20  Iteration 27/3560 Training loss: 3.5071 0.1800 sec/batch\n",
      "Epoch 1/20  Iteration 28/3560 Training loss: 3.4952 0.1806 sec/batch\n",
      "Epoch 1/20  Iteration 29/3560 Training loss: 3.4842 0.1753 sec/batch\n",
      "Epoch 1/20  Iteration 30/3560 Training loss: 3.4743 0.1825 sec/batch\n",
      "Epoch 1/20  Iteration 31/3560 Training loss: 3.4656 0.1770 sec/batch\n",
      "Epoch 1/20  Iteration 32/3560 Training loss: 3.4560 0.1800 sec/batch\n",
      "Epoch 1/20  Iteration 33/3560 Training loss: 3.4467 0.1757 sec/batch\n",
      "Epoch 1/20  Iteration 34/3560 Training loss: 3.4387 0.1817 sec/batch\n",
      "Epoch 1/20  Iteration 35/3560 Training loss: 3.4303 0.1757 sec/batch\n",
      "Epoch 1/20  Iteration 36/3560 Training loss: 3.4228 0.1905 sec/batch\n",
      "Epoch 1/20  Iteration 37/3560 Training loss: 3.4149 0.1958 sec/batch\n",
      "Epoch 1/20  Iteration 38/3560 Training loss: 3.4077 0.2091 sec/batch\n",
      "Epoch 1/20  Iteration 39/3560 Training loss: 3.4006 0.1769 sec/batch\n",
      "Epoch 1/20  Iteration 40/3560 Training loss: 3.3939 0.1829 sec/batch\n",
      "Epoch 1/20  Iteration 41/3560 Training loss: 3.3873 0.1773 sec/batch\n",
      "Epoch 1/20  Iteration 42/3560 Training loss: 3.3811 0.1806 sec/batch\n",
      "Epoch 1/20  Iteration 43/3560 Training loss: 3.3750 0.1818 sec/batch\n",
      "Epoch 1/20  Iteration 44/3560 Training loss: 3.3692 0.1830 sec/batch\n",
      "Epoch 1/20  Iteration 45/3560 Training loss: 3.3634 0.1747 sec/batch\n",
      "Epoch 1/20  Iteration 46/3560 Training loss: 3.3581 0.1875 sec/batch\n",
      "Epoch 1/20  Iteration 47/3560 Training loss: 3.3532 0.1784 sec/batch\n",
      "Epoch 1/20  Iteration 48/3560 Training loss: 3.3485 0.1877 sec/batch\n",
      "Epoch 1/20  Iteration 49/3560 Training loss: 3.3439 0.1807 sec/batch\n",
      "Epoch 1/20  Iteration 50/3560 Training loss: 3.3396 0.1835 sec/batch\n",
      "Epoch 1/20  Iteration 51/3560 Training loss: 3.3351 0.1834 sec/batch\n",
      "Epoch 1/20  Iteration 52/3560 Training loss: 3.3305 0.1967 sec/batch\n",
      "Epoch 1/20  Iteration 53/3560 Training loss: 3.3264 0.1761 sec/batch\n",
      "Epoch 1/20  Iteration 54/3560 Training loss: 3.3222 0.1843 sec/batch\n",
      "Epoch 1/20  Iteration 55/3560 Training loss: 3.3181 0.1759 sec/batch\n",
      "Epoch 1/20  Iteration 56/3560 Training loss: 3.3140 0.1859 sec/batch\n",
      "Epoch 1/20  Iteration 57/3560 Training loss: 3.3101 0.1998 sec/batch\n",
      "Epoch 1/20  Iteration 58/3560 Training loss: 3.3063 0.2556 sec/batch\n",
      "Epoch 1/20  Iteration 59/3560 Training loss: 3.3025 0.3137 sec/batch\n",
      "Epoch 1/20  Iteration 60/3560 Training loss: 3.2990 0.3266 sec/batch\n",
      "Epoch 1/20  Iteration 61/3560 Training loss: 3.2956 0.2893 sec/batch\n",
      "Epoch 1/20  Iteration 62/3560 Training loss: 3.2925 0.2080 sec/batch\n",
      "Epoch 1/20  Iteration 63/3560 Training loss: 3.2896 0.2250 sec/batch\n",
      "Epoch 1/20  Iteration 64/3560 Training loss: 3.2860 0.2368 sec/batch\n",
      "Epoch 1/20  Iteration 65/3560 Training loss: 3.2827 0.2194 sec/batch\n",
      "Epoch 1/20  Iteration 66/3560 Training loss: 3.2796 0.2200 sec/batch\n",
      "Epoch 1/20  Iteration 67/3560 Training loss: 3.2767 0.2232 sec/batch\n",
      "Epoch 1/20  Iteration 68/3560 Training loss: 3.2731 0.1836 sec/batch\n",
      "Epoch 1/20  Iteration 69/3560 Training loss: 3.2700 0.1929 sec/batch\n",
      "Epoch 1/20  Iteration 70/3560 Training loss: 3.2672 0.1760 sec/batch\n",
      "Epoch 1/20  Iteration 71/3560 Training loss: 3.2643 0.1858 sec/batch\n",
      "Epoch 1/20  Iteration 72/3560 Training loss: 3.2617 0.1766 sec/batch\n",
      "Epoch 1/20  Iteration 73/3560 Training loss: 3.2588 0.1869 sec/batch\n",
      "Epoch 1/20  Iteration 74/3560 Training loss: 3.2559 0.1769 sec/batch\n",
      "Epoch 1/20  Iteration 75/3560 Training loss: 3.2533 0.2073 sec/batch\n",
      "Epoch 1/20  Iteration 76/3560 Training loss: 3.2508 0.2869 sec/batch\n",
      "Epoch 1/20  Iteration 77/3560 Training loss: 3.2481 0.2543 sec/batch\n",
      "Epoch 1/20  Iteration 78/3560 Training loss: 3.2456 0.2402 sec/batch\n",
      "Epoch 1/20  Iteration 79/3560 Training loss: 3.2429 0.2423 sec/batch\n",
      "Epoch 1/20  Iteration 80/3560 Training loss: 3.2401 0.2003 sec/batch\n",
      "Epoch 1/20  Iteration 81/3560 Training loss: 3.2376 0.2440 sec/batch\n",
      "Epoch 1/20  Iteration 82/3560 Training loss: 3.2351 0.2318 sec/batch\n",
      "Epoch 1/20  Iteration 83/3560 Training loss: 3.2327 0.2441 sec/batch\n",
      "Epoch 1/20  Iteration 84/3560 Training loss: 3.2300 0.2245 sec/batch\n",
      "Epoch 1/20  Iteration 85/3560 Training loss: 3.2273 0.2609 sec/batch\n",
      "Epoch 1/20  Iteration 86/3560 Training loss: 3.2246 0.2885 sec/batch\n",
      "Epoch 1/20  Iteration 87/3560 Training loss: 3.2220 0.2260 sec/batch\n",
      "Epoch 1/20  Iteration 88/3560 Training loss: 3.2194 0.1804 sec/batch\n",
      "Epoch 1/20  Iteration 89/3560 Training loss: 3.2169 0.1826 sec/batch\n",
      "Epoch 1/20  Iteration 90/3560 Training loss: 3.2145 0.2084 sec/batch\n",
      "Epoch 1/20  Iteration 91/3560 Training loss: 3.2121 0.1899 sec/batch\n",
      "Epoch 1/20  Iteration 92/3560 Training loss: 3.2095 0.1864 sec/batch\n",
      "Epoch 1/20  Iteration 93/3560 Training loss: 3.2071 0.1884 sec/batch\n",
      "Epoch 1/20  Iteration 94/3560 Training loss: 3.2046 0.1847 sec/batch\n",
      "Epoch 1/20  Iteration 95/3560 Training loss: 3.2020 0.1802 sec/batch\n",
      "Epoch 1/20  Iteration 96/3560 Training loss: 3.1994 0.1827 sec/batch\n",
      "Epoch 1/20  Iteration 97/3560 Training loss: 3.1969 0.2552 sec/batch\n",
      "Epoch 1/20  Iteration 98/3560 Training loss: 3.1943 0.2363 sec/batch\n",
      "Epoch 1/20  Iteration 99/3560 Training loss: 3.1918 0.2524 sec/batch\n",
      "Epoch 1/20  Iteration 100/3560 Training loss: 3.1891 0.2448 sec/batch\n",
      "Epoch 1/20  Iteration 101/3560 Training loss: 3.1867 0.2169 sec/batch\n",
      "Epoch 1/20  Iteration 102/3560 Training loss: 3.1842 0.1913 sec/batch\n",
      "Epoch 1/20  Iteration 103/3560 Training loss: 3.1816 0.1838 sec/batch\n",
      "Epoch 1/20  Iteration 104/3560 Training loss: 3.1790 0.2498 sec/batch\n",
      "Epoch 1/20  Iteration 105/3560 Training loss: 3.1764 0.1781 sec/batch\n",
      "Epoch 1/20  Iteration 106/3560 Training loss: 3.1738 0.1812 sec/batch\n",
      "Epoch 1/20  Iteration 107/3560 Training loss: 3.1710 0.2753 sec/batch\n",
      "Epoch 1/20  Iteration 108/3560 Training loss: 3.1683 0.1917 sec/batch\n",
      "Epoch 1/20  Iteration 109/3560 Training loss: 3.1658 0.1841 sec/batch\n",
      "Epoch 1/20  Iteration 110/3560 Training loss: 3.1629 0.1810 sec/batch\n",
      "Epoch 1/20  Iteration 111/3560 Training loss: 3.1602 0.2375 sec/batch\n",
      "Epoch 1/20  Iteration 112/3560 Training loss: 3.1576 0.2112 sec/batch\n",
      "Epoch 1/20  Iteration 113/3560 Training loss: 3.1549 0.1849 sec/batch\n",
      "Epoch 1/20  Iteration 114/3560 Training loss: 3.1520 0.1860 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 115/3560 Training loss: 3.1492 0.2348 sec/batch\n",
      "Epoch 1/20  Iteration 116/3560 Training loss: 3.1463 0.2496 sec/batch\n",
      "Epoch 1/20  Iteration 117/3560 Training loss: 3.1435 0.1803 sec/batch\n",
      "Epoch 1/20  Iteration 118/3560 Training loss: 3.1408 0.1833 sec/batch\n",
      "Epoch 1/20  Iteration 119/3560 Training loss: 3.1383 0.1929 sec/batch\n",
      "Epoch 1/20  Iteration 120/3560 Training loss: 3.1355 0.1932 sec/batch\n",
      "Epoch 1/20  Iteration 121/3560 Training loss: 3.1329 0.1761 sec/batch\n",
      "Epoch 1/20  Iteration 122/3560 Training loss: 3.1303 0.2051 sec/batch\n",
      "Epoch 1/20  Iteration 123/3560 Training loss: 3.1275 0.2065 sec/batch\n",
      "Epoch 1/20  Iteration 124/3560 Training loss: 3.1249 0.1967 sec/batch\n",
      "Epoch 1/20  Iteration 125/3560 Training loss: 3.1221 0.1830 sec/batch\n",
      "Epoch 1/20  Iteration 126/3560 Training loss: 3.1192 0.1767 sec/batch\n",
      "Epoch 1/20  Iteration 127/3560 Training loss: 3.1165 0.1995 sec/batch\n",
      "Epoch 1/20  Iteration 128/3560 Training loss: 3.1138 0.1904 sec/batch\n",
      "Epoch 1/20  Iteration 129/3560 Training loss: 3.1110 0.1795 sec/batch\n",
      "Epoch 1/20  Iteration 130/3560 Training loss: 3.1082 0.1830 sec/batch\n",
      "Epoch 1/20  Iteration 131/3560 Training loss: 3.1055 0.1953 sec/batch\n",
      "Epoch 1/20  Iteration 132/3560 Training loss: 3.1027 0.2140 sec/batch\n",
      "Epoch 1/20  Iteration 133/3560 Training loss: 3.1000 0.2318 sec/batch\n",
      "Epoch 1/20  Iteration 134/3560 Training loss: 3.0972 0.2402 sec/batch\n",
      "Epoch 1/20  Iteration 135/3560 Training loss: 3.0941 0.1979 sec/batch\n",
      "Epoch 1/20  Iteration 136/3560 Training loss: 3.0913 0.2279 sec/batch\n",
      "Epoch 1/20  Iteration 137/3560 Training loss: 3.0885 0.2048 sec/batch\n",
      "Epoch 1/20  Iteration 138/3560 Training loss: 3.0856 0.2140 sec/batch\n",
      "Epoch 1/20  Iteration 139/3560 Training loss: 3.0829 0.1989 sec/batch\n",
      "Epoch 1/20  Iteration 140/3560 Training loss: 3.0800 0.1995 sec/batch\n",
      "Epoch 1/20  Iteration 141/3560 Training loss: 3.0774 0.2412 sec/batch\n",
      "Epoch 1/20  Iteration 142/3560 Training loss: 3.0745 0.2460 sec/batch\n",
      "Epoch 1/20  Iteration 143/3560 Training loss: 3.0718 0.2015 sec/batch\n",
      "Epoch 1/20  Iteration 144/3560 Training loss: 3.0690 0.1924 sec/batch\n",
      "Epoch 1/20  Iteration 145/3560 Training loss: 3.0662 0.1770 sec/batch\n",
      "Epoch 1/20  Iteration 146/3560 Training loss: 3.0636 0.1841 sec/batch\n",
      "Epoch 1/20  Iteration 147/3560 Training loss: 3.0608 0.1807 sec/batch\n",
      "Epoch 1/20  Iteration 148/3560 Training loss: 3.0582 0.1881 sec/batch\n",
      "Epoch 1/20  Iteration 149/3560 Training loss: 3.0554 0.1865 sec/batch\n",
      "Epoch 1/20  Iteration 150/3560 Training loss: 3.0526 0.1889 sec/batch\n",
      "Epoch 1/20  Iteration 151/3560 Training loss: 3.0500 0.1767 sec/batch\n",
      "Epoch 1/20  Iteration 152/3560 Training loss: 3.0476 0.1869 sec/batch\n",
      "Epoch 1/20  Iteration 153/3560 Training loss: 3.0450 0.1792 sec/batch\n",
      "Epoch 1/20  Iteration 154/3560 Training loss: 3.0424 0.1834 sec/batch\n",
      "Epoch 1/20  Iteration 155/3560 Training loss: 3.0396 0.1762 sec/batch\n",
      "Epoch 1/20  Iteration 156/3560 Training loss: 3.0369 0.1787 sec/batch\n",
      "Epoch 1/20  Iteration 157/3560 Training loss: 3.0342 0.1781 sec/batch\n",
      "Epoch 1/20  Iteration 158/3560 Training loss: 3.0315 0.1853 sec/batch\n",
      "Epoch 1/20  Iteration 159/3560 Training loss: 3.0287 0.1779 sec/batch\n",
      "Epoch 1/20  Iteration 160/3560 Training loss: 3.0261 0.1797 sec/batch\n",
      "Epoch 1/20  Iteration 161/3560 Training loss: 3.0234 0.1770 sec/batch\n",
      "Epoch 1/20  Iteration 162/3560 Training loss: 3.0206 0.1821 sec/batch\n",
      "Epoch 1/20  Iteration 163/3560 Training loss: 3.0178 0.1809 sec/batch\n",
      "Epoch 1/20  Iteration 164/3560 Training loss: 3.0152 0.1792 sec/batch\n",
      "Epoch 1/20  Iteration 165/3560 Training loss: 3.0126 0.1819 sec/batch\n",
      "Epoch 1/20  Iteration 166/3560 Training loss: 3.0099 0.1799 sec/batch\n",
      "Epoch 1/20  Iteration 167/3560 Training loss: 3.0074 0.1760 sec/batch\n",
      "Epoch 1/20  Iteration 168/3560 Training loss: 3.0048 0.1852 sec/batch\n",
      "Epoch 1/20  Iteration 169/3560 Training loss: 3.0023 0.1823 sec/batch\n",
      "Epoch 1/20  Iteration 170/3560 Training loss: 2.9996 0.1841 sec/batch\n",
      "Epoch 1/20  Iteration 171/3560 Training loss: 2.9972 0.1757 sec/batch\n",
      "Epoch 1/20  Iteration 172/3560 Training loss: 2.9948 0.1789 sec/batch\n",
      "Epoch 1/20  Iteration 173/3560 Training loss: 2.9926 0.1774 sec/batch\n",
      "Epoch 1/20  Iteration 174/3560 Training loss: 2.9903 0.1947 sec/batch\n",
      "Epoch 1/20  Iteration 175/3560 Training loss: 2.9880 0.1784 sec/batch\n",
      "Epoch 1/20  Iteration 176/3560 Training loss: 2.9856 0.1959 sec/batch\n",
      "Epoch 1/20  Iteration 177/3560 Training loss: 2.9830 0.1916 sec/batch\n",
      "Epoch 1/20  Iteration 178/3560 Training loss: 2.9805 0.1832 sec/batch\n",
      "Epoch 2/20  Iteration 179/3560 Training loss: 2.5859 0.1936 sec/batch\n",
      "Epoch 2/20  Iteration 180/3560 Training loss: 2.5443 0.1856 sec/batch\n",
      "Epoch 2/20  Iteration 181/3560 Training loss: 2.5354 0.1777 sec/batch\n",
      "Epoch 2/20  Iteration 182/3560 Training loss: 2.5330 0.1809 sec/batch\n",
      "Epoch 2/20  Iteration 183/3560 Training loss: 2.5308 0.1774 sec/batch\n",
      "Epoch 2/20  Iteration 184/3560 Training loss: 2.5277 0.1795 sec/batch\n",
      "Epoch 2/20  Iteration 185/3560 Training loss: 2.5290 0.1855 sec/batch\n",
      "Epoch 2/20  Iteration 186/3560 Training loss: 2.5298 0.1808 sec/batch\n",
      "Epoch 2/20  Iteration 187/3560 Training loss: 2.5291 0.1805 sec/batch\n",
      "Epoch 2/20  Iteration 188/3560 Training loss: 2.5265 0.2063 sec/batch\n",
      "Epoch 2/20  Iteration 189/3560 Training loss: 2.5244 0.2309 sec/batch\n",
      "Epoch 2/20  Iteration 190/3560 Training loss: 2.5237 0.1892 sec/batch\n",
      "Epoch 2/20  Iteration 191/3560 Training loss: 2.5221 0.1860 sec/batch\n",
      "Epoch 2/20  Iteration 192/3560 Training loss: 2.5235 0.1761 sec/batch\n",
      "Epoch 2/20  Iteration 193/3560 Training loss: 2.5222 0.1774 sec/batch\n",
      "Epoch 2/20  Iteration 194/3560 Training loss: 2.5215 0.1741 sec/batch\n",
      "Epoch 2/20  Iteration 195/3560 Training loss: 2.5207 0.1853 sec/batch\n",
      "Epoch 2/20  Iteration 196/3560 Training loss: 2.5215 0.1794 sec/batch\n",
      "Epoch 2/20  Iteration 197/3560 Training loss: 2.5205 0.1832 sec/batch\n",
      "Epoch 2/20  Iteration 198/3560 Training loss: 2.5183 0.1766 sec/batch\n",
      "Epoch 2/20  Iteration 199/3560 Training loss: 2.5166 0.1782 sec/batch\n",
      "Epoch 2/20  Iteration 200/3560 Training loss: 2.5167 0.1758 sec/batch\n",
      "Epoch 2/20  Iteration 201/3560 Training loss: 2.5154 0.1820 sec/batch\n",
      "Epoch 2/20  Iteration 202/3560 Training loss: 2.5137 0.1796 sec/batch\n",
      "Epoch 2/20  Iteration 203/3560 Training loss: 2.5119 0.1827 sec/batch\n",
      "Epoch 2/20  Iteration 204/3560 Training loss: 2.5109 0.1929 sec/batch\n",
      "Epoch 2/20  Iteration 205/3560 Training loss: 2.5094 0.1880 sec/batch\n",
      "Epoch 2/20  Iteration 206/3560 Training loss: 2.5081 0.1873 sec/batch\n",
      "Epoch 2/20  Iteration 207/3560 Training loss: 2.5073 0.2205 sec/batch\n",
      "Epoch 2/20  Iteration 208/3560 Training loss: 2.5063 0.1790 sec/batch\n",
      "Epoch 2/20  Iteration 209/3560 Training loss: 2.5058 0.1811 sec/batch\n",
      "Epoch 2/20  Iteration 210/3560 Training loss: 2.5042 0.1770 sec/batch\n",
      "Epoch 2/20  Iteration 211/3560 Training loss: 2.5026 0.1819 sec/batch\n",
      "Epoch 2/20  Iteration 212/3560 Training loss: 2.5016 0.1925 sec/batch\n",
      "Epoch 2/20  Iteration 213/3560 Training loss: 2.5003 0.1840 sec/batch\n",
      "Epoch 2/20  Iteration 214/3560 Training loss: 2.4996 0.1758 sec/batch\n",
      "Epoch 2/20  Iteration 215/3560 Training loss: 2.4983 0.1815 sec/batch\n",
      "Epoch 2/20  Iteration 216/3560 Training loss: 2.4965 0.1775 sec/batch\n",
      "Epoch 2/20  Iteration 217/3560 Training loss: 2.4951 0.1867 sec/batch\n",
      "Epoch 2/20  Iteration 218/3560 Training loss: 2.4937 0.1874 sec/batch\n",
      "Epoch 2/20  Iteration 219/3560 Training loss: 2.4922 0.1835 sec/batch\n",
      "Epoch 2/20  Iteration 220/3560 Training loss: 2.4909 0.1767 sec/batch\n",
      "Epoch 2/20  Iteration 221/3560 Training loss: 2.4896 0.1818 sec/batch\n",
      "Epoch 2/20  Iteration 222/3560 Training loss: 2.4881 0.1878 sec/batch\n",
      "Epoch 2/20  Iteration 223/3560 Training loss: 2.4866 0.1884 sec/batch\n",
      "Epoch 2/20  Iteration 224/3560 Training loss: 2.4849 0.1844 sec/batch\n",
      "Epoch 2/20  Iteration 225/3560 Training loss: 2.4840 0.1805 sec/batch\n",
      "Epoch 2/20  Iteration 226/3560 Training loss: 2.4829 0.1815 sec/batch\n",
      "Epoch 2/20  Iteration 227/3560 Training loss: 2.4818 0.1827 sec/batch\n",
      "Epoch 2/20  Iteration 228/3560 Training loss: 2.4812 0.1846 sec/batch\n",
      "Epoch 2/20  Iteration 229/3560 Training loss: 2.4799 0.1839 sec/batch\n",
      "Epoch 2/20  Iteration 230/3560 Training loss: 2.4789 0.1813 sec/batch\n",
      "Epoch 2/20  Iteration 231/3560 Training loss: 2.4777 0.1898 sec/batch\n",
      "Epoch 2/20  Iteration 232/3560 Training loss: 2.4765 0.2020 sec/batch\n",
      "Epoch 2/20  Iteration 233/3560 Training loss: 2.4755 0.1821 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 234/3560 Training loss: 2.4746 0.1850 sec/batch\n",
      "Epoch 2/20  Iteration 235/3560 Training loss: 2.4738 0.1785 sec/batch\n",
      "Epoch 2/20  Iteration 236/3560 Training loss: 2.4726 0.1809 sec/batch\n",
      "Epoch 2/20  Iteration 237/3560 Training loss: 2.4716 0.1810 sec/batch\n",
      "Epoch 2/20  Iteration 238/3560 Training loss: 2.4709 0.1866 sec/batch\n",
      "Epoch 2/20  Iteration 239/3560 Training loss: 2.4698 0.1815 sec/batch\n",
      "Epoch 2/20  Iteration 240/3560 Training loss: 2.4691 0.1895 sec/batch\n",
      "Epoch 2/20  Iteration 241/3560 Training loss: 2.4684 0.1759 sec/batch\n",
      "Epoch 2/20  Iteration 242/3560 Training loss: 2.4673 0.1922 sec/batch\n",
      "Epoch 2/20  Iteration 243/3560 Training loss: 2.4662 0.1906 sec/batch\n",
      "Epoch 2/20  Iteration 244/3560 Training loss: 2.4656 0.2037 sec/batch\n",
      "Epoch 2/20  Iteration 245/3560 Training loss: 2.4648 0.2015 sec/batch\n",
      "Epoch 2/20  Iteration 246/3560 Training loss: 2.4634 0.1954 sec/batch\n",
      "Epoch 2/20  Iteration 247/3560 Training loss: 2.4623 0.1945 sec/batch\n",
      "Epoch 2/20  Iteration 248/3560 Training loss: 2.4617 0.2087 sec/batch\n",
      "Epoch 2/20  Iteration 249/3560 Training loss: 2.4609 0.1875 sec/batch\n",
      "Epoch 2/20  Iteration 250/3560 Training loss: 2.4602 0.1908 sec/batch\n",
      "Epoch 2/20  Iteration 251/3560 Training loss: 2.4594 0.1782 sec/batch\n",
      "Epoch 2/20  Iteration 252/3560 Training loss: 2.4584 0.1862 sec/batch\n",
      "Epoch 2/20  Iteration 253/3560 Training loss: 2.4575 0.1891 sec/batch\n",
      "Epoch 2/20  Iteration 254/3560 Training loss: 2.4571 0.1881 sec/batch\n",
      "Epoch 2/20  Iteration 255/3560 Training loss: 2.4562 0.2682 sec/batch\n",
      "Epoch 2/20  Iteration 256/3560 Training loss: 2.4557 0.1851 sec/batch\n",
      "Epoch 2/20  Iteration 257/3560 Training loss: 2.4547 0.1830 sec/batch\n",
      "Epoch 2/20  Iteration 258/3560 Training loss: 2.4537 0.2133 sec/batch\n",
      "Epoch 2/20  Iteration 259/3560 Training loss: 2.4529 0.2285 sec/batch\n",
      "Epoch 2/20  Iteration 260/3560 Training loss: 2.4523 0.2037 sec/batch\n",
      "Epoch 2/20  Iteration 261/3560 Training loss: 2.4513 0.1832 sec/batch\n",
      "Epoch 2/20  Iteration 262/3560 Training loss: 2.4503 0.1819 sec/batch\n",
      "Epoch 2/20  Iteration 263/3560 Training loss: 2.4490 0.1777 sec/batch\n",
      "Epoch 2/20  Iteration 264/3560 Training loss: 2.4480 0.1824 sec/batch\n",
      "Epoch 2/20  Iteration 265/3560 Training loss: 2.4472 0.1834 sec/batch\n",
      "Epoch 2/20  Iteration 266/3560 Training loss: 2.4464 0.1945 sec/batch\n",
      "Epoch 2/20  Iteration 267/3560 Training loss: 2.4455 0.1803 sec/batch\n",
      "Epoch 2/20  Iteration 268/3560 Training loss: 2.4448 0.1818 sec/batch\n",
      "Epoch 2/20  Iteration 269/3560 Training loss: 2.4440 0.1768 sec/batch\n",
      "Epoch 2/20  Iteration 270/3560 Training loss: 2.4433 0.1877 sec/batch\n",
      "Epoch 2/20  Iteration 271/3560 Training loss: 2.4424 0.1950 sec/batch\n",
      "Epoch 2/20  Iteration 272/3560 Training loss: 2.4415 0.1932 sec/batch\n",
      "Epoch 2/20  Iteration 273/3560 Training loss: 2.4404 0.1808 sec/batch\n",
      "Epoch 2/20  Iteration 274/3560 Training loss: 2.4395 0.1804 sec/batch\n",
      "Epoch 2/20  Iteration 275/3560 Training loss: 2.4387 0.1763 sec/batch\n",
      "Epoch 2/20  Iteration 276/3560 Training loss: 2.4378 0.1823 sec/batch\n",
      "Epoch 2/20  Iteration 277/3560 Training loss: 2.4371 0.1803 sec/batch\n",
      "Epoch 2/20  Iteration 278/3560 Training loss: 2.4362 0.1819 sec/batch\n",
      "Epoch 2/20  Iteration 279/3560 Training loss: 2.4357 0.1824 sec/batch\n",
      "Epoch 2/20  Iteration 280/3560 Training loss: 2.4350 0.1894 sec/batch\n",
      "Epoch 2/20  Iteration 281/3560 Training loss: 2.4341 0.1806 sec/batch\n",
      "Epoch 2/20  Iteration 282/3560 Training loss: 2.4333 0.1862 sec/batch\n",
      "Epoch 2/20  Iteration 283/3560 Training loss: 2.4324 0.1796 sec/batch\n",
      "Epoch 2/20  Iteration 284/3560 Training loss: 2.4316 0.1894 sec/batch\n",
      "Epoch 2/20  Iteration 285/3560 Training loss: 2.4308 0.1815 sec/batch\n",
      "Epoch 2/20  Iteration 286/3560 Training loss: 2.4303 0.2173 sec/batch\n",
      "Epoch 2/20  Iteration 287/3560 Training loss: 2.4298 0.1853 sec/batch\n",
      "Epoch 2/20  Iteration 288/3560 Training loss: 2.4288 0.1908 sec/batch\n",
      "Epoch 2/20  Iteration 289/3560 Training loss: 2.4281 0.1759 sec/batch\n",
      "Epoch 2/20  Iteration 290/3560 Training loss: 2.4275 0.1798 sec/batch\n",
      "Epoch 2/20  Iteration 291/3560 Training loss: 2.4267 0.1863 sec/batch\n",
      "Epoch 2/20  Iteration 292/3560 Training loss: 2.4259 0.1924 sec/batch\n",
      "Epoch 2/20  Iteration 293/3560 Training loss: 2.4252 0.1987 sec/batch\n",
      "Epoch 2/20  Iteration 294/3560 Training loss: 2.4241 0.1935 sec/batch\n",
      "Epoch 2/20  Iteration 295/3560 Training loss: 2.4234 0.1952 sec/batch\n",
      "Epoch 2/20  Iteration 296/3560 Training loss: 2.4227 0.2024 sec/batch\n",
      "Epoch 2/20  Iteration 297/3560 Training loss: 2.4221 0.2327 sec/batch\n",
      "Epoch 2/20  Iteration 298/3560 Training loss: 2.4215 0.1926 sec/batch\n",
      "Epoch 2/20  Iteration 299/3560 Training loss: 2.4210 0.1797 sec/batch\n",
      "Epoch 2/20  Iteration 300/3560 Training loss: 2.4202 0.1796 sec/batch\n",
      "Epoch 2/20  Iteration 301/3560 Training loss: 2.4196 0.1856 sec/batch\n",
      "Epoch 2/20  Iteration 302/3560 Training loss: 2.4191 0.1767 sec/batch\n",
      "Epoch 2/20  Iteration 303/3560 Training loss: 2.4184 0.1886 sec/batch\n",
      "Epoch 2/20  Iteration 304/3560 Training loss: 2.4177 0.1807 sec/batch\n",
      "Epoch 2/20  Iteration 305/3560 Training loss: 2.4171 0.1845 sec/batch\n",
      "Epoch 2/20  Iteration 306/3560 Training loss: 2.4166 0.1772 sec/batch\n",
      "Epoch 2/20  Iteration 307/3560 Training loss: 2.4159 0.1833 sec/batch\n",
      "Epoch 2/20  Iteration 308/3560 Training loss: 2.4153 0.1850 sec/batch\n",
      "Epoch 2/20  Iteration 309/3560 Training loss: 2.4146 0.1885 sec/batch\n",
      "Epoch 2/20  Iteration 310/3560 Training loss: 2.4138 0.1765 sec/batch\n",
      "Epoch 2/20  Iteration 311/3560 Training loss: 2.4132 0.1794 sec/batch\n",
      "Epoch 2/20  Iteration 312/3560 Training loss: 2.4127 0.1823 sec/batch\n",
      "Epoch 2/20  Iteration 313/3560 Training loss: 2.4120 0.2102 sec/batch\n",
      "Epoch 2/20  Iteration 314/3560 Training loss: 2.4115 0.1850 sec/batch\n",
      "Epoch 2/20  Iteration 315/3560 Training loss: 2.4109 0.1846 sec/batch\n",
      "Epoch 2/20  Iteration 316/3560 Training loss: 2.4103 0.1770 sec/batch\n",
      "Epoch 2/20  Iteration 317/3560 Training loss: 2.4099 0.1825 sec/batch\n",
      "Epoch 2/20  Iteration 318/3560 Training loss: 2.4092 0.1799 sec/batch\n",
      "Epoch 2/20  Iteration 319/3560 Training loss: 2.4088 0.1841 sec/batch\n",
      "Epoch 2/20  Iteration 320/3560 Training loss: 2.4082 0.1831 sec/batch\n",
      "Epoch 2/20  Iteration 321/3560 Training loss: 2.4076 0.1803 sec/batch\n",
      "Epoch 2/20  Iteration 322/3560 Training loss: 2.4069 0.1791 sec/batch\n",
      "Epoch 2/20  Iteration 323/3560 Training loss: 2.4063 0.1887 sec/batch\n",
      "Epoch 2/20  Iteration 324/3560 Training loss: 2.4060 0.1778 sec/batch\n",
      "Epoch 2/20  Iteration 325/3560 Training loss: 2.4054 0.1867 sec/batch\n",
      "Epoch 2/20  Iteration 326/3560 Training loss: 2.4050 0.1825 sec/batch\n",
      "Epoch 2/20  Iteration 327/3560 Training loss: 2.4044 0.1811 sec/batch\n",
      "Epoch 2/20  Iteration 328/3560 Training loss: 2.4037 0.1794 sec/batch\n",
      "Epoch 2/20  Iteration 329/3560 Training loss: 2.4033 0.1852 sec/batch\n",
      "Epoch 2/20  Iteration 330/3560 Training loss: 2.4031 0.1820 sec/batch\n",
      "Epoch 2/20  Iteration 331/3560 Training loss: 2.4026 0.2653 sec/batch\n",
      "Epoch 2/20  Iteration 332/3560 Training loss: 2.4021 0.2179 sec/batch\n",
      "Epoch 2/20  Iteration 333/3560 Training loss: 2.4016 0.1858 sec/batch\n",
      "Epoch 2/20  Iteration 334/3560 Training loss: 2.4010 0.1800 sec/batch\n",
      "Epoch 2/20  Iteration 335/3560 Training loss: 2.4004 0.2011 sec/batch\n",
      "Epoch 2/20  Iteration 336/3560 Training loss: 2.3999 0.1972 sec/batch\n",
      "Epoch 2/20  Iteration 337/3560 Training loss: 2.3991 0.1820 sec/batch\n",
      "Epoch 2/20  Iteration 338/3560 Training loss: 2.3988 0.1846 sec/batch\n",
      "Epoch 2/20  Iteration 339/3560 Training loss: 2.3983 0.1819 sec/batch\n",
      "Epoch 2/20  Iteration 340/3560 Training loss: 2.3976 0.1975 sec/batch\n",
      "Epoch 2/20  Iteration 341/3560 Training loss: 2.3970 0.1920 sec/batch\n",
      "Epoch 2/20  Iteration 342/3560 Training loss: 2.3964 0.1755 sec/batch\n",
      "Epoch 2/20  Iteration 343/3560 Training loss: 2.3960 0.1794 sec/batch\n",
      "Epoch 2/20  Iteration 344/3560 Training loss: 2.3954 0.1827 sec/batch\n",
      "Epoch 2/20  Iteration 345/3560 Training loss: 2.3949 0.1825 sec/batch\n",
      "Epoch 2/20  Iteration 346/3560 Training loss: 2.3945 0.2684 sec/batch\n",
      "Epoch 2/20  Iteration 347/3560 Training loss: 2.3940 0.2430 sec/batch\n",
      "Epoch 2/20  Iteration 348/3560 Training loss: 2.3933 0.1880 sec/batch\n",
      "Epoch 2/20  Iteration 349/3560 Training loss: 2.3929 0.2018 sec/batch\n",
      "Epoch 2/20  Iteration 350/3560 Training loss: 2.3925 0.2210 sec/batch\n",
      "Epoch 2/20  Iteration 351/3560 Training loss: 2.3922 0.2708 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 352/3560 Training loss: 2.3920 0.2177 sec/batch\n",
      "Epoch 2/20  Iteration 353/3560 Training loss: 2.3917 0.2001 sec/batch\n",
      "Epoch 2/20  Iteration 354/3560 Training loss: 2.3912 0.1971 sec/batch\n",
      "Epoch 2/20  Iteration 355/3560 Training loss: 2.3906 0.2133 sec/batch\n",
      "Epoch 2/20  Iteration 356/3560 Training loss: 2.3900 0.1902 sec/batch\n",
      "Epoch 3/20  Iteration 357/3560 Training loss: 2.3390 0.1817 sec/batch\n",
      "Epoch 3/20  Iteration 358/3560 Training loss: 2.2990 0.1786 sec/batch\n",
      "Epoch 3/20  Iteration 359/3560 Training loss: 2.2893 0.1848 sec/batch\n",
      "Epoch 3/20  Iteration 360/3560 Training loss: 2.2872 0.1819 sec/batch\n",
      "Epoch 3/20  Iteration 361/3560 Training loss: 2.2858 0.1961 sec/batch\n",
      "Epoch 3/20  Iteration 362/3560 Training loss: 2.2839 0.1890 sec/batch\n",
      "Epoch 3/20  Iteration 363/3560 Training loss: 2.2846 0.2031 sec/batch\n",
      "Epoch 3/20  Iteration 364/3560 Training loss: 2.2861 0.1859 sec/batch\n",
      "Epoch 3/20  Iteration 365/3560 Training loss: 2.2875 0.1838 sec/batch\n",
      "Epoch 3/20  Iteration 366/3560 Training loss: 2.2868 0.2228 sec/batch\n",
      "Epoch 3/20  Iteration 367/3560 Training loss: 2.2851 0.2342 sec/batch\n",
      "Epoch 3/20  Iteration 368/3560 Training loss: 2.2845 0.2445 sec/batch\n",
      "Epoch 3/20  Iteration 369/3560 Training loss: 2.2840 0.2099 sec/batch\n",
      "Epoch 3/20  Iteration 370/3560 Training loss: 2.2862 0.1976 sec/batch\n",
      "Epoch 3/20  Iteration 371/3560 Training loss: 2.2856 0.2372 sec/batch\n",
      "Epoch 3/20  Iteration 372/3560 Training loss: 2.2850 0.1904 sec/batch\n",
      "Epoch 3/20  Iteration 373/3560 Training loss: 2.2846 0.1911 sec/batch\n",
      "Epoch 3/20  Iteration 374/3560 Training loss: 2.2863 0.1779 sec/batch\n",
      "Epoch 3/20  Iteration 375/3560 Training loss: 2.2866 0.1831 sec/batch\n",
      "Epoch 3/20  Iteration 376/3560 Training loss: 2.2854 0.1783 sec/batch\n",
      "Epoch 3/20  Iteration 377/3560 Training loss: 2.2845 0.1820 sec/batch\n",
      "Epoch 3/20  Iteration 378/3560 Training loss: 2.2857 0.1758 sec/batch\n",
      "Epoch 3/20  Iteration 379/3560 Training loss: 2.2849 0.1797 sec/batch\n",
      "Epoch 3/20  Iteration 380/3560 Training loss: 2.2844 0.1823 sec/batch\n",
      "Epoch 3/20  Iteration 381/3560 Training loss: 2.2835 0.2000 sec/batch\n",
      "Epoch 3/20  Iteration 382/3560 Training loss: 2.2828 0.1859 sec/batch\n",
      "Epoch 3/20  Iteration 383/3560 Training loss: 2.2821 0.1886 sec/batch\n",
      "Epoch 3/20  Iteration 384/3560 Training loss: 2.2819 0.1872 sec/batch\n",
      "Epoch 3/20  Iteration 385/3560 Training loss: 2.2821 0.1784 sec/batch\n",
      "Epoch 3/20  Iteration 386/3560 Training loss: 2.2818 0.1808 sec/batch\n",
      "Epoch 3/20  Iteration 387/3560 Training loss: 2.2818 0.1907 sec/batch\n",
      "Epoch 3/20  Iteration 388/3560 Training loss: 2.2810 0.1754 sec/batch\n",
      "Epoch 3/20  Iteration 389/3560 Training loss: 2.2806 0.1895 sec/batch\n",
      "Epoch 3/20  Iteration 390/3560 Training loss: 2.2808 0.1788 sec/batch\n",
      "Epoch 3/20  Iteration 391/3560 Training loss: 2.2804 0.1840 sec/batch\n",
      "Epoch 3/20  Iteration 392/3560 Training loss: 2.2802 0.2045 sec/batch\n",
      "Epoch 3/20  Iteration 393/3560 Training loss: 2.2798 0.1905 sec/batch\n",
      "Epoch 3/20  Iteration 394/3560 Training loss: 2.2783 0.1843 sec/batch\n",
      "Epoch 3/20  Iteration 395/3560 Training loss: 2.2774 0.1843 sec/batch\n",
      "Epoch 3/20  Iteration 396/3560 Training loss: 2.2765 0.1787 sec/batch\n",
      "Epoch 3/20  Iteration 397/3560 Training loss: 2.2759 0.1845 sec/batch\n",
      "Epoch 3/20  Iteration 398/3560 Training loss: 2.2752 0.1828 sec/batch\n",
      "Epoch 3/20  Iteration 399/3560 Training loss: 2.2745 0.1787 sec/batch\n",
      "Epoch 3/20  Iteration 400/3560 Training loss: 2.2735 0.1849 sec/batch\n",
      "Epoch 3/20  Iteration 401/3560 Training loss: 2.2731 0.2009 sec/batch\n",
      "Epoch 3/20  Iteration 402/3560 Training loss: 2.2717 0.1897 sec/batch\n",
      "Epoch 3/20  Iteration 403/3560 Training loss: 2.2716 0.1828 sec/batch\n",
      "Epoch 3/20  Iteration 404/3560 Training loss: 2.2708 0.1886 sec/batch\n",
      "Epoch 3/20  Iteration 405/3560 Training loss: 2.2703 0.2101 sec/batch\n",
      "Epoch 3/20  Iteration 406/3560 Training loss: 2.2704 0.1862 sec/batch\n",
      "Epoch 3/20  Iteration 407/3560 Training loss: 2.2697 0.1935 sec/batch\n",
      "Epoch 3/20  Iteration 408/3560 Training loss: 2.2697 0.2203 sec/batch\n",
      "Epoch 3/20  Iteration 409/3560 Training loss: 2.2691 0.3973 sec/batch\n",
      "Epoch 3/20  Iteration 410/3560 Training loss: 2.2685 0.2667 sec/batch\n",
      "Epoch 3/20  Iteration 411/3560 Training loss: 2.2678 0.2610 sec/batch\n",
      "Epoch 3/20  Iteration 412/3560 Training loss: 2.2674 0.2368 sec/batch\n",
      "Epoch 3/20  Iteration 413/3560 Training loss: 2.2670 0.2124 sec/batch\n",
      "Epoch 3/20  Iteration 414/3560 Training loss: 2.2665 0.1845 sec/batch\n",
      "Epoch 3/20  Iteration 415/3560 Training loss: 2.2659 0.1943 sec/batch\n",
      "Epoch 3/20  Iteration 416/3560 Training loss: 2.2659 0.2041 sec/batch\n",
      "Epoch 3/20  Iteration 417/3560 Training loss: 2.2660 0.2033 sec/batch\n",
      "Epoch 3/20  Iteration 418/3560 Training loss: 2.2667 0.1835 sec/batch\n",
      "Epoch 3/20  Iteration 419/3560 Training loss: 2.2674 0.1858 sec/batch\n",
      "Epoch 3/20  Iteration 420/3560 Training loss: 2.2679 0.1858 sec/batch\n",
      "Epoch 3/20  Iteration 421/3560 Training loss: 2.2681 0.1940 sec/batch\n",
      "Epoch 3/20  Iteration 422/3560 Training loss: 2.2685 0.1926 sec/batch\n",
      "Epoch 3/20  Iteration 423/3560 Training loss: 2.2687 0.1815 sec/batch\n",
      "Epoch 3/20  Iteration 424/3560 Training loss: 2.2677 0.1942 sec/batch\n",
      "Epoch 3/20  Iteration 425/3560 Training loss: 2.2671 0.1811 sec/batch\n",
      "Epoch 3/20  Iteration 426/3560 Training loss: 2.2667 0.1873 sec/batch\n",
      "Epoch 3/20  Iteration 427/3560 Training loss: 2.2665 0.1811 sec/batch\n",
      "Epoch 3/20  Iteration 428/3560 Training loss: 2.2663 0.1799 sec/batch\n",
      "Epoch 3/20  Iteration 429/3560 Training loss: 2.2661 0.1785 sec/batch\n",
      "Epoch 3/20  Iteration 430/3560 Training loss: 2.2655 0.1819 sec/batch\n",
      "Epoch 3/20  Iteration 431/3560 Training loss: 2.2648 0.1787 sec/batch\n",
      "Epoch 3/20  Iteration 432/3560 Training loss: 2.2648 0.1828 sec/batch\n",
      "Epoch 3/20  Iteration 433/3560 Training loss: 2.2643 0.1804 sec/batch\n",
      "Epoch 3/20  Iteration 434/3560 Training loss: 2.2640 0.1839 sec/batch\n",
      "Epoch 3/20  Iteration 435/3560 Training loss: 2.2633 0.2685 sec/batch\n",
      "Epoch 3/20  Iteration 436/3560 Training loss: 2.2627 0.1973 sec/batch\n",
      "Epoch 3/20  Iteration 437/3560 Training loss: 2.2621 0.1878 sec/batch\n",
      "Epoch 3/20  Iteration 438/3560 Training loss: 2.2618 0.1762 sec/batch\n",
      "Epoch 3/20  Iteration 439/3560 Training loss: 2.2611 0.2053 sec/batch\n",
      "Epoch 3/20  Iteration 440/3560 Training loss: 2.2604 0.1901 sec/batch\n",
      "Epoch 3/20  Iteration 441/3560 Training loss: 2.2595 0.1874 sec/batch\n",
      "Epoch 3/20  Iteration 442/3560 Training loss: 2.2589 0.1785 sec/batch\n",
      "Epoch 3/20  Iteration 443/3560 Training loss: 2.2583 0.1841 sec/batch\n",
      "Epoch 3/20  Iteration 444/3560 Training loss: 2.2578 0.2020 sec/batch\n",
      "Epoch 3/20  Iteration 445/3560 Training loss: 2.2571 0.1858 sec/batch\n",
      "Epoch 3/20  Iteration 446/3560 Training loss: 2.2568 0.1855 sec/batch\n",
      "Epoch 3/20  Iteration 447/3560 Training loss: 2.2562 0.1771 sec/batch\n",
      "Epoch 3/20  Iteration 448/3560 Training loss: 2.2558 0.1976 sec/batch\n",
      "Epoch 3/20  Iteration 449/3560 Training loss: 2.2553 0.2909 sec/batch\n",
      "Epoch 3/20  Iteration 450/3560 Training loss: 2.2546 0.2132 sec/batch\n",
      "Epoch 3/20  Iteration 451/3560 Training loss: 2.2540 0.2125 sec/batch\n",
      "Epoch 3/20  Iteration 452/3560 Training loss: 2.2535 0.2272 sec/batch\n",
      "Epoch 3/20  Iteration 453/3560 Training loss: 2.2530 0.2048 sec/batch\n",
      "Epoch 3/20  Iteration 454/3560 Training loss: 2.2525 0.2125 sec/batch\n",
      "Epoch 3/20  Iteration 455/3560 Training loss: 2.2520 0.2391 sec/batch\n",
      "Epoch 3/20  Iteration 456/3560 Training loss: 2.2513 0.2434 sec/batch\n",
      "Epoch 3/20  Iteration 457/3560 Training loss: 2.2511 0.2118 sec/batch\n",
      "Epoch 3/20  Iteration 458/3560 Training loss: 2.2508 0.1971 sec/batch\n",
      "Epoch 3/20  Iteration 459/3560 Training loss: 2.2501 0.2032 sec/batch\n",
      "Epoch 3/20  Iteration 460/3560 Training loss: 2.2496 0.2000 sec/batch\n",
      "Epoch 3/20  Iteration 461/3560 Training loss: 2.2490 0.1953 sec/batch\n",
      "Epoch 3/20  Iteration 462/3560 Training loss: 2.2485 0.1959 sec/batch\n",
      "Epoch 3/20  Iteration 463/3560 Training loss: 2.2481 0.1973 sec/batch\n",
      "Epoch 3/20  Iteration 464/3560 Training loss: 2.2478 0.1882 sec/batch\n",
      "Epoch 3/20  Iteration 465/3560 Training loss: 2.2476 0.1814 sec/batch\n",
      "Epoch 3/20  Iteration 466/3560 Training loss: 2.2471 0.1758 sec/batch\n",
      "Epoch 3/20  Iteration 467/3560 Training loss: 2.2467 0.1833 sec/batch\n",
      "Epoch 3/20  Iteration 468/3560 Training loss: 2.2464 0.1776 sec/batch\n",
      "Epoch 3/20  Iteration 469/3560 Training loss: 2.2459 0.1991 sec/batch\n",
      "Epoch 3/20  Iteration 470/3560 Training loss: 2.2454 0.1791 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 471/3560 Training loss: 2.2449 0.1818 sec/batch\n",
      "Epoch 3/20  Iteration 472/3560 Training loss: 2.2442 0.1763 sec/batch\n",
      "Epoch 3/20  Iteration 473/3560 Training loss: 2.2438 0.2145 sec/batch\n",
      "Epoch 3/20  Iteration 474/3560 Training loss: 2.2433 0.1805 sec/batch\n",
      "Epoch 3/20  Iteration 475/3560 Training loss: 2.2431 0.1833 sec/batch\n",
      "Epoch 3/20  Iteration 476/3560 Training loss: 2.2428 0.1757 sec/batch\n",
      "Epoch 3/20  Iteration 477/3560 Training loss: 2.2426 0.1827 sec/batch\n",
      "Epoch 3/20  Iteration 478/3560 Training loss: 2.2421 0.1774 sec/batch\n",
      "Epoch 3/20  Iteration 479/3560 Training loss: 2.2417 0.1799 sec/batch\n",
      "Epoch 3/20  Iteration 480/3560 Training loss: 2.2415 0.1758 sec/batch\n",
      "Epoch 3/20  Iteration 481/3560 Training loss: 2.2411 0.1794 sec/batch\n",
      "Epoch 3/20  Iteration 482/3560 Training loss: 2.2405 0.1794 sec/batch\n",
      "Epoch 3/20  Iteration 483/3560 Training loss: 2.2402 0.1791 sec/batch\n",
      "Epoch 3/20  Iteration 484/3560 Training loss: 2.2400 0.1915 sec/batch\n",
      "Epoch 3/20  Iteration 485/3560 Training loss: 2.2397 0.1929 sec/batch\n",
      "Epoch 3/20  Iteration 486/3560 Training loss: 2.2394 0.1809 sec/batch\n",
      "Epoch 3/20  Iteration 487/3560 Training loss: 2.2390 0.1873 sec/batch\n",
      "Epoch 3/20  Iteration 488/3560 Training loss: 2.2384 0.1774 sec/batch\n",
      "Epoch 3/20  Iteration 489/3560 Training loss: 2.2381 0.1823 sec/batch\n",
      "Epoch 3/20  Iteration 490/3560 Training loss: 2.2379 0.1743 sec/batch\n",
      "Epoch 3/20  Iteration 491/3560 Training loss: 2.2375 0.1817 sec/batch\n",
      "Epoch 3/20  Iteration 492/3560 Training loss: 2.2373 0.1855 sec/batch\n",
      "Epoch 3/20  Iteration 493/3560 Training loss: 2.2370 0.1843 sec/batch\n",
      "Epoch 3/20  Iteration 494/3560 Training loss: 2.2368 0.1749 sec/batch\n",
      "Epoch 3/20  Iteration 495/3560 Training loss: 2.2367 0.1834 sec/batch\n",
      "Epoch 3/20  Iteration 496/3560 Training loss: 2.2363 0.1754 sec/batch\n",
      "Epoch 3/20  Iteration 497/3560 Training loss: 2.2362 0.2122 sec/batch\n",
      "Epoch 3/20  Iteration 498/3560 Training loss: 2.2358 0.1933 sec/batch\n",
      "Epoch 3/20  Iteration 499/3560 Training loss: 2.2355 0.1804 sec/batch\n",
      "Epoch 3/20  Iteration 500/3560 Training loss: 2.2352 0.1957 sec/batch\n",
      "Epoch 3/20  Iteration 501/3560 Training loss: 2.2348 0.1804 sec/batch\n",
      "Epoch 3/20  Iteration 502/3560 Training loss: 2.2347 0.1791 sec/batch\n",
      "Epoch 3/20  Iteration 503/3560 Training loss: 2.2344 0.1864 sec/batch\n",
      "Epoch 3/20  Iteration 504/3560 Training loss: 2.2343 0.1951 sec/batch\n",
      "Epoch 3/20  Iteration 505/3560 Training loss: 2.2340 0.2025 sec/batch\n",
      "Epoch 3/20  Iteration 506/3560 Training loss: 2.2336 0.1760 sec/batch\n",
      "Epoch 3/20  Iteration 507/3560 Training loss: 2.2333 0.1846 sec/batch\n",
      "Epoch 3/20  Iteration 508/3560 Training loss: 2.2332 0.1858 sec/batch\n",
      "Epoch 3/20  Iteration 509/3560 Training loss: 2.2330 0.1895 sec/batch\n",
      "Epoch 3/20  Iteration 510/3560 Training loss: 2.2328 0.1779 sec/batch\n",
      "Epoch 3/20  Iteration 511/3560 Training loss: 2.2324 0.1928 sec/batch\n",
      "Epoch 3/20  Iteration 512/3560 Training loss: 2.2321 0.1814 sec/batch\n",
      "Epoch 3/20  Iteration 513/3560 Training loss: 2.2318 0.1860 sec/batch\n",
      "Epoch 3/20  Iteration 514/3560 Training loss: 2.2314 0.1832 sec/batch\n",
      "Epoch 3/20  Iteration 515/3560 Training loss: 2.2310 0.1849 sec/batch\n",
      "Epoch 3/20  Iteration 516/3560 Training loss: 2.2309 0.1817 sec/batch\n",
      "Epoch 3/20  Iteration 517/3560 Training loss: 2.2307 0.1899 sec/batch\n",
      "Epoch 3/20  Iteration 518/3560 Training loss: 2.2303 0.1891 sec/batch\n",
      "Epoch 3/20  Iteration 519/3560 Training loss: 2.2299 0.1803 sec/batch\n",
      "Epoch 3/20  Iteration 520/3560 Training loss: 2.2296 0.1802 sec/batch\n",
      "Epoch 3/20  Iteration 521/3560 Training loss: 2.2294 0.1826 sec/batch\n",
      "Epoch 3/20  Iteration 522/3560 Training loss: 2.2290 0.1811 sec/batch\n",
      "Epoch 3/20  Iteration 523/3560 Training loss: 2.2287 0.1884 sec/batch\n",
      "Epoch 3/20  Iteration 524/3560 Training loss: 2.2287 0.1923 sec/batch\n",
      "Epoch 3/20  Iteration 525/3560 Training loss: 2.2284 0.2044 sec/batch\n",
      "Epoch 3/20  Iteration 526/3560 Training loss: 2.2280 0.1754 sec/batch\n",
      "Epoch 3/20  Iteration 527/3560 Training loss: 2.2277 0.2243 sec/batch\n",
      "Epoch 3/20  Iteration 528/3560 Training loss: 2.2274 0.1778 sec/batch\n",
      "Epoch 3/20  Iteration 529/3560 Training loss: 2.2272 0.1846 sec/batch\n",
      "Epoch 3/20  Iteration 530/3560 Training loss: 2.2270 0.1784 sec/batch\n",
      "Epoch 3/20  Iteration 531/3560 Training loss: 2.2268 0.1832 sec/batch\n",
      "Epoch 3/20  Iteration 532/3560 Training loss: 2.2266 0.1777 sec/batch\n",
      "Epoch 3/20  Iteration 533/3560 Training loss: 2.2262 0.1912 sec/batch\n",
      "Epoch 3/20  Iteration 534/3560 Training loss: 2.2259 0.1864 sec/batch\n",
      "Epoch 4/20  Iteration 535/3560 Training loss: 2.2180 0.1816 sec/batch\n",
      "Epoch 4/20  Iteration 536/3560 Training loss: 2.1800 0.1828 sec/batch\n",
      "Epoch 4/20  Iteration 537/3560 Training loss: 2.1683 0.1797 sec/batch\n",
      "Epoch 4/20  Iteration 538/3560 Training loss: 2.1649 0.1800 sec/batch\n",
      "Epoch 4/20  Iteration 539/3560 Training loss: 2.1637 0.1877 sec/batch\n",
      "Epoch 4/20  Iteration 540/3560 Training loss: 2.1604 0.1794 sec/batch\n",
      "Epoch 4/20  Iteration 541/3560 Training loss: 2.1617 0.1817 sec/batch\n",
      "Epoch 4/20  Iteration 542/3560 Training loss: 2.1627 0.1812 sec/batch\n",
      "Epoch 4/20  Iteration 543/3560 Training loss: 2.1665 0.1831 sec/batch\n",
      "Epoch 4/20  Iteration 544/3560 Training loss: 2.1647 0.1785 sec/batch\n",
      "Epoch 4/20  Iteration 545/3560 Training loss: 2.1622 0.1778 sec/batch\n",
      "Epoch 4/20  Iteration 546/3560 Training loss: 2.1604 0.1756 sec/batch\n",
      "Epoch 4/20  Iteration 547/3560 Training loss: 2.1607 0.1899 sec/batch\n",
      "Epoch 4/20  Iteration 548/3560 Training loss: 2.1635 0.1775 sec/batch\n",
      "Epoch 4/20  Iteration 549/3560 Training loss: 2.1629 0.1851 sec/batch\n",
      "Epoch 4/20  Iteration 550/3560 Training loss: 2.1619 0.1767 sec/batch\n",
      "Epoch 4/20  Iteration 551/3560 Training loss: 2.1621 0.1839 sec/batch\n",
      "Epoch 4/20  Iteration 552/3560 Training loss: 2.1641 0.1809 sec/batch\n",
      "Epoch 4/20  Iteration 553/3560 Training loss: 2.1643 0.2051 sec/batch\n",
      "Epoch 4/20  Iteration 554/3560 Training loss: 2.1634 0.1800 sec/batch\n",
      "Epoch 4/20  Iteration 555/3560 Training loss: 2.1626 0.1800 sec/batch\n",
      "Epoch 4/20  Iteration 556/3560 Training loss: 2.1638 0.1883 sec/batch\n",
      "Epoch 4/20  Iteration 557/3560 Training loss: 2.1633 0.1853 sec/batch\n",
      "Epoch 4/20  Iteration 558/3560 Training loss: 2.1626 0.1897 sec/batch\n",
      "Epoch 4/20  Iteration 559/3560 Training loss: 2.1620 0.2023 sec/batch\n",
      "Epoch 4/20  Iteration 560/3560 Training loss: 2.1613 0.1980 sec/batch\n",
      "Epoch 4/20  Iteration 561/3560 Training loss: 2.1605 0.2223 sec/batch\n",
      "Epoch 4/20  Iteration 562/3560 Training loss: 2.1604 0.2855 sec/batch\n",
      "Epoch 4/20  Iteration 563/3560 Training loss: 2.1612 0.2515 sec/batch\n",
      "Epoch 4/20  Iteration 564/3560 Training loss: 2.1612 0.2224 sec/batch\n",
      "Epoch 4/20  Iteration 565/3560 Training loss: 2.1613 0.1796 sec/batch\n",
      "Epoch 4/20  Iteration 566/3560 Training loss: 2.1606 0.1820 sec/batch\n",
      "Epoch 4/20  Iteration 567/3560 Training loss: 2.1602 0.1951 sec/batch\n",
      "Epoch 4/20  Iteration 568/3560 Training loss: 2.1605 0.1947 sec/batch\n",
      "Epoch 4/20  Iteration 569/3560 Training loss: 2.1602 0.1818 sec/batch\n",
      "Epoch 4/20  Iteration 570/3560 Training loss: 2.1597 0.1817 sec/batch\n",
      "Epoch 4/20  Iteration 571/3560 Training loss: 2.1592 0.1817 sec/batch\n",
      "Epoch 4/20  Iteration 572/3560 Training loss: 2.1580 0.1857 sec/batch\n",
      "Epoch 4/20  Iteration 573/3560 Training loss: 2.1571 0.1769 sec/batch\n",
      "Epoch 4/20  Iteration 574/3560 Training loss: 2.1563 0.1916 sec/batch\n",
      "Epoch 4/20  Iteration 575/3560 Training loss: 2.1557 0.2762 sec/batch\n",
      "Epoch 4/20  Iteration 576/3560 Training loss: 2.1554 0.2224 sec/batch\n",
      "Epoch 4/20  Iteration 577/3560 Training loss: 2.1548 0.2055 sec/batch\n",
      "Epoch 4/20  Iteration 578/3560 Training loss: 2.1540 0.2379 sec/batch\n",
      "Epoch 4/20  Iteration 579/3560 Training loss: 2.1537 0.2378 sec/batch\n",
      "Epoch 4/20  Iteration 580/3560 Training loss: 2.1523 0.1840 sec/batch\n",
      "Epoch 4/20  Iteration 581/3560 Training loss: 2.1523 0.1824 sec/batch\n",
      "Epoch 4/20  Iteration 582/3560 Training loss: 2.1516 0.1798 sec/batch\n",
      "Epoch 4/20  Iteration 583/3560 Training loss: 2.1512 0.1837 sec/batch\n",
      "Epoch 4/20  Iteration 584/3560 Training loss: 2.1517 0.1798 sec/batch\n",
      "Epoch 4/20  Iteration 585/3560 Training loss: 2.1510 0.1824 sec/batch\n",
      "Epoch 4/20  Iteration 586/3560 Training loss: 2.1513 0.1789 sec/batch\n",
      "Epoch 4/20  Iteration 587/3560 Training loss: 2.1508 0.1851 sec/batch\n",
      "Epoch 4/20  Iteration 588/3560 Training loss: 2.1504 0.1768 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 589/3560 Training loss: 2.1495 0.1849 sec/batch\n",
      "Epoch 4/20  Iteration 590/3560 Training loss: 2.1495 0.1815 sec/batch\n",
      "Epoch 4/20  Iteration 591/3560 Training loss: 2.1493 0.1928 sec/batch\n",
      "Epoch 4/20  Iteration 592/3560 Training loss: 2.1489 0.1800 sec/batch\n",
      "Epoch 4/20  Iteration 593/3560 Training loss: 2.1483 0.1825 sec/batch\n",
      "Epoch 4/20  Iteration 594/3560 Training loss: 2.1484 0.1763 sec/batch\n",
      "Epoch 4/20  Iteration 595/3560 Training loss: 2.1481 0.1823 sec/batch\n",
      "Epoch 4/20  Iteration 596/3560 Training loss: 2.1483 0.1736 sec/batch\n",
      "Epoch 4/20  Iteration 597/3560 Training loss: 2.1484 0.1803 sec/batch\n",
      "Epoch 4/20  Iteration 598/3560 Training loss: 2.1482 0.1798 sec/batch\n",
      "Epoch 4/20  Iteration 599/3560 Training loss: 2.1479 0.1859 sec/batch\n",
      "Epoch 4/20  Iteration 600/3560 Training loss: 2.1479 0.1783 sec/batch\n",
      "Epoch 4/20  Iteration 601/3560 Training loss: 2.1477 0.1801 sec/batch\n",
      "Epoch 4/20  Iteration 602/3560 Training loss: 2.1469 0.1787 sec/batch\n",
      "Epoch 4/20  Iteration 603/3560 Training loss: 2.1465 0.1816 sec/batch\n",
      "Epoch 4/20  Iteration 604/3560 Training loss: 2.1462 0.1787 sec/batch\n",
      "Epoch 4/20  Iteration 605/3560 Training loss: 2.1463 0.1867 sec/batch\n",
      "Epoch 4/20  Iteration 606/3560 Training loss: 2.1463 0.1982 sec/batch\n",
      "Epoch 4/20  Iteration 607/3560 Training loss: 2.1464 0.1812 sec/batch\n",
      "Epoch 4/20  Iteration 608/3560 Training loss: 2.1459 0.1767 sec/batch\n",
      "Epoch 4/20  Iteration 609/3560 Training loss: 2.1456 0.1840 sec/batch\n",
      "Epoch 4/20  Iteration 610/3560 Training loss: 2.1458 0.1891 sec/batch\n",
      "Epoch 4/20  Iteration 611/3560 Training loss: 2.1454 0.1987 sec/batch\n",
      "Epoch 4/20  Iteration 612/3560 Training loss: 2.1453 0.2115 sec/batch\n",
      "Epoch 4/20  Iteration 613/3560 Training loss: 2.1448 0.1866 sec/batch\n",
      "Epoch 4/20  Iteration 614/3560 Training loss: 2.1445 0.1844 sec/batch\n",
      "Epoch 4/20  Iteration 615/3560 Training loss: 2.1439 0.1797 sec/batch\n",
      "Epoch 4/20  Iteration 616/3560 Training loss: 2.1437 0.1802 sec/batch\n",
      "Epoch 4/20  Iteration 617/3560 Training loss: 2.1431 0.1795 sec/batch\n",
      "Epoch 4/20  Iteration 618/3560 Training loss: 2.1427 0.1808 sec/batch\n",
      "Epoch 4/20  Iteration 619/3560 Training loss: 2.1419 0.1836 sec/batch\n",
      "Epoch 4/20  Iteration 620/3560 Training loss: 2.1414 0.1849 sec/batch\n",
      "Epoch 4/20  Iteration 621/3560 Training loss: 2.1411 0.1795 sec/batch\n",
      "Epoch 4/20  Iteration 622/3560 Training loss: 2.1406 0.1835 sec/batch\n",
      "Epoch 4/20  Iteration 623/3560 Training loss: 2.1400 0.1771 sec/batch\n",
      "Epoch 4/20  Iteration 624/3560 Training loss: 2.1399 0.1867 sec/batch\n",
      "Epoch 4/20  Iteration 625/3560 Training loss: 2.1396 0.1796 sec/batch\n",
      "Epoch 4/20  Iteration 626/3560 Training loss: 2.1393 0.1837 sec/batch\n",
      "Epoch 4/20  Iteration 627/3560 Training loss: 2.1388 0.1789 sec/batch\n",
      "Epoch 4/20  Iteration 628/3560 Training loss: 2.1383 0.1842 sec/batch\n",
      "Epoch 4/20  Iteration 629/3560 Training loss: 2.1378 0.1838 sec/batch\n",
      "Epoch 4/20  Iteration 630/3560 Training loss: 2.1374 0.1977 sec/batch\n",
      "Epoch 4/20  Iteration 631/3560 Training loss: 2.1371 0.1842 sec/batch\n",
      "Epoch 4/20  Iteration 632/3560 Training loss: 2.1368 0.1801 sec/batch\n",
      "Epoch 4/20  Iteration 633/3560 Training loss: 2.1363 0.1933 sec/batch\n",
      "Epoch 4/20  Iteration 634/3560 Training loss: 2.1359 0.1933 sec/batch\n",
      "Epoch 4/20  Iteration 635/3560 Training loss: 2.1357 0.1833 sec/batch\n",
      "Epoch 4/20  Iteration 636/3560 Training loss: 2.1354 0.1869 sec/batch\n",
      "Epoch 4/20  Iteration 637/3560 Training loss: 2.1349 0.1755 sec/batch\n",
      "Epoch 4/20  Iteration 638/3560 Training loss: 2.1346 0.1761 sec/batch\n",
      "Epoch 4/20  Iteration 639/3560 Training loss: 2.1342 0.1788 sec/batch\n",
      "Epoch 4/20  Iteration 640/3560 Training loss: 2.1339 0.1808 sec/batch\n",
      "Epoch 4/20  Iteration 641/3560 Training loss: 2.1336 0.1792 sec/batch\n",
      "Epoch 4/20  Iteration 642/3560 Training loss: 2.1334 0.1883 sec/batch\n",
      "Epoch 4/20  Iteration 643/3560 Training loss: 2.1332 0.1789 sec/batch\n",
      "Epoch 4/20  Iteration 644/3560 Training loss: 2.1329 0.1816 sec/batch\n",
      "Epoch 4/20  Iteration 645/3560 Training loss: 2.1327 0.1793 sec/batch\n",
      "Epoch 4/20  Iteration 646/3560 Training loss: 2.1324 0.1806 sec/batch\n",
      "Epoch 4/20  Iteration 647/3560 Training loss: 2.1322 0.1767 sec/batch\n",
      "Epoch 4/20  Iteration 648/3560 Training loss: 2.1319 0.1854 sec/batch\n",
      "Epoch 4/20  Iteration 649/3560 Training loss: 2.1314 0.1768 sec/batch\n",
      "Epoch 4/20  Iteration 650/3560 Training loss: 2.1309 0.1812 sec/batch\n",
      "Epoch 4/20  Iteration 651/3560 Training loss: 2.1306 0.1778 sec/batch\n",
      "Epoch 4/20  Iteration 652/3560 Training loss: 2.1304 0.1805 sec/batch\n",
      "Epoch 4/20  Iteration 653/3560 Training loss: 2.1303 0.1771 sec/batch\n",
      "Epoch 4/20  Iteration 654/3560 Training loss: 2.1300 0.1873 sec/batch\n",
      "Epoch 4/20  Iteration 655/3560 Training loss: 2.1300 0.1764 sec/batch\n",
      "Epoch 4/20  Iteration 656/3560 Training loss: 2.1296 0.1847 sec/batch\n",
      "Epoch 4/20  Iteration 657/3560 Training loss: 2.1293 0.1759 sec/batch\n",
      "Epoch 4/20  Iteration 658/3560 Training loss: 2.1292 0.1835 sec/batch\n",
      "Epoch 4/20  Iteration 659/3560 Training loss: 2.1290 0.1791 sec/batch\n",
      "Epoch 4/20  Iteration 660/3560 Training loss: 2.1285 0.1816 sec/batch\n",
      "Epoch 4/20  Iteration 661/3560 Training loss: 2.1284 0.1776 sec/batch\n",
      "Epoch 4/20  Iteration 662/3560 Training loss: 2.1282 0.2017 sec/batch\n",
      "Epoch 4/20  Iteration 663/3560 Training loss: 2.1281 0.1779 sec/batch\n",
      "Epoch 4/20  Iteration 664/3560 Training loss: 2.1278 0.1839 sec/batch\n",
      "Epoch 4/20  Iteration 665/3560 Training loss: 2.1275 0.1830 sec/batch\n",
      "Epoch 4/20  Iteration 666/3560 Training loss: 2.1271 0.1958 sec/batch\n",
      "Epoch 4/20  Iteration 667/3560 Training loss: 2.1269 0.1971 sec/batch\n",
      "Epoch 4/20  Iteration 668/3560 Training loss: 2.1268 0.1805 sec/batch\n",
      "Epoch 4/20  Iteration 669/3560 Training loss: 2.1266 0.1800 sec/batch\n",
      "Epoch 4/20  Iteration 670/3560 Training loss: 2.1264 0.1834 sec/batch\n",
      "Epoch 4/20  Iteration 671/3560 Training loss: 2.1262 0.1784 sec/batch\n",
      "Epoch 4/20  Iteration 672/3560 Training loss: 2.1260 0.1831 sec/batch\n",
      "Epoch 4/20  Iteration 673/3560 Training loss: 2.1261 0.1773 sec/batch\n",
      "Epoch 4/20  Iteration 674/3560 Training loss: 2.1257 0.1812 sec/batch\n",
      "Epoch 4/20  Iteration 675/3560 Training loss: 2.1257 0.1772 sec/batch\n",
      "Epoch 4/20  Iteration 676/3560 Training loss: 2.1255 0.1807 sec/batch\n",
      "Epoch 4/20  Iteration 677/3560 Training loss: 2.1253 0.1764 sec/batch\n",
      "Epoch 4/20  Iteration 678/3560 Training loss: 2.1251 0.1842 sec/batch\n",
      "Epoch 4/20  Iteration 679/3560 Training loss: 2.1248 0.1767 sec/batch\n",
      "Epoch 4/20  Iteration 680/3560 Training loss: 2.1247 0.1837 sec/batch\n",
      "Epoch 4/20  Iteration 681/3560 Training loss: 2.1246 0.1767 sec/batch\n",
      "Epoch 4/20  Iteration 682/3560 Training loss: 2.1245 0.1787 sec/batch\n",
      "Epoch 4/20  Iteration 683/3560 Training loss: 2.1244 0.1761 sec/batch\n",
      "Epoch 4/20  Iteration 684/3560 Training loss: 2.1240 0.1926 sec/batch\n",
      "Epoch 4/20  Iteration 685/3560 Training loss: 2.1238 0.1822 sec/batch\n",
      "Epoch 4/20  Iteration 686/3560 Training loss: 2.1239 0.1953 sec/batch\n",
      "Epoch 4/20  Iteration 687/3560 Training loss: 2.1238 0.1973 sec/batch\n",
      "Epoch 4/20  Iteration 688/3560 Training loss: 2.1236 0.1851 sec/batch\n",
      "Epoch 4/20  Iteration 689/3560 Training loss: 2.1234 0.1963 sec/batch\n",
      "Epoch 4/20  Iteration 690/3560 Training loss: 2.1232 0.1933 sec/batch\n",
      "Epoch 4/20  Iteration 691/3560 Training loss: 2.1230 0.1813 sec/batch\n",
      "Epoch 4/20  Iteration 692/3560 Training loss: 2.1227 0.1834 sec/batch\n",
      "Epoch 4/20  Iteration 693/3560 Training loss: 2.1223 0.1856 sec/batch\n",
      "Epoch 4/20  Iteration 694/3560 Training loss: 2.1223 0.1791 sec/batch\n",
      "Epoch 4/20  Iteration 695/3560 Training loss: 2.1222 0.1791 sec/batch\n",
      "Epoch 4/20  Iteration 696/3560 Training loss: 2.1220 0.1860 sec/batch\n",
      "Epoch 4/20  Iteration 697/3560 Training loss: 2.1218 0.1773 sec/batch\n",
      "Epoch 4/20  Iteration 698/3560 Training loss: 2.1216 0.1812 sec/batch\n",
      "Epoch 4/20  Iteration 699/3560 Training loss: 2.1214 0.1755 sec/batch\n",
      "Epoch 4/20  Iteration 700/3560 Training loss: 2.1212 0.1864 sec/batch\n",
      "Epoch 4/20  Iteration 701/3560 Training loss: 2.1211 0.1746 sec/batch\n",
      "Epoch 4/20  Iteration 702/3560 Training loss: 2.1211 0.1785 sec/batch\n",
      "Epoch 4/20  Iteration 703/3560 Training loss: 2.1210 0.1791 sec/batch\n",
      "Epoch 4/20  Iteration 704/3560 Training loss: 2.1208 0.1813 sec/batch\n",
      "Epoch 4/20  Iteration 705/3560 Training loss: 2.1205 0.1764 sec/batch\n",
      "Epoch 4/20  Iteration 706/3560 Training loss: 2.1203 0.1801 sec/batch\n",
      "Epoch 4/20  Iteration 707/3560 Training loss: 2.1202 0.1743 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 708/3560 Training loss: 2.1201 0.1819 sec/batch\n",
      "Epoch 4/20  Iteration 709/3560 Training loss: 2.1201 0.1936 sec/batch\n",
      "Epoch 4/20  Iteration 710/3560 Training loss: 2.1199 0.1811 sec/batch\n",
      "Epoch 4/20  Iteration 711/3560 Training loss: 2.1197 0.1803 sec/batch\n",
      "Epoch 4/20  Iteration 712/3560 Training loss: 2.1195 0.1821 sec/batch\n",
      "Epoch 5/20  Iteration 713/3560 Training loss: 2.1401 0.1782 sec/batch\n",
      "Epoch 5/20  Iteration 714/3560 Training loss: 2.0991 0.1811 sec/batch\n",
      "Epoch 5/20  Iteration 715/3560 Training loss: 2.0918 0.1774 sec/batch\n",
      "Epoch 5/20  Iteration 716/3560 Training loss: 2.0875 0.1799 sec/batch\n",
      "Epoch 5/20  Iteration 717/3560 Training loss: 2.0840 0.1753 sec/batch\n",
      "Epoch 5/20  Iteration 718/3560 Training loss: 2.0793 0.2066 sec/batch\n",
      "Epoch 5/20  Iteration 719/3560 Training loss: 2.0798 0.1755 sec/batch\n",
      "Epoch 5/20  Iteration 720/3560 Training loss: 2.0806 0.1890 sec/batch\n",
      "Epoch 5/20  Iteration 721/3560 Training loss: 2.0833 0.1934 sec/batch\n",
      "Epoch 5/20  Iteration 722/3560 Training loss: 2.0825 0.1983 sec/batch\n",
      "Epoch 5/20  Iteration 723/3560 Training loss: 2.0797 0.1803 sec/batch\n",
      "Epoch 5/20  Iteration 724/3560 Training loss: 2.0786 0.1801 sec/batch\n",
      "Epoch 5/20  Iteration 725/3560 Training loss: 2.0780 0.1794 sec/batch\n",
      "Epoch 5/20  Iteration 726/3560 Training loss: 2.0799 0.1803 sec/batch\n",
      "Epoch 5/20  Iteration 727/3560 Training loss: 2.0793 0.1785 sec/batch\n",
      "Epoch 5/20  Iteration 728/3560 Training loss: 2.0783 0.1806 sec/batch\n",
      "Epoch 5/20  Iteration 729/3560 Training loss: 2.0781 0.1775 sec/batch\n",
      "Epoch 5/20  Iteration 730/3560 Training loss: 2.0804 0.1775 sec/batch\n",
      "Epoch 5/20  Iteration 731/3560 Training loss: 2.0802 0.1748 sec/batch\n",
      "Epoch 5/20  Iteration 732/3560 Training loss: 2.0795 0.1819 sec/batch\n",
      "Epoch 5/20  Iteration 733/3560 Training loss: 2.0784 0.1803 sec/batch\n",
      "Epoch 5/20  Iteration 734/3560 Training loss: 2.0792 0.1790 sec/batch\n",
      "Epoch 5/20  Iteration 735/3560 Training loss: 2.0789 0.1802 sec/batch\n",
      "Epoch 5/20  Iteration 736/3560 Training loss: 2.0783 0.1834 sec/batch\n",
      "Epoch 5/20  Iteration 737/3560 Training loss: 2.0778 0.1759 sec/batch\n",
      "Epoch 5/20  Iteration 738/3560 Training loss: 2.0773 0.1794 sec/batch\n",
      "Epoch 5/20  Iteration 739/3560 Training loss: 2.0766 0.1768 sec/batch\n",
      "Epoch 5/20  Iteration 740/3560 Training loss: 2.0769 0.1824 sec/batch\n",
      "Epoch 5/20  Iteration 741/3560 Training loss: 2.0780 0.1754 sec/batch\n",
      "Epoch 5/20  Iteration 742/3560 Training loss: 2.0784 0.1793 sec/batch\n",
      "Epoch 5/20  Iteration 743/3560 Training loss: 2.0782 0.1785 sec/batch\n",
      "Epoch 5/20  Iteration 744/3560 Training loss: 2.0771 0.1850 sec/batch\n",
      "Epoch 5/20  Iteration 745/3560 Training loss: 2.0767 0.1735 sec/batch\n",
      "Epoch 5/20  Iteration 746/3560 Training loss: 2.0773 0.2106 sec/batch\n",
      "Epoch 5/20  Iteration 747/3560 Training loss: 2.0770 0.1830 sec/batch\n",
      "Epoch 5/20  Iteration 748/3560 Training loss: 2.0765 0.1919 sec/batch\n",
      "Epoch 5/20  Iteration 749/3560 Training loss: 2.0759 0.1881 sec/batch\n",
      "Epoch 5/20  Iteration 750/3560 Training loss: 2.0746 0.1862 sec/batch\n",
      "Epoch 5/20  Iteration 751/3560 Training loss: 2.0739 0.1819 sec/batch\n",
      "Epoch 5/20  Iteration 752/3560 Training loss: 2.0730 0.1891 sec/batch\n",
      "Epoch 5/20  Iteration 753/3560 Training loss: 2.0724 0.1833 sec/batch\n",
      "Epoch 5/20  Iteration 754/3560 Training loss: 2.0722 0.1806 sec/batch\n",
      "Epoch 5/20  Iteration 755/3560 Training loss: 2.0716 0.2434 sec/batch\n",
      "Epoch 5/20  Iteration 756/3560 Training loss: 2.0709 0.2355 sec/batch\n",
      "Epoch 5/20  Iteration 757/3560 Training loss: 2.0708 0.2320 sec/batch\n",
      "Epoch 5/20  Iteration 758/3560 Training loss: 2.0693 0.2216 sec/batch\n",
      "Epoch 5/20  Iteration 759/3560 Training loss: 2.0692 0.2375 sec/batch\n",
      "Epoch 5/20  Iteration 760/3560 Training loss: 2.0686 0.2438 sec/batch\n",
      "Epoch 5/20  Iteration 761/3560 Training loss: 2.0682 0.2353 sec/batch\n",
      "Epoch 5/20  Iteration 762/3560 Training loss: 2.0687 0.2432 sec/batch\n",
      "Epoch 5/20  Iteration 763/3560 Training loss: 2.0680 0.2302 sec/batch\n",
      "Epoch 5/20  Iteration 764/3560 Training loss: 2.0684 0.1833 sec/batch\n",
      "Epoch 5/20  Iteration 765/3560 Training loss: 2.0680 0.1960 sec/batch\n",
      "Epoch 5/20  Iteration 766/3560 Training loss: 2.0677 0.2183 sec/batch\n",
      "Epoch 5/20  Iteration 767/3560 Training loss: 2.0673 0.2134 sec/batch\n",
      "Epoch 5/20  Iteration 768/3560 Training loss: 2.0672 0.2139 sec/batch\n",
      "Epoch 5/20  Iteration 769/3560 Training loss: 2.0673 0.2171 sec/batch\n",
      "Epoch 5/20  Iteration 770/3560 Training loss: 2.0668 0.2725 sec/batch\n",
      "Epoch 5/20  Iteration 771/3560 Training loss: 2.0663 0.2536 sec/batch\n",
      "Epoch 5/20  Iteration 772/3560 Training loss: 2.0667 0.2596 sec/batch\n",
      "Epoch 5/20  Iteration 773/3560 Training loss: 2.0663 0.2817 sec/batch\n",
      "Epoch 5/20  Iteration 774/3560 Training loss: 2.0667 0.1918 sec/batch\n",
      "Epoch 5/20  Iteration 775/3560 Training loss: 2.0669 0.1874 sec/batch\n",
      "Epoch 5/20  Iteration 776/3560 Training loss: 2.0670 0.1958 sec/batch\n",
      "Epoch 5/20  Iteration 777/3560 Training loss: 2.0666 0.1869 sec/batch\n",
      "Epoch 5/20  Iteration 778/3560 Training loss: 2.0668 0.1837 sec/batch\n",
      "Epoch 5/20  Iteration 779/3560 Training loss: 2.0667 0.1823 sec/batch\n",
      "Epoch 5/20  Iteration 780/3560 Training loss: 2.0660 0.1787 sec/batch\n",
      "Epoch 5/20  Iteration 781/3560 Training loss: 2.0658 0.1796 sec/batch\n",
      "Epoch 5/20  Iteration 782/3560 Training loss: 2.0657 0.1826 sec/batch\n",
      "Epoch 5/20  Iteration 783/3560 Training loss: 2.0659 0.1814 sec/batch\n",
      "Epoch 5/20  Iteration 784/3560 Training loss: 2.0661 0.1786 sec/batch\n",
      "Epoch 5/20  Iteration 785/3560 Training loss: 2.0663 0.1842 sec/batch\n",
      "Epoch 5/20  Iteration 786/3560 Training loss: 2.0659 0.1766 sec/batch\n",
      "Epoch 5/20  Iteration 787/3560 Training loss: 2.0657 0.1819 sec/batch\n",
      "Epoch 5/20  Iteration 788/3560 Training loss: 2.0660 0.1765 sec/batch\n",
      "Epoch 5/20  Iteration 789/3560 Training loss: 2.0657 0.1862 sec/batch\n",
      "Epoch 5/20  Iteration 790/3560 Training loss: 2.0657 0.2151 sec/batch\n",
      "Epoch 5/20  Iteration 791/3560 Training loss: 2.0653 0.2362 sec/batch\n",
      "Epoch 5/20  Iteration 792/3560 Training loss: 2.0649 0.2374 sec/batch\n",
      "Epoch 5/20  Iteration 793/3560 Training loss: 2.0642 0.2364 sec/batch\n",
      "Epoch 5/20  Iteration 794/3560 Training loss: 2.0642 0.2419 sec/batch\n",
      "Epoch 5/20  Iteration 795/3560 Training loss: 2.0638 0.2726 sec/batch\n",
      "Epoch 5/20  Iteration 796/3560 Training loss: 2.0636 0.2501 sec/batch\n",
      "Epoch 5/20  Iteration 797/3560 Training loss: 2.0629 0.2376 sec/batch\n",
      "Epoch 5/20  Iteration 798/3560 Training loss: 2.0625 0.1970 sec/batch\n",
      "Epoch 5/20  Iteration 799/3560 Training loss: 2.0622 0.1864 sec/batch\n",
      "Epoch 5/20  Iteration 800/3560 Training loss: 2.0618 0.1799 sec/batch\n",
      "Epoch 5/20  Iteration 801/3560 Training loss: 2.0612 0.2487 sec/batch\n",
      "Epoch 5/20  Iteration 802/3560 Training loss: 2.0612 0.2587 sec/batch\n",
      "Epoch 5/20  Iteration 803/3560 Training loss: 2.0609 0.2439 sec/batch\n",
      "Epoch 5/20  Iteration 804/3560 Training loss: 2.0606 0.1881 sec/batch\n",
      "Epoch 5/20  Iteration 805/3560 Training loss: 2.0600 0.1914 sec/batch\n",
      "Epoch 5/20  Iteration 806/3560 Training loss: 2.0596 0.1998 sec/batch\n",
      "Epoch 5/20  Iteration 807/3560 Training loss: 2.0591 0.2060 sec/batch\n",
      "Epoch 5/20  Iteration 808/3560 Training loss: 2.0588 0.1802 sec/batch\n",
      "Epoch 5/20  Iteration 809/3560 Training loss: 2.0586 0.1891 sec/batch\n",
      "Epoch 5/20  Iteration 810/3560 Training loss: 2.0581 0.1928 sec/batch\n",
      "Epoch 5/20  Iteration 811/3560 Training loss: 2.0576 0.2337 sec/batch\n",
      "Epoch 5/20  Iteration 812/3560 Training loss: 2.0572 0.1848 sec/batch\n",
      "Epoch 5/20  Iteration 813/3560 Training loss: 2.0571 0.1823 sec/batch\n",
      "Epoch 5/20  Iteration 814/3560 Training loss: 2.0570 0.1812 sec/batch\n",
      "Epoch 5/20  Iteration 815/3560 Training loss: 2.0565 0.1785 sec/batch\n",
      "Epoch 5/20  Iteration 816/3560 Training loss: 2.0561 0.1782 sec/batch\n",
      "Epoch 5/20  Iteration 817/3560 Training loss: 2.0558 0.1864 sec/batch\n",
      "Epoch 5/20  Iteration 818/3560 Training loss: 2.0556 0.1843 sec/batch\n",
      "Epoch 5/20  Iteration 819/3560 Training loss: 2.0554 0.1791 sec/batch\n",
      "Epoch 5/20  Iteration 820/3560 Training loss: 2.0552 0.1836 sec/batch\n",
      "Epoch 5/20  Iteration 821/3560 Training loss: 2.0550 0.2227 sec/batch\n",
      "Epoch 5/20  Iteration 822/3560 Training loss: 2.0548 0.1951 sec/batch\n",
      "Epoch 5/20  Iteration 823/3560 Training loss: 2.0547 0.2020 sec/batch\n",
      "Epoch 5/20  Iteration 824/3560 Training loss: 2.0545 0.1798 sec/batch\n",
      "Epoch 5/20  Iteration 825/3560 Training loss: 2.0543 0.1840 sec/batch\n",
      "Epoch 5/20  Iteration 826/3560 Training loss: 2.0540 0.1816 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 827/3560 Training loss: 2.0537 0.1791 sec/batch\n",
      "Epoch 5/20  Iteration 828/3560 Training loss: 2.0532 0.1811 sec/batch\n",
      "Epoch 5/20  Iteration 829/3560 Training loss: 2.0531 0.1865 sec/batch\n",
      "Epoch 5/20  Iteration 830/3560 Training loss: 2.0529 0.1935 sec/batch\n",
      "Epoch 5/20  Iteration 831/3560 Training loss: 2.0527 0.4470 sec/batch\n",
      "Epoch 5/20  Iteration 832/3560 Training loss: 2.0526 0.3063 sec/batch\n",
      "Epoch 5/20  Iteration 833/3560 Training loss: 2.0526 0.3130 sec/batch\n",
      "Epoch 5/20  Iteration 834/3560 Training loss: 2.0523 0.2478 sec/batch\n",
      "Epoch 5/20  Iteration 835/3560 Training loss: 2.0520 0.2258 sec/batch\n",
      "Epoch 5/20  Iteration 836/3560 Training loss: 2.0519 0.2335 sec/batch\n",
      "Epoch 5/20  Iteration 837/3560 Training loss: 2.0518 0.2015 sec/batch\n",
      "Epoch 5/20  Iteration 838/3560 Training loss: 2.0513 0.1922 sec/batch\n",
      "Epoch 5/20  Iteration 839/3560 Training loss: 2.0513 0.2018 sec/batch\n",
      "Epoch 5/20  Iteration 840/3560 Training loss: 2.0512 0.1808 sec/batch\n",
      "Epoch 5/20  Iteration 841/3560 Training loss: 2.0511 0.1887 sec/batch\n",
      "Epoch 5/20  Iteration 842/3560 Training loss: 2.0509 0.1843 sec/batch\n",
      "Epoch 5/20  Iteration 843/3560 Training loss: 2.0507 0.1866 sec/batch\n",
      "Epoch 5/20  Iteration 844/3560 Training loss: 2.0503 0.1991 sec/batch\n",
      "Epoch 5/20  Iteration 845/3560 Training loss: 2.0502 0.1945 sec/batch\n",
      "Epoch 5/20  Iteration 846/3560 Training loss: 2.0502 0.1826 sec/batch\n",
      "Epoch 5/20  Iteration 847/3560 Training loss: 2.0500 0.1854 sec/batch\n",
      "Epoch 5/20  Iteration 848/3560 Training loss: 2.0499 0.1774 sec/batch\n",
      "Epoch 5/20  Iteration 849/3560 Training loss: 2.0498 0.1806 sec/batch\n",
      "Epoch 5/20  Iteration 850/3560 Training loss: 2.0497 0.1812 sec/batch\n",
      "Epoch 5/20  Iteration 851/3560 Training loss: 2.0497 0.1818 sec/batch\n",
      "Epoch 5/20  Iteration 852/3560 Training loss: 2.0494 0.1752 sec/batch\n",
      "Epoch 5/20  Iteration 853/3560 Training loss: 2.0494 0.1814 sec/batch\n",
      "Epoch 5/20  Iteration 854/3560 Training loss: 2.0493 0.1800 sec/batch\n",
      "Epoch 5/20  Iteration 855/3560 Training loss: 2.0491 0.1811 sec/batch\n",
      "Epoch 5/20  Iteration 856/3560 Training loss: 2.0489 0.1761 sec/batch\n",
      "Epoch 5/20  Iteration 857/3560 Training loss: 2.0486 0.1853 sec/batch\n",
      "Epoch 5/20  Iteration 858/3560 Training loss: 2.0486 0.1811 sec/batch\n",
      "Epoch 5/20  Iteration 859/3560 Training loss: 2.0485 0.1815 sec/batch\n",
      "Epoch 5/20  Iteration 860/3560 Training loss: 2.0485 0.1764 sec/batch\n",
      "Epoch 5/20  Iteration 861/3560 Training loss: 2.0484 0.1951 sec/batch\n",
      "Epoch 5/20  Iteration 862/3560 Training loss: 2.0481 0.1798 sec/batch\n",
      "Epoch 5/20  Iteration 863/3560 Training loss: 2.0478 0.1798 sec/batch\n",
      "Epoch 5/20  Iteration 864/3560 Training loss: 2.0479 0.1778 sec/batch\n",
      "Epoch 5/20  Iteration 865/3560 Training loss: 2.0478 0.1779 sec/batch\n",
      "Epoch 5/20  Iteration 866/3560 Training loss: 2.0477 0.1835 sec/batch\n",
      "Epoch 5/20  Iteration 867/3560 Training loss: 2.0476 0.1850 sec/batch\n",
      "Epoch 5/20  Iteration 868/3560 Training loss: 2.0474 0.1831 sec/batch\n",
      "Epoch 5/20  Iteration 869/3560 Training loss: 2.0473 0.1859 sec/batch\n",
      "Epoch 5/20  Iteration 870/3560 Training loss: 2.0471 0.2204 sec/batch\n",
      "Epoch 5/20  Iteration 871/3560 Training loss: 2.0467 0.2276 sec/batch\n",
      "Epoch 5/20  Iteration 872/3560 Training loss: 2.0468 0.2394 sec/batch\n",
      "Epoch 5/20  Iteration 873/3560 Training loss: 2.0468 0.2294 sec/batch\n",
      "Epoch 5/20  Iteration 874/3560 Training loss: 2.0466 0.1940 sec/batch\n",
      "Epoch 5/20  Iteration 875/3560 Training loss: 2.0465 0.1870 sec/batch\n",
      "Epoch 5/20  Iteration 876/3560 Training loss: 2.0464 0.1831 sec/batch\n",
      "Epoch 5/20  Iteration 877/3560 Training loss: 2.0463 0.2191 sec/batch\n",
      "Epoch 5/20  Iteration 878/3560 Training loss: 2.0462 0.1984 sec/batch\n",
      "Epoch 5/20  Iteration 879/3560 Training loss: 2.0462 0.1958 sec/batch\n",
      "Epoch 5/20  Iteration 880/3560 Training loss: 2.0463 0.2063 sec/batch\n",
      "Epoch 5/20  Iteration 881/3560 Training loss: 2.0461 0.2009 sec/batch\n",
      "Epoch 5/20  Iteration 882/3560 Training loss: 2.0458 0.1838 sec/batch\n",
      "Epoch 5/20  Iteration 883/3560 Training loss: 2.0456 0.1819 sec/batch\n",
      "Epoch 5/20  Iteration 884/3560 Training loss: 2.0454 0.2036 sec/batch\n",
      "Epoch 5/20  Iteration 885/3560 Training loss: 2.0454 0.2052 sec/batch\n",
      "Epoch 5/20  Iteration 886/3560 Training loss: 2.0454 0.1932 sec/batch\n",
      "Epoch 5/20  Iteration 887/3560 Training loss: 2.0454 0.1841 sec/batch\n",
      "Epoch 5/20  Iteration 888/3560 Training loss: 2.0452 0.1824 sec/batch\n",
      "Epoch 5/20  Iteration 889/3560 Training loss: 2.0450 0.1844 sec/batch\n",
      "Epoch 5/20  Iteration 890/3560 Training loss: 2.0449 0.1774 sec/batch\n",
      "Epoch 6/20  Iteration 891/3560 Training loss: 2.0739 0.1802 sec/batch\n",
      "Epoch 6/20  Iteration 892/3560 Training loss: 2.0371 0.1822 sec/batch\n",
      "Epoch 6/20  Iteration 893/3560 Training loss: 2.0260 0.1946 sec/batch\n",
      "Epoch 6/20  Iteration 894/3560 Training loss: 2.0200 0.1837 sec/batch\n",
      "Epoch 6/20  Iteration 895/3560 Training loss: 2.0169 0.1853 sec/batch\n",
      "Epoch 6/20  Iteration 896/3560 Training loss: 2.0117 0.1810 sec/batch\n",
      "Epoch 6/20  Iteration 897/3560 Training loss: 2.0129 0.1789 sec/batch\n",
      "Epoch 6/20  Iteration 898/3560 Training loss: 2.0129 0.1751 sec/batch\n",
      "Epoch 6/20  Iteration 899/3560 Training loss: 2.0156 0.2059 sec/batch\n",
      "Epoch 6/20  Iteration 900/3560 Training loss: 2.0150 0.1775 sec/batch\n",
      "Epoch 6/20  Iteration 901/3560 Training loss: 2.0118 0.1812 sec/batch\n",
      "Epoch 6/20  Iteration 902/3560 Training loss: 2.0098 0.1816 sec/batch\n",
      "Epoch 6/20  Iteration 903/3560 Training loss: 2.0098 0.1822 sec/batch\n",
      "Epoch 6/20  Iteration 904/3560 Training loss: 2.0122 0.1842 sec/batch\n",
      "Epoch 6/20  Iteration 905/3560 Training loss: 2.0116 0.1814 sec/batch\n",
      "Epoch 6/20  Iteration 906/3560 Training loss: 2.0106 0.1766 sec/batch\n",
      "Epoch 6/20  Iteration 907/3560 Training loss: 2.0102 0.1801 sec/batch\n",
      "Epoch 6/20  Iteration 908/3560 Training loss: 2.0125 0.1761 sec/batch\n",
      "Epoch 6/20  Iteration 909/3560 Training loss: 2.0128 0.1782 sec/batch\n",
      "Epoch 6/20  Iteration 910/3560 Training loss: 2.0128 0.3142 sec/batch\n",
      "Epoch 6/20  Iteration 911/3560 Training loss: 2.0119 0.1999 sec/batch\n",
      "Epoch 6/20  Iteration 912/3560 Training loss: 2.0129 0.2003 sec/batch\n",
      "Epoch 6/20  Iteration 913/3560 Training loss: 2.0123 0.2041 sec/batch\n",
      "Epoch 6/20  Iteration 914/3560 Training loss: 2.0117 0.1959 sec/batch\n",
      "Epoch 6/20  Iteration 915/3560 Training loss: 2.0115 0.1884 sec/batch\n",
      "Epoch 6/20  Iteration 916/3560 Training loss: 2.0112 0.2051 sec/batch\n",
      "Epoch 6/20  Iteration 917/3560 Training loss: 2.0103 0.1828 sec/batch\n",
      "Epoch 6/20  Iteration 918/3560 Training loss: 2.0105 0.1850 sec/batch\n",
      "Epoch 6/20  Iteration 919/3560 Training loss: 2.0114 0.1826 sec/batch\n",
      "Epoch 6/20  Iteration 920/3560 Training loss: 2.0116 0.1888 sec/batch\n",
      "Epoch 6/20  Iteration 921/3560 Training loss: 2.0111 0.1787 sec/batch\n",
      "Epoch 6/20  Iteration 922/3560 Training loss: 2.0105 0.1924 sec/batch\n",
      "Epoch 6/20  Iteration 923/3560 Training loss: 2.0103 0.1825 sec/batch\n",
      "Epoch 6/20  Iteration 924/3560 Training loss: 2.0110 0.2133 sec/batch\n",
      "Epoch 6/20  Iteration 925/3560 Training loss: 2.0105 0.2481 sec/batch\n",
      "Epoch 6/20  Iteration 926/3560 Training loss: 2.0101 0.1910 sec/batch\n",
      "Epoch 6/20  Iteration 927/3560 Training loss: 2.0096 0.1824 sec/batch\n",
      "Epoch 6/20  Iteration 928/3560 Training loss: 2.0084 0.1741 sec/batch\n",
      "Epoch 6/20  Iteration 929/3560 Training loss: 2.0074 0.1828 sec/batch\n",
      "Epoch 6/20  Iteration 930/3560 Training loss: 2.0067 0.1793 sec/batch\n",
      "Epoch 6/20  Iteration 931/3560 Training loss: 2.0059 0.1894 sec/batch\n",
      "Epoch 6/20  Iteration 932/3560 Training loss: 2.0059 0.1853 sec/batch\n",
      "Epoch 6/20  Iteration 933/3560 Training loss: 2.0053 0.1889 sec/batch\n",
      "Epoch 6/20  Iteration 934/3560 Training loss: 2.0043 0.2554 sec/batch\n",
      "Epoch 6/20  Iteration 935/3560 Training loss: 2.0044 0.1904 sec/batch\n",
      "Epoch 6/20  Iteration 936/3560 Training loss: 2.0031 0.1898 sec/batch\n",
      "Epoch 6/20  Iteration 937/3560 Training loss: 2.0032 0.1954 sec/batch\n",
      "Epoch 6/20  Iteration 938/3560 Training loss: 2.0027 0.2102 sec/batch\n",
      "Epoch 6/20  Iteration 939/3560 Training loss: 2.0024 0.1927 sec/batch\n",
      "Epoch 6/20  Iteration 940/3560 Training loss: 2.0030 0.1908 sec/batch\n",
      "Epoch 6/20  Iteration 941/3560 Training loss: 2.0022 0.1839 sec/batch\n",
      "Epoch 6/20  Iteration 942/3560 Training loss: 2.0028 0.2002 sec/batch\n",
      "Epoch 6/20  Iteration 943/3560 Training loss: 2.0025 0.1913 sec/batch\n",
      "Epoch 6/20  Iteration 944/3560 Training loss: 2.0022 0.1820 sec/batch\n",
      "Epoch 6/20  Iteration 945/3560 Training loss: 2.0015 0.1857 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 946/3560 Training loss: 2.0017 0.1802 sec/batch\n",
      "Epoch 6/20  Iteration 947/3560 Training loss: 2.0016 0.1789 sec/batch\n",
      "Epoch 6/20  Iteration 948/3560 Training loss: 2.0012 0.1827 sec/batch\n",
      "Epoch 6/20  Iteration 949/3560 Training loss: 2.0009 0.1782 sec/batch\n",
      "Epoch 6/20  Iteration 950/3560 Training loss: 2.0013 0.1803 sec/batch\n",
      "Epoch 6/20  Iteration 951/3560 Training loss: 2.0010 0.1798 sec/batch\n",
      "Epoch 6/20  Iteration 952/3560 Training loss: 2.0015 0.1958 sec/batch\n",
      "Epoch 6/20  Iteration 953/3560 Training loss: 2.0018 0.1903 sec/batch\n",
      "Epoch 6/20  Iteration 954/3560 Training loss: 2.0018 0.1864 sec/batch\n",
      "Epoch 6/20  Iteration 955/3560 Training loss: 2.0014 0.1781 sec/batch\n",
      "Epoch 6/20  Iteration 956/3560 Training loss: 2.0017 0.1818 sec/batch\n",
      "Epoch 6/20  Iteration 957/3560 Training loss: 2.0018 0.1761 sec/batch\n",
      "Epoch 6/20  Iteration 958/3560 Training loss: 2.0010 0.1826 sec/batch\n",
      "Epoch 6/20  Iteration 959/3560 Training loss: 2.0008 0.1788 sec/batch\n",
      "Epoch 6/20  Iteration 960/3560 Training loss: 2.0006 0.1814 sec/batch\n",
      "Epoch 6/20  Iteration 961/3560 Training loss: 2.0010 0.1761 sec/batch\n",
      "Epoch 6/20  Iteration 962/3560 Training loss: 2.0010 0.1823 sec/batch\n",
      "Epoch 6/20  Iteration 963/3560 Training loss: 2.0011 0.1795 sec/batch\n",
      "Epoch 6/20  Iteration 964/3560 Training loss: 2.0009 0.1795 sec/batch\n",
      "Epoch 6/20  Iteration 965/3560 Training loss: 2.0005 0.1842 sec/batch\n",
      "Epoch 6/20  Iteration 966/3560 Training loss: 2.0009 0.1848 sec/batch\n",
      "Epoch 6/20  Iteration 967/3560 Training loss: 2.0005 0.1814 sec/batch\n",
      "Epoch 6/20  Iteration 968/3560 Training loss: 2.0005 0.1780 sec/batch\n",
      "Epoch 6/20  Iteration 969/3560 Training loss: 2.0000 0.1776 sec/batch\n",
      "Epoch 6/20  Iteration 970/3560 Training loss: 1.9997 0.1793 sec/batch\n",
      "Epoch 6/20  Iteration 971/3560 Training loss: 1.9991 0.1766 sec/batch\n",
      "Epoch 6/20  Iteration 972/3560 Training loss: 1.9993 0.1793 sec/batch\n",
      "Epoch 6/20  Iteration 973/3560 Training loss: 1.9988 0.1772 sec/batch\n",
      "Epoch 6/20  Iteration 974/3560 Training loss: 1.9987 0.1769 sec/batch\n",
      "Epoch 6/20  Iteration 975/3560 Training loss: 1.9981 0.1753 sec/batch\n",
      "Epoch 6/20  Iteration 976/3560 Training loss: 1.9977 0.1870 sec/batch\n",
      "Epoch 6/20  Iteration 977/3560 Training loss: 1.9975 0.1775 sec/batch\n",
      "Epoch 6/20  Iteration 978/3560 Training loss: 1.9971 0.1949 sec/batch\n",
      "Epoch 6/20  Iteration 979/3560 Training loss: 1.9967 0.2283 sec/batch\n",
      "Epoch 6/20  Iteration 980/3560 Training loss: 1.9967 0.2163 sec/batch\n",
      "Epoch 6/20  Iteration 981/3560 Training loss: 1.9964 0.1996 sec/batch\n",
      "Epoch 6/20  Iteration 982/3560 Training loss: 1.9961 0.1809 sec/batch\n",
      "Epoch 6/20  Iteration 983/3560 Training loss: 1.9956 0.1802 sec/batch\n",
      "Epoch 6/20  Iteration 984/3560 Training loss: 1.9952 0.1855 sec/batch\n",
      "Epoch 6/20  Iteration 985/3560 Training loss: 1.9947 0.1782 sec/batch\n",
      "Epoch 6/20  Iteration 986/3560 Training loss: 1.9946 0.1815 sec/batch\n",
      "Epoch 6/20  Iteration 987/3560 Training loss: 1.9943 0.1792 sec/batch\n",
      "Epoch 6/20  Iteration 988/3560 Training loss: 1.9939 0.1812 sec/batch\n",
      "Epoch 6/20  Iteration 989/3560 Training loss: 1.9935 0.1827 sec/batch\n",
      "Epoch 6/20  Iteration 990/3560 Training loss: 1.9930 0.1834 sec/batch\n",
      "Epoch 6/20  Iteration 991/3560 Training loss: 1.9930 0.1763 sec/batch\n",
      "Epoch 6/20  Iteration 992/3560 Training loss: 1.9928 0.1795 sec/batch\n",
      "Epoch 6/20  Iteration 993/3560 Training loss: 1.9924 0.1861 sec/batch\n",
      "Epoch 6/20  Iteration 994/3560 Training loss: 1.9921 0.1847 sec/batch\n",
      "Epoch 6/20  Iteration 995/3560 Training loss: 1.9918 0.1807 sec/batch\n",
      "Epoch 6/20  Iteration 996/3560 Training loss: 1.9917 0.1811 sec/batch\n",
      "Epoch 6/20  Iteration 997/3560 Training loss: 1.9915 0.1775 sec/batch\n",
      "Epoch 6/20  Iteration 998/3560 Training loss: 1.9914 0.1876 sec/batch\n",
      "Epoch 6/20  Iteration 999/3560 Training loss: 1.9914 0.1776 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3560 Training loss: 1.9912 0.1839 sec/batch\n",
      "Epoch 6/20  Iteration 1001/3560 Training loss: 1.9910 0.1871 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3560 Training loss: 1.9907 0.1823 sec/batch\n",
      "Epoch 6/20  Iteration 1003/3560 Training loss: 1.9905 0.1992 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3560 Training loss: 1.9903 0.1908 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3560 Training loss: 1.9900 0.1821 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3560 Training loss: 1.9896 0.1844 sec/batch\n",
      "Epoch 6/20  Iteration 1007/3560 Training loss: 1.9894 0.1834 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3560 Training loss: 1.9893 0.2018 sec/batch\n",
      "Epoch 6/20  Iteration 1009/3560 Training loss: 1.9892 0.1916 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3560 Training loss: 1.9890 0.1841 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3560 Training loss: 1.9890 0.1818 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3560 Training loss: 1.9887 0.1898 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3560 Training loss: 1.9885 0.1834 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3560 Training loss: 1.9884 0.1835 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3560 Training loss: 1.9883 0.1815 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3560 Training loss: 1.9879 0.1868 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3560 Training loss: 1.9879 0.1786 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3560 Training loss: 1.9879 0.1801 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3560 Training loss: 1.9878 0.1839 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3560 Training loss: 1.9877 0.1820 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3560 Training loss: 1.9874 0.1841 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3560 Training loss: 1.9871 0.1797 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3560 Training loss: 1.9870 0.1790 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3560 Training loss: 1.9870 0.1806 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3560 Training loss: 1.9868 0.1787 sec/batch\n",
      "Epoch 6/20  Iteration 1026/3560 Training loss: 1.9868 0.1801 sec/batch\n",
      "Epoch 6/20  Iteration 1027/3560 Training loss: 1.9867 0.1798 sec/batch\n",
      "Epoch 6/20  Iteration 1028/3560 Training loss: 1.9867 0.1802 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3560 Training loss: 1.9868 0.1777 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3560 Training loss: 1.9865 0.1818 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3560 Training loss: 1.9866 0.1777 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3560 Training loss: 1.9865 0.1868 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3560 Training loss: 1.9864 0.1952 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3560 Training loss: 1.9863 0.1997 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3560 Training loss: 1.9860 0.1792 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3560 Training loss: 1.9860 0.1966 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3560 Training loss: 1.9860 0.1900 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3560 Training loss: 1.9861 0.1821 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3560 Training loss: 1.9860 0.1801 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3560 Training loss: 1.9857 0.1804 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3560 Training loss: 1.9854 0.1864 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3560 Training loss: 1.9855 0.1796 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3560 Training loss: 1.9854 0.1859 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3560 Training loss: 1.9854 0.1803 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3560 Training loss: 1.9853 0.1779 sec/batch\n",
      "Epoch 6/20  Iteration 1046/3560 Training loss: 1.9851 0.1797 sec/batch\n",
      "Epoch 6/20  Iteration 1047/3560 Training loss: 1.9850 0.1774 sec/batch\n",
      "Epoch 6/20  Iteration 1048/3560 Training loss: 1.9849 0.1888 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3560 Training loss: 1.9846 0.1760 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3560 Training loss: 1.9846 0.1838 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3560 Training loss: 1.9846 0.1794 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3560 Training loss: 1.9844 0.1919 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3560 Training loss: 1.9844 0.1880 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3560 Training loss: 1.9843 0.1976 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3560 Training loss: 1.9842 0.1951 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3560 Training loss: 1.9842 0.1868 sec/batch\n",
      "Epoch 6/20  Iteration 1057/3560 Training loss: 1.9841 0.1778 sec/batch\n",
      "Epoch 6/20  Iteration 1058/3560 Training loss: 1.9843 0.1857 sec/batch\n",
      "Epoch 6/20  Iteration 1059/3560 Training loss: 1.9842 0.1895 sec/batch\n",
      "Epoch 6/20  Iteration 1060/3560 Training loss: 1.9841 0.1861 sec/batch\n",
      "Epoch 6/20  Iteration 1061/3560 Training loss: 1.9838 0.2589 sec/batch\n",
      "Epoch 6/20  Iteration 1062/3560 Training loss: 1.9836 0.2230 sec/batch\n",
      "Epoch 6/20  Iteration 1063/3560 Training loss: 1.9836 0.1929 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 1064/3560 Training loss: 1.9836 0.2345 sec/batch\n",
      "Epoch 6/20  Iteration 1065/3560 Training loss: 1.9836 0.1917 sec/batch\n",
      "Epoch 6/20  Iteration 1066/3560 Training loss: 1.9835 0.2081 sec/batch\n",
      "Epoch 6/20  Iteration 1067/3560 Training loss: 1.9833 0.1874 sec/batch\n",
      "Epoch 6/20  Iteration 1068/3560 Training loss: 1.9832 0.2040 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3560 Training loss: 2.0192 0.1925 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3560 Training loss: 1.9819 0.2460 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3560 Training loss: 1.9689 0.2152 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3560 Training loss: 1.9650 0.1841 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3560 Training loss: 1.9608 0.1876 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3560 Training loss: 1.9530 0.2359 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3560 Training loss: 1.9543 0.2394 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3560 Training loss: 1.9556 0.1807 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3560 Training loss: 1.9580 0.1928 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3560 Training loss: 1.9585 0.2036 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3560 Training loss: 1.9558 0.1961 sec/batch\n",
      "Epoch 7/20  Iteration 1080/3560 Training loss: 1.9546 0.1870 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3560 Training loss: 1.9551 0.1914 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3560 Training loss: 1.9568 0.1894 sec/batch\n",
      "Epoch 7/20  Iteration 1083/3560 Training loss: 1.9563 0.1921 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3560 Training loss: 1.9547 0.1992 sec/batch\n",
      "Epoch 7/20  Iteration 1085/3560 Training loss: 1.9553 0.2238 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3560 Training loss: 1.9572 0.1877 sec/batch\n",
      "Epoch 7/20  Iteration 1087/3560 Training loss: 1.9575 0.1937 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3560 Training loss: 1.9581 0.2254 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3560 Training loss: 1.9573 0.2062 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3560 Training loss: 1.9586 0.1831 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3560 Training loss: 1.9581 0.2087 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3560 Training loss: 1.9572 0.2315 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3560 Training loss: 1.9569 0.2618 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3560 Training loss: 1.9558 0.2895 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3560 Training loss: 1.9546 0.2587 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3560 Training loss: 1.9551 0.2525 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3560 Training loss: 1.9562 0.2357 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3560 Training loss: 1.9562 0.2546 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3560 Training loss: 1.9559 0.2250 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3560 Training loss: 1.9553 0.1849 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3560 Training loss: 1.9552 0.1865 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3560 Training loss: 1.9556 0.1784 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3560 Training loss: 1.9549 0.1867 sec/batch\n",
      "Epoch 7/20  Iteration 1104/3560 Training loss: 1.9546 0.1739 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3560 Training loss: 1.9539 0.1862 sec/batch\n",
      "Epoch 7/20  Iteration 1106/3560 Training loss: 1.9529 0.1920 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3560 Training loss: 1.9519 0.1888 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3560 Training loss: 1.9513 0.1849 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3560 Training loss: 1.9507 0.1821 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3560 Training loss: 1.9508 0.1900 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3560 Training loss: 1.9502 0.1805 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3560 Training loss: 1.9494 0.1792 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3560 Training loss: 1.9495 0.1947 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3560 Training loss: 1.9481 0.1892 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3560 Training loss: 1.9481 0.1800 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3560 Training loss: 1.9476 0.1830 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3560 Training loss: 1.9475 0.1753 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3560 Training loss: 1.9481 0.1872 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3560 Training loss: 1.9475 0.1787 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3560 Training loss: 1.9482 0.1766 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3560 Training loss: 1.9481 0.1878 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3560 Training loss: 1.9479 0.1745 sec/batch\n",
      "Epoch 7/20  Iteration 1123/3560 Training loss: 1.9474 0.1829 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3560 Training loss: 1.9477 0.1930 sec/batch\n",
      "Epoch 7/20  Iteration 1125/3560 Training loss: 1.9478 0.1803 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3560 Training loss: 1.9476 0.1791 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3560 Training loss: 1.9471 0.1849 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3560 Training loss: 1.9473 0.1788 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3560 Training loss: 1.9470 0.1803 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3560 Training loss: 1.9476 0.1734 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3560 Training loss: 1.9479 0.1794 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3560 Training loss: 1.9480 0.1825 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3560 Training loss: 1.9477 0.1804 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3560 Training loss: 1.9480 0.1869 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3560 Training loss: 1.9482 0.1916 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3560 Training loss: 1.9478 0.1919 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3560 Training loss: 1.9475 0.2117 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3560 Training loss: 1.9475 0.1841 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3560 Training loss: 1.9479 0.1781 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3560 Training loss: 1.9480 0.1768 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3560 Training loss: 1.9482 0.2142 sec/batch\n",
      "Epoch 7/20  Iteration 1142/3560 Training loss: 1.9478 0.2025 sec/batch\n",
      "Epoch 7/20  Iteration 1143/3560 Training loss: 1.9477 0.1846 sec/batch\n",
      "Epoch 7/20  Iteration 1144/3560 Training loss: 1.9481 0.1780 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3560 Training loss: 1.9478 0.1837 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3560 Training loss: 1.9480 0.1854 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3560 Training loss: 1.9476 0.1898 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3560 Training loss: 1.9475 0.1980 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3560 Training loss: 1.9470 0.1955 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3560 Training loss: 1.9471 0.1811 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3560 Training loss: 1.9467 0.1775 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3560 Training loss: 1.9464 0.1866 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3560 Training loss: 1.9458 0.1922 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3560 Training loss: 1.9455 0.1880 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3560 Training loss: 1.9453 0.1905 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3560 Training loss: 1.9449 0.1747 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3560 Training loss: 1.9445 0.1816 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3560 Training loss: 1.9446 0.1802 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3560 Training loss: 1.9443 0.1848 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3560 Training loss: 1.9442 0.1802 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3560 Training loss: 1.9436 0.1923 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3560 Training loss: 1.9433 0.1946 sec/batch\n",
      "Epoch 7/20  Iteration 1163/3560 Training loss: 1.9430 0.1924 sec/batch\n",
      "Epoch 7/20  Iteration 1164/3560 Training loss: 1.9428 0.2019 sec/batch\n",
      "Epoch 7/20  Iteration 1165/3560 Training loss: 1.9426 0.1777 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3560 Training loss: 1.9422 0.1775 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3560 Training loss: 1.9418 0.1865 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3560 Training loss: 1.9413 0.1828 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3560 Training loss: 1.9412 0.2081 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3560 Training loss: 1.9411 0.1821 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3560 Training loss: 1.9408 0.1791 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3560 Training loss: 1.9405 0.1761 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3560 Training loss: 1.9402 0.1865 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3560 Training loss: 1.9402 0.1979 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3560 Training loss: 1.9400 0.1818 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3560 Training loss: 1.9400 0.1861 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3560 Training loss: 1.9400 0.1982 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3560 Training loss: 1.9398 0.2009 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3560 Training loss: 1.9396 0.1818 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3560 Training loss: 1.9394 0.1869 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1181/3560 Training loss: 1.9393 0.1790 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3560 Training loss: 1.9391 0.1885 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3560 Training loss: 1.9388 0.1948 sec/batch\n",
      "Epoch 7/20  Iteration 1184/3560 Training loss: 1.9383 0.1875 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3560 Training loss: 1.9382 0.1843 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3560 Training loss: 1.9381 0.1836 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3560 Training loss: 1.9381 0.1888 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3560 Training loss: 1.9379 0.1763 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3560 Training loss: 1.9379 0.1857 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3560 Training loss: 1.9377 0.1951 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3560 Training loss: 1.9374 0.1932 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3560 Training loss: 1.9374 0.1773 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3560 Training loss: 1.9372 0.1766 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3560 Training loss: 1.9368 0.1756 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3560 Training loss: 1.9368 0.1892 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3560 Training loss: 1.9368 0.2337 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3560 Training loss: 1.9368 0.1971 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3560 Training loss: 1.9366 0.1806 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3560 Training loss: 1.9363 0.1935 sec/batch\n",
      "Epoch 7/20  Iteration 1200/3560 Training loss: 1.9359 0.1943 sec/batch\n",
      "Epoch 7/20  Iteration 1201/3560 Training loss: 1.9358 0.1884 sec/batch\n",
      "Epoch 7/20  Iteration 1202/3560 Training loss: 1.9358 0.1941 sec/batch\n",
      "Epoch 7/20  Iteration 1203/3560 Training loss: 1.9358 0.1889 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3560 Training loss: 1.9357 0.1989 sec/batch\n",
      "Epoch 7/20  Iteration 1205/3560 Training loss: 1.9356 0.1775 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3560 Training loss: 1.9356 0.1858 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3560 Training loss: 1.9357 0.1767 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3560 Training loss: 1.9355 0.1777 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3560 Training loss: 1.9356 0.1908 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3560 Training loss: 1.9354 0.1806 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3560 Training loss: 1.9353 0.1753 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3560 Training loss: 1.9353 0.1864 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3560 Training loss: 1.9351 0.1824 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3560 Training loss: 1.9351 0.1883 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3560 Training loss: 1.9351 0.1778 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3560 Training loss: 1.9352 0.1921 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3560 Training loss: 1.9351 0.2018 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3560 Training loss: 1.9348 0.1900 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3560 Training loss: 1.9346 0.1835 sec/batch\n",
      "Epoch 7/20  Iteration 1220/3560 Training loss: 1.9347 0.1749 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3560 Training loss: 1.9347 0.1821 sec/batch\n",
      "Epoch 7/20  Iteration 1222/3560 Training loss: 1.9348 0.1818 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3560 Training loss: 1.9347 0.1889 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3560 Training loss: 1.9346 0.2011 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3560 Training loss: 1.9346 0.1779 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3560 Training loss: 1.9345 0.1851 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3560 Training loss: 1.9343 0.1921 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3560 Training loss: 1.9343 0.1971 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3560 Training loss: 1.9344 0.1835 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3560 Training loss: 1.9343 0.1828 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3560 Training loss: 1.9343 0.1812 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3560 Training loss: 1.9342 0.1904 sec/batch\n",
      "Epoch 7/20  Iteration 1233/3560 Training loss: 1.9342 0.1794 sec/batch\n",
      "Epoch 7/20  Iteration 1234/3560 Training loss: 1.9340 0.1834 sec/batch\n",
      "Epoch 7/20  Iteration 1235/3560 Training loss: 1.9340 0.1777 sec/batch\n",
      "Epoch 7/20  Iteration 1236/3560 Training loss: 1.9342 0.1765 sec/batch\n",
      "Epoch 7/20  Iteration 1237/3560 Training loss: 1.9341 0.1804 sec/batch\n",
      "Epoch 7/20  Iteration 1238/3560 Training loss: 1.9340 0.1840 sec/batch\n",
      "Epoch 7/20  Iteration 1239/3560 Training loss: 1.9338 0.1932 sec/batch\n",
      "Epoch 7/20  Iteration 1240/3560 Training loss: 1.9336 0.2442 sec/batch\n",
      "Epoch 7/20  Iteration 1241/3560 Training loss: 1.9337 0.2562 sec/batch\n",
      "Epoch 7/20  Iteration 1242/3560 Training loss: 1.9337 0.2409 sec/batch\n",
      "Epoch 7/20  Iteration 1243/3560 Training loss: 1.9337 0.2742 sec/batch\n",
      "Epoch 7/20  Iteration 1244/3560 Training loss: 1.9336 0.2998 sec/batch\n",
      "Epoch 7/20  Iteration 1245/3560 Training loss: 1.9334 0.1878 sec/batch\n",
      "Epoch 7/20  Iteration 1246/3560 Training loss: 1.9334 0.1874 sec/batch\n",
      "Epoch 8/20  Iteration 1247/3560 Training loss: 1.9840 0.2283 sec/batch\n",
      "Epoch 8/20  Iteration 1248/3560 Training loss: 1.9441 0.2625 sec/batch\n",
      "Epoch 8/20  Iteration 1249/3560 Training loss: 1.9277 0.2348 sec/batch\n",
      "Epoch 8/20  Iteration 1250/3560 Training loss: 1.9208 0.1864 sec/batch\n",
      "Epoch 8/20  Iteration 1251/3560 Training loss: 1.9169 0.1922 sec/batch\n",
      "Epoch 8/20  Iteration 1252/3560 Training loss: 1.9096 0.2176 sec/batch\n",
      "Epoch 8/20  Iteration 1253/3560 Training loss: 1.9120 0.1929 sec/batch\n",
      "Epoch 8/20  Iteration 1254/3560 Training loss: 1.9114 0.1787 sec/batch\n",
      "Epoch 8/20  Iteration 1255/3560 Training loss: 1.9136 0.1778 sec/batch\n",
      "Epoch 8/20  Iteration 1256/3560 Training loss: 1.9145 0.1771 sec/batch\n",
      "Epoch 8/20  Iteration 1257/3560 Training loss: 1.9126 0.1779 sec/batch\n",
      "Epoch 8/20  Iteration 1258/3560 Training loss: 1.9107 0.1871 sec/batch\n",
      "Epoch 8/20  Iteration 1259/3560 Training loss: 1.9106 0.1815 sec/batch\n",
      "Epoch 8/20  Iteration 1260/3560 Training loss: 1.9133 0.1765 sec/batch\n",
      "Epoch 8/20  Iteration 1261/3560 Training loss: 1.9127 0.1863 sec/batch\n",
      "Epoch 8/20  Iteration 1262/3560 Training loss: 1.9116 0.1787 sec/batch\n",
      "Epoch 8/20  Iteration 1263/3560 Training loss: 1.9114 0.1859 sec/batch\n",
      "Epoch 8/20  Iteration 1264/3560 Training loss: 1.9134 0.1853 sec/batch\n",
      "Epoch 8/20  Iteration 1265/3560 Training loss: 1.9137 0.1770 sec/batch\n",
      "Epoch 8/20  Iteration 1266/3560 Training loss: 1.9136 0.1882 sec/batch\n",
      "Epoch 8/20  Iteration 1267/3560 Training loss: 1.9125 0.1772 sec/batch\n",
      "Epoch 8/20  Iteration 1268/3560 Training loss: 1.9132 0.1836 sec/batch\n",
      "Epoch 8/20  Iteration 1269/3560 Training loss: 1.9127 0.1788 sec/batch\n",
      "Epoch 8/20  Iteration 1270/3560 Training loss: 1.9124 0.1793 sec/batch\n",
      "Epoch 8/20  Iteration 1271/3560 Training loss: 1.9123 0.1745 sec/batch\n",
      "Epoch 8/20  Iteration 1272/3560 Training loss: 1.9117 0.1819 sec/batch\n",
      "Epoch 8/20  Iteration 1273/3560 Training loss: 1.9107 0.1753 sec/batch\n",
      "Epoch 8/20  Iteration 1274/3560 Training loss: 1.9111 0.1961 sec/batch\n",
      "Epoch 8/20  Iteration 1275/3560 Training loss: 1.9116 0.1999 sec/batch\n",
      "Epoch 8/20  Iteration 1276/3560 Training loss: 1.9123 0.1976 sec/batch\n",
      "Epoch 8/20  Iteration 1277/3560 Training loss: 1.9122 0.1859 sec/batch\n",
      "Epoch 8/20  Iteration 1278/3560 Training loss: 1.9116 0.2348 sec/batch\n",
      "Epoch 8/20  Iteration 1279/3560 Training loss: 1.9115 0.2493 sec/batch\n",
      "Epoch 8/20  Iteration 1280/3560 Training loss: 1.9121 0.2570 sec/batch\n",
      "Epoch 8/20  Iteration 1281/3560 Training loss: 1.9115 0.2456 sec/batch\n",
      "Epoch 8/20  Iteration 1282/3560 Training loss: 1.9111 0.2056 sec/batch\n",
      "Epoch 8/20  Iteration 1283/3560 Training loss: 1.9106 0.1953 sec/batch\n",
      "Epoch 8/20  Iteration 1284/3560 Training loss: 1.9096 0.1894 sec/batch\n",
      "Epoch 8/20  Iteration 1285/3560 Training loss: 1.9083 0.2085 sec/batch\n",
      "Epoch 8/20  Iteration 1286/3560 Training loss: 1.9080 0.1823 sec/batch\n",
      "Epoch 8/20  Iteration 1287/3560 Training loss: 1.9075 0.1962 sec/batch\n",
      "Epoch 8/20  Iteration 1288/3560 Training loss: 1.9078 0.1904 sec/batch\n",
      "Epoch 8/20  Iteration 1289/3560 Training loss: 1.9073 0.1882 sec/batch\n",
      "Epoch 8/20  Iteration 1290/3560 Training loss: 1.9065 0.1878 sec/batch\n",
      "Epoch 8/20  Iteration 1291/3560 Training loss: 1.9066 0.1840 sec/batch\n",
      "Epoch 8/20  Iteration 1292/3560 Training loss: 1.9054 0.2365 sec/batch\n",
      "Epoch 8/20  Iteration 1293/3560 Training loss: 1.9053 0.2219 sec/batch\n",
      "Epoch 8/20  Iteration 1294/3560 Training loss: 1.9047 0.2166 sec/batch\n",
      "Epoch 8/20  Iteration 1295/3560 Training loss: 1.9045 0.2314 sec/batch\n",
      "Epoch 8/20  Iteration 1296/3560 Training loss: 1.9053 0.2043 sec/batch\n",
      "Epoch 8/20  Iteration 1297/3560 Training loss: 1.9046 0.2037 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1298/3560 Training loss: 1.9053 0.2192 sec/batch\n",
      "Epoch 8/20  Iteration 1299/3560 Training loss: 1.9052 0.2058 sec/batch\n",
      "Epoch 8/20  Iteration 1300/3560 Training loss: 1.9050 0.2096 sec/batch\n",
      "Epoch 8/20  Iteration 1301/3560 Training loss: 1.9046 0.2278 sec/batch\n",
      "Epoch 8/20  Iteration 1302/3560 Training loss: 1.9048 0.2125 sec/batch\n",
      "Epoch 8/20  Iteration 1303/3560 Training loss: 1.9050 0.2197 sec/batch\n",
      "Epoch 8/20  Iteration 1304/3560 Training loss: 1.9045 0.1840 sec/batch\n",
      "Epoch 8/20  Iteration 1305/3560 Training loss: 1.9041 0.1793 sec/batch\n",
      "Epoch 8/20  Iteration 1306/3560 Training loss: 1.9045 0.1849 sec/batch\n",
      "Epoch 8/20  Iteration 1307/3560 Training loss: 1.9042 0.1754 sec/batch\n",
      "Epoch 8/20  Iteration 1308/3560 Training loss: 1.9047 0.1920 sec/batch\n",
      "Epoch 8/20  Iteration 1309/3560 Training loss: 1.9051 0.2258 sec/batch\n",
      "Epoch 8/20  Iteration 1310/3560 Training loss: 1.9051 0.2215 sec/batch\n",
      "Epoch 8/20  Iteration 1311/3560 Training loss: 1.9050 0.1999 sec/batch\n",
      "Epoch 8/20  Iteration 1312/3560 Training loss: 1.9052 0.1754 sec/batch\n",
      "Epoch 8/20  Iteration 1313/3560 Training loss: 1.9052 0.1811 sec/batch\n",
      "Epoch 8/20  Iteration 1314/3560 Training loss: 1.9047 0.1995 sec/batch\n",
      "Epoch 8/20  Iteration 1315/3560 Training loss: 1.9047 0.1867 sec/batch\n",
      "Epoch 8/20  Iteration 1316/3560 Training loss: 1.9047 0.1770 sec/batch\n",
      "Epoch 8/20  Iteration 1317/3560 Training loss: 1.9051 0.1761 sec/batch\n",
      "Epoch 8/20  Iteration 1318/3560 Training loss: 1.9051 0.1828 sec/batch\n",
      "Epoch 8/20  Iteration 1319/3560 Training loss: 1.9053 0.1764 sec/batch\n",
      "Epoch 8/20  Iteration 1320/3560 Training loss: 1.9051 0.1808 sec/batch\n",
      "Epoch 8/20  Iteration 1321/3560 Training loss: 1.9048 0.1764 sec/batch\n",
      "Epoch 8/20  Iteration 1322/3560 Training loss: 1.9051 0.1770 sec/batch\n",
      "Epoch 8/20  Iteration 1323/3560 Training loss: 1.9050 0.1762 sec/batch\n",
      "Epoch 8/20  Iteration 1324/3560 Training loss: 1.9051 0.1782 sec/batch\n",
      "Epoch 8/20  Iteration 1325/3560 Training loss: 1.9046 0.1753 sec/batch\n",
      "Epoch 8/20  Iteration 1326/3560 Training loss: 1.9044 0.1784 sec/batch\n",
      "Epoch 8/20  Iteration 1327/3560 Training loss: 1.9038 0.1751 sec/batch\n",
      "Epoch 8/20  Iteration 1328/3560 Training loss: 1.9039 0.2014 sec/batch\n",
      "Epoch 8/20  Iteration 1329/3560 Training loss: 1.9034 0.2028 sec/batch\n",
      "Epoch 8/20  Iteration 1330/3560 Training loss: 1.9032 0.1972 sec/batch\n",
      "Epoch 8/20  Iteration 1331/3560 Training loss: 1.9027 0.1963 sec/batch\n",
      "Epoch 8/20  Iteration 1332/3560 Training loss: 1.9023 0.1917 sec/batch\n",
      "Epoch 8/20  Iteration 1333/3560 Training loss: 1.9020 0.1909 sec/batch\n",
      "Epoch 8/20  Iteration 1334/3560 Training loss: 1.9018 0.1934 sec/batch\n",
      "Epoch 8/20  Iteration 1335/3560 Training loss: 1.9014 0.1910 sec/batch\n",
      "Epoch 8/20  Iteration 1336/3560 Training loss: 1.9014 0.2291 sec/batch\n",
      "Epoch 8/20  Iteration 1337/3560 Training loss: 1.9011 0.1784 sec/batch\n",
      "Epoch 8/20  Iteration 1338/3560 Training loss: 1.9009 0.1934 sec/batch\n",
      "Epoch 8/20  Iteration 1339/3560 Training loss: 1.9005 0.1828 sec/batch\n",
      "Epoch 8/20  Iteration 1340/3560 Training loss: 1.9001 0.1895 sec/batch\n",
      "Epoch 8/20  Iteration 1341/3560 Training loss: 1.8998 0.1913 sec/batch\n",
      "Epoch 8/20  Iteration 1342/3560 Training loss: 1.8998 0.1851 sec/batch\n",
      "Epoch 8/20  Iteration 1343/3560 Training loss: 1.8996 0.2100 sec/batch\n",
      "Epoch 8/20  Iteration 1344/3560 Training loss: 1.8991 0.1957 sec/batch\n",
      "Epoch 8/20  Iteration 1345/3560 Training loss: 1.8988 0.2009 sec/batch\n",
      "Epoch 8/20  Iteration 1346/3560 Training loss: 1.8983 0.2099 sec/batch\n",
      "Epoch 8/20  Iteration 1347/3560 Training loss: 1.8982 0.1956 sec/batch\n",
      "Epoch 8/20  Iteration 1348/3560 Training loss: 1.8981 0.2244 sec/batch\n",
      "Epoch 8/20  Iteration 1349/3560 Training loss: 1.8979 0.1918 sec/batch\n",
      "Epoch 8/20  Iteration 1350/3560 Training loss: 1.8977 0.1852 sec/batch\n",
      "Epoch 8/20  Iteration 1351/3560 Training loss: 1.8974 0.2140 sec/batch\n",
      "Epoch 8/20  Iteration 1352/3560 Training loss: 1.8973 0.1889 sec/batch\n",
      "Epoch 8/20  Iteration 1353/3560 Training loss: 1.8972 0.2221 sec/batch\n",
      "Epoch 8/20  Iteration 1354/3560 Training loss: 1.8971 0.2794 sec/batch\n",
      "Epoch 8/20  Iteration 1355/3560 Training loss: 1.8971 0.2396 sec/batch\n",
      "Epoch 8/20  Iteration 1356/3560 Training loss: 1.8970 0.1931 sec/batch\n",
      "Epoch 8/20  Iteration 1357/3560 Training loss: 1.8969 0.1929 sec/batch\n",
      "Epoch 8/20  Iteration 1358/3560 Training loss: 1.8968 0.1983 sec/batch\n",
      "Epoch 8/20  Iteration 1359/3560 Training loss: 1.8966 0.2468 sec/batch\n",
      "Epoch 8/20  Iteration 1360/3560 Training loss: 1.8964 0.2015 sec/batch\n",
      "Epoch 8/20  Iteration 1361/3560 Training loss: 1.8961 0.2200 sec/batch\n",
      "Epoch 8/20  Iteration 1362/3560 Training loss: 1.8957 0.2165 sec/batch\n",
      "Epoch 8/20  Iteration 1363/3560 Training loss: 1.8957 0.2009 sec/batch\n",
      "Epoch 8/20  Iteration 1364/3560 Training loss: 1.8955 0.1996 sec/batch\n",
      "Epoch 8/20  Iteration 1365/3560 Training loss: 1.8955 0.2032 sec/batch\n",
      "Epoch 8/20  Iteration 1366/3560 Training loss: 1.8954 0.2228 sec/batch\n",
      "Epoch 8/20  Iteration 1367/3560 Training loss: 1.8953 0.2325 sec/batch\n",
      "Epoch 8/20  Iteration 1368/3560 Training loss: 1.8950 0.4600 sec/batch\n",
      "Epoch 8/20  Iteration 1369/3560 Training loss: 1.8948 0.3945 sec/batch\n",
      "Epoch 8/20  Iteration 1370/3560 Training loss: 1.8949 0.4166 sec/batch\n",
      "Epoch 8/20  Iteration 1371/3560 Training loss: 1.8948 0.3963 sec/batch\n",
      "Epoch 8/20  Iteration 1372/3560 Training loss: 1.8944 0.4230 sec/batch\n",
      "Epoch 8/20  Iteration 1373/3560 Training loss: 1.8945 0.4104 sec/batch\n",
      "Epoch 8/20  Iteration 1374/3560 Training loss: 1.8945 0.3520 sec/batch\n",
      "Epoch 8/20  Iteration 1375/3560 Training loss: 1.8944 0.3387 sec/batch\n",
      "Epoch 8/20  Iteration 1376/3560 Training loss: 1.8943 0.3665 sec/batch\n",
      "Epoch 8/20  Iteration 1377/3560 Training loss: 1.8940 0.3833 sec/batch\n",
      "Epoch 8/20  Iteration 1378/3560 Training loss: 1.8936 0.3594 sec/batch\n",
      "Epoch 8/20  Iteration 1379/3560 Training loss: 1.8936 0.3636 sec/batch\n",
      "Epoch 8/20  Iteration 1380/3560 Training loss: 1.8936 0.4204 sec/batch\n",
      "Epoch 8/20  Iteration 1381/3560 Training loss: 1.8935 0.3157 sec/batch\n",
      "Epoch 8/20  Iteration 1382/3560 Training loss: 1.8936 0.2907 sec/batch\n",
      "Epoch 8/20  Iteration 1383/3560 Training loss: 1.8937 0.3013 sec/batch\n",
      "Epoch 8/20  Iteration 1384/3560 Training loss: 1.8937 0.2674 sec/batch\n",
      "Epoch 8/20  Iteration 1385/3560 Training loss: 1.8938 0.2875 sec/batch\n",
      "Epoch 8/20  Iteration 1386/3560 Training loss: 1.8936 0.2913 sec/batch\n",
      "Epoch 8/20  Iteration 1387/3560 Training loss: 1.8939 0.4009 sec/batch\n",
      "Epoch 8/20  Iteration 1388/3560 Training loss: 1.8937 0.4383 sec/batch\n",
      "Epoch 8/20  Iteration 1389/3560 Training loss: 1.8936 0.3904 sec/batch\n",
      "Epoch 8/20  Iteration 1390/3560 Training loss: 1.8936 0.4367 sec/batch\n",
      "Epoch 8/20  Iteration 1391/3560 Training loss: 1.8935 0.4632 sec/batch\n",
      "Epoch 8/20  Iteration 1392/3560 Training loss: 1.8935 0.3936 sec/batch\n",
      "Epoch 8/20  Iteration 1393/3560 Training loss: 1.8935 0.3794 sec/batch\n",
      "Epoch 8/20  Iteration 1394/3560 Training loss: 1.8936 0.3112 sec/batch\n",
      "Epoch 8/20  Iteration 1395/3560 Training loss: 1.8936 0.3452 sec/batch\n",
      "Epoch 8/20  Iteration 1396/3560 Training loss: 1.8934 0.3440 sec/batch\n",
      "Epoch 8/20  Iteration 1397/3560 Training loss: 1.8931 0.3214 sec/batch\n",
      "Epoch 8/20  Iteration 1398/3560 Training loss: 1.8932 0.3239 sec/batch\n",
      "Epoch 8/20  Iteration 1399/3560 Training loss: 1.8932 0.3345 sec/batch\n",
      "Epoch 8/20  Iteration 1400/3560 Training loss: 1.8931 0.3083 sec/batch\n",
      "Epoch 8/20  Iteration 1401/3560 Training loss: 1.8931 0.2927 sec/batch\n",
      "Epoch 8/20  Iteration 1402/3560 Training loss: 1.8930 0.2903 sec/batch\n",
      "Epoch 8/20  Iteration 1403/3560 Training loss: 1.8930 0.3055 sec/batch\n",
      "Epoch 8/20  Iteration 1404/3560 Training loss: 1.8930 0.2808 sec/batch\n",
      "Epoch 8/20  Iteration 1405/3560 Training loss: 1.8927 0.3086 sec/batch\n",
      "Epoch 8/20  Iteration 1406/3560 Training loss: 1.8929 0.3107 sec/batch\n",
      "Epoch 8/20  Iteration 1407/3560 Training loss: 1.8929 0.3305 sec/batch\n",
      "Epoch 8/20  Iteration 1408/3560 Training loss: 1.8928 0.2974 sec/batch\n",
      "Epoch 8/20  Iteration 1409/3560 Training loss: 1.8929 0.3339 sec/batch\n",
      "Epoch 8/20  Iteration 1410/3560 Training loss: 1.8929 0.2903 sec/batch\n",
      "Epoch 8/20  Iteration 1411/3560 Training loss: 1.8929 0.2952 sec/batch\n",
      "Epoch 8/20  Iteration 1412/3560 Training loss: 1.8928 0.2985 sec/batch\n",
      "Epoch 8/20  Iteration 1413/3560 Training loss: 1.8928 0.2882 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1414/3560 Training loss: 1.8930 0.3637 sec/batch\n",
      "Epoch 8/20  Iteration 1415/3560 Training loss: 1.8929 0.3924 sec/batch\n",
      "Epoch 8/20  Iteration 1416/3560 Training loss: 1.8928 0.3965 sec/batch\n",
      "Epoch 8/20  Iteration 1417/3560 Training loss: 1.8926 0.2812 sec/batch\n",
      "Epoch 8/20  Iteration 1418/3560 Training loss: 1.8924 0.4059 sec/batch\n",
      "Epoch 8/20  Iteration 1419/3560 Training loss: 1.8925 0.3609 sec/batch\n",
      "Epoch 8/20  Iteration 1420/3560 Training loss: 1.8925 0.3293 sec/batch\n",
      "Epoch 8/20  Iteration 1421/3560 Training loss: 1.8925 0.3292 sec/batch\n",
      "Epoch 8/20  Iteration 1422/3560 Training loss: 1.8925 0.3872 sec/batch\n",
      "Epoch 8/20  Iteration 1423/3560 Training loss: 1.8923 0.3167 sec/batch\n",
      "Epoch 8/20  Iteration 1424/3560 Training loss: 1.8923 0.3393 sec/batch\n",
      "Epoch 9/20  Iteration 1425/3560 Training loss: 1.9522 0.3249 sec/batch\n",
      "Epoch 9/20  Iteration 1426/3560 Training loss: 1.9123 0.3677 sec/batch\n",
      "Epoch 9/20  Iteration 1427/3560 Training loss: 1.8967 0.3783 sec/batch\n",
      "Epoch 9/20  Iteration 1428/3560 Training loss: 1.8911 0.4178 sec/batch\n",
      "Epoch 9/20  Iteration 1429/3560 Training loss: 1.8842 0.3856 sec/batch\n",
      "Epoch 9/20  Iteration 1430/3560 Training loss: 1.8768 0.3235 sec/batch\n",
      "Epoch 9/20  Iteration 1431/3560 Training loss: 1.8788 0.3176 sec/batch\n",
      "Epoch 9/20  Iteration 1432/3560 Training loss: 1.8777 0.2843 sec/batch\n",
      "Epoch 9/20  Iteration 1433/3560 Training loss: 1.8802 0.3111 sec/batch\n",
      "Epoch 9/20  Iteration 1434/3560 Training loss: 1.8798 0.3394 sec/batch\n",
      "Epoch 9/20  Iteration 1435/3560 Training loss: 1.8768 0.2685 sec/batch\n",
      "Epoch 9/20  Iteration 1436/3560 Training loss: 1.8752 0.3335 sec/batch\n",
      "Epoch 9/20  Iteration 1437/3560 Training loss: 1.8750 0.3113 sec/batch\n",
      "Epoch 9/20  Iteration 1438/3560 Training loss: 1.8772 0.3188 sec/batch\n",
      "Epoch 9/20  Iteration 1439/3560 Training loss: 1.8761 0.3544 sec/batch\n",
      "Epoch 9/20  Iteration 1440/3560 Training loss: 1.8743 0.3060 sec/batch\n",
      "Epoch 9/20  Iteration 1441/3560 Training loss: 1.8741 0.3242 sec/batch\n",
      "Epoch 9/20  Iteration 1442/3560 Training loss: 1.8756 0.3427 sec/batch\n",
      "Epoch 9/20  Iteration 1443/3560 Training loss: 1.8758 0.3256 sec/batch\n",
      "Epoch 9/20  Iteration 1444/3560 Training loss: 1.8766 0.3312 sec/batch\n",
      "Epoch 9/20  Iteration 1445/3560 Training loss: 1.8755 0.3376 sec/batch\n",
      "Epoch 9/20  Iteration 1446/3560 Training loss: 1.8767 0.3092 sec/batch\n",
      "Epoch 9/20  Iteration 1447/3560 Training loss: 1.8759 0.3368 sec/batch\n",
      "Epoch 9/20  Iteration 1448/3560 Training loss: 1.8756 0.3120 sec/batch\n",
      "Epoch 9/20  Iteration 1449/3560 Training loss: 1.8749 0.3177 sec/batch\n",
      "Epoch 9/20  Iteration 1450/3560 Training loss: 1.8737 0.2896 sec/batch\n",
      "Epoch 9/20  Iteration 1451/3560 Training loss: 1.8726 0.3009 sec/batch\n",
      "Epoch 9/20  Iteration 1452/3560 Training loss: 1.8731 0.3045 sec/batch\n",
      "Epoch 9/20  Iteration 1453/3560 Training loss: 1.8742 0.2899 sec/batch\n",
      "Epoch 9/20  Iteration 1454/3560 Training loss: 1.8748 0.2928 sec/batch\n",
      "Epoch 9/20  Iteration 1455/3560 Training loss: 1.8743 0.2885 sec/batch\n",
      "Epoch 9/20  Iteration 1456/3560 Training loss: 1.8735 0.2869 sec/batch\n",
      "Epoch 9/20  Iteration 1457/3560 Training loss: 1.8736 0.2968 sec/batch\n",
      "Epoch 9/20  Iteration 1458/3560 Training loss: 1.8742 0.2864 sec/batch\n",
      "Epoch 9/20  Iteration 1459/3560 Training loss: 1.8739 0.3024 sec/batch\n",
      "Epoch 9/20  Iteration 1460/3560 Training loss: 1.8736 0.3331 sec/batch\n",
      "Epoch 9/20  Iteration 1461/3560 Training loss: 1.8733 0.3073 sec/batch\n",
      "Epoch 9/20  Iteration 1462/3560 Training loss: 1.8723 0.3430 sec/batch\n",
      "Epoch 9/20  Iteration 1463/3560 Training loss: 1.8708 0.2977 sec/batch\n",
      "Epoch 9/20  Iteration 1464/3560 Training loss: 1.8703 0.3041 sec/batch\n",
      "Epoch 9/20  Iteration 1465/3560 Training loss: 1.8698 0.2869 sec/batch\n",
      "Epoch 9/20  Iteration 1466/3560 Training loss: 1.8701 0.3386 sec/batch\n",
      "Epoch 9/20  Iteration 1467/3560 Training loss: 1.8695 0.2989 sec/batch\n",
      "Epoch 9/20  Iteration 1468/3560 Training loss: 1.8689 0.2888 sec/batch\n",
      "Epoch 9/20  Iteration 1469/3560 Training loss: 1.8691 0.3044 sec/batch\n",
      "Epoch 9/20  Iteration 1470/3560 Training loss: 1.8678 0.2844 sec/batch\n",
      "Epoch 9/20  Iteration 1471/3560 Training loss: 1.8675 0.3061 sec/batch\n",
      "Epoch 9/20  Iteration 1472/3560 Training loss: 1.8667 0.2805 sec/batch\n",
      "Epoch 9/20  Iteration 1473/3560 Training loss: 1.8664 0.3100 sec/batch\n",
      "Epoch 9/20  Iteration 1474/3560 Training loss: 1.8671 0.2851 sec/batch\n",
      "Epoch 9/20  Iteration 1475/3560 Training loss: 1.8667 0.2938 sec/batch\n",
      "Epoch 9/20  Iteration 1476/3560 Training loss: 1.8675 0.2771 sec/batch\n",
      "Epoch 9/20  Iteration 1477/3560 Training loss: 1.8674 0.3154 sec/batch\n",
      "Epoch 9/20  Iteration 1478/3560 Training loss: 1.8671 0.2938 sec/batch\n",
      "Epoch 9/20  Iteration 1479/3560 Training loss: 1.8667 0.3152 sec/batch\n",
      "Epoch 9/20  Iteration 1480/3560 Training loss: 1.8670 0.3080 sec/batch\n",
      "Epoch 9/20  Iteration 1481/3560 Training loss: 1.8675 0.3188 sec/batch\n",
      "Epoch 9/20  Iteration 1482/3560 Training loss: 1.8672 0.3006 sec/batch\n",
      "Epoch 9/20  Iteration 1483/3560 Training loss: 1.8669 0.3126 sec/batch\n",
      "Epoch 9/20  Iteration 1484/3560 Training loss: 1.8674 0.3310 sec/batch\n",
      "Epoch 9/20  Iteration 1485/3560 Training loss: 1.8672 0.2858 sec/batch\n",
      "Epoch 9/20  Iteration 1486/3560 Training loss: 1.8680 0.3365 sec/batch\n",
      "Epoch 9/20  Iteration 1487/3560 Training loss: 1.8683 0.2946 sec/batch\n",
      "Epoch 9/20  Iteration 1488/3560 Training loss: 1.8686 0.2889 sec/batch\n",
      "Epoch 9/20  Iteration 1489/3560 Training loss: 1.8686 0.2892 sec/batch\n",
      "Epoch 9/20  Iteration 1490/3560 Training loss: 1.8691 0.3090 sec/batch\n",
      "Epoch 9/20  Iteration 1491/3560 Training loss: 1.8691 0.3010 sec/batch\n",
      "Epoch 9/20  Iteration 1492/3560 Training loss: 1.8688 0.2792 sec/batch\n",
      "Epoch 9/20  Iteration 1493/3560 Training loss: 1.8688 0.2935 sec/batch\n",
      "Epoch 9/20  Iteration 1494/3560 Training loss: 1.8687 0.3029 sec/batch\n",
      "Epoch 9/20  Iteration 1495/3560 Training loss: 1.8690 0.2967 sec/batch\n",
      "Epoch 9/20  Iteration 1496/3560 Training loss: 1.8692 0.3325 sec/batch\n",
      "Epoch 9/20  Iteration 1497/3560 Training loss: 1.8696 0.2896 sec/batch\n",
      "Epoch 9/20  Iteration 1498/3560 Training loss: 1.8692 0.3100 sec/batch\n",
      "Epoch 9/20  Iteration 1499/3560 Training loss: 1.8690 0.3184 sec/batch\n",
      "Epoch 9/20  Iteration 1500/3560 Training loss: 1.8692 0.3396 sec/batch\n",
      "Epoch 9/20  Iteration 1501/3560 Training loss: 1.8690 0.2859 sec/batch\n",
      "Epoch 9/20  Iteration 1502/3560 Training loss: 1.8692 0.3066 sec/batch\n",
      "Epoch 9/20  Iteration 1503/3560 Training loss: 1.8688 0.3119 sec/batch\n",
      "Epoch 9/20  Iteration 1504/3560 Training loss: 1.8686 0.3185 sec/batch\n",
      "Epoch 9/20  Iteration 1505/3560 Training loss: 1.8681 0.2659 sec/batch\n",
      "Epoch 9/20  Iteration 1506/3560 Training loss: 1.8682 0.2856 sec/batch\n",
      "Epoch 9/20  Iteration 1507/3560 Training loss: 1.8678 0.2788 sec/batch\n",
      "Epoch 9/20  Iteration 1508/3560 Training loss: 1.8676 0.3049 sec/batch\n",
      "Epoch 9/20  Iteration 1509/3560 Training loss: 1.8671 0.3000 sec/batch\n",
      "Epoch 9/20  Iteration 1510/3560 Training loss: 1.8667 0.3074 sec/batch\n",
      "Epoch 9/20  Iteration 1511/3560 Training loss: 1.8666 0.3103 sec/batch\n",
      "Epoch 9/20  Iteration 1512/3560 Training loss: 1.8661 0.3159 sec/batch\n",
      "Epoch 9/20  Iteration 1513/3560 Training loss: 1.8658 0.3133 sec/batch\n",
      "Epoch 9/20  Iteration 1514/3560 Training loss: 1.8659 0.3360 sec/batch\n",
      "Epoch 9/20  Iteration 1515/3560 Training loss: 1.8656 0.3263 sec/batch\n",
      "Epoch 9/20  Iteration 1516/3560 Training loss: 1.8655 0.3434 sec/batch\n",
      "Epoch 9/20  Iteration 1517/3560 Training loss: 1.8650 0.3632 sec/batch\n",
      "Epoch 9/20  Iteration 1518/3560 Training loss: 1.8646 0.3334 sec/batch\n",
      "Epoch 9/20  Iteration 1519/3560 Training loss: 1.8642 0.2971 sec/batch\n",
      "Epoch 9/20  Iteration 1520/3560 Training loss: 1.8641 0.3173 sec/batch\n",
      "Epoch 9/20  Iteration 1521/3560 Training loss: 1.8640 0.2743 sec/batch\n",
      "Epoch 9/20  Iteration 1522/3560 Training loss: 1.8636 0.2838 sec/batch\n",
      "Epoch 9/20  Iteration 1523/3560 Training loss: 1.8632 0.3054 sec/batch\n",
      "Epoch 9/20  Iteration 1524/3560 Training loss: 1.8627 0.2779 sec/batch\n",
      "Epoch 9/20  Iteration 1525/3560 Training loss: 1.8626 0.3439 sec/batch\n",
      "Epoch 9/20  Iteration 1526/3560 Training loss: 1.8626 0.3041 sec/batch\n",
      "Epoch 9/20  Iteration 1527/3560 Training loss: 1.8623 0.3025 sec/batch\n",
      "Epoch 9/20  Iteration 1528/3560 Training loss: 1.8620 0.2869 sec/batch\n",
      "Epoch 9/20  Iteration 1529/3560 Training loss: 1.8617 0.2794 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1530/3560 Training loss: 1.8617 0.2967 sec/batch\n",
      "Epoch 9/20  Iteration 1531/3560 Training loss: 1.8616 0.2958 sec/batch\n",
      "Epoch 9/20  Iteration 1532/3560 Training loss: 1.8616 0.3291 sec/batch\n",
      "Epoch 9/20  Iteration 1533/3560 Training loss: 1.8616 0.3474 sec/batch\n",
      "Epoch 9/20  Iteration 1534/3560 Training loss: 1.8615 0.3379 sec/batch\n",
      "Epoch 9/20  Iteration 1535/3560 Training loss: 1.8614 0.3551 sec/batch\n",
      "Epoch 9/20  Iteration 1536/3560 Training loss: 1.8613 0.2831 sec/batch\n",
      "Epoch 9/20  Iteration 1537/3560 Training loss: 1.8612 0.3340 sec/batch\n",
      "Epoch 9/20  Iteration 1538/3560 Training loss: 1.8610 0.3131 sec/batch\n",
      "Epoch 9/20  Iteration 1539/3560 Training loss: 1.8608 0.3797 sec/batch\n",
      "Epoch 9/20  Iteration 1540/3560 Training loss: 1.8604 0.2854 sec/batch\n",
      "Epoch 9/20  Iteration 1541/3560 Training loss: 1.8602 0.3434 sec/batch\n",
      "Epoch 9/20  Iteration 1542/3560 Training loss: 1.8602 0.3236 sec/batch\n",
      "Epoch 9/20  Iteration 1543/3560 Training loss: 1.8601 0.3734 sec/batch\n",
      "Epoch 9/20  Iteration 1544/3560 Training loss: 1.8600 0.2891 sec/batch\n",
      "Epoch 9/20  Iteration 1545/3560 Training loss: 1.8600 0.3400 sec/batch\n",
      "Epoch 9/20  Iteration 1546/3560 Training loss: 1.8597 0.3716 sec/batch\n",
      "Epoch 9/20  Iteration 1547/3560 Training loss: 1.8595 0.2885 sec/batch\n",
      "Epoch 9/20  Iteration 1548/3560 Training loss: 1.8595 0.3350 sec/batch\n",
      "Epoch 9/20  Iteration 1549/3560 Training loss: 1.8594 0.3373 sec/batch\n",
      "Epoch 9/20  Iteration 1550/3560 Training loss: 1.8591 0.3426 sec/batch\n",
      "Epoch 9/20  Iteration 1551/3560 Training loss: 1.8591 0.3531 sec/batch\n",
      "Epoch 9/20  Iteration 1552/3560 Training loss: 1.8592 0.3388 sec/batch\n",
      "Epoch 9/20  Iteration 1553/3560 Training loss: 1.8591 0.3279 sec/batch\n",
      "Epoch 9/20  Iteration 1554/3560 Training loss: 1.8591 0.3249 sec/batch\n",
      "Epoch 9/20  Iteration 1555/3560 Training loss: 1.8588 0.3482 sec/batch\n",
      "Epoch 9/20  Iteration 1556/3560 Training loss: 1.8585 0.4136 sec/batch\n",
      "Epoch 9/20  Iteration 1557/3560 Training loss: 1.8585 0.3233 sec/batch\n",
      "Epoch 9/20  Iteration 1558/3560 Training loss: 1.8584 0.3187 sec/batch\n",
      "Epoch 9/20  Iteration 1559/3560 Training loss: 1.8584 0.3213 sec/batch\n",
      "Epoch 9/20  Iteration 1560/3560 Training loss: 1.8585 0.3368 sec/batch\n",
      "Epoch 9/20  Iteration 1561/3560 Training loss: 1.8586 0.3323 sec/batch\n",
      "Epoch 9/20  Iteration 1562/3560 Training loss: 1.8587 0.4498 sec/batch\n",
      "Epoch 9/20  Iteration 1563/3560 Training loss: 1.8588 0.4034 sec/batch\n",
      "Epoch 9/20  Iteration 1564/3560 Training loss: 1.8586 0.3715 sec/batch\n",
      "Epoch 9/20  Iteration 1565/3560 Training loss: 1.8588 0.2765 sec/batch\n",
      "Epoch 9/20  Iteration 1566/3560 Training loss: 1.8587 0.3121 sec/batch\n",
      "Epoch 9/20  Iteration 1567/3560 Training loss: 1.8586 0.2906 sec/batch\n",
      "Epoch 9/20  Iteration 1568/3560 Training loss: 1.8586 0.2695 sec/batch\n",
      "Epoch 9/20  Iteration 1569/3560 Training loss: 1.8585 0.3617 sec/batch\n",
      "Epoch 9/20  Iteration 1570/3560 Training loss: 1.8585 0.3626 sec/batch\n",
      "Epoch 9/20  Iteration 1571/3560 Training loss: 1.8586 0.3838 sec/batch\n",
      "Epoch 9/20  Iteration 1572/3560 Training loss: 1.8587 0.2906 sec/batch\n",
      "Epoch 9/20  Iteration 1573/3560 Training loss: 1.8587 0.3392 sec/batch\n",
      "Epoch 9/20  Iteration 1574/3560 Training loss: 1.8584 0.2955 sec/batch\n",
      "Epoch 9/20  Iteration 1575/3560 Training loss: 1.8582 0.3597 sec/batch\n",
      "Epoch 9/20  Iteration 1576/3560 Training loss: 1.8582 0.2738 sec/batch\n",
      "Epoch 9/20  Iteration 1577/3560 Training loss: 1.8582 0.3268 sec/batch\n",
      "Epoch 9/20  Iteration 1578/3560 Training loss: 1.8582 0.3028 sec/batch\n",
      "Epoch 9/20  Iteration 1579/3560 Training loss: 1.8582 0.3404 sec/batch\n",
      "Epoch 9/20  Iteration 1580/3560 Training loss: 1.8581 0.3363 sec/batch\n",
      "Epoch 9/20  Iteration 1581/3560 Training loss: 1.8581 0.3427 sec/batch\n",
      "Epoch 9/20  Iteration 1582/3560 Training loss: 1.8580 0.3397 sec/batch\n",
      "Epoch 9/20  Iteration 1583/3560 Training loss: 1.8578 0.4107 sec/batch\n",
      "Epoch 9/20  Iteration 1584/3560 Training loss: 1.8579 0.3666 sec/batch\n",
      "Epoch 9/20  Iteration 1585/3560 Training loss: 1.8580 0.4100 sec/batch\n",
      "Epoch 9/20  Iteration 1586/3560 Training loss: 1.8580 0.3809 sec/batch\n",
      "Epoch 9/20  Iteration 1587/3560 Training loss: 1.8580 0.3514 sec/batch\n",
      "Epoch 9/20  Iteration 1588/3560 Training loss: 1.8580 0.3040 sec/batch\n",
      "Epoch 9/20  Iteration 1589/3560 Training loss: 1.8579 0.3441 sec/batch\n",
      "Epoch 9/20  Iteration 1590/3560 Training loss: 1.8578 0.3371 sec/batch\n",
      "Epoch 9/20  Iteration 1591/3560 Training loss: 1.8579 0.3417 sec/batch\n",
      "Epoch 9/20  Iteration 1592/3560 Training loss: 1.8582 0.2877 sec/batch\n",
      "Epoch 9/20  Iteration 1593/3560 Training loss: 1.8581 0.3633 sec/batch\n",
      "Epoch 9/20  Iteration 1594/3560 Training loss: 1.8580 0.3319 sec/batch\n",
      "Epoch 9/20  Iteration 1595/3560 Training loss: 1.8578 0.3254 sec/batch\n",
      "Epoch 9/20  Iteration 1596/3560 Training loss: 1.8577 0.2882 sec/batch\n",
      "Epoch 9/20  Iteration 1597/3560 Training loss: 1.8578 0.2739 sec/batch\n",
      "Epoch 9/20  Iteration 1598/3560 Training loss: 1.8578 0.3183 sec/batch\n",
      "Epoch 9/20  Iteration 1599/3560 Training loss: 1.8578 0.2531 sec/batch\n",
      "Epoch 9/20  Iteration 1600/3560 Training loss: 1.8578 0.2890 sec/batch\n",
      "Epoch 9/20  Iteration 1601/3560 Training loss: 1.8576 0.2876 sec/batch\n",
      "Epoch 9/20  Iteration 1602/3560 Training loss: 1.8576 0.3010 sec/batch\n",
      "Epoch 10/20  Iteration 1603/3560 Training loss: 1.9157 0.3080 sec/batch\n",
      "Epoch 10/20  Iteration 1604/3560 Training loss: 1.8797 0.3301 sec/batch\n",
      "Epoch 10/20  Iteration 1605/3560 Training loss: 1.8642 0.3004 sec/batch\n",
      "Epoch 10/20  Iteration 1606/3560 Training loss: 1.8579 0.3243 sec/batch\n",
      "Epoch 10/20  Iteration 1607/3560 Training loss: 1.8508 0.3064 sec/batch\n",
      "Epoch 10/20  Iteration 1608/3560 Training loss: 1.8422 0.3995 sec/batch\n",
      "Epoch 10/20  Iteration 1609/3560 Training loss: 1.8446 0.3345 sec/batch\n",
      "Epoch 10/20  Iteration 1610/3560 Training loss: 1.8429 0.4617 sec/batch\n",
      "Epoch 10/20  Iteration 1611/3560 Training loss: 1.8467 0.2726 sec/batch\n",
      "Epoch 10/20  Iteration 1612/3560 Training loss: 1.8469 0.3153 sec/batch\n",
      "Epoch 10/20  Iteration 1613/3560 Training loss: 1.8443 0.3997 sec/batch\n",
      "Epoch 10/20  Iteration 1614/3560 Training loss: 1.8421 0.2973 sec/batch\n",
      "Epoch 10/20  Iteration 1615/3560 Training loss: 1.8420 0.3553 sec/batch\n",
      "Epoch 10/20  Iteration 1616/3560 Training loss: 1.8451 0.3752 sec/batch\n",
      "Epoch 10/20  Iteration 1617/3560 Training loss: 1.8440 0.3494 sec/batch\n",
      "Epoch 10/20  Iteration 1618/3560 Training loss: 1.8421 0.3088 sec/batch\n",
      "Epoch 10/20  Iteration 1619/3560 Training loss: 1.8418 0.3036 sec/batch\n",
      "Epoch 10/20  Iteration 1620/3560 Training loss: 1.8438 0.2891 sec/batch\n",
      "Epoch 10/20  Iteration 1621/3560 Training loss: 1.8440 0.2845 sec/batch\n",
      "Epoch 10/20  Iteration 1622/3560 Training loss: 1.8439 0.2992 sec/batch\n",
      "Epoch 10/20  Iteration 1623/3560 Training loss: 1.8428 0.2743 sec/batch\n",
      "Epoch 10/20  Iteration 1624/3560 Training loss: 1.8433 0.2850 sec/batch\n",
      "Epoch 10/20  Iteration 1625/3560 Training loss: 1.8432 0.3113 sec/batch\n",
      "Epoch 10/20  Iteration 1626/3560 Training loss: 1.8429 0.3424 sec/batch\n",
      "Epoch 10/20  Iteration 1627/3560 Training loss: 1.8424 0.3232 sec/batch\n",
      "Epoch 10/20  Iteration 1628/3560 Training loss: 1.8415 0.3003 sec/batch\n",
      "Epoch 10/20  Iteration 1629/3560 Training loss: 1.8404 0.4076 sec/batch\n",
      "Epoch 10/20  Iteration 1630/3560 Training loss: 1.8412 0.3149 sec/batch\n",
      "Epoch 10/20  Iteration 1631/3560 Training loss: 1.8422 0.3051 sec/batch\n",
      "Epoch 10/20  Iteration 1632/3560 Training loss: 1.8426 0.3177 sec/batch\n",
      "Epoch 10/20  Iteration 1633/3560 Training loss: 1.8423 0.3107 sec/batch\n",
      "Epoch 10/20  Iteration 1634/3560 Training loss: 1.8416 0.3039 sec/batch\n",
      "Epoch 10/20  Iteration 1635/3560 Training loss: 1.8417 0.2628 sec/batch\n",
      "Epoch 10/20  Iteration 1636/3560 Training loss: 1.8425 0.2931 sec/batch\n",
      "Epoch 10/20  Iteration 1637/3560 Training loss: 1.8420 0.2869 sec/batch\n",
      "Epoch 10/20  Iteration 1638/3560 Training loss: 1.8415 0.2863 sec/batch\n",
      "Epoch 10/20  Iteration 1639/3560 Training loss: 1.8410 0.3094 sec/batch\n",
      "Epoch 10/20  Iteration 1640/3560 Training loss: 1.8399 0.3231 sec/batch\n",
      "Epoch 10/20  Iteration 1641/3560 Training loss: 1.8389 0.3056 sec/batch\n",
      "Epoch 10/20  Iteration 1642/3560 Training loss: 1.8383 0.4077 sec/batch\n",
      "Epoch 10/20  Iteration 1643/3560 Training loss: 1.8378 0.3388 sec/batch\n",
      "Epoch 10/20  Iteration 1644/3560 Training loss: 1.8381 0.3161 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1645/3560 Training loss: 1.8376 0.3488 sec/batch\n",
      "Epoch 10/20  Iteration 1646/3560 Training loss: 1.8369 0.3350 sec/batch\n",
      "Epoch 10/20  Iteration 1647/3560 Training loss: 1.8370 0.3174 sec/batch\n",
      "Epoch 10/20  Iteration 1648/3560 Training loss: 1.8361 0.2645 sec/batch\n",
      "Epoch 10/20  Iteration 1649/3560 Training loss: 1.8360 0.3083 sec/batch\n",
      "Epoch 10/20  Iteration 1650/3560 Training loss: 1.8357 0.3100 sec/batch\n",
      "Epoch 10/20  Iteration 1651/3560 Training loss: 1.8355 0.3346 sec/batch\n",
      "Epoch 10/20  Iteration 1652/3560 Training loss: 1.8364 0.2881 sec/batch\n",
      "Epoch 10/20  Iteration 1653/3560 Training loss: 1.8360 0.3342 sec/batch\n",
      "Epoch 10/20  Iteration 1654/3560 Training loss: 1.8368 0.3147 sec/batch\n",
      "Epoch 10/20  Iteration 1655/3560 Training loss: 1.8366 0.3208 sec/batch\n",
      "Epoch 10/20  Iteration 1656/3560 Training loss: 1.8364 0.3256 sec/batch\n",
      "Epoch 10/20  Iteration 1657/3560 Training loss: 1.8360 0.3176 sec/batch\n",
      "Epoch 10/20  Iteration 1658/3560 Training loss: 1.8363 0.3715 sec/batch\n",
      "Epoch 10/20  Iteration 1659/3560 Training loss: 1.8365 0.3159 sec/batch\n",
      "Epoch 10/20  Iteration 1660/3560 Training loss: 1.8361 0.3400 sec/batch\n",
      "Epoch 10/20  Iteration 1661/3560 Training loss: 1.8356 0.3800 sec/batch\n",
      "Epoch 10/20  Iteration 1662/3560 Training loss: 1.8361 0.3168 sec/batch\n",
      "Epoch 10/20  Iteration 1663/3560 Training loss: 1.8360 0.3571 sec/batch\n",
      "Epoch 10/20  Iteration 1664/3560 Training loss: 1.8366 0.4500 sec/batch\n",
      "Epoch 10/20  Iteration 1665/3560 Training loss: 1.8368 0.3197 sec/batch\n",
      "Epoch 10/20  Iteration 1666/3560 Training loss: 1.8373 0.3827 sec/batch\n",
      "Epoch 10/20  Iteration 1667/3560 Training loss: 1.8372 0.6223 sec/batch\n",
      "Epoch 10/20  Iteration 1668/3560 Training loss: 1.8374 0.5686 sec/batch\n",
      "Epoch 10/20  Iteration 1669/3560 Training loss: 1.8376 0.6825 sec/batch\n",
      "Epoch 10/20  Iteration 1670/3560 Training loss: 1.8372 0.5866 sec/batch\n",
      "Epoch 10/20  Iteration 1671/3560 Training loss: 1.8371 0.4793 sec/batch\n",
      "Epoch 10/20  Iteration 1672/3560 Training loss: 1.8370 0.4845 sec/batch\n",
      "Epoch 10/20  Iteration 1673/3560 Training loss: 1.8374 0.4656 sec/batch\n",
      "Epoch 10/20  Iteration 1674/3560 Training loss: 1.8376 0.4017 sec/batch\n",
      "Epoch 10/20  Iteration 1675/3560 Training loss: 1.8380 0.4801 sec/batch\n",
      "Epoch 10/20  Iteration 1676/3560 Training loss: 1.8377 0.3764 sec/batch\n",
      "Epoch 10/20  Iteration 1677/3560 Training loss: 1.8376 0.4705 sec/batch\n",
      "Epoch 10/20  Iteration 1678/3560 Training loss: 1.8380 0.4286 sec/batch\n",
      "Epoch 10/20  Iteration 1679/3560 Training loss: 1.8377 0.4388 sec/batch\n",
      "Epoch 10/20  Iteration 1680/3560 Training loss: 1.8379 0.4188 sec/batch\n",
      "Epoch 10/20  Iteration 1681/3560 Training loss: 1.8374 0.4969 sec/batch\n",
      "Epoch 10/20  Iteration 1682/3560 Training loss: 1.8373 0.6465 sec/batch\n",
      "Epoch 10/20  Iteration 1683/3560 Training loss: 1.8368 0.5309 sec/batch\n",
      "Epoch 10/20  Iteration 1684/3560 Training loss: 1.8369 0.4436 sec/batch\n",
      "Epoch 10/20  Iteration 1685/3560 Training loss: 1.8364 0.6138 sec/batch\n",
      "Epoch 10/20  Iteration 1686/3560 Training loss: 1.8363 0.4030 sec/batch\n",
      "Epoch 10/20  Iteration 1687/3560 Training loss: 1.8358 0.4492 sec/batch\n",
      "Epoch 10/20  Iteration 1688/3560 Training loss: 1.8355 0.4237 sec/batch\n",
      "Epoch 10/20  Iteration 1689/3560 Training loss: 1.8353 0.4996 sec/batch\n",
      "Epoch 10/20  Iteration 1690/3560 Training loss: 1.8349 0.5409 sec/batch\n",
      "Epoch 10/20  Iteration 1691/3560 Training loss: 1.8345 0.4694 sec/batch\n",
      "Epoch 10/20  Iteration 1692/3560 Training loss: 1.8346 0.5033 sec/batch\n",
      "Epoch 10/20  Iteration 1693/3560 Training loss: 1.8343 0.3819 sec/batch\n",
      "Epoch 10/20  Iteration 1694/3560 Training loss: 1.8340 0.5002 sec/batch\n",
      "Epoch 10/20  Iteration 1695/3560 Training loss: 1.8337 0.5138 sec/batch\n",
      "Epoch 10/20  Iteration 1696/3560 Training loss: 1.8335 0.5296 sec/batch\n",
      "Epoch 10/20  Iteration 1697/3560 Training loss: 1.8332 0.4584 sec/batch\n",
      "Epoch 10/20  Iteration 1698/3560 Training loss: 1.8331 0.4730 sec/batch\n",
      "Epoch 10/20  Iteration 1699/3560 Training loss: 1.8329 0.4536 sec/batch\n",
      "Epoch 10/20  Iteration 1700/3560 Training loss: 1.8325 0.5102 sec/batch\n",
      "Epoch 10/20  Iteration 1701/3560 Training loss: 1.8322 0.4395 sec/batch\n",
      "Epoch 10/20  Iteration 1702/3560 Training loss: 1.8317 0.5397 sec/batch\n",
      "Epoch 10/20  Iteration 1703/3560 Training loss: 1.8317 0.5311 sec/batch\n",
      "Epoch 10/20  Iteration 1704/3560 Training loss: 1.8316 0.4608 sec/batch\n",
      "Epoch 10/20  Iteration 1705/3560 Training loss: 1.8314 0.4426 sec/batch\n",
      "Epoch 10/20  Iteration 1706/3560 Training loss: 1.8312 0.5718 sec/batch\n",
      "Epoch 10/20  Iteration 1707/3560 Training loss: 1.8309 0.5049 sec/batch\n",
      "Epoch 10/20  Iteration 1708/3560 Training loss: 1.8308 0.5702 sec/batch\n",
      "Epoch 10/20  Iteration 1709/3560 Training loss: 1.8308 0.5485 sec/batch\n",
      "Epoch 10/20  Iteration 1710/3560 Training loss: 1.8307 0.5158 sec/batch\n",
      "Epoch 10/20  Iteration 1711/3560 Training loss: 1.8308 0.5155 sec/batch\n",
      "Epoch 10/20  Iteration 1712/3560 Training loss: 1.8308 0.4082 sec/batch\n",
      "Epoch 10/20  Iteration 1713/3560 Training loss: 1.8306 0.4322 sec/batch\n",
      "Epoch 10/20  Iteration 1714/3560 Training loss: 1.8304 0.4400 sec/batch\n",
      "Epoch 10/20  Iteration 1715/3560 Training loss: 1.8302 0.4574 sec/batch\n",
      "Epoch 10/20  Iteration 1716/3560 Training loss: 1.8299 0.3998 sec/batch\n",
      "Epoch 10/20  Iteration 1717/3560 Training loss: 1.8297 0.5055 sec/batch\n",
      "Epoch 10/20  Iteration 1718/3560 Training loss: 1.8292 0.4215 sec/batch\n",
      "Epoch 10/20  Iteration 1719/3560 Training loss: 1.8291 0.4847 sec/batch\n",
      "Epoch 10/20  Iteration 1720/3560 Training loss: 1.8291 0.4746 sec/batch\n",
      "Epoch 10/20  Iteration 1721/3560 Training loss: 1.8291 0.4584 sec/batch\n",
      "Epoch 10/20  Iteration 1722/3560 Training loss: 1.8291 0.5311 sec/batch\n",
      "Epoch 10/20  Iteration 1723/3560 Training loss: 1.8290 0.5329 sec/batch\n",
      "Epoch 10/20  Iteration 1724/3560 Training loss: 1.8287 0.4380 sec/batch\n",
      "Epoch 10/20  Iteration 1725/3560 Training loss: 1.8284 0.4448 sec/batch\n",
      "Epoch 10/20  Iteration 1726/3560 Training loss: 1.8285 0.4456 sec/batch\n",
      "Epoch 10/20  Iteration 1727/3560 Training loss: 1.8284 0.4918 sec/batch\n",
      "Epoch 10/20  Iteration 1728/3560 Training loss: 1.8280 0.9651 sec/batch\n",
      "Epoch 10/20  Iteration 1729/3560 Training loss: 1.8281 0.8138 sec/batch\n",
      "Epoch 10/20  Iteration 1730/3560 Training loss: 1.8283 0.6124 sec/batch\n",
      "Epoch 10/20  Iteration 1731/3560 Training loss: 1.8283 0.7321 sec/batch\n",
      "Epoch 10/20  Iteration 1732/3560 Training loss: 1.8281 0.5174 sec/batch\n",
      "Epoch 10/20  Iteration 1733/3560 Training loss: 1.8279 0.5228 sec/batch\n",
      "Epoch 10/20  Iteration 1734/3560 Training loss: 1.8276 0.4319 sec/batch\n",
      "Epoch 10/20  Iteration 1735/3560 Training loss: 1.8276 0.5113 sec/batch\n",
      "Epoch 10/20  Iteration 1736/3560 Training loss: 1.8278 0.5105 sec/batch\n",
      "Epoch 10/20  Iteration 1737/3560 Training loss: 1.8278 0.4633 sec/batch\n",
      "Epoch 10/20  Iteration 1738/3560 Training loss: 1.8279 0.4790 sec/batch\n",
      "Epoch 10/20  Iteration 1739/3560 Training loss: 1.8279 0.4967 sec/batch\n",
      "Epoch 10/20  Iteration 1740/3560 Training loss: 1.8280 0.4223 sec/batch\n",
      "Epoch 10/20  Iteration 1741/3560 Training loss: 1.8281 0.4268 sec/batch\n",
      "Epoch 10/20  Iteration 1742/3560 Training loss: 1.8279 0.4844 sec/batch\n",
      "Epoch 10/20  Iteration 1743/3560 Training loss: 1.8282 0.3866 sec/batch\n",
      "Epoch 10/20  Iteration 1744/3560 Training loss: 1.8280 0.3839 sec/batch\n",
      "Epoch 10/20  Iteration 1745/3560 Training loss: 1.8279 0.4682 sec/batch\n",
      "Epoch 10/20  Iteration 1746/3560 Training loss: 1.8280 0.4407 sec/batch\n",
      "Epoch 10/20  Iteration 1747/3560 Training loss: 1.8279 0.4865 sec/batch\n",
      "Epoch 10/20  Iteration 1748/3560 Training loss: 1.8280 0.4729 sec/batch\n",
      "Epoch 10/20  Iteration 1749/3560 Training loss: 1.8280 0.5880 sec/batch\n",
      "Epoch 10/20  Iteration 1750/3560 Training loss: 1.8281 0.4956 sec/batch\n",
      "Epoch 10/20  Iteration 1751/3560 Training loss: 1.8281 0.4091 sec/batch\n",
      "Epoch 10/20  Iteration 1752/3560 Training loss: 1.8280 0.4782 sec/batch\n",
      "Epoch 10/20  Iteration 1753/3560 Training loss: 1.8277 0.4360 sec/batch\n",
      "Epoch 10/20  Iteration 1754/3560 Training loss: 1.8277 0.4754 sec/batch\n",
      "Epoch 10/20  Iteration 1755/3560 Training loss: 1.8277 0.3818 sec/batch\n",
      "Epoch 10/20  Iteration 1756/3560 Training loss: 1.8278 0.3714 sec/batch\n",
      "Epoch 10/20  Iteration 1757/3560 Training loss: 1.8277 0.3702 sec/batch\n",
      "Epoch 10/20  Iteration 1758/3560 Training loss: 1.8277 0.3890 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1759/3560 Training loss: 1.8278 0.4787 sec/batch\n",
      "Epoch 10/20  Iteration 1760/3560 Training loss: 1.8277 0.4117 sec/batch\n",
      "Epoch 10/20  Iteration 1761/3560 Training loss: 1.8275 0.4701 sec/batch\n",
      "Epoch 10/20  Iteration 1762/3560 Training loss: 1.8277 0.4666 sec/batch\n",
      "Epoch 10/20  Iteration 1763/3560 Training loss: 1.8278 0.4931 sec/batch\n",
      "Epoch 10/20  Iteration 1764/3560 Training loss: 1.8277 0.5144 sec/batch\n",
      "Epoch 10/20  Iteration 1765/3560 Training loss: 1.8278 0.4485 sec/batch\n",
      "Epoch 10/20  Iteration 1766/3560 Training loss: 1.8278 0.4507 sec/batch\n",
      "Epoch 10/20  Iteration 1767/3560 Training loss: 1.8277 0.4597 sec/batch\n",
      "Epoch 10/20  Iteration 1768/3560 Training loss: 1.8277 0.4685 sec/batch\n",
      "Epoch 10/20  Iteration 1769/3560 Training loss: 1.8277 0.5097 sec/batch\n",
      "Epoch 10/20  Iteration 1770/3560 Training loss: 1.8280 0.5633 sec/batch\n",
      "Epoch 10/20  Iteration 1771/3560 Training loss: 1.8279 0.7735 sec/batch\n",
      "Epoch 10/20  Iteration 1772/3560 Training loss: 1.8279 0.7095 sec/batch\n",
      "Epoch 10/20  Iteration 1773/3560 Training loss: 1.8278 0.5278 sec/batch\n",
      "Epoch 10/20  Iteration 1774/3560 Training loss: 1.8276 0.6536 sec/batch\n",
      "Epoch 10/20  Iteration 1775/3560 Training loss: 1.8277 0.4749 sec/batch\n",
      "Epoch 10/20  Iteration 1776/3560 Training loss: 1.8278 0.6324 sec/batch\n",
      "Epoch 10/20  Iteration 1777/3560 Training loss: 1.8278 0.4484 sec/batch\n",
      "Epoch 10/20  Iteration 1778/3560 Training loss: 1.8277 0.5400 sec/batch\n",
      "Epoch 10/20  Iteration 1779/3560 Training loss: 1.8276 0.5097 sec/batch\n",
      "Epoch 10/20  Iteration 1780/3560 Training loss: 1.8276 0.5031 sec/batch\n",
      "Epoch 11/20  Iteration 1781/3560 Training loss: 1.8853 0.5193 sec/batch\n",
      "Epoch 11/20  Iteration 1782/3560 Training loss: 1.8478 0.5745 sec/batch\n",
      "Epoch 11/20  Iteration 1783/3560 Training loss: 1.8323 0.4645 sec/batch\n",
      "Epoch 11/20  Iteration 1784/3560 Training loss: 1.8295 0.4371 sec/batch\n",
      "Epoch 11/20  Iteration 1785/3560 Training loss: 1.8238 0.4352 sec/batch\n",
      "Epoch 11/20  Iteration 1786/3560 Training loss: 1.8146 0.6730 sec/batch\n",
      "Epoch 11/20  Iteration 1787/3560 Training loss: 1.8158 0.5153 sec/batch\n",
      "Epoch 11/20  Iteration 1788/3560 Training loss: 1.8152 0.5215 sec/batch\n",
      "Epoch 11/20  Iteration 1789/3560 Training loss: 1.8172 0.4987 sec/batch\n",
      "Epoch 11/20  Iteration 1790/3560 Training loss: 1.8171 0.4500 sec/batch\n",
      "Epoch 11/20  Iteration 1791/3560 Training loss: 1.8134 0.4884 sec/batch\n",
      "Epoch 11/20  Iteration 1792/3560 Training loss: 1.8124 0.3677 sec/batch\n",
      "Epoch 11/20  Iteration 1793/3560 Training loss: 1.8128 0.4239 sec/batch\n",
      "Epoch 11/20  Iteration 1794/3560 Training loss: 1.8156 0.4455 sec/batch\n",
      "Epoch 11/20  Iteration 1795/3560 Training loss: 1.8148 0.4198 sec/batch\n",
      "Epoch 11/20  Iteration 1796/3560 Training loss: 1.8132 0.4130 sec/batch\n",
      "Epoch 11/20  Iteration 1797/3560 Training loss: 1.8132 0.3949 sec/batch\n",
      "Epoch 11/20  Iteration 1798/3560 Training loss: 1.8151 0.4420 sec/batch\n",
      "Epoch 11/20  Iteration 1799/3560 Training loss: 1.8149 0.4783 sec/batch\n",
      "Epoch 11/20  Iteration 1800/3560 Training loss: 1.8155 0.4164 sec/batch\n",
      "Epoch 11/20  Iteration 1801/3560 Training loss: 1.8149 0.3995 sec/batch\n",
      "Epoch 11/20  Iteration 1802/3560 Training loss: 1.8156 0.3822 sec/batch\n",
      "Epoch 11/20  Iteration 1803/3560 Training loss: 1.8150 0.4368 sec/batch\n",
      "Epoch 11/20  Iteration 1804/3560 Training loss: 1.8146 0.5132 sec/batch\n",
      "Epoch 11/20  Iteration 1805/3560 Training loss: 1.8141 0.4044 sec/batch\n",
      "Epoch 11/20  Iteration 1806/3560 Training loss: 1.8129 0.4425 sec/batch\n",
      "Epoch 11/20  Iteration 1807/3560 Training loss: 1.8121 0.4217 sec/batch\n",
      "Epoch 11/20  Iteration 1808/3560 Training loss: 1.8127 0.4387 sec/batch\n",
      "Epoch 11/20  Iteration 1809/3560 Training loss: 1.8135 0.4225 sec/batch\n",
      "Epoch 11/20  Iteration 1810/3560 Training loss: 1.8138 0.3753 sec/batch\n",
      "Epoch 11/20  Iteration 1811/3560 Training loss: 1.8138 0.3973 sec/batch\n",
      "Epoch 11/20  Iteration 1812/3560 Training loss: 1.8133 0.4076 sec/batch\n",
      "Epoch 11/20  Iteration 1813/3560 Training loss: 1.8133 0.4608 sec/batch\n",
      "Epoch 11/20  Iteration 1814/3560 Training loss: 1.8143 0.4119 sec/batch\n",
      "Epoch 11/20  Iteration 1815/3560 Training loss: 1.8139 0.4098 sec/batch\n",
      "Epoch 11/20  Iteration 1816/3560 Training loss: 1.8138 0.4899 sec/batch\n",
      "Epoch 11/20  Iteration 1817/3560 Training loss: 1.8133 0.4559 sec/batch\n",
      "Epoch 11/20  Iteration 1818/3560 Training loss: 1.8123 0.4667 sec/batch\n",
      "Epoch 11/20  Iteration 1819/3560 Training loss: 1.8112 0.4047 sec/batch\n",
      "Epoch 11/20  Iteration 1820/3560 Training loss: 1.8108 0.3887 sec/batch\n",
      "Epoch 11/20  Iteration 1821/3560 Training loss: 1.8104 0.3884 sec/batch\n",
      "Epoch 11/20  Iteration 1822/3560 Training loss: 1.8108 0.5060 sec/batch\n",
      "Epoch 11/20  Iteration 1823/3560 Training loss: 1.8104 0.3824 sec/batch\n",
      "Epoch 11/20  Iteration 1824/3560 Training loss: 1.8097 0.4526 sec/batch\n",
      "Epoch 11/20  Iteration 1825/3560 Training loss: 1.8097 0.3909 sec/batch\n",
      "Epoch 11/20  Iteration 1826/3560 Training loss: 1.8085 0.4029 sec/batch\n",
      "Epoch 11/20  Iteration 1827/3560 Training loss: 1.8082 0.3961 sec/batch\n",
      "Epoch 11/20  Iteration 1828/3560 Training loss: 1.8077 0.4234 sec/batch\n",
      "Epoch 11/20  Iteration 1829/3560 Training loss: 1.8076 0.4002 sec/batch\n",
      "Epoch 11/20  Iteration 1830/3560 Training loss: 1.8083 0.4209 sec/batch\n",
      "Epoch 11/20  Iteration 1831/3560 Training loss: 1.8077 0.3614 sec/batch\n",
      "Epoch 11/20  Iteration 1832/3560 Training loss: 1.8087 0.3960 sec/batch\n",
      "Epoch 11/20  Iteration 1833/3560 Training loss: 1.8085 0.4362 sec/batch\n",
      "Epoch 11/20  Iteration 1834/3560 Training loss: 1.8083 0.4455 sec/batch\n",
      "Epoch 11/20  Iteration 1835/3560 Training loss: 1.8077 0.3987 sec/batch\n",
      "Epoch 11/20  Iteration 1836/3560 Training loss: 1.8080 0.4364 sec/batch\n",
      "Epoch 11/20  Iteration 1837/3560 Training loss: 1.8083 0.4706 sec/batch\n",
      "Epoch 11/20  Iteration 1838/3560 Training loss: 1.8081 0.3992 sec/batch\n",
      "Epoch 11/20  Iteration 1839/3560 Training loss: 1.8076 0.3953 sec/batch\n",
      "Epoch 11/20  Iteration 1840/3560 Training loss: 1.8082 0.4397 sec/batch\n",
      "Epoch 11/20  Iteration 1841/3560 Training loss: 1.8080 0.3971 sec/batch\n",
      "Epoch 11/20  Iteration 1842/3560 Training loss: 1.8087 0.5474 sec/batch\n",
      "Epoch 11/20  Iteration 1843/3560 Training loss: 1.8091 0.3724 sec/batch\n",
      "Epoch 11/20  Iteration 1844/3560 Training loss: 1.8092 0.5067 sec/batch\n",
      "Epoch 11/20  Iteration 1845/3560 Training loss: 1.8090 0.3924 sec/batch\n",
      "Epoch 11/20  Iteration 1846/3560 Training loss: 1.8094 0.4226 sec/batch\n",
      "Epoch 11/20  Iteration 1847/3560 Training loss: 1.8095 0.4203 sec/batch\n",
      "Epoch 11/20  Iteration 1848/3560 Training loss: 1.8091 0.3928 sec/batch\n",
      "Epoch 11/20  Iteration 1849/3560 Training loss: 1.8090 0.3678 sec/batch\n",
      "Epoch 11/20  Iteration 1850/3560 Training loss: 1.8090 0.3900 sec/batch\n",
      "Epoch 11/20  Iteration 1851/3560 Training loss: 1.8095 0.4241 sec/batch\n",
      "Epoch 11/20  Iteration 1852/3560 Training loss: 1.8098 0.3340 sec/batch\n",
      "Epoch 11/20  Iteration 1853/3560 Training loss: 1.8101 0.4523 sec/batch\n",
      "Epoch 11/20  Iteration 1854/3560 Training loss: 1.8099 0.4072 sec/batch\n",
      "Epoch 11/20  Iteration 1855/3560 Training loss: 1.8098 0.4363 sec/batch\n",
      "Epoch 11/20  Iteration 1856/3560 Training loss: 1.8101 0.4375 sec/batch\n",
      "Epoch 11/20  Iteration 1857/3560 Training loss: 1.8100 0.4325 sec/batch\n",
      "Epoch 11/20  Iteration 1858/3560 Training loss: 1.8102 0.3920 sec/batch\n",
      "Epoch 11/20  Iteration 1859/3560 Training loss: 1.8098 0.3820 sec/batch\n",
      "Epoch 11/20  Iteration 1860/3560 Training loss: 1.8097 0.4102 sec/batch\n",
      "Epoch 11/20  Iteration 1861/3560 Training loss: 1.8091 0.4746 sec/batch\n",
      "Epoch 11/20  Iteration 1862/3560 Training loss: 1.8094 0.3782 sec/batch\n",
      "Epoch 11/20  Iteration 1863/3560 Training loss: 1.8089 0.4606 sec/batch\n",
      "Epoch 11/20  Iteration 1864/3560 Training loss: 1.8088 0.3946 sec/batch\n",
      "Epoch 11/20  Iteration 1865/3560 Training loss: 1.8084 0.5052 sec/batch\n",
      "Epoch 11/20  Iteration 1866/3560 Training loss: 1.8082 0.4868 sec/batch\n",
      "Epoch 11/20  Iteration 1867/3560 Training loss: 1.8081 0.4669 sec/batch\n",
      "Epoch 11/20  Iteration 1868/3560 Training loss: 1.8077 0.3774 sec/batch\n",
      "Epoch 11/20  Iteration 1869/3560 Training loss: 1.8073 0.4134 sec/batch\n",
      "Epoch 11/20  Iteration 1870/3560 Training loss: 1.8074 0.4028 sec/batch\n",
      "Epoch 11/20  Iteration 1871/3560 Training loss: 1.8072 0.4064 sec/batch\n",
      "Epoch 11/20  Iteration 1872/3560 Training loss: 1.8070 0.3721 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1873/3560 Training loss: 1.8065 0.3976 sec/batch\n",
      "Epoch 11/20  Iteration 1874/3560 Training loss: 1.8062 0.4597 sec/batch\n",
      "Epoch 11/20  Iteration 1875/3560 Training loss: 1.8058 0.4311 sec/batch\n",
      "Epoch 11/20  Iteration 1876/3560 Training loss: 1.8058 0.4511 sec/batch\n",
      "Epoch 11/20  Iteration 1877/3560 Training loss: 1.8056 0.4262 sec/batch\n",
      "Epoch 11/20  Iteration 1878/3560 Training loss: 1.8052 0.3959 sec/batch\n",
      "Epoch 11/20  Iteration 1879/3560 Training loss: 1.8048 0.4318 sec/batch\n",
      "Epoch 11/20  Iteration 1880/3560 Training loss: 1.8043 0.4434 sec/batch\n",
      "Epoch 11/20  Iteration 1881/3560 Training loss: 1.8043 0.4335 sec/batch\n",
      "Epoch 11/20  Iteration 1882/3560 Training loss: 1.8041 0.4199 sec/batch\n",
      "Epoch 11/20  Iteration 1883/3560 Training loss: 1.8038 0.4431 sec/batch\n",
      "Epoch 11/20  Iteration 1884/3560 Training loss: 1.8037 0.4660 sec/batch\n",
      "Epoch 11/20  Iteration 1885/3560 Training loss: 1.8035 0.3805 sec/batch\n",
      "Epoch 11/20  Iteration 1886/3560 Training loss: 1.8033 0.4211 sec/batch\n",
      "Epoch 11/20  Iteration 1887/3560 Training loss: 1.8033 0.3983 sec/batch\n",
      "Epoch 11/20  Iteration 1888/3560 Training loss: 1.8033 0.4214 sec/batch\n",
      "Epoch 11/20  Iteration 1889/3560 Training loss: 1.8033 0.3894 sec/batch\n",
      "Epoch 11/20  Iteration 1890/3560 Training loss: 1.8032 0.4195 sec/batch\n",
      "Epoch 11/20  Iteration 1891/3560 Training loss: 1.8030 0.5049 sec/batch\n",
      "Epoch 11/20  Iteration 1892/3560 Training loss: 1.8029 0.2808 sec/batch\n",
      "Epoch 11/20  Iteration 1893/3560 Training loss: 1.8029 0.5125 sec/batch\n",
      "Epoch 11/20  Iteration 1894/3560 Training loss: 1.8028 0.4247 sec/batch\n",
      "Epoch 11/20  Iteration 1895/3560 Training loss: 1.8025 0.3999 sec/batch\n",
      "Epoch 11/20  Iteration 1896/3560 Training loss: 1.8022 0.4088 sec/batch\n",
      "Epoch 11/20  Iteration 1897/3560 Training loss: 1.8021 0.3620 sec/batch\n",
      "Epoch 11/20  Iteration 1898/3560 Training loss: 1.8021 0.3833 sec/batch\n",
      "Epoch 11/20  Iteration 1899/3560 Training loss: 1.8021 0.4082 sec/batch\n",
      "Epoch 11/20  Iteration 1900/3560 Training loss: 1.8020 0.4688 sec/batch\n",
      "Epoch 11/20  Iteration 1901/3560 Training loss: 1.8021 0.4124 sec/batch\n",
      "Epoch 11/20  Iteration 1902/3560 Training loss: 1.8018 0.4510 sec/batch\n",
      "Epoch 11/20  Iteration 1903/3560 Training loss: 1.8015 0.3709 sec/batch\n",
      "Epoch 11/20  Iteration 1904/3560 Training loss: 1.8016 0.4573 sec/batch\n",
      "Epoch 11/20  Iteration 1905/3560 Training loss: 1.8016 0.4040 sec/batch\n",
      "Epoch 11/20  Iteration 1906/3560 Training loss: 1.8012 0.4293 sec/batch\n",
      "Epoch 11/20  Iteration 1907/3560 Training loss: 1.8013 0.3997 sec/batch\n",
      "Epoch 11/20  Iteration 1908/3560 Training loss: 1.8015 0.4025 sec/batch\n",
      "Epoch 11/20  Iteration 1909/3560 Training loss: 1.8014 0.4516 sec/batch\n",
      "Epoch 11/20  Iteration 1910/3560 Training loss: 1.8012 0.3666 sec/batch\n",
      "Epoch 11/20  Iteration 1911/3560 Training loss: 1.8009 0.4236 sec/batch\n",
      "Epoch 11/20  Iteration 1912/3560 Training loss: 1.8007 0.4405 sec/batch\n",
      "Epoch 11/20  Iteration 1913/3560 Training loss: 1.8007 0.4946 sec/batch\n",
      "Epoch 11/20  Iteration 1914/3560 Training loss: 1.8007 0.4820 sec/batch\n",
      "Epoch 11/20  Iteration 1915/3560 Training loss: 1.8008 0.3757 sec/batch\n",
      "Epoch 11/20  Iteration 1916/3560 Training loss: 1.8008 0.3999 sec/batch\n",
      "Epoch 11/20  Iteration 1917/3560 Training loss: 1.8008 0.3573 sec/batch\n",
      "Epoch 11/20  Iteration 1918/3560 Training loss: 1.8009 0.4282 sec/batch\n",
      "Epoch 11/20  Iteration 1919/3560 Training loss: 1.8010 0.4377 sec/batch\n",
      "Epoch 11/20  Iteration 1920/3560 Training loss: 1.8009 0.4214 sec/batch\n",
      "Epoch 11/20  Iteration 1921/3560 Training loss: 1.8012 0.4243 sec/batch\n",
      "Epoch 11/20  Iteration 1922/3560 Training loss: 1.8011 0.4119 sec/batch\n",
      "Epoch 11/20  Iteration 1923/3560 Training loss: 1.8011 0.4941 sec/batch\n",
      "Epoch 11/20  Iteration 1924/3560 Training loss: 1.8012 0.3610 sec/batch\n",
      "Epoch 11/20  Iteration 1925/3560 Training loss: 1.8010 0.4370 sec/batch\n",
      "Epoch 11/20  Iteration 1926/3560 Training loss: 1.8011 0.4105 sec/batch\n",
      "Epoch 11/20  Iteration 1927/3560 Training loss: 1.8012 0.4754 sec/batch\n",
      "Epoch 11/20  Iteration 1928/3560 Training loss: 1.8014 0.4066 sec/batch\n",
      "Epoch 11/20  Iteration 1929/3560 Training loss: 1.8014 0.3813 sec/batch\n",
      "Epoch 11/20  Iteration 1930/3560 Training loss: 1.8013 0.4055 sec/batch\n",
      "Epoch 11/20  Iteration 1931/3560 Training loss: 1.8011 0.5020 sec/batch\n",
      "Epoch 11/20  Iteration 1932/3560 Training loss: 1.8011 0.4304 sec/batch\n",
      "Epoch 11/20  Iteration 1933/3560 Training loss: 1.8011 0.4301 sec/batch\n",
      "Epoch 11/20  Iteration 1934/3560 Training loss: 1.8013 0.4782 sec/batch\n",
      "Epoch 11/20  Iteration 1935/3560 Training loss: 1.8013 0.4236 sec/batch\n",
      "Epoch 11/20  Iteration 1936/3560 Training loss: 1.8013 0.3912 sec/batch\n",
      "Epoch 11/20  Iteration 1937/3560 Training loss: 1.8013 0.4797 sec/batch\n",
      "Epoch 11/20  Iteration 1938/3560 Training loss: 1.8013 0.4399 sec/batch\n",
      "Epoch 11/20  Iteration 1939/3560 Training loss: 1.8011 0.4479 sec/batch\n",
      "Epoch 11/20  Iteration 1940/3560 Training loss: 1.8013 0.4782 sec/batch\n",
      "Epoch 11/20  Iteration 1941/3560 Training loss: 1.8015 0.3931 sec/batch\n",
      "Epoch 11/20  Iteration 1942/3560 Training loss: 1.8014 0.4185 sec/batch\n",
      "Epoch 11/20  Iteration 1943/3560 Training loss: 1.8015 0.4149 sec/batch\n",
      "Epoch 11/20  Iteration 1944/3560 Training loss: 1.8015 0.4060 sec/batch\n",
      "Epoch 11/20  Iteration 1945/3560 Training loss: 1.8015 0.3994 sec/batch\n",
      "Epoch 11/20  Iteration 1946/3560 Training loss: 1.8014 0.3689 sec/batch\n",
      "Epoch 11/20  Iteration 1947/3560 Training loss: 1.8015 0.4846 sec/batch\n",
      "Epoch 11/20  Iteration 1948/3560 Training loss: 1.8018 0.3477 sec/batch\n",
      "Epoch 11/20  Iteration 1949/3560 Training loss: 1.8018 0.3923 sec/batch\n",
      "Epoch 11/20  Iteration 1950/3560 Training loss: 1.8017 0.3742 sec/batch\n",
      "Epoch 11/20  Iteration 1951/3560 Training loss: 1.8016 0.5236 sec/batch\n",
      "Epoch 11/20  Iteration 1952/3560 Training loss: 1.8015 0.3941 sec/batch\n",
      "Epoch 11/20  Iteration 1953/3560 Training loss: 1.8015 0.4441 sec/batch\n",
      "Epoch 11/20  Iteration 1954/3560 Training loss: 1.8015 0.3707 sec/batch\n",
      "Epoch 11/20  Iteration 1955/3560 Training loss: 1.8016 0.3985 sec/batch\n",
      "Epoch 11/20  Iteration 1956/3560 Training loss: 1.8015 0.4899 sec/batch\n",
      "Epoch 11/20  Iteration 1957/3560 Training loss: 1.8014 0.4201 sec/batch\n",
      "Epoch 11/20  Iteration 1958/3560 Training loss: 1.8014 0.4848 sec/batch\n",
      "Epoch 12/20  Iteration 1959/3560 Training loss: 1.8664 0.4020 sec/batch\n",
      "Epoch 12/20  Iteration 1960/3560 Training loss: 1.8273 0.4236 sec/batch\n",
      "Epoch 12/20  Iteration 1961/3560 Training loss: 1.8159 0.4151 sec/batch\n",
      "Epoch 12/20  Iteration 1962/3560 Training loss: 1.8097 0.4548 sec/batch\n",
      "Epoch 12/20  Iteration 1963/3560 Training loss: 1.8038 0.4176 sec/batch\n",
      "Epoch 12/20  Iteration 1964/3560 Training loss: 1.7941 0.4849 sec/batch\n",
      "Epoch 12/20  Iteration 1965/3560 Training loss: 1.7943 0.3333 sec/batch\n",
      "Epoch 12/20  Iteration 1966/3560 Training loss: 1.7941 0.3518 sec/batch\n",
      "Epoch 12/20  Iteration 1967/3560 Training loss: 1.7967 0.5023 sec/batch\n",
      "Epoch 12/20  Iteration 1968/3560 Training loss: 1.7957 0.3446 sec/batch\n",
      "Epoch 12/20  Iteration 1969/3560 Training loss: 1.7934 0.5318 sec/batch\n",
      "Epoch 12/20  Iteration 1970/3560 Training loss: 1.7920 0.4300 sec/batch\n",
      "Epoch 12/20  Iteration 1971/3560 Training loss: 1.7918 0.4156 sec/batch\n",
      "Epoch 12/20  Iteration 1972/3560 Training loss: 1.7941 0.4504 sec/batch\n",
      "Epoch 12/20  Iteration 1973/3560 Training loss: 1.7934 0.4055 sec/batch\n",
      "Epoch 12/20  Iteration 1974/3560 Training loss: 1.7917 0.4968 sec/batch\n",
      "Epoch 12/20  Iteration 1975/3560 Training loss: 1.7916 0.3750 sec/batch\n",
      "Epoch 12/20  Iteration 1976/3560 Training loss: 1.7927 0.3718 sec/batch\n",
      "Epoch 12/20  Iteration 1977/3560 Training loss: 1.7924 0.4647 sec/batch\n",
      "Epoch 12/20  Iteration 1978/3560 Training loss: 1.7925 0.4807 sec/batch\n",
      "Epoch 12/20  Iteration 1979/3560 Training loss: 1.7919 0.3630 sec/batch\n",
      "Epoch 12/20  Iteration 1980/3560 Training loss: 1.7925 0.4851 sec/batch\n",
      "Epoch 12/20  Iteration 1981/3560 Training loss: 1.7919 0.4659 sec/batch\n",
      "Epoch 12/20  Iteration 1982/3560 Training loss: 1.7914 0.3776 sec/batch\n",
      "Epoch 12/20  Iteration 1983/3560 Training loss: 1.7909 0.4276 sec/batch\n",
      "Epoch 12/20  Iteration 1984/3560 Training loss: 1.7899 0.4215 sec/batch\n",
      "Epoch 12/20  Iteration 1985/3560 Training loss: 1.7891 0.3980 sec/batch\n",
      "Epoch 12/20  Iteration 1986/3560 Training loss: 1.7898 0.4183 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 1987/3560 Training loss: 1.7908 0.3532 sec/batch\n",
      "Epoch 12/20  Iteration 1988/3560 Training loss: 1.7907 0.4029 sec/batch\n",
      "Epoch 12/20  Iteration 1989/3560 Training loss: 1.7906 0.5065 sec/batch\n",
      "Epoch 12/20  Iteration 1990/3560 Training loss: 1.7899 0.4187 sec/batch\n",
      "Epoch 12/20  Iteration 1991/3560 Training loss: 1.7900 0.4008 sec/batch\n",
      "Epoch 12/20  Iteration 1992/3560 Training loss: 1.7908 0.4027 sec/batch\n",
      "Epoch 12/20  Iteration 1993/3560 Training loss: 1.7908 0.4601 sec/batch\n",
      "Epoch 12/20  Iteration 1994/3560 Training loss: 1.7904 0.4213 sec/batch\n",
      "Epoch 12/20  Iteration 1995/3560 Training loss: 1.7900 0.3911 sec/batch\n",
      "Epoch 12/20  Iteration 1996/3560 Training loss: 1.7891 0.4409 sec/batch\n",
      "Epoch 12/20  Iteration 1997/3560 Training loss: 1.7878 0.4097 sec/batch\n",
      "Epoch 12/20  Iteration 1998/3560 Training loss: 1.7873 0.4613 sec/batch\n",
      "Epoch 12/20  Iteration 1999/3560 Training loss: 1.7866 0.3997 sec/batch\n",
      "Epoch 12/20  Iteration 2000/3560 Training loss: 1.7872 0.4118 sec/batch\n",
      "Epoch 12/20  Iteration 2001/3560 Training loss: 1.7869 0.4452 sec/batch\n",
      "Epoch 12/20  Iteration 2002/3560 Training loss: 1.7862 0.3996 sec/batch\n",
      "Epoch 12/20  Iteration 2003/3560 Training loss: 1.7864 0.4046 sec/batch\n",
      "Epoch 12/20  Iteration 2004/3560 Training loss: 1.7853 0.4369 sec/batch\n",
      "Epoch 12/20  Iteration 2005/3560 Training loss: 1.7849 0.4077 sec/batch\n",
      "Epoch 12/20  Iteration 2006/3560 Training loss: 1.7843 0.4107 sec/batch\n",
      "Epoch 12/20  Iteration 2007/3560 Training loss: 1.7840 0.3550 sec/batch\n",
      "Epoch 12/20  Iteration 2008/3560 Training loss: 1.7848 0.3994 sec/batch\n",
      "Epoch 12/20  Iteration 2009/3560 Training loss: 1.7842 0.4991 sec/batch\n",
      "Epoch 12/20  Iteration 2010/3560 Training loss: 1.7850 0.4467 sec/batch\n",
      "Epoch 12/20  Iteration 2011/3560 Training loss: 1.7849 0.4694 sec/batch\n",
      "Epoch 12/20  Iteration 2012/3560 Training loss: 1.7849 0.3980 sec/batch\n",
      "Epoch 12/20  Iteration 2013/3560 Training loss: 1.7845 0.3804 sec/batch\n",
      "Epoch 12/20  Iteration 2014/3560 Training loss: 1.7847 0.3906 sec/batch\n",
      "Epoch 12/20  Iteration 2015/3560 Training loss: 1.7850 0.4612 sec/batch\n",
      "Epoch 12/20  Iteration 2016/3560 Training loss: 1.7849 0.3787 sec/batch\n",
      "Epoch 12/20  Iteration 2017/3560 Training loss: 1.7843 0.4536 sec/batch\n",
      "Epoch 12/20  Iteration 2018/3560 Training loss: 1.7849 0.4513 sec/batch\n",
      "Epoch 12/20  Iteration 2019/3560 Training loss: 1.7848 0.4120 sec/batch\n",
      "Epoch 12/20  Iteration 2020/3560 Training loss: 1.7854 0.4022 sec/batch\n",
      "Epoch 12/20  Iteration 2021/3560 Training loss: 1.7858 0.3472 sec/batch\n",
      "Epoch 12/20  Iteration 2022/3560 Training loss: 1.7859 0.4510 sec/batch\n",
      "Epoch 12/20  Iteration 2023/3560 Training loss: 1.7858 0.3894 sec/batch\n",
      "Epoch 12/20  Iteration 2024/3560 Training loss: 1.7862 0.4118 sec/batch\n",
      "Epoch 12/20  Iteration 2025/3560 Training loss: 1.7863 0.4473 sec/batch\n",
      "Epoch 12/20  Iteration 2026/3560 Training loss: 1.7860 0.3931 sec/batch\n",
      "Epoch 12/20  Iteration 2027/3560 Training loss: 1.7860 0.5429 sec/batch\n",
      "Epoch 12/20  Iteration 2028/3560 Training loss: 1.7858 0.4058 sec/batch\n",
      "Epoch 12/20  Iteration 2029/3560 Training loss: 1.7865 0.5069 sec/batch\n",
      "Epoch 12/20  Iteration 2030/3560 Training loss: 1.7867 0.4133 sec/batch\n",
      "Epoch 12/20  Iteration 2031/3560 Training loss: 1.7870 0.4120 sec/batch\n",
      "Epoch 12/20  Iteration 2032/3560 Training loss: 1.7867 0.3934 sec/batch\n",
      "Epoch 12/20  Iteration 2033/3560 Training loss: 1.7867 0.3820 sec/batch\n",
      "Epoch 12/20  Iteration 2034/3560 Training loss: 1.7869 0.4205 sec/batch\n",
      "Epoch 12/20  Iteration 2035/3560 Training loss: 1.7868 0.4434 sec/batch\n",
      "Epoch 12/20  Iteration 2036/3560 Training loss: 1.7870 0.4681 sec/batch\n",
      "Epoch 12/20  Iteration 2037/3560 Training loss: 1.7867 0.4418 sec/batch\n",
      "Epoch 12/20  Iteration 2038/3560 Training loss: 1.7867 0.4192 sec/batch\n",
      "Epoch 12/20  Iteration 2039/3560 Training loss: 1.7862 0.3925 sec/batch\n",
      "Epoch 12/20  Iteration 2040/3560 Training loss: 1.7864 0.3926 sec/batch\n",
      "Epoch 12/20  Iteration 2041/3560 Training loss: 1.7861 0.4123 sec/batch\n",
      "Epoch 12/20  Iteration 2042/3560 Training loss: 1.7860 0.4783 sec/batch\n",
      "Epoch 12/20  Iteration 2043/3560 Training loss: 1.7855 0.3495 sec/batch\n",
      "Epoch 12/20  Iteration 2044/3560 Training loss: 1.7851 0.3750 sec/batch\n",
      "Epoch 12/20  Iteration 2045/3560 Training loss: 1.7850 0.4271 sec/batch\n",
      "Epoch 12/20  Iteration 2046/3560 Training loss: 1.7847 0.4336 sec/batch\n",
      "Epoch 12/20  Iteration 2047/3560 Training loss: 1.7843 0.4289 sec/batch\n",
      "Epoch 12/20  Iteration 2048/3560 Training loss: 1.7844 0.4363 sec/batch\n",
      "Epoch 12/20  Iteration 2049/3560 Training loss: 1.7841 0.4576 sec/batch\n",
      "Epoch 12/20  Iteration 2050/3560 Training loss: 1.7839 0.4992 sec/batch\n",
      "Epoch 12/20  Iteration 2051/3560 Training loss: 1.7836 0.3335 sec/batch\n",
      "Epoch 12/20  Iteration 2052/3560 Training loss: 1.7833 0.4082 sec/batch\n",
      "Epoch 12/20  Iteration 2053/3560 Training loss: 1.7830 0.3634 sec/batch\n",
      "Epoch 12/20  Iteration 2054/3560 Training loss: 1.7828 0.3877 sec/batch\n",
      "Epoch 12/20  Iteration 2055/3560 Training loss: 1.7827 0.4843 sec/batch\n",
      "Epoch 12/20  Iteration 2056/3560 Training loss: 1.7823 0.3778 sec/batch\n",
      "Epoch 12/20  Iteration 2057/3560 Training loss: 1.7819 0.4424 sec/batch\n",
      "Epoch 12/20  Iteration 2058/3560 Training loss: 1.7814 0.4371 sec/batch\n",
      "Epoch 12/20  Iteration 2059/3560 Training loss: 1.7812 0.3874 sec/batch\n",
      "Epoch 12/20  Iteration 2060/3560 Training loss: 1.7812 0.4415 sec/batch\n",
      "Epoch 12/20  Iteration 2061/3560 Training loss: 1.7810 0.3992 sec/batch\n",
      "Epoch 12/20  Iteration 2062/3560 Training loss: 1.7807 0.4059 sec/batch\n",
      "Epoch 12/20  Iteration 2063/3560 Training loss: 1.7805 0.3797 sec/batch\n",
      "Epoch 12/20  Iteration 2064/3560 Training loss: 1.7804 0.4145 sec/batch\n",
      "Epoch 12/20  Iteration 2065/3560 Training loss: 1.7803 0.4313 sec/batch\n",
      "Epoch 12/20  Iteration 2066/3560 Training loss: 1.7803 0.4182 sec/batch\n",
      "Epoch 12/20  Iteration 2067/3560 Training loss: 1.7803 0.4797 sec/batch\n",
      "Epoch 12/20  Iteration 2068/3560 Training loss: 1.7803 0.4157 sec/batch\n",
      "Epoch 12/20  Iteration 2069/3560 Training loss: 1.7803 0.3903 sec/batch\n",
      "Epoch 12/20  Iteration 2070/3560 Training loss: 1.7801 0.4234 sec/batch\n",
      "Epoch 12/20  Iteration 2071/3560 Training loss: 1.7800 0.4516 sec/batch\n",
      "Epoch 12/20  Iteration 2072/3560 Training loss: 1.7798 0.4125 sec/batch\n",
      "Epoch 12/20  Iteration 2073/3560 Training loss: 1.7796 0.3972 sec/batch\n",
      "Epoch 12/20  Iteration 2074/3560 Training loss: 1.7792 0.4803 sec/batch\n",
      "Epoch 12/20  Iteration 2075/3560 Training loss: 1.7792 0.4288 sec/batch\n",
      "Epoch 12/20  Iteration 2076/3560 Training loss: 1.7792 0.4219 sec/batch\n",
      "Epoch 12/20  Iteration 2077/3560 Training loss: 1.7791 0.4551 sec/batch\n",
      "Epoch 12/20  Iteration 2078/3560 Training loss: 1.7791 0.4074 sec/batch\n",
      "Epoch 12/20  Iteration 2079/3560 Training loss: 1.7791 0.4080 sec/batch\n",
      "Epoch 12/20  Iteration 2080/3560 Training loss: 1.7788 0.3894 sec/batch\n",
      "Epoch 12/20  Iteration 2081/3560 Training loss: 1.7784 0.4169 sec/batch\n",
      "Epoch 12/20  Iteration 2082/3560 Training loss: 1.7785 0.4141 sec/batch\n",
      "Epoch 12/20  Iteration 2083/3560 Training loss: 1.7784 0.3988 sec/batch\n",
      "Epoch 12/20  Iteration 2084/3560 Training loss: 1.7780 0.4048 sec/batch\n",
      "Epoch 12/20  Iteration 2085/3560 Training loss: 1.7781 0.3952 sec/batch\n",
      "Epoch 12/20  Iteration 2086/3560 Training loss: 1.7783 0.4388 sec/batch\n",
      "Epoch 12/20  Iteration 2087/3560 Training loss: 1.7782 0.3989 sec/batch\n",
      "Epoch 12/20  Iteration 2088/3560 Training loss: 1.7780 0.3986 sec/batch\n",
      "Epoch 12/20  Iteration 2089/3560 Training loss: 1.7777 0.3953 sec/batch\n",
      "Epoch 12/20  Iteration 2090/3560 Training loss: 1.7775 0.3908 sec/batch\n",
      "Epoch 12/20  Iteration 2091/3560 Training loss: 1.7775 0.4634 sec/batch\n",
      "Epoch 12/20  Iteration 2092/3560 Training loss: 1.7775 0.3739 sec/batch\n",
      "Epoch 12/20  Iteration 2093/3560 Training loss: 1.7776 0.4150 sec/batch\n",
      "Epoch 12/20  Iteration 2094/3560 Training loss: 1.7776 0.4779 sec/batch\n",
      "Epoch 12/20  Iteration 2095/3560 Training loss: 1.7777 0.4189 sec/batch\n",
      "Epoch 12/20  Iteration 2096/3560 Training loss: 1.7777 0.4521 sec/batch\n",
      "Epoch 12/20  Iteration 2097/3560 Training loss: 1.7779 0.4028 sec/batch\n",
      "Epoch 12/20  Iteration 2098/3560 Training loss: 1.7778 0.3918 sec/batch\n",
      "Epoch 12/20  Iteration 2099/3560 Training loss: 1.7780 0.4259 sec/batch\n",
      "Epoch 12/20  Iteration 2100/3560 Training loss: 1.7779 0.4115 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 2101/3560 Training loss: 1.7778 0.3892 sec/batch\n",
      "Epoch 12/20  Iteration 2102/3560 Training loss: 1.7778 0.3891 sec/batch\n",
      "Epoch 12/20  Iteration 2103/3560 Training loss: 1.7776 0.3517 sec/batch\n",
      "Epoch 12/20  Iteration 2104/3560 Training loss: 1.7777 0.4956 sec/batch\n",
      "Epoch 12/20  Iteration 2105/3560 Training loss: 1.7777 0.3952 sec/batch\n",
      "Epoch 12/20  Iteration 2106/3560 Training loss: 1.7778 0.3716 sec/batch\n",
      "Epoch 12/20  Iteration 2107/3560 Training loss: 1.7778 0.4050 sec/batch\n",
      "Epoch 12/20  Iteration 2108/3560 Training loss: 1.7777 0.4422 sec/batch\n",
      "Epoch 12/20  Iteration 2109/3560 Training loss: 1.7774 0.4011 sec/batch\n",
      "Epoch 12/20  Iteration 2110/3560 Training loss: 1.7774 0.4029 sec/batch\n",
      "Epoch 12/20  Iteration 2111/3560 Training loss: 1.7775 0.4145 sec/batch\n",
      "Epoch 12/20  Iteration 2112/3560 Training loss: 1.7776 0.4335 sec/batch\n",
      "Epoch 12/20  Iteration 2113/3560 Training loss: 1.7776 0.3999 sec/batch\n",
      "Epoch 12/20  Iteration 2114/3560 Training loss: 1.7776 0.4677 sec/batch\n",
      "Epoch 12/20  Iteration 2115/3560 Training loss: 1.7776 0.4147 sec/batch\n",
      "Epoch 12/20  Iteration 2116/3560 Training loss: 1.7776 0.4235 sec/batch\n",
      "Epoch 12/20  Iteration 2117/3560 Training loss: 1.7773 0.4045 sec/batch\n",
      "Epoch 12/20  Iteration 2118/3560 Training loss: 1.7775 0.3748 sec/batch\n",
      "Epoch 12/20  Iteration 2119/3560 Training loss: 1.7776 0.3946 sec/batch\n",
      "Epoch 12/20  Iteration 2120/3560 Training loss: 1.7776 0.3808 sec/batch\n",
      "Epoch 12/20  Iteration 2121/3560 Training loss: 1.7777 0.4581 sec/batch\n",
      "Epoch 12/20  Iteration 2122/3560 Training loss: 1.7777 0.3966 sec/batch\n",
      "Epoch 12/20  Iteration 2123/3560 Training loss: 1.7777 0.3578 sec/batch\n",
      "Epoch 12/20  Iteration 2124/3560 Training loss: 1.7777 0.4866 sec/batch\n",
      "Epoch 12/20  Iteration 2125/3560 Training loss: 1.7777 0.4346 sec/batch\n",
      "Epoch 12/20  Iteration 2126/3560 Training loss: 1.7781 0.4842 sec/batch\n",
      "Epoch 12/20  Iteration 2127/3560 Training loss: 1.7780 0.3886 sec/batch\n",
      "Epoch 12/20  Iteration 2128/3560 Training loss: 1.7780 0.4540 sec/batch\n",
      "Epoch 12/20  Iteration 2129/3560 Training loss: 1.7778 0.3886 sec/batch\n",
      "Epoch 12/20  Iteration 2130/3560 Training loss: 1.7776 0.4166 sec/batch\n",
      "Epoch 12/20  Iteration 2131/3560 Training loss: 1.7777 0.4097 sec/batch\n",
      "Epoch 12/20  Iteration 2132/3560 Training loss: 1.7778 0.4071 sec/batch\n",
      "Epoch 12/20  Iteration 2133/3560 Training loss: 1.7779 0.4967 sec/batch\n",
      "Epoch 12/20  Iteration 2134/3560 Training loss: 1.7778 0.4121 sec/batch\n",
      "Epoch 12/20  Iteration 2135/3560 Training loss: 1.7777 0.4299 sec/batch\n",
      "Epoch 12/20  Iteration 2136/3560 Training loss: 1.7778 0.4116 sec/batch\n",
      "Epoch 13/20  Iteration 2137/3560 Training loss: 1.8387 0.3689 sec/batch\n",
      "Epoch 13/20  Iteration 2138/3560 Training loss: 1.8007 0.4249 sec/batch\n",
      "Epoch 13/20  Iteration 2139/3560 Training loss: 1.7928 0.3973 sec/batch\n",
      "Epoch 13/20  Iteration 2140/3560 Training loss: 1.7885 0.3958 sec/batch\n",
      "Epoch 13/20  Iteration 2141/3560 Training loss: 1.7817 0.3865 sec/batch\n",
      "Epoch 13/20  Iteration 2142/3560 Training loss: 1.7723 0.4134 sec/batch\n",
      "Epoch 13/20  Iteration 2143/3560 Training loss: 1.7726 0.4167 sec/batch\n",
      "Epoch 13/20  Iteration 2144/3560 Training loss: 1.7716 0.4278 sec/batch\n",
      "Epoch 13/20  Iteration 2145/3560 Training loss: 1.7748 0.4416 sec/batch\n",
      "Epoch 13/20  Iteration 2146/3560 Training loss: 1.7741 0.5132 sec/batch\n",
      "Epoch 13/20  Iteration 2147/3560 Training loss: 1.7704 0.3300 sec/batch\n",
      "Epoch 13/20  Iteration 2148/3560 Training loss: 1.7678 0.3990 sec/batch\n",
      "Epoch 13/20  Iteration 2149/3560 Training loss: 1.7677 0.4105 sec/batch\n",
      "Epoch 13/20  Iteration 2150/3560 Training loss: 1.7703 0.4907 sec/batch\n",
      "Epoch 13/20  Iteration 2151/3560 Training loss: 1.7700 0.4388 sec/batch\n",
      "Epoch 13/20  Iteration 2152/3560 Training loss: 1.7684 0.4217 sec/batch\n",
      "Epoch 13/20  Iteration 2153/3560 Training loss: 1.7685 0.4390 sec/batch\n",
      "Epoch 13/20  Iteration 2154/3560 Training loss: 1.7707 0.3847 sec/batch\n",
      "Epoch 13/20  Iteration 2155/3560 Training loss: 1.7708 0.4320 sec/batch\n",
      "Epoch 13/20  Iteration 2156/3560 Training loss: 1.7715 0.3784 sec/batch\n",
      "Epoch 13/20  Iteration 2157/3560 Training loss: 1.7710 0.4750 sec/batch\n",
      "Epoch 13/20  Iteration 2158/3560 Training loss: 1.7713 0.3659 sec/batch\n",
      "Epoch 13/20  Iteration 2159/3560 Training loss: 1.7708 0.3975 sec/batch\n",
      "Epoch 13/20  Iteration 2160/3560 Training loss: 1.7706 0.4053 sec/batch\n",
      "Epoch 13/20  Iteration 2161/3560 Training loss: 1.7702 0.3730 sec/batch\n",
      "Epoch 13/20  Iteration 2162/3560 Training loss: 1.7691 0.3988 sec/batch\n",
      "Epoch 13/20  Iteration 2163/3560 Training loss: 1.7681 0.4221 sec/batch\n",
      "Epoch 13/20  Iteration 2164/3560 Training loss: 1.7685 0.4673 sec/batch\n",
      "Epoch 13/20  Iteration 2165/3560 Training loss: 1.7696 0.4459 sec/batch\n",
      "Epoch 13/20  Iteration 2166/3560 Training loss: 1.7700 0.3797 sec/batch\n",
      "Epoch 13/20  Iteration 2167/3560 Training loss: 1.7694 0.3911 sec/batch\n",
      "Epoch 13/20  Iteration 2168/3560 Training loss: 1.7691 0.3579 sec/batch\n",
      "Epoch 13/20  Iteration 2169/3560 Training loss: 1.7693 0.4474 sec/batch\n",
      "Epoch 13/20  Iteration 2170/3560 Training loss: 1.7701 0.4218 sec/batch\n",
      "Epoch 13/20  Iteration 2171/3560 Training loss: 1.7695 0.5213 sec/batch\n",
      "Epoch 13/20  Iteration 2172/3560 Training loss: 1.7692 0.3881 sec/batch\n",
      "Epoch 13/20  Iteration 2173/3560 Training loss: 1.7688 0.3962 sec/batch\n",
      "Epoch 13/20  Iteration 2174/3560 Training loss: 1.7679 0.4473 sec/batch\n",
      "Epoch 13/20  Iteration 2175/3560 Training loss: 1.7669 0.3881 sec/batch\n",
      "Epoch 13/20  Iteration 2176/3560 Training loss: 1.7664 0.4082 sec/batch\n",
      "Epoch 13/20  Iteration 2177/3560 Training loss: 1.7660 0.4149 sec/batch\n",
      "Epoch 13/20  Iteration 2178/3560 Training loss: 1.7666 0.3984 sec/batch\n",
      "Epoch 13/20  Iteration 2179/3560 Training loss: 1.7662 0.3837 sec/batch\n",
      "Epoch 13/20  Iteration 2180/3560 Training loss: 1.7656 0.3758 sec/batch\n",
      "Epoch 13/20  Iteration 2181/3560 Training loss: 1.7658 0.5112 sec/batch\n",
      "Epoch 13/20  Iteration 2182/3560 Training loss: 1.7645 0.3440 sec/batch\n",
      "Epoch 13/20  Iteration 2183/3560 Training loss: 1.7640 0.3986 sec/batch\n",
      "Epoch 13/20  Iteration 2184/3560 Training loss: 1.7635 0.4610 sec/batch\n",
      "Epoch 13/20  Iteration 2185/3560 Training loss: 1.7633 0.4131 sec/batch\n",
      "Epoch 13/20  Iteration 2186/3560 Training loss: 1.7641 0.4239 sec/batch\n",
      "Epoch 13/20  Iteration 2187/3560 Training loss: 1.7636 0.3804 sec/batch\n",
      "Epoch 13/20  Iteration 2188/3560 Training loss: 1.7645 0.3897 sec/batch\n",
      "Epoch 13/20  Iteration 2189/3560 Training loss: 1.7643 0.4538 sec/batch\n",
      "Epoch 13/20  Iteration 2190/3560 Training loss: 1.7643 0.4287 sec/batch\n",
      "Epoch 13/20  Iteration 2191/3560 Training loss: 1.7641 0.4069 sec/batch\n",
      "Epoch 13/20  Iteration 2192/3560 Training loss: 1.7644 0.4019 sec/batch\n",
      "Epoch 13/20  Iteration 2193/3560 Training loss: 1.7647 0.4560 sec/batch\n",
      "Epoch 13/20  Iteration 2194/3560 Training loss: 1.7644 0.4192 sec/batch\n",
      "Epoch 13/20  Iteration 2195/3560 Training loss: 1.7640 0.3622 sec/batch\n",
      "Epoch 13/20  Iteration 2196/3560 Training loss: 1.7647 0.4212 sec/batch\n",
      "Epoch 13/20  Iteration 2197/3560 Training loss: 1.7647 0.4312 sec/batch\n",
      "Epoch 13/20  Iteration 2198/3560 Training loss: 1.7655 0.4051 sec/batch\n",
      "Epoch 13/20  Iteration 2199/3560 Training loss: 1.7657 0.4037 sec/batch\n",
      "Epoch 13/20  Iteration 2200/3560 Training loss: 1.7658 0.3875 sec/batch\n",
      "Epoch 13/20  Iteration 2201/3560 Training loss: 1.7657 0.4135 sec/batch\n",
      "Epoch 13/20  Iteration 2202/3560 Training loss: 1.7660 0.4168 sec/batch\n",
      "Epoch 13/20  Iteration 2203/3560 Training loss: 1.7661 0.4392 sec/batch\n",
      "Epoch 13/20  Iteration 2204/3560 Training loss: 1.7657 0.4371 sec/batch\n",
      "Epoch 13/20  Iteration 2205/3560 Training loss: 1.7657 0.3941 sec/batch\n",
      "Epoch 13/20  Iteration 2206/3560 Training loss: 1.7657 0.4480 sec/batch\n",
      "Epoch 13/20  Iteration 2207/3560 Training loss: 1.7663 0.3794 sec/batch\n",
      "Epoch 13/20  Iteration 2208/3560 Training loss: 1.7664 0.3792 sec/batch\n",
      "Epoch 13/20  Iteration 2209/3560 Training loss: 1.7668 0.4022 sec/batch\n",
      "Epoch 13/20  Iteration 2210/3560 Training loss: 1.7666 0.4634 sec/batch\n",
      "Epoch 13/20  Iteration 2211/3560 Training loss: 1.7664 0.4250 sec/batch\n",
      "Epoch 13/20  Iteration 2212/3560 Training loss: 1.7666 0.3613 sec/batch\n",
      "Epoch 13/20  Iteration 2213/3560 Training loss: 1.7664 0.4127 sec/batch\n",
      "Epoch 13/20  Iteration 2214/3560 Training loss: 1.7666 0.4476 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2215/3560 Training loss: 1.7663 0.3919 sec/batch\n",
      "Epoch 13/20  Iteration 2216/3560 Training loss: 1.7662 0.3970 sec/batch\n",
      "Epoch 13/20  Iteration 2217/3560 Training loss: 1.7657 0.4308 sec/batch\n",
      "Epoch 13/20  Iteration 2218/3560 Training loss: 1.7659 0.4078 sec/batch\n",
      "Epoch 13/20  Iteration 2219/3560 Training loss: 1.7655 0.4077 sec/batch\n",
      "Epoch 13/20  Iteration 2220/3560 Training loss: 1.7655 0.3947 sec/batch\n",
      "Epoch 13/20  Iteration 2221/3560 Training loss: 1.7650 0.4301 sec/batch\n",
      "Epoch 13/20  Iteration 2222/3560 Training loss: 1.7647 0.4129 sec/batch\n",
      "Epoch 13/20  Iteration 2223/3560 Training loss: 1.7644 0.5099 sec/batch\n",
      "Epoch 13/20  Iteration 2224/3560 Training loss: 1.7641 0.3936 sec/batch\n",
      "Epoch 13/20  Iteration 2225/3560 Training loss: 1.7637 0.4423 sec/batch\n",
      "Epoch 13/20  Iteration 2226/3560 Training loss: 1.7638 0.3561 sec/batch\n",
      "Epoch 13/20  Iteration 2227/3560 Training loss: 1.7635 0.4698 sec/batch\n",
      "Epoch 13/20  Iteration 2228/3560 Training loss: 1.7632 0.4482 sec/batch\n",
      "Epoch 13/20  Iteration 2229/3560 Training loss: 1.7628 0.4129 sec/batch\n",
      "Epoch 13/20  Iteration 2230/3560 Training loss: 1.7624 0.4319 sec/batch\n",
      "Epoch 13/20  Iteration 2231/3560 Training loss: 1.7622 0.4214 sec/batch\n",
      "Epoch 13/20  Iteration 2232/3560 Training loss: 1.7622 0.3973 sec/batch\n",
      "Epoch 13/20  Iteration 2233/3560 Training loss: 1.7620 0.4213 sec/batch\n",
      "Epoch 13/20  Iteration 2234/3560 Training loss: 1.7616 0.3966 sec/batch\n",
      "Epoch 13/20  Iteration 2235/3560 Training loss: 1.7613 0.4063 sec/batch\n",
      "Epoch 13/20  Iteration 2236/3560 Training loss: 1.7606 0.4611 sec/batch\n",
      "Epoch 13/20  Iteration 2237/3560 Training loss: 1.7606 0.5506 sec/batch\n",
      "Epoch 13/20  Iteration 2238/3560 Training loss: 1.7605 0.5454 sec/batch\n",
      "Epoch 13/20  Iteration 2239/3560 Training loss: 1.7603 0.4311 sec/batch\n",
      "Epoch 13/20  Iteration 2240/3560 Training loss: 1.7602 0.5270 sec/batch\n",
      "Epoch 13/20  Iteration 2241/3560 Training loss: 1.7600 0.5762 sec/batch\n",
      "Epoch 13/20  Iteration 2242/3560 Training loss: 1.7599 0.3966 sec/batch\n",
      "Epoch 13/20  Iteration 2243/3560 Training loss: 1.7599 0.5049 sec/batch\n",
      "Epoch 13/20  Iteration 2244/3560 Training loss: 1.7599 0.4349 sec/batch\n",
      "Epoch 13/20  Iteration 2245/3560 Training loss: 1.7599 0.3813 sec/batch\n",
      "Epoch 13/20  Iteration 2246/3560 Training loss: 1.7599 0.4201 sec/batch\n",
      "Epoch 13/20  Iteration 2247/3560 Training loss: 1.7597 0.4163 sec/batch\n",
      "Epoch 13/20  Iteration 2248/3560 Training loss: 1.7596 0.4017 sec/batch\n",
      "Epoch 13/20  Iteration 2249/3560 Training loss: 1.7595 0.4083 sec/batch\n",
      "Epoch 13/20  Iteration 2250/3560 Training loss: 1.7593 0.4368 sec/batch\n",
      "Epoch 13/20  Iteration 2251/3560 Training loss: 1.7591 0.4709 sec/batch\n",
      "Epoch 13/20  Iteration 2252/3560 Training loss: 1.7587 0.4538 sec/batch\n",
      "Epoch 13/20  Iteration 2253/3560 Training loss: 1.7587 0.4312 sec/batch\n",
      "Epoch 13/20  Iteration 2254/3560 Training loss: 1.7587 0.4060 sec/batch\n",
      "Epoch 13/20  Iteration 2255/3560 Training loss: 1.7585 0.3737 sec/batch\n",
      "Epoch 13/20  Iteration 2256/3560 Training loss: 1.7585 0.4112 sec/batch\n",
      "Epoch 13/20  Iteration 2257/3560 Training loss: 1.7585 0.3658 sec/batch\n",
      "Epoch 13/20  Iteration 2258/3560 Training loss: 1.7581 0.3900 sec/batch\n",
      "Epoch 13/20  Iteration 2259/3560 Training loss: 1.7578 0.4465 sec/batch\n",
      "Epoch 13/20  Iteration 2260/3560 Training loss: 1.7579 0.5019 sec/batch\n",
      "Epoch 13/20  Iteration 2261/3560 Training loss: 1.7579 0.4046 sec/batch\n",
      "Epoch 13/20  Iteration 2262/3560 Training loss: 1.7574 0.4312 sec/batch\n",
      "Epoch 13/20  Iteration 2263/3560 Training loss: 1.7575 0.4016 sec/batch\n",
      "Epoch 13/20  Iteration 2264/3560 Training loss: 1.7576 0.4009 sec/batch\n",
      "Epoch 13/20  Iteration 2265/3560 Training loss: 1.7576 0.4492 sec/batch\n",
      "Epoch 13/20  Iteration 2266/3560 Training loss: 1.7574 0.3809 sec/batch\n",
      "Epoch 13/20  Iteration 2267/3560 Training loss: 1.7570 0.4286 sec/batch\n",
      "Epoch 13/20  Iteration 2268/3560 Training loss: 1.7569 0.4536 sec/batch\n",
      "Epoch 13/20  Iteration 2269/3560 Training loss: 1.7570 0.4520 sec/batch\n",
      "Epoch 13/20  Iteration 2270/3560 Training loss: 1.7570 0.4197 sec/batch\n",
      "Epoch 13/20  Iteration 2271/3560 Training loss: 1.7570 0.4398 sec/batch\n",
      "Epoch 13/20  Iteration 2272/3560 Training loss: 1.7570 0.3862 sec/batch\n",
      "Epoch 13/20  Iteration 2273/3560 Training loss: 1.7571 0.3907 sec/batch\n",
      "Epoch 13/20  Iteration 2274/3560 Training loss: 1.7572 0.4595 sec/batch\n",
      "Epoch 13/20  Iteration 2275/3560 Training loss: 1.7573 0.3666 sec/batch\n",
      "Epoch 13/20  Iteration 2276/3560 Training loss: 1.7571 0.4574 sec/batch\n",
      "Epoch 13/20  Iteration 2277/3560 Training loss: 1.7574 0.3815 sec/batch\n",
      "Epoch 13/20  Iteration 2278/3560 Training loss: 1.7573 0.4075 sec/batch\n",
      "Epoch 13/20  Iteration 2279/3560 Training loss: 1.7572 0.4093 sec/batch\n",
      "Epoch 13/20  Iteration 2280/3560 Training loss: 1.7573 0.4584 sec/batch\n",
      "Epoch 13/20  Iteration 2281/3560 Training loss: 1.7571 0.4113 sec/batch\n",
      "Epoch 13/20  Iteration 2282/3560 Training loss: 1.7572 0.3949 sec/batch\n",
      "Epoch 13/20  Iteration 2283/3560 Training loss: 1.7573 0.4188 sec/batch\n",
      "Epoch 13/20  Iteration 2284/3560 Training loss: 1.7574 0.4278 sec/batch\n",
      "Epoch 13/20  Iteration 2285/3560 Training loss: 1.7575 0.3770 sec/batch\n",
      "Epoch 13/20  Iteration 2286/3560 Training loss: 1.7574 0.4212 sec/batch\n",
      "Epoch 13/20  Iteration 2287/3560 Training loss: 1.7572 0.4265 sec/batch\n",
      "Epoch 13/20  Iteration 2288/3560 Training loss: 1.7572 0.4092 sec/batch\n",
      "Epoch 13/20  Iteration 2289/3560 Training loss: 1.7573 0.4085 sec/batch\n",
      "Epoch 13/20  Iteration 2290/3560 Training loss: 1.7574 0.4366 sec/batch\n",
      "Epoch 13/20  Iteration 2291/3560 Training loss: 1.7574 0.4657 sec/batch\n",
      "Epoch 13/20  Iteration 2292/3560 Training loss: 1.7573 0.3439 sec/batch\n",
      "Epoch 13/20  Iteration 2293/3560 Training loss: 1.7574 0.4798 sec/batch\n",
      "Epoch 13/20  Iteration 2294/3560 Training loss: 1.7574 0.3799 sec/batch\n",
      "Epoch 13/20  Iteration 2295/3560 Training loss: 1.7571 0.3986 sec/batch\n",
      "Epoch 13/20  Iteration 2296/3560 Training loss: 1.7573 0.3719 sec/batch\n",
      "Epoch 13/20  Iteration 2297/3560 Training loss: 1.7574 0.3680 sec/batch\n",
      "Epoch 13/20  Iteration 2298/3560 Training loss: 1.7574 0.4255 sec/batch\n",
      "Epoch 13/20  Iteration 2299/3560 Training loss: 1.7575 0.4253 sec/batch\n",
      "Epoch 13/20  Iteration 2300/3560 Training loss: 1.7576 0.4363 sec/batch\n",
      "Epoch 13/20  Iteration 2301/3560 Training loss: 1.7575 0.4654 sec/batch\n",
      "Epoch 13/20  Iteration 2302/3560 Training loss: 1.7575 0.4385 sec/batch\n",
      "Epoch 13/20  Iteration 2303/3560 Training loss: 1.7576 0.3881 sec/batch\n",
      "Epoch 13/20  Iteration 2304/3560 Training loss: 1.7579 0.4046 sec/batch\n",
      "Epoch 13/20  Iteration 2305/3560 Training loss: 1.7579 0.4352 sec/batch\n",
      "Epoch 13/20  Iteration 2306/3560 Training loss: 1.7579 0.4364 sec/batch\n",
      "Epoch 13/20  Iteration 2307/3560 Training loss: 1.7578 0.3978 sec/batch\n",
      "Epoch 13/20  Iteration 2308/3560 Training loss: 1.7576 0.4269 sec/batch\n",
      "Epoch 13/20  Iteration 2309/3560 Training loss: 1.7578 0.4230 sec/batch\n",
      "Epoch 13/20  Iteration 2310/3560 Training loss: 1.7578 0.3891 sec/batch\n",
      "Epoch 13/20  Iteration 2311/3560 Training loss: 1.7578 0.3891 sec/batch\n",
      "Epoch 13/20  Iteration 2312/3560 Training loss: 1.7577 0.3803 sec/batch\n",
      "Epoch 13/20  Iteration 2313/3560 Training loss: 1.7576 0.4624 sec/batch\n",
      "Epoch 13/20  Iteration 2314/3560 Training loss: 1.7576 0.4044 sec/batch\n",
      "Epoch 14/20  Iteration 2315/3560 Training loss: 1.8291 0.4340 sec/batch\n",
      "Epoch 14/20  Iteration 2316/3560 Training loss: 1.7872 0.4356 sec/batch\n",
      "Epoch 14/20  Iteration 2317/3560 Training loss: 1.7719 0.4186 sec/batch\n",
      "Epoch 14/20  Iteration 2318/3560 Training loss: 1.7704 0.4131 sec/batch\n",
      "Epoch 14/20  Iteration 2319/3560 Training loss: 1.7638 0.3718 sec/batch\n",
      "Epoch 14/20  Iteration 2320/3560 Training loss: 1.7551 0.4447 sec/batch\n",
      "Epoch 14/20  Iteration 2321/3560 Training loss: 1.7557 0.4394 sec/batch\n",
      "Epoch 14/20  Iteration 2322/3560 Training loss: 1.7551 0.3787 sec/batch\n",
      "Epoch 14/20  Iteration 2323/3560 Training loss: 1.7574 0.3996 sec/batch\n",
      "Epoch 14/20  Iteration 2324/3560 Training loss: 1.7554 0.4458 sec/batch\n",
      "Epoch 14/20  Iteration 2325/3560 Training loss: 1.7526 0.4075 sec/batch\n",
      "Epoch 14/20  Iteration 2326/3560 Training loss: 1.7516 0.4268 sec/batch\n",
      "Epoch 14/20  Iteration 2327/3560 Training loss: 1.7514 0.4193 sec/batch\n",
      "Epoch 14/20  Iteration 2328/3560 Training loss: 1.7533 0.4424 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2329/3560 Training loss: 1.7526 0.3969 sec/batch\n",
      "Epoch 14/20  Iteration 2330/3560 Training loss: 1.7505 0.3726 sec/batch\n",
      "Epoch 14/20  Iteration 2331/3560 Training loss: 1.7502 0.4127 sec/batch\n",
      "Epoch 14/20  Iteration 2332/3560 Training loss: 1.7516 0.3951 sec/batch\n",
      "Epoch 14/20  Iteration 2333/3560 Training loss: 1.7521 0.3942 sec/batch\n",
      "Epoch 14/20  Iteration 2334/3560 Training loss: 1.7530 0.3911 sec/batch\n",
      "Epoch 14/20  Iteration 2335/3560 Training loss: 1.7522 0.4107 sec/batch\n",
      "Epoch 14/20  Iteration 2336/3560 Training loss: 1.7528 0.4123 sec/batch\n",
      "Epoch 14/20  Iteration 2337/3560 Training loss: 1.7519 0.4572 sec/batch\n",
      "Epoch 14/20  Iteration 2338/3560 Training loss: 1.7512 0.4392 sec/batch\n",
      "Epoch 14/20  Iteration 2339/3560 Training loss: 1.7507 0.4351 sec/batch\n",
      "Epoch 14/20  Iteration 2340/3560 Training loss: 1.7492 0.4293 sec/batch\n",
      "Epoch 14/20  Iteration 2341/3560 Training loss: 1.7482 0.4212 sec/batch\n",
      "Epoch 14/20  Iteration 2342/3560 Training loss: 1.7487 0.3948 sec/batch\n",
      "Epoch 14/20  Iteration 2343/3560 Training loss: 1.7496 0.4130 sec/batch\n",
      "Epoch 14/20  Iteration 2344/3560 Training loss: 1.7499 0.4053 sec/batch\n",
      "Epoch 14/20  Iteration 2345/3560 Training loss: 1.7496 0.4275 sec/batch\n",
      "Epoch 14/20  Iteration 2346/3560 Training loss: 1.7490 0.4398 sec/batch\n",
      "Epoch 14/20  Iteration 2347/3560 Training loss: 1.7490 0.4059 sec/batch\n",
      "Epoch 14/20  Iteration 2348/3560 Training loss: 1.7497 0.3967 sec/batch\n",
      "Epoch 14/20  Iteration 2349/3560 Training loss: 1.7493 0.4480 sec/batch\n",
      "Epoch 14/20  Iteration 2350/3560 Training loss: 1.7491 0.4188 sec/batch\n",
      "Epoch 14/20  Iteration 2351/3560 Training loss: 1.7488 0.4323 sec/batch\n",
      "Epoch 14/20  Iteration 2352/3560 Training loss: 1.7478 0.3761 sec/batch\n",
      "Epoch 14/20  Iteration 2353/3560 Training loss: 1.7464 0.3981 sec/batch\n",
      "Epoch 14/20  Iteration 2354/3560 Training loss: 1.7459 0.3913 sec/batch\n",
      "Epoch 14/20  Iteration 2355/3560 Training loss: 1.7455 0.4268 sec/batch\n",
      "Epoch 14/20  Iteration 2356/3560 Training loss: 1.7458 0.4391 sec/batch\n",
      "Epoch 14/20  Iteration 2357/3560 Training loss: 1.7455 0.4481 sec/batch\n",
      "Epoch 14/20  Iteration 2358/3560 Training loss: 1.7449 0.4576 sec/batch\n",
      "Epoch 14/20  Iteration 2359/3560 Training loss: 1.7452 0.4025 sec/batch\n",
      "Epoch 14/20  Iteration 2360/3560 Training loss: 1.7442 0.3542 sec/batch\n",
      "Epoch 14/20  Iteration 2361/3560 Training loss: 1.7442 0.4076 sec/batch\n",
      "Epoch 14/20  Iteration 2362/3560 Training loss: 1.7436 0.4231 sec/batch\n",
      "Epoch 14/20  Iteration 2363/3560 Training loss: 1.7434 0.4751 sec/batch\n",
      "Epoch 14/20  Iteration 2364/3560 Training loss: 1.7443 0.4058 sec/batch\n",
      "Epoch 14/20  Iteration 2365/3560 Training loss: 1.7438 0.4382 sec/batch\n",
      "Epoch 14/20  Iteration 2366/3560 Training loss: 1.7446 0.4916 sec/batch\n",
      "Epoch 14/20  Iteration 2367/3560 Training loss: 1.7445 0.4007 sec/batch\n",
      "Epoch 14/20  Iteration 2368/3560 Training loss: 1.7445 0.4380 sec/batch\n",
      "Epoch 14/20  Iteration 2369/3560 Training loss: 1.7440 0.4024 sec/batch\n",
      "Epoch 14/20  Iteration 2370/3560 Training loss: 1.7443 0.4338 sec/batch\n",
      "Epoch 14/20  Iteration 2371/3560 Training loss: 1.7446 0.3457 sec/batch\n",
      "Epoch 14/20  Iteration 2372/3560 Training loss: 1.7444 0.3612 sec/batch\n",
      "Epoch 14/20  Iteration 2373/3560 Training loss: 1.7439 0.4154 sec/batch\n",
      "Epoch 14/20  Iteration 2374/3560 Training loss: 1.7447 0.4418 sec/batch\n",
      "Epoch 14/20  Iteration 2375/3560 Training loss: 1.7444 0.4061 sec/batch\n",
      "Epoch 14/20  Iteration 2376/3560 Training loss: 1.7452 0.4295 sec/batch\n",
      "Epoch 14/20  Iteration 2377/3560 Training loss: 1.7455 0.4195 sec/batch\n",
      "Epoch 14/20  Iteration 2378/3560 Training loss: 1.7457 0.4256 sec/batch\n",
      "Epoch 14/20  Iteration 2379/3560 Training loss: 1.7456 0.3772 sec/batch\n",
      "Epoch 14/20  Iteration 2380/3560 Training loss: 1.7458 0.4214 sec/batch\n",
      "Epoch 14/20  Iteration 2381/3560 Training loss: 1.7460 0.3883 sec/batch\n",
      "Epoch 14/20  Iteration 2382/3560 Training loss: 1.7456 0.4871 sec/batch\n",
      "Epoch 14/20  Iteration 2383/3560 Training loss: 1.7456 0.3786 sec/batch\n",
      "Epoch 14/20  Iteration 2384/3560 Training loss: 1.7454 0.4108 sec/batch\n",
      "Epoch 14/20  Iteration 2385/3560 Training loss: 1.7459 0.4243 sec/batch\n",
      "Epoch 14/20  Iteration 2386/3560 Training loss: 1.7461 0.4095 sec/batch\n",
      "Epoch 14/20  Iteration 2387/3560 Training loss: 1.7466 0.4122 sec/batch\n",
      "Epoch 14/20  Iteration 2388/3560 Training loss: 1.7464 0.4538 sec/batch\n",
      "Epoch 14/20  Iteration 2389/3560 Training loss: 1.7463 0.4101 sec/batch\n",
      "Epoch 14/20  Iteration 2390/3560 Training loss: 1.7465 0.4267 sec/batch\n",
      "Epoch 14/20  Iteration 2391/3560 Training loss: 1.7463 0.3577 sec/batch\n",
      "Epoch 14/20  Iteration 2392/3560 Training loss: 1.7465 0.4057 sec/batch\n",
      "Epoch 14/20  Iteration 2393/3560 Training loss: 1.7459 0.3786 sec/batch\n",
      "Epoch 14/20  Iteration 2394/3560 Training loss: 1.7458 0.4411 sec/batch\n",
      "Epoch 14/20  Iteration 2395/3560 Training loss: 1.7453 0.4336 sec/batch\n",
      "Epoch 14/20  Iteration 2396/3560 Training loss: 1.7456 0.4273 sec/batch\n",
      "Epoch 14/20  Iteration 2397/3560 Training loss: 1.7453 0.4186 sec/batch\n",
      "Epoch 14/20  Iteration 2398/3560 Training loss: 1.7452 0.3907 sec/batch\n",
      "Epoch 14/20  Iteration 2399/3560 Training loss: 1.7448 0.3852 sec/batch\n",
      "Epoch 14/20  Iteration 2400/3560 Training loss: 1.7445 0.4147 sec/batch\n",
      "Epoch 14/20  Iteration 2401/3560 Training loss: 1.7442 0.3814 sec/batch\n",
      "Epoch 14/20  Iteration 2402/3560 Training loss: 1.7440 0.4199 sec/batch\n",
      "Epoch 14/20  Iteration 2403/3560 Training loss: 1.7436 0.4524 sec/batch\n",
      "Epoch 14/20  Iteration 2404/3560 Training loss: 1.7438 0.4277 sec/batch\n",
      "Epoch 14/20  Iteration 2405/3560 Training loss: 1.7435 0.3976 sec/batch\n",
      "Epoch 14/20  Iteration 2406/3560 Training loss: 1.7434 0.4315 sec/batch\n",
      "Epoch 14/20  Iteration 2407/3560 Training loss: 1.7430 0.3926 sec/batch\n",
      "Epoch 14/20  Iteration 2408/3560 Training loss: 1.7426 0.4029 sec/batch\n",
      "Epoch 14/20  Iteration 2409/3560 Training loss: 1.7423 0.4424 sec/batch\n",
      "Epoch 14/20  Iteration 2410/3560 Training loss: 1.7423 0.3373 sec/batch\n",
      "Epoch 14/20  Iteration 2411/3560 Training loss: 1.7422 0.4302 sec/batch\n",
      "Epoch 14/20  Iteration 2412/3560 Training loss: 1.7418 0.4042 sec/batch\n",
      "Epoch 14/20  Iteration 2413/3560 Training loss: 1.7414 0.4375 sec/batch\n",
      "Epoch 14/20  Iteration 2414/3560 Training loss: 1.7410 0.4430 sec/batch\n",
      "Epoch 14/20  Iteration 2415/3560 Training loss: 1.7410 0.4040 sec/batch\n",
      "Epoch 14/20  Iteration 2416/3560 Training loss: 1.7408 0.4329 sec/batch\n",
      "Epoch 14/20  Iteration 2417/3560 Training loss: 1.7407 0.3770 sec/batch\n",
      "Epoch 14/20  Iteration 2418/3560 Training loss: 1.7405 0.3875 sec/batch\n",
      "Epoch 14/20  Iteration 2419/3560 Training loss: 1.7402 0.3902 sec/batch\n",
      "Epoch 14/20  Iteration 2420/3560 Training loss: 1.7401 0.4143 sec/batch\n",
      "Epoch 14/20  Iteration 2421/3560 Training loss: 1.7401 0.3794 sec/batch\n",
      "Epoch 14/20  Iteration 2422/3560 Training loss: 1.7400 0.3879 sec/batch\n",
      "Epoch 14/20  Iteration 2423/3560 Training loss: 1.7402 0.4502 sec/batch\n",
      "Epoch 14/20  Iteration 2424/3560 Training loss: 1.7402 0.4419 sec/batch\n",
      "Epoch 14/20  Iteration 2425/3560 Training loss: 1.7400 0.4690 sec/batch\n",
      "Epoch 14/20  Iteration 2426/3560 Training loss: 1.7398 0.3702 sec/batch\n",
      "Epoch 14/20  Iteration 2427/3560 Training loss: 1.7397 0.3920 sec/batch\n",
      "Epoch 14/20  Iteration 2428/3560 Training loss: 1.7396 0.4068 sec/batch\n",
      "Epoch 14/20  Iteration 2429/3560 Training loss: 1.7394 0.4128 sec/batch\n",
      "Epoch 14/20  Iteration 2430/3560 Training loss: 1.7392 0.3770 sec/batch\n",
      "Epoch 14/20  Iteration 2431/3560 Training loss: 1.7391 0.3918 sec/batch\n",
      "Epoch 14/20  Iteration 2432/3560 Training loss: 1.7392 0.4397 sec/batch\n",
      "Epoch 14/20  Iteration 2433/3560 Training loss: 1.7392 0.4344 sec/batch\n",
      "Epoch 14/20  Iteration 2434/3560 Training loss: 1.7392 0.4406 sec/batch\n",
      "Epoch 14/20  Iteration 2435/3560 Training loss: 1.7391 0.4528 sec/batch\n",
      "Epoch 14/20  Iteration 2436/3560 Training loss: 1.7388 0.4216 sec/batch\n",
      "Epoch 14/20  Iteration 2437/3560 Training loss: 1.7386 0.4189 sec/batch\n",
      "Epoch 14/20  Iteration 2438/3560 Training loss: 1.7387 0.3960 sec/batch\n",
      "Epoch 14/20  Iteration 2439/3560 Training loss: 1.7385 0.4259 sec/batch\n",
      "Epoch 14/20  Iteration 2440/3560 Training loss: 1.7381 0.3841 sec/batch\n",
      "Epoch 14/20  Iteration 2441/3560 Training loss: 1.7382 0.4448 sec/batch\n",
      "Epoch 14/20  Iteration 2442/3560 Training loss: 1.7383 0.4223 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2443/3560 Training loss: 1.7382 0.3939 sec/batch\n",
      "Epoch 14/20  Iteration 2444/3560 Training loss: 1.7380 0.4146 sec/batch\n",
      "Epoch 14/20  Iteration 2445/3560 Training loss: 1.7378 0.4110 sec/batch\n",
      "Epoch 14/20  Iteration 2446/3560 Training loss: 1.7375 0.3844 sec/batch\n",
      "Epoch 14/20  Iteration 2447/3560 Training loss: 1.7376 0.3715 sec/batch\n",
      "Epoch 14/20  Iteration 2448/3560 Training loss: 1.7377 0.3856 sec/batch\n",
      "Epoch 14/20  Iteration 2449/3560 Training loss: 1.7377 0.4011 sec/batch\n",
      "Epoch 14/20  Iteration 2450/3560 Training loss: 1.7377 0.4086 sec/batch\n",
      "Epoch 14/20  Iteration 2451/3560 Training loss: 1.7378 0.4031 sec/batch\n",
      "Epoch 14/20  Iteration 2452/3560 Training loss: 1.7378 0.4097 sec/batch\n",
      "Epoch 14/20  Iteration 2453/3560 Training loss: 1.7380 0.4487 sec/batch\n",
      "Epoch 14/20  Iteration 2454/3560 Training loss: 1.7378 0.4335 sec/batch\n",
      "Epoch 14/20  Iteration 2455/3560 Training loss: 1.7381 0.4223 sec/batch\n",
      "Epoch 14/20  Iteration 2456/3560 Training loss: 1.7381 0.4164 sec/batch\n",
      "Epoch 14/20  Iteration 2457/3560 Training loss: 1.7380 0.3714 sec/batch\n",
      "Epoch 14/20  Iteration 2458/3560 Training loss: 1.7381 0.3755 sec/batch\n",
      "Epoch 14/20  Iteration 2459/3560 Training loss: 1.7379 0.4126 sec/batch\n",
      "Epoch 14/20  Iteration 2460/3560 Training loss: 1.7379 0.4361 sec/batch\n",
      "Epoch 14/20  Iteration 2461/3560 Training loss: 1.7380 0.4183 sec/batch\n",
      "Epoch 14/20  Iteration 2462/3560 Training loss: 1.7382 0.4120 sec/batch\n",
      "Epoch 14/20  Iteration 2463/3560 Training loss: 1.7382 0.4570 sec/batch\n",
      "Epoch 14/20  Iteration 2464/3560 Training loss: 1.7381 0.4652 sec/batch\n",
      "Epoch 14/20  Iteration 2465/3560 Training loss: 1.7379 0.4402 sec/batch\n",
      "Epoch 14/20  Iteration 2466/3560 Training loss: 1.7379 0.3246 sec/batch\n",
      "Epoch 14/20  Iteration 2467/3560 Training loss: 1.7379 0.4009 sec/batch\n",
      "Epoch 14/20  Iteration 2468/3560 Training loss: 1.7380 0.4014 sec/batch\n",
      "Epoch 14/20  Iteration 2469/3560 Training loss: 1.7380 0.3921 sec/batch\n",
      "Epoch 14/20  Iteration 2470/3560 Training loss: 1.7380 0.4167 sec/batch\n",
      "Epoch 14/20  Iteration 2471/3560 Training loss: 1.7380 0.4450 sec/batch\n",
      "Epoch 14/20  Iteration 2472/3560 Training loss: 1.7380 0.3777 sec/batch\n",
      "Epoch 14/20  Iteration 2473/3560 Training loss: 1.7377 0.4360 sec/batch\n",
      "Epoch 14/20  Iteration 2474/3560 Training loss: 1.7379 0.4088 sec/batch\n",
      "Epoch 14/20  Iteration 2475/3560 Training loss: 1.7381 0.4303 sec/batch\n",
      "Epoch 14/20  Iteration 2476/3560 Training loss: 1.7381 0.4153 sec/batch\n",
      "Epoch 14/20  Iteration 2477/3560 Training loss: 1.7382 0.3871 sec/batch\n",
      "Epoch 14/20  Iteration 2478/3560 Training loss: 1.7382 0.4328 sec/batch\n",
      "Epoch 14/20  Iteration 2479/3560 Training loss: 1.7383 0.4014 sec/batch\n",
      "Epoch 14/20  Iteration 2480/3560 Training loss: 1.7383 0.4408 sec/batch\n",
      "Epoch 14/20  Iteration 2481/3560 Training loss: 1.7383 0.3794 sec/batch\n",
      "Epoch 14/20  Iteration 2482/3560 Training loss: 1.7387 0.4249 sec/batch\n",
      "Epoch 14/20  Iteration 2483/3560 Training loss: 1.7387 0.4401 sec/batch\n",
      "Epoch 14/20  Iteration 2484/3560 Training loss: 1.7388 0.4387 sec/batch\n",
      "Epoch 14/20  Iteration 2485/3560 Training loss: 1.7387 0.4434 sec/batch\n",
      "Epoch 14/20  Iteration 2486/3560 Training loss: 1.7386 0.4001 sec/batch\n",
      "Epoch 14/20  Iteration 2487/3560 Training loss: 1.7387 0.4625 sec/batch\n",
      "Epoch 14/20  Iteration 2488/3560 Training loss: 1.7387 0.3656 sec/batch\n",
      "Epoch 14/20  Iteration 2489/3560 Training loss: 1.7388 0.4323 sec/batch\n",
      "Epoch 14/20  Iteration 2490/3560 Training loss: 1.7387 0.4163 sec/batch\n",
      "Epoch 14/20  Iteration 2491/3560 Training loss: 1.7386 0.4385 sec/batch\n",
      "Epoch 14/20  Iteration 2492/3560 Training loss: 1.7387 0.4638 sec/batch\n",
      "Epoch 15/20  Iteration 2493/3560 Training loss: 1.8065 0.4261 sec/batch\n",
      "Epoch 15/20  Iteration 2494/3560 Training loss: 1.7723 0.3843 sec/batch\n",
      "Epoch 15/20  Iteration 2495/3560 Training loss: 1.7570 0.3720 sec/batch\n",
      "Epoch 15/20  Iteration 2496/3560 Training loss: 1.7527 0.4013 sec/batch\n",
      "Epoch 15/20  Iteration 2497/3560 Training loss: 1.7469 0.3887 sec/batch\n",
      "Epoch 15/20  Iteration 2498/3560 Training loss: 1.7375 0.4196 sec/batch\n",
      "Epoch 15/20  Iteration 2499/3560 Training loss: 1.7373 0.4118 sec/batch\n",
      "Epoch 15/20  Iteration 2500/3560 Training loss: 1.7360 0.4393 sec/batch\n",
      "Epoch 15/20  Iteration 2501/3560 Training loss: 1.7394 0.4660 sec/batch\n",
      "Epoch 15/20  Iteration 2502/3560 Training loss: 1.7385 0.4066 sec/batch\n",
      "Epoch 15/20  Iteration 2503/3560 Training loss: 1.7344 0.4080 sec/batch\n",
      "Epoch 15/20  Iteration 2504/3560 Training loss: 1.7333 0.3787 sec/batch\n",
      "Epoch 15/20  Iteration 2505/3560 Training loss: 1.7331 0.3877 sec/batch\n",
      "Epoch 15/20  Iteration 2506/3560 Training loss: 1.7358 0.4091 sec/batch\n",
      "Epoch 15/20  Iteration 2507/3560 Training loss: 1.7352 0.3999 sec/batch\n",
      "Epoch 15/20  Iteration 2508/3560 Training loss: 1.7326 0.4205 sec/batch\n",
      "Epoch 15/20  Iteration 2509/3560 Training loss: 1.7324 0.4271 sec/batch\n",
      "Epoch 15/20  Iteration 2510/3560 Training loss: 1.7346 0.5226 sec/batch\n",
      "Epoch 15/20  Iteration 2511/3560 Training loss: 1.7350 0.4017 sec/batch\n",
      "Epoch 15/20  Iteration 2512/3560 Training loss: 1.7355 0.4157 sec/batch\n",
      "Epoch 15/20  Iteration 2513/3560 Training loss: 1.7350 0.4302 sec/batch\n",
      "Epoch 15/20  Iteration 2514/3560 Training loss: 1.7352 0.4186 sec/batch\n",
      "Epoch 15/20  Iteration 2515/3560 Training loss: 1.7345 0.3684 sec/batch\n",
      "Epoch 15/20  Iteration 2516/3560 Training loss: 1.7341 0.4067 sec/batch\n",
      "Epoch 15/20  Iteration 2517/3560 Training loss: 1.7337 0.3815 sec/batch\n",
      "Epoch 15/20  Iteration 2518/3560 Training loss: 1.7326 0.3941 sec/batch\n",
      "Epoch 15/20  Iteration 2519/3560 Training loss: 1.7316 0.4276 sec/batch\n",
      "Epoch 15/20  Iteration 2520/3560 Training loss: 1.7324 0.5251 sec/batch\n",
      "Epoch 15/20  Iteration 2521/3560 Training loss: 1.7335 0.4363 sec/batch\n",
      "Epoch 15/20  Iteration 2522/3560 Training loss: 1.7340 0.4238 sec/batch\n",
      "Epoch 15/20  Iteration 2523/3560 Training loss: 1.7334 0.4431 sec/batch\n",
      "Epoch 15/20  Iteration 2524/3560 Training loss: 1.7330 0.4791 sec/batch\n",
      "Epoch 15/20  Iteration 2525/3560 Training loss: 1.7332 0.3581 sec/batch\n",
      "Epoch 15/20  Iteration 2526/3560 Training loss: 1.7340 0.3952 sec/batch\n",
      "Epoch 15/20  Iteration 2527/3560 Training loss: 1.7339 0.4207 sec/batch\n",
      "Epoch 15/20  Iteration 2528/3560 Training loss: 1.7334 0.4755 sec/batch\n",
      "Epoch 15/20  Iteration 2529/3560 Training loss: 1.7327 0.4818 sec/batch\n",
      "Epoch 15/20  Iteration 2530/3560 Training loss: 1.7320 0.3766 sec/batch\n",
      "Epoch 15/20  Iteration 2531/3560 Training loss: 1.7306 0.4740 sec/batch\n",
      "Epoch 15/20  Iteration 2532/3560 Training loss: 1.7302 0.3636 sec/batch\n",
      "Epoch 15/20  Iteration 2533/3560 Training loss: 1.7297 0.3990 sec/batch\n",
      "Epoch 15/20  Iteration 2534/3560 Training loss: 1.7303 0.3936 sec/batch\n",
      "Epoch 15/20  Iteration 2535/3560 Training loss: 1.7300 0.4559 sec/batch\n",
      "Epoch 15/20  Iteration 2536/3560 Training loss: 1.7294 0.3903 sec/batch\n",
      "Epoch 15/20  Iteration 2537/3560 Training loss: 1.7296 0.4123 sec/batch\n",
      "Epoch 15/20  Iteration 2538/3560 Training loss: 1.7285 0.4401 sec/batch\n",
      "Epoch 15/20  Iteration 2539/3560 Training loss: 1.7282 0.3916 sec/batch\n",
      "Epoch 15/20  Iteration 2540/3560 Training loss: 1.7275 0.4733 sec/batch\n",
      "Epoch 15/20  Iteration 2541/3560 Training loss: 1.7276 0.3804 sec/batch\n",
      "Epoch 15/20  Iteration 2542/3560 Training loss: 1.7283 0.4085 sec/batch\n",
      "Epoch 15/20  Iteration 2543/3560 Training loss: 1.7277 0.3974 sec/batch\n",
      "Epoch 15/20  Iteration 2544/3560 Training loss: 1.7285 0.4164 sec/batch\n",
      "Epoch 15/20  Iteration 2545/3560 Training loss: 1.7285 0.4039 sec/batch\n",
      "Epoch 15/20  Iteration 2546/3560 Training loss: 1.7285 0.3733 sec/batch\n",
      "Epoch 15/20  Iteration 2547/3560 Training loss: 1.7279 0.3925 sec/batch\n",
      "Epoch 15/20  Iteration 2548/3560 Training loss: 1.7281 0.4619 sec/batch\n",
      "Epoch 15/20  Iteration 2549/3560 Training loss: 1.7283 0.4420 sec/batch\n",
      "Epoch 15/20  Iteration 2550/3560 Training loss: 1.7280 0.4167 sec/batch\n",
      "Epoch 15/20  Iteration 2551/3560 Training loss: 1.7274 0.4670 sec/batch\n",
      "Epoch 15/20  Iteration 2552/3560 Training loss: 1.7281 0.3777 sec/batch\n",
      "Epoch 15/20  Iteration 2553/3560 Training loss: 1.7280 0.3851 sec/batch\n",
      "Epoch 15/20  Iteration 2554/3560 Training loss: 1.7290 0.4000 sec/batch\n",
      "Epoch 15/20  Iteration 2555/3560 Training loss: 1.7295 0.4178 sec/batch\n",
      "Epoch 15/20  Iteration 2556/3560 Training loss: 1.7297 0.4085 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2557/3560 Training loss: 1.7296 0.4921 sec/batch\n",
      "Epoch 15/20  Iteration 2558/3560 Training loss: 1.7298 0.4010 sec/batch\n",
      "Epoch 15/20  Iteration 2559/3560 Training loss: 1.7299 0.4931 sec/batch\n",
      "Epoch 15/20  Iteration 2560/3560 Training loss: 1.7296 0.4009 sec/batch\n",
      "Epoch 15/20  Iteration 2561/3560 Training loss: 1.7295 0.4242 sec/batch\n",
      "Epoch 15/20  Iteration 2562/3560 Training loss: 1.7295 0.4413 sec/batch\n",
      "Epoch 15/20  Iteration 2563/3560 Training loss: 1.7301 0.4480 sec/batch\n",
      "Epoch 15/20  Iteration 2564/3560 Training loss: 1.7303 0.4060 sec/batch\n",
      "Epoch 15/20  Iteration 2565/3560 Training loss: 1.7307 0.4231 sec/batch\n",
      "Epoch 15/20  Iteration 2566/3560 Training loss: 1.7305 0.4520 sec/batch\n",
      "Epoch 15/20  Iteration 2567/3560 Training loss: 1.7303 0.5078 sec/batch\n",
      "Epoch 15/20  Iteration 2568/3560 Training loss: 1.7306 0.4485 sec/batch\n",
      "Epoch 15/20  Iteration 2569/3560 Training loss: 1.7304 0.4253 sec/batch\n",
      "Epoch 15/20  Iteration 2570/3560 Training loss: 1.7305 0.5523 sec/batch\n",
      "Epoch 15/20  Iteration 2571/3560 Training loss: 1.7300 0.5342 sec/batch\n",
      "Epoch 15/20  Iteration 2572/3560 Training loss: 1.7299 0.3554 sec/batch\n",
      "Epoch 15/20  Iteration 2573/3560 Training loss: 1.7294 0.4278 sec/batch\n",
      "Epoch 15/20  Iteration 2574/3560 Training loss: 1.7296 0.4075 sec/batch\n",
      "Epoch 15/20  Iteration 2575/3560 Training loss: 1.7290 0.4765 sec/batch\n",
      "Epoch 15/20  Iteration 2576/3560 Training loss: 1.7289 0.3774 sec/batch\n",
      "Epoch 15/20  Iteration 2577/3560 Training loss: 1.7284 0.4339 sec/batch\n",
      "Epoch 15/20  Iteration 2578/3560 Training loss: 1.7282 0.5521 sec/batch\n",
      "Epoch 15/20  Iteration 2579/3560 Training loss: 1.7280 0.5245 sec/batch\n",
      "Epoch 15/20  Iteration 2580/3560 Training loss: 1.7276 0.4294 sec/batch\n",
      "Epoch 15/20  Iteration 2581/3560 Training loss: 1.7272 0.5359 sec/batch\n",
      "Epoch 15/20  Iteration 2582/3560 Training loss: 1.7273 0.5469 sec/batch\n",
      "Epoch 15/20  Iteration 2583/3560 Training loss: 1.7270 0.4151 sec/batch\n",
      "Epoch 15/20  Iteration 2584/3560 Training loss: 1.7267 0.3776 sec/batch\n",
      "Epoch 15/20  Iteration 2585/3560 Training loss: 1.7263 0.5444 sec/batch\n",
      "Epoch 15/20  Iteration 2586/3560 Training loss: 1.7260 0.4493 sec/batch\n",
      "Epoch 15/20  Iteration 2587/3560 Training loss: 1.7256 0.4693 sec/batch\n",
      "Epoch 15/20  Iteration 2588/3560 Training loss: 1.7255 0.4872 sec/batch\n",
      "Epoch 15/20  Iteration 2589/3560 Training loss: 1.7254 0.5311 sec/batch\n",
      "Epoch 15/20  Iteration 2590/3560 Training loss: 1.7249 0.5384 sec/batch\n",
      "Epoch 15/20  Iteration 2591/3560 Training loss: 1.7245 0.3408 sec/batch\n",
      "Epoch 15/20  Iteration 2592/3560 Training loss: 1.7240 0.4359 sec/batch\n",
      "Epoch 15/20  Iteration 2593/3560 Training loss: 1.7241 0.4369 sec/batch\n",
      "Epoch 15/20  Iteration 2594/3560 Training loss: 1.7240 0.4569 sec/batch\n",
      "Epoch 15/20  Iteration 2595/3560 Training loss: 1.7238 0.4416 sec/batch\n",
      "Epoch 15/20  Iteration 2596/3560 Training loss: 1.7237 0.4924 sec/batch\n",
      "Epoch 15/20  Iteration 2597/3560 Training loss: 1.7235 0.4809 sec/batch\n",
      "Epoch 15/20  Iteration 2598/3560 Training loss: 1.7233 0.5122 sec/batch\n",
      "Epoch 15/20  Iteration 2599/3560 Training loss: 1.7233 0.4959 sec/batch\n",
      "Epoch 15/20  Iteration 2600/3560 Training loss: 1.7232 0.4962 sec/batch\n",
      "Epoch 15/20  Iteration 2601/3560 Training loss: 1.7232 0.3954 sec/batch\n",
      "Epoch 15/20  Iteration 2602/3560 Training loss: 1.7233 0.4433 sec/batch\n",
      "Epoch 15/20  Iteration 2603/3560 Training loss: 1.7232 0.4993 sec/batch\n",
      "Epoch 15/20  Iteration 2604/3560 Training loss: 1.7231 0.4329 sec/batch\n",
      "Epoch 15/20  Iteration 2605/3560 Training loss: 1.7230 0.3552 sec/batch\n",
      "Epoch 15/20  Iteration 2606/3560 Training loss: 1.7229 0.4629 sec/batch\n",
      "Epoch 15/20  Iteration 2607/3560 Training loss: 1.7226 0.4672 sec/batch\n",
      "Epoch 15/20  Iteration 2608/3560 Training loss: 1.7224 0.4098 sec/batch\n",
      "Epoch 15/20  Iteration 2609/3560 Training loss: 1.7223 0.4791 sec/batch\n",
      "Epoch 15/20  Iteration 2610/3560 Training loss: 1.7224 0.4352 sec/batch\n",
      "Epoch 15/20  Iteration 2611/3560 Training loss: 1.7224 0.5286 sec/batch\n",
      "Epoch 15/20  Iteration 2612/3560 Training loss: 1.7224 0.5263 sec/batch\n",
      "Epoch 15/20  Iteration 2613/3560 Training loss: 1.7223 0.3334 sec/batch\n",
      "Epoch 15/20  Iteration 2614/3560 Training loss: 1.7221 0.4945 sec/batch\n",
      "Epoch 15/20  Iteration 2615/3560 Training loss: 1.7218 0.5054 sec/batch\n",
      "Epoch 15/20  Iteration 2616/3560 Training loss: 1.7217 0.5122 sec/batch\n",
      "Epoch 15/20  Iteration 2617/3560 Training loss: 1.7217 0.4452 sec/batch\n",
      "Epoch 15/20  Iteration 2618/3560 Training loss: 1.7212 0.5429 sec/batch\n",
      "Epoch 15/20  Iteration 2619/3560 Training loss: 1.7213 0.4207 sec/batch\n",
      "Epoch 15/20  Iteration 2620/3560 Training loss: 1.7214 0.4404 sec/batch\n",
      "Epoch 15/20  Iteration 2621/3560 Training loss: 1.7213 0.3841 sec/batch\n",
      "Epoch 15/20  Iteration 2622/3560 Training loss: 1.7211 0.4985 sec/batch\n",
      "Epoch 15/20  Iteration 2623/3560 Training loss: 1.7208 0.4710 sec/batch\n",
      "Epoch 15/20  Iteration 2624/3560 Training loss: 1.7205 0.5020 sec/batch\n",
      "Epoch 15/20  Iteration 2625/3560 Training loss: 1.7206 0.4166 sec/batch\n",
      "Epoch 15/20  Iteration 2626/3560 Training loss: 1.7206 0.4608 sec/batch\n",
      "Epoch 15/20  Iteration 2627/3560 Training loss: 1.7206 0.5547 sec/batch\n",
      "Epoch 15/20  Iteration 2628/3560 Training loss: 1.7207 0.7429 sec/batch\n",
      "Epoch 15/20  Iteration 2629/3560 Training loss: 1.7208 0.7500 sec/batch\n",
      "Epoch 15/20  Iteration 2630/3560 Training loss: 1.7209 0.5243 sec/batch\n",
      "Epoch 15/20  Iteration 2631/3560 Training loss: 1.7210 0.4611 sec/batch\n",
      "Epoch 15/20  Iteration 2632/3560 Training loss: 1.7208 0.5007 sec/batch\n",
      "Epoch 15/20  Iteration 2633/3560 Training loss: 1.7212 0.4408 sec/batch\n",
      "Epoch 15/20  Iteration 2634/3560 Training loss: 1.7211 0.5002 sec/batch\n",
      "Epoch 15/20  Iteration 2635/3560 Training loss: 1.7210 0.5217 sec/batch\n",
      "Epoch 15/20  Iteration 2636/3560 Training loss: 1.7211 0.4980 sec/batch\n",
      "Epoch 15/20  Iteration 2637/3560 Training loss: 1.7209 0.5131 sec/batch\n",
      "Epoch 15/20  Iteration 2638/3560 Training loss: 1.7210 0.4770 sec/batch\n",
      "Epoch 15/20  Iteration 2639/3560 Training loss: 1.7211 0.4567 sec/batch\n",
      "Epoch 15/20  Iteration 2640/3560 Training loss: 1.7213 0.4673 sec/batch\n",
      "Epoch 15/20  Iteration 2641/3560 Training loss: 1.7213 0.6000 sec/batch\n",
      "Epoch 15/20  Iteration 2642/3560 Training loss: 1.7211 0.3971 sec/batch\n",
      "Epoch 15/20  Iteration 2643/3560 Training loss: 1.7209 0.5210 sec/batch\n",
      "Epoch 15/20  Iteration 2644/3560 Training loss: 1.7210 0.3777 sec/batch\n",
      "Epoch 15/20  Iteration 2645/3560 Training loss: 1.7210 0.5955 sec/batch\n",
      "Epoch 15/20  Iteration 2646/3560 Training loss: 1.7211 0.5492 sec/batch\n",
      "Epoch 15/20  Iteration 2647/3560 Training loss: 1.7211 0.5523 sec/batch\n",
      "Epoch 15/20  Iteration 2648/3560 Training loss: 1.7211 0.5573 sec/batch\n",
      "Epoch 15/20  Iteration 2649/3560 Training loss: 1.7211 0.5696 sec/batch\n",
      "Epoch 15/20  Iteration 2650/3560 Training loss: 1.7212 0.4317 sec/batch\n",
      "Epoch 15/20  Iteration 2651/3560 Training loss: 1.7209 0.4832 sec/batch\n",
      "Epoch 15/20  Iteration 2652/3560 Training loss: 1.7210 0.4550 sec/batch\n",
      "Epoch 15/20  Iteration 2653/3560 Training loss: 1.7212 0.4487 sec/batch\n",
      "Epoch 15/20  Iteration 2654/3560 Training loss: 1.7212 0.5607 sec/batch\n",
      "Epoch 15/20  Iteration 2655/3560 Training loss: 1.7213 0.6921 sec/batch\n",
      "Epoch 15/20  Iteration 2656/3560 Training loss: 1.7213 0.4832 sec/batch\n",
      "Epoch 15/20  Iteration 2657/3560 Training loss: 1.7213 0.5297 sec/batch\n",
      "Epoch 15/20  Iteration 2658/3560 Training loss: 1.7213 0.4568 sec/batch\n",
      "Epoch 15/20  Iteration 2659/3560 Training loss: 1.7214 0.4318 sec/batch\n",
      "Epoch 15/20  Iteration 2660/3560 Training loss: 1.7218 0.4592 sec/batch\n",
      "Epoch 15/20  Iteration 2661/3560 Training loss: 1.7217 0.4350 sec/batch\n",
      "Epoch 15/20  Iteration 2662/3560 Training loss: 1.7217 0.4486 sec/batch\n",
      "Epoch 15/20  Iteration 2663/3560 Training loss: 1.7215 0.4700 sec/batch\n",
      "Epoch 15/20  Iteration 2664/3560 Training loss: 1.7214 0.5734 sec/batch\n",
      "Epoch 15/20  Iteration 2665/3560 Training loss: 1.7216 0.5469 sec/batch\n",
      "Epoch 15/20  Iteration 2666/3560 Training loss: 1.7216 0.5489 sec/batch\n",
      "Epoch 15/20  Iteration 2667/3560 Training loss: 1.7217 0.5244 sec/batch\n",
      "Epoch 15/20  Iteration 2668/3560 Training loss: 1.7216 0.5023 sec/batch\n",
      "Epoch 15/20  Iteration 2669/3560 Training loss: 1.7215 0.4666 sec/batch\n",
      "Epoch 15/20  Iteration 2670/3560 Training loss: 1.7216 0.4340 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2671/3560 Training loss: 1.7977 0.4718 sec/batch\n",
      "Epoch 16/20  Iteration 2672/3560 Training loss: 1.7526 0.4575 sec/batch\n",
      "Epoch 16/20  Iteration 2673/3560 Training loss: 1.7397 0.4345 sec/batch\n",
      "Epoch 16/20  Iteration 2674/3560 Training loss: 1.7343 0.4619 sec/batch\n",
      "Epoch 16/20  Iteration 2675/3560 Training loss: 1.7294 0.4901 sec/batch\n",
      "Epoch 16/20  Iteration 2676/3560 Training loss: 1.7209 0.5323 sec/batch\n",
      "Epoch 16/20  Iteration 2677/3560 Training loss: 1.7215 0.4283 sec/batch\n",
      "Epoch 16/20  Iteration 2678/3560 Training loss: 1.7213 0.4085 sec/batch\n",
      "Epoch 16/20  Iteration 2679/3560 Training loss: 1.7234 0.4045 sec/batch\n",
      "Epoch 16/20  Iteration 2680/3560 Training loss: 1.7220 0.4683 sec/batch\n",
      "Epoch 16/20  Iteration 2681/3560 Training loss: 1.7191 0.3776 sec/batch\n",
      "Epoch 16/20  Iteration 2682/3560 Training loss: 1.7175 0.3934 sec/batch\n",
      "Epoch 16/20  Iteration 2683/3560 Training loss: 1.7171 0.4394 sec/batch\n",
      "Epoch 16/20  Iteration 2684/3560 Training loss: 1.7184 0.4572 sec/batch\n",
      "Epoch 16/20  Iteration 2685/3560 Training loss: 1.7179 0.4741 sec/batch\n",
      "Epoch 16/20  Iteration 2686/3560 Training loss: 1.7161 0.4901 sec/batch\n",
      "Epoch 16/20  Iteration 2687/3560 Training loss: 1.7162 0.4341 sec/batch\n",
      "Epoch 16/20  Iteration 2688/3560 Training loss: 1.7176 0.4472 sec/batch\n",
      "Epoch 16/20  Iteration 2689/3560 Training loss: 1.7182 0.3875 sec/batch\n",
      "Epoch 16/20  Iteration 2690/3560 Training loss: 1.7189 0.3992 sec/batch\n",
      "Epoch 16/20  Iteration 2691/3560 Training loss: 1.7178 0.4063 sec/batch\n",
      "Epoch 16/20  Iteration 2692/3560 Training loss: 1.7182 0.4240 sec/batch\n",
      "Epoch 16/20  Iteration 2693/3560 Training loss: 1.7175 0.4808 sec/batch\n",
      "Epoch 16/20  Iteration 2694/3560 Training loss: 1.7169 0.4348 sec/batch\n",
      "Epoch 16/20  Iteration 2695/3560 Training loss: 1.7167 0.4291 sec/batch\n",
      "Epoch 16/20  Iteration 2696/3560 Training loss: 1.7158 0.5175 sec/batch\n",
      "Epoch 16/20  Iteration 2697/3560 Training loss: 1.7147 0.4349 sec/batch\n",
      "Epoch 16/20  Iteration 2698/3560 Training loss: 1.7153 0.5223 sec/batch\n",
      "Epoch 16/20  Iteration 2699/3560 Training loss: 1.7163 0.6051 sec/batch\n",
      "Epoch 16/20  Iteration 2700/3560 Training loss: 1.7166 0.6356 sec/batch\n",
      "Epoch 16/20  Iteration 2701/3560 Training loss: 1.7161 0.4290 sec/batch\n",
      "Epoch 16/20  Iteration 2702/3560 Training loss: 1.7156 0.5736 sec/batch\n",
      "Epoch 16/20  Iteration 2703/3560 Training loss: 1.7157 0.4868 sec/batch\n",
      "Epoch 16/20  Iteration 2704/3560 Training loss: 1.7163 0.5427 sec/batch\n",
      "Epoch 16/20  Iteration 2705/3560 Training loss: 1.7160 0.4353 sec/batch\n",
      "Epoch 16/20  Iteration 2706/3560 Training loss: 1.7157 0.5731 sec/batch\n",
      "Epoch 16/20  Iteration 2707/3560 Training loss: 1.7152 0.4430 sec/batch\n",
      "Epoch 16/20  Iteration 2708/3560 Training loss: 1.7143 0.4458 sec/batch\n",
      "Epoch 16/20  Iteration 2709/3560 Training loss: 1.7130 0.4864 sec/batch\n",
      "Epoch 16/20  Iteration 2710/3560 Training loss: 1.7127 0.4001 sec/batch\n",
      "Epoch 16/20  Iteration 2711/3560 Training loss: 1.7124 0.4682 sec/batch\n",
      "Epoch 16/20  Iteration 2712/3560 Training loss: 1.7128 0.4335 sec/batch\n",
      "Epoch 16/20  Iteration 2713/3560 Training loss: 1.7124 0.4710 sec/batch\n",
      "Epoch 16/20  Iteration 2714/3560 Training loss: 1.7116 0.4920 sec/batch\n",
      "Epoch 16/20  Iteration 2715/3560 Training loss: 1.7118 0.4560 sec/batch\n",
      "Epoch 16/20  Iteration 2716/3560 Training loss: 1.7106 0.4159 sec/batch\n",
      "Epoch 16/20  Iteration 2717/3560 Training loss: 1.7106 0.4057 sec/batch\n",
      "Epoch 16/20  Iteration 2718/3560 Training loss: 1.7099 0.3959 sec/batch\n",
      "Epoch 16/20  Iteration 2719/3560 Training loss: 1.7098 0.4043 sec/batch\n",
      "Epoch 16/20  Iteration 2720/3560 Training loss: 1.7103 0.4270 sec/batch\n",
      "Epoch 16/20  Iteration 2721/3560 Training loss: 1.7097 0.3952 sec/batch\n",
      "Epoch 16/20  Iteration 2722/3560 Training loss: 1.7106 0.4379 sec/batch\n",
      "Epoch 16/20  Iteration 2723/3560 Training loss: 1.7105 0.4293 sec/batch\n",
      "Epoch 16/20  Iteration 2724/3560 Training loss: 1.7107 0.4264 sec/batch\n",
      "Epoch 16/20  Iteration 2725/3560 Training loss: 1.7104 0.4516 sec/batch\n",
      "Epoch 16/20  Iteration 2726/3560 Training loss: 1.7106 0.3890 sec/batch\n",
      "Epoch 16/20  Iteration 2727/3560 Training loss: 1.7108 0.4300 sec/batch\n",
      "Epoch 16/20  Iteration 2728/3560 Training loss: 1.7108 0.4435 sec/batch\n",
      "Epoch 16/20  Iteration 2729/3560 Training loss: 1.7104 0.4023 sec/batch\n",
      "Epoch 16/20  Iteration 2730/3560 Training loss: 1.7111 0.6232 sec/batch\n",
      "Epoch 16/20  Iteration 2731/3560 Training loss: 1.7108 0.5180 sec/batch\n",
      "Epoch 16/20  Iteration 2732/3560 Training loss: 1.7118 0.4375 sec/batch\n",
      "Epoch 16/20  Iteration 2733/3560 Training loss: 1.7122 0.4417 sec/batch\n",
      "Epoch 16/20  Iteration 2734/3560 Training loss: 1.7123 0.5394 sec/batch\n",
      "Epoch 16/20  Iteration 2735/3560 Training loss: 1.7123 0.4201 sec/batch\n",
      "Epoch 16/20  Iteration 2736/3560 Training loss: 1.7124 0.4064 sec/batch\n",
      "Epoch 16/20  Iteration 2737/3560 Training loss: 1.7125 0.3957 sec/batch\n",
      "Epoch 16/20  Iteration 2738/3560 Training loss: 1.7121 0.4359 sec/batch\n",
      "Epoch 16/20  Iteration 2739/3560 Training loss: 1.7122 0.3707 sec/batch\n",
      "Epoch 16/20  Iteration 2740/3560 Training loss: 1.7122 0.4290 sec/batch\n",
      "Epoch 16/20  Iteration 2741/3560 Training loss: 1.7128 0.4617 sec/batch\n",
      "Epoch 16/20  Iteration 2742/3560 Training loss: 1.7130 0.3919 sec/batch\n",
      "Epoch 16/20  Iteration 2743/3560 Training loss: 1.7135 0.4400 sec/batch\n",
      "Epoch 16/20  Iteration 2744/3560 Training loss: 1.7132 0.4502 sec/batch\n",
      "Epoch 16/20  Iteration 2745/3560 Training loss: 1.7131 0.4636 sec/batch\n",
      "Epoch 16/20  Iteration 2746/3560 Training loss: 1.7133 0.4753 sec/batch\n",
      "Epoch 16/20  Iteration 2747/3560 Training loss: 1.7132 0.4718 sec/batch\n",
      "Epoch 16/20  Iteration 2748/3560 Training loss: 1.7133 0.4093 sec/batch\n",
      "Epoch 16/20  Iteration 2749/3560 Training loss: 1.7129 0.4404 sec/batch\n",
      "Epoch 16/20  Iteration 2750/3560 Training loss: 1.7128 0.4943 sec/batch\n",
      "Epoch 16/20  Iteration 2751/3560 Training loss: 1.7123 0.3984 sec/batch\n",
      "Epoch 16/20  Iteration 2752/3560 Training loss: 1.7125 0.4873 sec/batch\n",
      "Epoch 16/20  Iteration 2753/3560 Training loss: 1.7120 0.4894 sec/batch\n",
      "Epoch 16/20  Iteration 2754/3560 Training loss: 1.7119 0.4515 sec/batch\n",
      "Epoch 16/20  Iteration 2755/3560 Training loss: 1.7115 0.3933 sec/batch\n",
      "Epoch 16/20  Iteration 2756/3560 Training loss: 1.7112 0.4610 sec/batch\n",
      "Epoch 16/20  Iteration 2757/3560 Training loss: 1.7110 0.3784 sec/batch\n",
      "Epoch 16/20  Iteration 2758/3560 Training loss: 1.7107 0.3922 sec/batch\n",
      "Epoch 16/20  Iteration 2759/3560 Training loss: 1.7102 0.4021 sec/batch\n",
      "Epoch 16/20  Iteration 2760/3560 Training loss: 1.7103 0.3856 sec/batch\n",
      "Epoch 16/20  Iteration 2761/3560 Training loss: 1.7100 0.4582 sec/batch\n",
      "Epoch 16/20  Iteration 2762/3560 Training loss: 1.7098 0.4420 sec/batch\n",
      "Epoch 16/20  Iteration 2763/3560 Training loss: 1.7095 0.4336 sec/batch\n",
      "Epoch 16/20  Iteration 2764/3560 Training loss: 1.7093 0.4395 sec/batch\n",
      "Epoch 16/20  Iteration 2765/3560 Training loss: 1.7090 0.4233 sec/batch\n",
      "Epoch 16/20  Iteration 2766/3560 Training loss: 1.7088 0.3602 sec/batch\n",
      "Epoch 16/20  Iteration 2767/3560 Training loss: 1.7088 0.5209 sec/batch\n",
      "Epoch 16/20  Iteration 2768/3560 Training loss: 1.7084 0.4219 sec/batch\n",
      "Epoch 16/20  Iteration 2769/3560 Training loss: 1.7082 0.5102 sec/batch\n",
      "Epoch 16/20  Iteration 2770/3560 Training loss: 1.7077 0.4343 sec/batch\n",
      "Epoch 16/20  Iteration 2771/3560 Training loss: 1.7077 0.4707 sec/batch\n",
      "Epoch 16/20  Iteration 2772/3560 Training loss: 1.7075 0.5461 sec/batch\n",
      "Epoch 16/20  Iteration 2773/3560 Training loss: 1.7074 0.3997 sec/batch\n",
      "Epoch 16/20  Iteration 2774/3560 Training loss: 1.7072 0.4654 sec/batch\n",
      "Epoch 16/20  Iteration 2775/3560 Training loss: 1.7069 0.3984 sec/batch\n",
      "Epoch 16/20  Iteration 2776/3560 Training loss: 1.7068 0.4864 sec/batch\n",
      "Epoch 16/20  Iteration 2777/3560 Training loss: 1.7068 0.4660 sec/batch\n",
      "Epoch 16/20  Iteration 2778/3560 Training loss: 1.7068 0.5365 sec/batch\n",
      "Epoch 16/20  Iteration 2779/3560 Training loss: 1.7068 0.4853 sec/batch\n",
      "Epoch 16/20  Iteration 2780/3560 Training loss: 1.7069 0.4804 sec/batch\n",
      "Epoch 16/20  Iteration 2781/3560 Training loss: 1.7068 0.5533 sec/batch\n",
      "Epoch 16/20  Iteration 2782/3560 Training loss: 1.7067 0.5092 sec/batch\n",
      "Epoch 16/20  Iteration 2783/3560 Training loss: 1.7066 0.4295 sec/batch\n",
      "Epoch 16/20  Iteration 2784/3560 Training loss: 1.7065 0.4801 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2785/3560 Training loss: 1.7063 0.3840 sec/batch\n",
      "Epoch 16/20  Iteration 2786/3560 Training loss: 1.7059 0.3964 sec/batch\n",
      "Epoch 16/20  Iteration 2787/3560 Training loss: 1.7059 0.3842 sec/batch\n",
      "Epoch 16/20  Iteration 2788/3560 Training loss: 1.7059 0.4726 sec/batch\n",
      "Epoch 16/20  Iteration 2789/3560 Training loss: 1.7059 0.3929 sec/batch\n",
      "Epoch 16/20  Iteration 2790/3560 Training loss: 1.7059 0.5220 sec/batch\n",
      "Epoch 16/20  Iteration 2791/3560 Training loss: 1.7060 0.5136 sec/batch\n",
      "Epoch 16/20  Iteration 2792/3560 Training loss: 1.7055 0.4491 sec/batch\n",
      "Epoch 16/20  Iteration 2793/3560 Training loss: 1.7052 0.4710 sec/batch\n",
      "Epoch 16/20  Iteration 2794/3560 Training loss: 1.7053 0.4108 sec/batch\n",
      "Epoch 16/20  Iteration 2795/3560 Training loss: 1.7052 0.3967 sec/batch\n",
      "Epoch 16/20  Iteration 2796/3560 Training loss: 1.7048 0.4280 sec/batch\n",
      "Epoch 16/20  Iteration 2797/3560 Training loss: 1.7049 0.4565 sec/batch\n",
      "Epoch 16/20  Iteration 2798/3560 Training loss: 1.7050 0.4167 sec/batch\n",
      "Epoch 16/20  Iteration 2799/3560 Training loss: 1.7049 0.4407 sec/batch\n",
      "Epoch 16/20  Iteration 2800/3560 Training loss: 1.7047 0.4250 sec/batch\n",
      "Epoch 16/20  Iteration 2801/3560 Training loss: 1.7045 0.4587 sec/batch\n",
      "Epoch 16/20  Iteration 2802/3560 Training loss: 1.7042 0.4456 sec/batch\n",
      "Epoch 16/20  Iteration 2803/3560 Training loss: 1.7043 0.4202 sec/batch\n",
      "Epoch 16/20  Iteration 2804/3560 Training loss: 1.7043 0.4373 sec/batch\n",
      "Epoch 16/20  Iteration 2805/3560 Training loss: 1.7042 0.3478 sec/batch\n",
      "Epoch 16/20  Iteration 2806/3560 Training loss: 1.7043 0.4194 sec/batch\n",
      "Epoch 16/20  Iteration 2807/3560 Training loss: 1.7045 0.4121 sec/batch\n",
      "Epoch 16/20  Iteration 2808/3560 Training loss: 1.7045 0.4390 sec/batch\n",
      "Epoch 16/20  Iteration 2809/3560 Training loss: 1.7046 0.4254 sec/batch\n",
      "Epoch 16/20  Iteration 2810/3560 Training loss: 1.7045 0.4033 sec/batch\n",
      "Epoch 16/20  Iteration 2811/3560 Training loss: 1.7047 0.4615 sec/batch\n",
      "Epoch 16/20  Iteration 2812/3560 Training loss: 1.7048 0.4007 sec/batch\n",
      "Epoch 16/20  Iteration 2813/3560 Training loss: 1.7047 0.3898 sec/batch\n",
      "Epoch 16/20  Iteration 2814/3560 Training loss: 1.7049 0.4299 sec/batch\n",
      "Epoch 16/20  Iteration 2815/3560 Training loss: 1.7048 0.4073 sec/batch\n",
      "Epoch 16/20  Iteration 2816/3560 Training loss: 1.7049 0.3752 sec/batch\n",
      "Epoch 16/20  Iteration 2817/3560 Training loss: 1.7050 0.4092 sec/batch\n",
      "Epoch 16/20  Iteration 2818/3560 Training loss: 1.7052 0.4088 sec/batch\n",
      "Epoch 16/20  Iteration 2819/3560 Training loss: 1.7053 0.4369 sec/batch\n",
      "Epoch 16/20  Iteration 2820/3560 Training loss: 1.7052 0.4138 sec/batch\n",
      "Epoch 16/20  Iteration 2821/3560 Training loss: 1.7049 0.5060 sec/batch\n",
      "Epoch 16/20  Iteration 2822/3560 Training loss: 1.7050 0.4026 sec/batch\n",
      "Epoch 16/20  Iteration 2823/3560 Training loss: 1.7051 0.4349 sec/batch\n",
      "Epoch 16/20  Iteration 2824/3560 Training loss: 1.7052 0.4413 sec/batch\n",
      "Epoch 16/20  Iteration 2825/3560 Training loss: 1.7053 0.3971 sec/batch\n",
      "Epoch 16/20  Iteration 2826/3560 Training loss: 1.7053 0.4152 sec/batch\n",
      "Epoch 16/20  Iteration 2827/3560 Training loss: 1.7053 0.4529 sec/batch\n",
      "Epoch 16/20  Iteration 2828/3560 Training loss: 1.7053 0.4783 sec/batch\n",
      "Epoch 16/20  Iteration 2829/3560 Training loss: 1.7051 0.4068 sec/batch\n",
      "Epoch 16/20  Iteration 2830/3560 Training loss: 1.7053 0.4393 sec/batch\n",
      "Epoch 16/20  Iteration 2831/3560 Training loss: 1.7054 0.4036 sec/batch\n",
      "Epoch 16/20  Iteration 2832/3560 Training loss: 1.7054 0.4129 sec/batch\n",
      "Epoch 16/20  Iteration 2833/3560 Training loss: 1.7056 0.4304 sec/batch\n",
      "Epoch 16/20  Iteration 2834/3560 Training loss: 1.7056 0.3665 sec/batch\n",
      "Epoch 16/20  Iteration 2835/3560 Training loss: 1.7056 0.4218 sec/batch\n",
      "Epoch 16/20  Iteration 2836/3560 Training loss: 1.7056 0.4120 sec/batch\n",
      "Epoch 16/20  Iteration 2837/3560 Training loss: 1.7057 0.4275 sec/batch\n",
      "Epoch 16/20  Iteration 2838/3560 Training loss: 1.7061 0.5048 sec/batch\n",
      "Epoch 16/20  Iteration 2839/3560 Training loss: 1.7060 0.4490 sec/batch\n",
      "Epoch 16/20  Iteration 2840/3560 Training loss: 1.7061 0.4244 sec/batch\n",
      "Epoch 16/20  Iteration 2841/3560 Training loss: 1.7060 0.4953 sec/batch\n",
      "Epoch 16/20  Iteration 2842/3560 Training loss: 1.7059 0.4372 sec/batch\n",
      "Epoch 16/20  Iteration 2843/3560 Training loss: 1.7060 0.4563 sec/batch\n",
      "Epoch 16/20  Iteration 2844/3560 Training loss: 1.7060 0.4872 sec/batch\n",
      "Epoch 16/20  Iteration 2845/3560 Training loss: 1.7061 0.4556 sec/batch\n",
      "Epoch 16/20  Iteration 2846/3560 Training loss: 1.7060 0.4997 sec/batch\n",
      "Epoch 16/20  Iteration 2847/3560 Training loss: 1.7059 0.6332 sec/batch\n",
      "Epoch 16/20  Iteration 2848/3560 Training loss: 1.7060 0.6605 sec/batch\n",
      "Epoch 17/20  Iteration 2849/3560 Training loss: 1.7844 0.4897 sec/batch\n",
      "Epoch 17/20  Iteration 2850/3560 Training loss: 1.7428 0.4373 sec/batch\n",
      "Epoch 17/20  Iteration 2851/3560 Training loss: 1.7300 0.4152 sec/batch\n",
      "Epoch 17/20  Iteration 2852/3560 Training loss: 1.7248 0.4266 sec/batch\n",
      "Epoch 17/20  Iteration 2853/3560 Training loss: 1.7163 0.3705 sec/batch\n",
      "Epoch 17/20  Iteration 2854/3560 Training loss: 1.7064 0.4116 sec/batch\n",
      "Epoch 17/20  Iteration 2855/3560 Training loss: 1.7068 0.4472 sec/batch\n",
      "Epoch 17/20  Iteration 2856/3560 Training loss: 1.7054 0.3576 sec/batch\n",
      "Epoch 17/20  Iteration 2857/3560 Training loss: 1.7079 0.3802 sec/batch\n",
      "Epoch 17/20  Iteration 2858/3560 Training loss: 1.7067 0.4227 sec/batch\n",
      "Epoch 17/20  Iteration 2859/3560 Training loss: 1.7038 0.3978 sec/batch\n",
      "Epoch 17/20  Iteration 2860/3560 Training loss: 1.7021 0.4608 sec/batch\n",
      "Epoch 17/20  Iteration 2861/3560 Training loss: 1.7021 0.4426 sec/batch\n",
      "Epoch 17/20  Iteration 2862/3560 Training loss: 1.7047 0.4223 sec/batch\n",
      "Epoch 17/20  Iteration 2863/3560 Training loss: 1.7039 0.3986 sec/batch\n",
      "Epoch 17/20  Iteration 2864/3560 Training loss: 1.7018 0.3676 sec/batch\n",
      "Epoch 17/20  Iteration 2865/3560 Training loss: 1.7013 0.4697 sec/batch\n",
      "Epoch 17/20  Iteration 2866/3560 Training loss: 1.7024 0.4012 sec/batch\n",
      "Epoch 17/20  Iteration 2867/3560 Training loss: 1.7028 0.4616 sec/batch\n",
      "Epoch 17/20  Iteration 2868/3560 Training loss: 1.7033 0.4344 sec/batch\n",
      "Epoch 17/20  Iteration 2869/3560 Training loss: 1.7023 0.4475 sec/batch\n",
      "Epoch 17/20  Iteration 2870/3560 Training loss: 1.7026 0.3964 sec/batch\n",
      "Epoch 17/20  Iteration 2871/3560 Training loss: 1.7018 0.4074 sec/batch\n",
      "Epoch 17/20  Iteration 2872/3560 Training loss: 1.7017 0.3650 sec/batch\n",
      "Epoch 17/20  Iteration 2873/3560 Training loss: 1.7012 0.4043 sec/batch\n",
      "Epoch 17/20  Iteration 2874/3560 Training loss: 1.6999 0.4753 sec/batch\n",
      "Epoch 17/20  Iteration 2875/3560 Training loss: 1.6988 0.3541 sec/batch\n",
      "Epoch 17/20  Iteration 2876/3560 Training loss: 1.6996 0.3896 sec/batch\n",
      "Epoch 17/20  Iteration 2877/3560 Training loss: 1.7006 0.3962 sec/batch\n",
      "Epoch 17/20  Iteration 2878/3560 Training loss: 1.7007 0.4743 sec/batch\n",
      "Epoch 17/20  Iteration 2879/3560 Training loss: 1.7007 0.4222 sec/batch\n",
      "Epoch 17/20  Iteration 2880/3560 Training loss: 1.7000 0.3956 sec/batch\n",
      "Epoch 17/20  Iteration 2881/3560 Training loss: 1.7003 0.4202 sec/batch\n",
      "Epoch 17/20  Iteration 2882/3560 Training loss: 1.7011 0.4045 sec/batch\n",
      "Epoch 17/20  Iteration 2883/3560 Training loss: 1.7006 0.3481 sec/batch\n",
      "Epoch 17/20  Iteration 2884/3560 Training loss: 1.7001 0.4475 sec/batch\n",
      "Epoch 17/20  Iteration 2885/3560 Training loss: 1.6995 0.3860 sec/batch\n",
      "Epoch 17/20  Iteration 2886/3560 Training loss: 1.6984 0.4595 sec/batch\n",
      "Epoch 17/20  Iteration 2887/3560 Training loss: 1.6974 0.4230 sec/batch\n",
      "Epoch 17/20  Iteration 2888/3560 Training loss: 1.6969 0.3973 sec/batch\n",
      "Epoch 17/20  Iteration 2889/3560 Training loss: 1.6966 0.4046 sec/batch\n",
      "Epoch 17/20  Iteration 2890/3560 Training loss: 1.6971 0.3907 sec/batch\n",
      "Epoch 17/20  Iteration 2891/3560 Training loss: 1.6967 0.4150 sec/batch\n",
      "Epoch 17/20  Iteration 2892/3560 Training loss: 1.6961 0.4053 sec/batch\n",
      "Epoch 17/20  Iteration 2893/3560 Training loss: 1.6965 0.4134 sec/batch\n",
      "Epoch 17/20  Iteration 2894/3560 Training loss: 1.6953 0.4439 sec/batch\n",
      "Epoch 17/20  Iteration 2895/3560 Training loss: 1.6950 0.4317 sec/batch\n",
      "Epoch 17/20  Iteration 2896/3560 Training loss: 1.6944 0.4267 sec/batch\n",
      "Epoch 17/20  Iteration 2897/3560 Training loss: 1.6943 0.4388 sec/batch\n",
      "Epoch 17/20  Iteration 2898/3560 Training loss: 1.6952 0.4612 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 2899/3560 Training loss: 1.6948 0.4164 sec/batch\n",
      "Epoch 17/20  Iteration 2900/3560 Training loss: 1.6957 0.5644 sec/batch\n",
      "Epoch 17/20  Iteration 2901/3560 Training loss: 1.6954 0.3917 sec/batch\n",
      "Epoch 17/20  Iteration 2902/3560 Training loss: 1.6954 0.4046 sec/batch\n",
      "Epoch 17/20  Iteration 2903/3560 Training loss: 1.6953 0.3930 sec/batch\n",
      "Epoch 17/20  Iteration 2904/3560 Training loss: 1.6955 0.4232 sec/batch\n",
      "Epoch 17/20  Iteration 2905/3560 Training loss: 1.6960 0.4529 sec/batch\n",
      "Epoch 17/20  Iteration 2906/3560 Training loss: 1.6956 0.6049 sec/batch\n",
      "Epoch 17/20  Iteration 2907/3560 Training loss: 1.6952 0.4638 sec/batch\n",
      "Epoch 17/20  Iteration 2908/3560 Training loss: 1.6959 0.4357 sec/batch\n",
      "Epoch 17/20  Iteration 2909/3560 Training loss: 1.6958 0.4479 sec/batch\n",
      "Epoch 17/20  Iteration 2910/3560 Training loss: 1.6966 0.4136 sec/batch\n",
      "Epoch 17/20  Iteration 2911/3560 Training loss: 1.6970 0.4204 sec/batch\n",
      "Epoch 17/20  Iteration 2912/3560 Training loss: 1.6972 0.4308 sec/batch\n",
      "Epoch 17/20  Iteration 2913/3560 Training loss: 1.6972 0.3990 sec/batch\n",
      "Epoch 17/20  Iteration 2914/3560 Training loss: 1.6974 0.4250 sec/batch\n",
      "Epoch 17/20  Iteration 2915/3560 Training loss: 1.6976 0.4311 sec/batch\n",
      "Epoch 17/20  Iteration 2916/3560 Training loss: 1.6973 0.4284 sec/batch\n",
      "Epoch 17/20  Iteration 2917/3560 Training loss: 1.6972 0.4568 sec/batch\n",
      "Epoch 17/20  Iteration 2918/3560 Training loss: 1.6971 0.4026 sec/batch\n",
      "Epoch 17/20  Iteration 2919/3560 Training loss: 1.6978 0.4046 sec/batch\n",
      "Epoch 17/20  Iteration 2920/3560 Training loss: 1.6979 0.3886 sec/batch\n",
      "Epoch 17/20  Iteration 2921/3560 Training loss: 1.6984 0.5483 sec/batch\n",
      "Epoch 17/20  Iteration 2922/3560 Training loss: 1.6981 0.3990 sec/batch\n",
      "Epoch 17/20  Iteration 2923/3560 Training loss: 1.6981 0.4337 sec/batch\n",
      "Epoch 17/20  Iteration 2924/3560 Training loss: 1.6984 0.4722 sec/batch\n",
      "Epoch 17/20  Iteration 2925/3560 Training loss: 1.6983 0.4271 sec/batch\n",
      "Epoch 17/20  Iteration 2926/3560 Training loss: 1.6985 0.4392 sec/batch\n",
      "Epoch 17/20  Iteration 2927/3560 Training loss: 1.6980 0.4350 sec/batch\n",
      "Epoch 17/20  Iteration 2928/3560 Training loss: 1.6978 0.3829 sec/batch\n",
      "Epoch 17/20  Iteration 2929/3560 Training loss: 1.6973 0.4003 sec/batch\n",
      "Epoch 17/20  Iteration 2930/3560 Training loss: 1.6974 0.4147 sec/batch\n",
      "Epoch 17/20  Iteration 2931/3560 Training loss: 1.6969 0.4163 sec/batch\n",
      "Epoch 17/20  Iteration 2932/3560 Training loss: 1.6969 0.3374 sec/batch\n",
      "Epoch 17/20  Iteration 2933/3560 Training loss: 1.6965 0.3837 sec/batch\n",
      "Epoch 17/20  Iteration 2934/3560 Training loss: 1.6963 0.4572 sec/batch\n",
      "Epoch 17/20  Iteration 2935/3560 Training loss: 1.6961 0.4048 sec/batch\n",
      "Epoch 17/20  Iteration 2936/3560 Training loss: 1.6959 0.3858 sec/batch\n",
      "Epoch 17/20  Iteration 2937/3560 Training loss: 1.6954 0.4911 sec/batch\n",
      "Epoch 17/20  Iteration 2938/3560 Training loss: 1.6955 0.4247 sec/batch\n",
      "Epoch 17/20  Iteration 2939/3560 Training loss: 1.6952 0.4122 sec/batch\n",
      "Epoch 17/20  Iteration 2940/3560 Training loss: 1.6949 0.4197 sec/batch\n",
      "Epoch 17/20  Iteration 2941/3560 Training loss: 1.6946 0.4421 sec/batch\n",
      "Epoch 17/20  Iteration 2942/3560 Training loss: 1.6943 0.4057 sec/batch\n",
      "Epoch 17/20  Iteration 2943/3560 Training loss: 1.6939 0.4292 sec/batch\n",
      "Epoch 17/20  Iteration 2944/3560 Training loss: 1.6938 0.4239 sec/batch\n",
      "Epoch 17/20  Iteration 2945/3560 Training loss: 1.6938 0.4173 sec/batch\n",
      "Epoch 17/20  Iteration 2946/3560 Training loss: 1.6934 0.4358 sec/batch\n",
      "Epoch 17/20  Iteration 2947/3560 Training loss: 1.6931 0.4521 sec/batch\n",
      "Epoch 17/20  Iteration 2948/3560 Training loss: 1.6928 0.3672 sec/batch\n",
      "Epoch 17/20  Iteration 2949/3560 Training loss: 1.6927 0.3879 sec/batch\n",
      "Epoch 17/20  Iteration 2950/3560 Training loss: 1.6927 0.4133 sec/batch\n",
      "Epoch 17/20  Iteration 2951/3560 Training loss: 1.6927 0.4087 sec/batch\n",
      "Epoch 17/20  Iteration 2952/3560 Training loss: 1.6925 0.3879 sec/batch\n",
      "Epoch 17/20  Iteration 2953/3560 Training loss: 1.6923 0.4141 sec/batch\n",
      "Epoch 17/20  Iteration 2954/3560 Training loss: 1.6922 0.3598 sec/batch\n",
      "Epoch 17/20  Iteration 2955/3560 Training loss: 1.6922 0.5425 sec/batch\n",
      "Epoch 17/20  Iteration 2956/3560 Training loss: 1.6922 0.4919 sec/batch\n",
      "Epoch 17/20  Iteration 2957/3560 Training loss: 1.6921 0.4779 sec/batch\n",
      "Epoch 17/20  Iteration 2958/3560 Training loss: 1.6921 0.3941 sec/batch\n",
      "Epoch 17/20  Iteration 2959/3560 Training loss: 1.6920 0.4044 sec/batch\n",
      "Epoch 17/20  Iteration 2960/3560 Training loss: 1.6920 0.4938 sec/batch\n",
      "Epoch 17/20  Iteration 2961/3560 Training loss: 1.6919 0.3309 sec/batch\n",
      "Epoch 17/20  Iteration 2962/3560 Training loss: 1.6918 0.3991 sec/batch\n",
      "Epoch 17/20  Iteration 2963/3560 Training loss: 1.6916 0.4444 sec/batch\n",
      "Epoch 17/20  Iteration 2964/3560 Training loss: 1.6912 0.4755 sec/batch\n",
      "Epoch 17/20  Iteration 2965/3560 Training loss: 1.6913 0.4243 sec/batch\n",
      "Epoch 17/20  Iteration 2966/3560 Training loss: 1.6913 0.4627 sec/batch\n",
      "Epoch 17/20  Iteration 2967/3560 Training loss: 1.6912 0.3862 sec/batch\n",
      "Epoch 17/20  Iteration 2968/3560 Training loss: 1.6912 0.4100 sec/batch\n",
      "Epoch 17/20  Iteration 2969/3560 Training loss: 1.6912 0.4026 sec/batch\n",
      "Epoch 17/20  Iteration 2970/3560 Training loss: 1.6909 0.4160 sec/batch\n",
      "Epoch 17/20  Iteration 2971/3560 Training loss: 1.6906 0.3400 sec/batch\n",
      "Epoch 17/20  Iteration 2972/3560 Training loss: 1.6907 0.4292 sec/batch\n",
      "Epoch 17/20  Iteration 2973/3560 Training loss: 1.6906 0.4472 sec/batch\n",
      "Epoch 17/20  Iteration 2974/3560 Training loss: 1.6902 0.5135 sec/batch\n",
      "Epoch 17/20  Iteration 2975/3560 Training loss: 1.6904 0.3709 sec/batch\n",
      "Epoch 17/20  Iteration 2976/3560 Training loss: 1.6905 0.4404 sec/batch\n",
      "Epoch 17/20  Iteration 2977/3560 Training loss: 1.6905 0.3935 sec/batch\n",
      "Epoch 17/20  Iteration 2978/3560 Training loss: 1.6903 0.3803 sec/batch\n",
      "Epoch 17/20  Iteration 2979/3560 Training loss: 1.6900 0.4521 sec/batch\n",
      "Epoch 17/20  Iteration 2980/3560 Training loss: 1.6897 0.3996 sec/batch\n",
      "Epoch 17/20  Iteration 2981/3560 Training loss: 1.6898 0.4071 sec/batch\n",
      "Epoch 17/20  Iteration 2982/3560 Training loss: 1.6898 0.4706 sec/batch\n",
      "Epoch 17/20  Iteration 2983/3560 Training loss: 1.6898 0.4018 sec/batch\n",
      "Epoch 17/20  Iteration 2984/3560 Training loss: 1.6898 0.4504 sec/batch\n",
      "Epoch 17/20  Iteration 2985/3560 Training loss: 1.6900 0.4076 sec/batch\n",
      "Epoch 17/20  Iteration 2986/3560 Training loss: 1.6901 0.4409 sec/batch\n",
      "Epoch 17/20  Iteration 2987/3560 Training loss: 1.6902 0.4388 sec/batch\n",
      "Epoch 17/20  Iteration 2988/3560 Training loss: 1.6901 0.3745 sec/batch\n",
      "Epoch 17/20  Iteration 2989/3560 Training loss: 1.6904 0.3928 sec/batch\n",
      "Epoch 17/20  Iteration 2990/3560 Training loss: 1.6904 0.3846 sec/batch\n",
      "Epoch 17/20  Iteration 2991/3560 Training loss: 1.6903 0.4131 sec/batch\n",
      "Epoch 17/20  Iteration 2992/3560 Training loss: 1.6905 0.4304 sec/batch\n",
      "Epoch 17/20  Iteration 2993/3560 Training loss: 1.6904 0.4638 sec/batch\n",
      "Epoch 17/20  Iteration 2994/3560 Training loss: 1.6904 0.4194 sec/batch\n",
      "Epoch 17/20  Iteration 2995/3560 Training loss: 1.6905 0.4274 sec/batch\n",
      "Epoch 17/20  Iteration 2996/3560 Training loss: 1.6906 0.3967 sec/batch\n",
      "Epoch 17/20  Iteration 2997/3560 Training loss: 1.6907 0.3862 sec/batch\n",
      "Epoch 17/20  Iteration 2998/3560 Training loss: 1.6905 0.4105 sec/batch\n",
      "Epoch 17/20  Iteration 2999/3560 Training loss: 1.6903 0.3929 sec/batch\n",
      "Epoch 17/20  Iteration 3000/3560 Training loss: 1.6903 0.4085 sec/batch\n",
      "Epoch 17/20  Iteration 3001/3560 Training loss: 1.6905 0.4408 sec/batch\n",
      "Epoch 17/20  Iteration 3002/3560 Training loss: 1.6906 0.4146 sec/batch\n",
      "Epoch 17/20  Iteration 3003/3560 Training loss: 1.6907 0.4470 sec/batch\n",
      "Epoch 17/20  Iteration 3004/3560 Training loss: 1.6907 0.4421 sec/batch\n",
      "Epoch 17/20  Iteration 3005/3560 Training loss: 1.6908 0.4370 sec/batch\n",
      "Epoch 17/20  Iteration 3006/3560 Training loss: 1.6909 0.3865 sec/batch\n",
      "Epoch 17/20  Iteration 3007/3560 Training loss: 1.6906 0.4525 sec/batch\n",
      "Epoch 17/20  Iteration 3008/3560 Training loss: 1.6908 0.3646 sec/batch\n",
      "Epoch 17/20  Iteration 3009/3560 Training loss: 1.6910 0.3993 sec/batch\n",
      "Epoch 17/20  Iteration 3010/3560 Training loss: 1.6910 0.4111 sec/batch\n",
      "Epoch 17/20  Iteration 3011/3560 Training loss: 1.6911 0.4800 sec/batch\n",
      "Epoch 17/20  Iteration 3012/3560 Training loss: 1.6911 0.4106 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 3013/3560 Training loss: 1.6910 0.4167 sec/batch\n",
      "Epoch 17/20  Iteration 3014/3560 Training loss: 1.6911 0.4250 sec/batch\n",
      "Epoch 17/20  Iteration 3015/3560 Training loss: 1.6912 0.3932 sec/batch\n",
      "Epoch 17/20  Iteration 3016/3560 Training loss: 1.6915 0.3988 sec/batch\n",
      "Epoch 17/20  Iteration 3017/3560 Training loss: 1.6915 0.3832 sec/batch\n",
      "Epoch 17/20  Iteration 3018/3560 Training loss: 1.6915 0.4055 sec/batch\n",
      "Epoch 17/20  Iteration 3019/3560 Training loss: 1.6915 0.3674 sec/batch\n",
      "Epoch 17/20  Iteration 3020/3560 Training loss: 1.6913 0.4332 sec/batch\n",
      "Epoch 17/20  Iteration 3021/3560 Training loss: 1.6915 0.4103 sec/batch\n",
      "Epoch 17/20  Iteration 3022/3560 Training loss: 1.6916 0.4738 sec/batch\n",
      "Epoch 17/20  Iteration 3023/3560 Training loss: 1.6916 0.3944 sec/batch\n",
      "Epoch 17/20  Iteration 3024/3560 Training loss: 1.6915 0.4646 sec/batch\n",
      "Epoch 17/20  Iteration 3025/3560 Training loss: 1.6914 0.3744 sec/batch\n",
      "Epoch 17/20  Iteration 3026/3560 Training loss: 1.6915 0.4211 sec/batch\n",
      "Epoch 18/20  Iteration 3027/3560 Training loss: 1.7729 0.3859 sec/batch\n",
      "Epoch 18/20  Iteration 3028/3560 Training loss: 1.7325 0.4372 sec/batch\n",
      "Epoch 18/20  Iteration 3029/3560 Training loss: 1.7184 0.3714 sec/batch\n",
      "Epoch 18/20  Iteration 3030/3560 Training loss: 1.7136 0.4384 sec/batch\n",
      "Epoch 18/20  Iteration 3031/3560 Training loss: 1.7043 0.4533 sec/batch\n",
      "Epoch 18/20  Iteration 3032/3560 Training loss: 1.6946 0.3944 sec/batch\n",
      "Epoch 18/20  Iteration 3033/3560 Training loss: 1.6944 0.4647 sec/batch\n",
      "Epoch 18/20  Iteration 3034/3560 Training loss: 1.6936 0.4264 sec/batch\n",
      "Epoch 18/20  Iteration 3035/3560 Training loss: 1.6941 0.4456 sec/batch\n",
      "Epoch 18/20  Iteration 3036/3560 Training loss: 1.6919 0.3894 sec/batch\n",
      "Epoch 18/20  Iteration 3037/3560 Training loss: 1.6894 0.3964 sec/batch\n",
      "Epoch 18/20  Iteration 3038/3560 Training loss: 1.6882 0.4334 sec/batch\n",
      "Epoch 18/20  Iteration 3039/3560 Training loss: 1.6879 0.4139 sec/batch\n",
      "Epoch 18/20  Iteration 3040/3560 Training loss: 1.6898 0.4038 sec/batch\n",
      "Epoch 18/20  Iteration 3041/3560 Training loss: 1.6893 0.4235 sec/batch\n",
      "Epoch 18/20  Iteration 3042/3560 Training loss: 1.6881 0.4806 sec/batch\n",
      "Epoch 18/20  Iteration 3043/3560 Training loss: 1.6884 0.3876 sec/batch\n",
      "Epoch 18/20  Iteration 3044/3560 Training loss: 1.6898 0.4171 sec/batch\n",
      "Epoch 18/20  Iteration 3045/3560 Training loss: 1.6906 0.5116 sec/batch\n",
      "Epoch 18/20  Iteration 3046/3560 Training loss: 1.6909 0.4275 sec/batch\n",
      "Epoch 18/20  Iteration 3047/3560 Training loss: 1.6903 0.3708 sec/batch\n",
      "Epoch 18/20  Iteration 3048/3560 Training loss: 1.6905 0.3992 sec/batch\n",
      "Epoch 18/20  Iteration 3049/3560 Training loss: 1.6897 0.4397 sec/batch\n",
      "Epoch 18/20  Iteration 3050/3560 Training loss: 1.6894 0.5030 sec/batch\n",
      "Epoch 18/20  Iteration 3051/3560 Training loss: 1.6892 0.4007 sec/batch\n",
      "Epoch 18/20  Iteration 3052/3560 Training loss: 1.6883 0.4442 sec/batch\n",
      "Epoch 18/20  Iteration 3053/3560 Training loss: 1.6871 0.4201 sec/batch\n",
      "Epoch 18/20  Iteration 3054/3560 Training loss: 1.6876 0.4008 sec/batch\n",
      "Epoch 18/20  Iteration 3055/3560 Training loss: 1.6886 0.3978 sec/batch\n",
      "Epoch 18/20  Iteration 3056/3560 Training loss: 1.6888 0.4475 sec/batch\n",
      "Epoch 18/20  Iteration 3057/3560 Training loss: 1.6887 0.3851 sec/batch\n",
      "Epoch 18/20  Iteration 3058/3560 Training loss: 1.6878 0.4044 sec/batch\n",
      "Epoch 18/20  Iteration 3059/3560 Training loss: 1.6882 0.4353 sec/batch\n",
      "Epoch 18/20  Iteration 3060/3560 Training loss: 1.6888 0.4366 sec/batch\n",
      "Epoch 18/20  Iteration 3061/3560 Training loss: 1.6886 0.3994 sec/batch\n",
      "Epoch 18/20  Iteration 3062/3560 Training loss: 1.6883 0.4502 sec/batch\n",
      "Epoch 18/20  Iteration 3063/3560 Training loss: 1.6876 0.5016 sec/batch\n",
      "Epoch 18/20  Iteration 3064/3560 Training loss: 1.6865 0.4774 sec/batch\n",
      "Epoch 18/20  Iteration 3065/3560 Training loss: 1.6854 0.3893 sec/batch\n",
      "Epoch 18/20  Iteration 3066/3560 Training loss: 1.6848 0.4163 sec/batch\n",
      "Epoch 18/20  Iteration 3067/3560 Training loss: 1.6843 0.3813 sec/batch\n",
      "Epoch 18/20  Iteration 3068/3560 Training loss: 1.6849 0.4567 sec/batch\n",
      "Epoch 18/20  Iteration 3069/3560 Training loss: 1.6846 0.4294 sec/batch\n",
      "Epoch 18/20  Iteration 3070/3560 Training loss: 1.6839 0.4322 sec/batch\n",
      "Epoch 18/20  Iteration 3071/3560 Training loss: 1.6841 0.4280 sec/batch\n",
      "Epoch 18/20  Iteration 3072/3560 Training loss: 1.6831 0.3944 sec/batch\n",
      "Epoch 18/20  Iteration 3073/3560 Training loss: 1.6829 0.4338 sec/batch\n",
      "Epoch 18/20  Iteration 3074/3560 Training loss: 1.6824 0.3873 sec/batch\n",
      "Epoch 18/20  Iteration 3075/3560 Training loss: 1.6823 0.3871 sec/batch\n",
      "Epoch 18/20  Iteration 3076/3560 Training loss: 1.6831 0.4186 sec/batch\n",
      "Epoch 18/20  Iteration 3077/3560 Training loss: 1.6826 0.3800 sec/batch\n",
      "Epoch 18/20  Iteration 3078/3560 Training loss: 1.6834 0.4638 sec/batch\n",
      "Epoch 18/20  Iteration 3079/3560 Training loss: 1.6835 0.4233 sec/batch\n",
      "Epoch 18/20  Iteration 3080/3560 Training loss: 1.6834 0.4234 sec/batch\n",
      "Epoch 18/20  Iteration 3081/3560 Training loss: 1.6830 0.4233 sec/batch\n",
      "Epoch 18/20  Iteration 3082/3560 Training loss: 1.6832 0.4190 sec/batch\n",
      "Epoch 18/20  Iteration 3083/3560 Training loss: 1.6835 0.4349 sec/batch\n",
      "Epoch 18/20  Iteration 3084/3560 Training loss: 1.6832 0.3814 sec/batch\n",
      "Epoch 18/20  Iteration 3085/3560 Training loss: 1.6827 0.4130 sec/batch\n",
      "Epoch 18/20  Iteration 3086/3560 Training loss: 1.6834 0.3838 sec/batch\n",
      "Epoch 18/20  Iteration 3087/3560 Training loss: 1.6831 0.4087 sec/batch\n",
      "Epoch 18/20  Iteration 3088/3560 Training loss: 1.6840 0.4490 sec/batch\n",
      "Epoch 18/20  Iteration 3089/3560 Training loss: 1.6845 0.4657 sec/batch\n",
      "Epoch 18/20  Iteration 3090/3560 Training loss: 1.6848 0.4204 sec/batch\n",
      "Epoch 18/20  Iteration 3091/3560 Training loss: 1.6848 0.4219 sec/batch\n",
      "Epoch 18/20  Iteration 3092/3560 Training loss: 1.6851 0.4390 sec/batch\n",
      "Epoch 18/20  Iteration 3093/3560 Training loss: 1.6852 0.3975 sec/batch\n",
      "Epoch 18/20  Iteration 3094/3560 Training loss: 1.6848 0.3896 sec/batch\n",
      "Epoch 18/20  Iteration 3095/3560 Training loss: 1.6848 0.4050 sec/batch\n",
      "Epoch 18/20  Iteration 3096/3560 Training loss: 1.6848 0.3974 sec/batch\n",
      "Epoch 18/20  Iteration 3097/3560 Training loss: 1.6856 0.4290 sec/batch\n",
      "Epoch 18/20  Iteration 3098/3560 Training loss: 1.6858 0.4384 sec/batch\n",
      "Epoch 18/20  Iteration 3099/3560 Training loss: 1.6862 0.4366 sec/batch\n",
      "Epoch 18/20  Iteration 3100/3560 Training loss: 1.6859 0.4236 sec/batch\n",
      "Epoch 18/20  Iteration 3101/3560 Training loss: 1.6859 0.4293 sec/batch\n",
      "Epoch 18/20  Iteration 3102/3560 Training loss: 1.6861 0.4539 sec/batch\n",
      "Epoch 18/20  Iteration 3103/3560 Training loss: 1.6859 0.3793 sec/batch\n",
      "Epoch 18/20  Iteration 3104/3560 Training loss: 1.6859 0.4415 sec/batch\n",
      "Epoch 18/20  Iteration 3105/3560 Training loss: 1.6855 0.3638 sec/batch\n",
      "Epoch 18/20  Iteration 3106/3560 Training loss: 1.6853 0.4445 sec/batch\n",
      "Epoch 18/20  Iteration 3107/3560 Training loss: 1.6848 0.4326 sec/batch\n",
      "Epoch 18/20  Iteration 3108/3560 Training loss: 1.6849 0.4451 sec/batch\n",
      "Epoch 18/20  Iteration 3109/3560 Training loss: 1.6846 0.4608 sec/batch\n",
      "Epoch 18/20  Iteration 3110/3560 Training loss: 1.6845 0.4287 sec/batch\n",
      "Epoch 18/20  Iteration 3111/3560 Training loss: 1.6840 0.4356 sec/batch\n",
      "Epoch 18/20  Iteration 3112/3560 Training loss: 1.6838 0.3744 sec/batch\n",
      "Epoch 18/20  Iteration 3113/3560 Training loss: 1.6836 0.3893 sec/batch\n",
      "Epoch 18/20  Iteration 3114/3560 Training loss: 1.6834 0.4104 sec/batch\n",
      "Epoch 18/20  Iteration 3115/3560 Training loss: 1.6829 0.3984 sec/batch\n",
      "Epoch 18/20  Iteration 3116/3560 Training loss: 1.6829 0.3627 sec/batch\n",
      "Epoch 18/20  Iteration 3117/3560 Training loss: 1.6825 0.4290 sec/batch\n",
      "Epoch 18/20  Iteration 3118/3560 Training loss: 1.6822 0.4756 sec/batch\n",
      "Epoch 18/20  Iteration 3119/3560 Training loss: 1.6819 0.3865 sec/batch\n",
      "Epoch 18/20  Iteration 3120/3560 Training loss: 1.6816 0.4122 sec/batch\n",
      "Epoch 18/20  Iteration 3121/3560 Training loss: 1.6814 0.4461 sec/batch\n",
      "Epoch 18/20  Iteration 3122/3560 Training loss: 1.6815 0.4153 sec/batch\n",
      "Epoch 18/20  Iteration 3123/3560 Training loss: 1.6814 0.3694 sec/batch\n",
      "Epoch 18/20  Iteration 3124/3560 Training loss: 1.6809 0.3955 sec/batch\n",
      "Epoch 18/20  Iteration 3125/3560 Training loss: 1.6806 0.3989 sec/batch\n",
      "Epoch 18/20  Iteration 3126/3560 Training loss: 1.6802 0.4025 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3127/3560 Training loss: 1.6802 0.4401 sec/batch\n",
      "Epoch 18/20  Iteration 3128/3560 Training loss: 1.6801 0.4737 sec/batch\n",
      "Epoch 18/20  Iteration 3129/3560 Training loss: 1.6800 0.4586 sec/batch\n",
      "Epoch 18/20  Iteration 3130/3560 Training loss: 1.6798 0.4419 sec/batch\n",
      "Epoch 18/20  Iteration 3131/3560 Training loss: 1.6797 0.4195 sec/batch\n",
      "Epoch 18/20  Iteration 3132/3560 Training loss: 1.6797 0.3945 sec/batch\n",
      "Epoch 18/20  Iteration 3133/3560 Training loss: 1.6797 0.3720 sec/batch\n",
      "Epoch 18/20  Iteration 3134/3560 Training loss: 1.6797 0.4137 sec/batch\n",
      "Epoch 18/20  Iteration 3135/3560 Training loss: 1.6797 0.3679 sec/batch\n",
      "Epoch 18/20  Iteration 3136/3560 Training loss: 1.6797 0.4638 sec/batch\n",
      "Epoch 18/20  Iteration 3137/3560 Training loss: 1.6797 0.4245 sec/batch\n",
      "Epoch 18/20  Iteration 3138/3560 Training loss: 1.6796 0.3692 sec/batch\n",
      "Epoch 18/20  Iteration 3139/3560 Training loss: 1.6795 0.4941 sec/batch\n",
      "Epoch 18/20  Iteration 3140/3560 Training loss: 1.6793 0.3971 sec/batch\n",
      "Epoch 18/20  Iteration 3141/3560 Training loss: 1.6791 0.3952 sec/batch\n",
      "Epoch 18/20  Iteration 3142/3560 Training loss: 1.6787 0.4189 sec/batch\n",
      "Epoch 18/20  Iteration 3143/3560 Training loss: 1.6787 0.3954 sec/batch\n",
      "Epoch 18/20  Iteration 3144/3560 Training loss: 1.6787 0.3984 sec/batch\n",
      "Epoch 18/20  Iteration 3145/3560 Training loss: 1.6787 0.4169 sec/batch\n",
      "Epoch 18/20  Iteration 3146/3560 Training loss: 1.6786 0.4647 sec/batch\n",
      "Epoch 18/20  Iteration 3147/3560 Training loss: 1.6786 0.4657 sec/batch\n",
      "Epoch 18/20  Iteration 3148/3560 Training loss: 1.6783 0.4309 sec/batch\n",
      "Epoch 18/20  Iteration 3149/3560 Training loss: 1.6780 0.4752 sec/batch\n",
      "Epoch 18/20  Iteration 3150/3560 Training loss: 1.6780 0.3984 sec/batch\n",
      "Epoch 18/20  Iteration 3151/3560 Training loss: 1.6779 0.3792 sec/batch\n",
      "Epoch 18/20  Iteration 3152/3560 Training loss: 1.6775 0.3804 sec/batch\n",
      "Epoch 18/20  Iteration 3153/3560 Training loss: 1.6776 0.4412 sec/batch\n",
      "Epoch 18/20  Iteration 3154/3560 Training loss: 1.6777 0.4550 sec/batch\n",
      "Epoch 18/20  Iteration 3155/3560 Training loss: 1.6776 0.4009 sec/batch\n",
      "Epoch 18/20  Iteration 3156/3560 Training loss: 1.6774 0.4090 sec/batch\n",
      "Epoch 18/20  Iteration 3157/3560 Training loss: 1.6771 0.4140 sec/batch\n",
      "Epoch 18/20  Iteration 3158/3560 Training loss: 1.6769 0.4959 sec/batch\n",
      "Epoch 18/20  Iteration 3159/3560 Training loss: 1.6770 0.4371 sec/batch\n",
      "Epoch 18/20  Iteration 3160/3560 Training loss: 1.6771 0.4091 sec/batch\n",
      "Epoch 18/20  Iteration 3161/3560 Training loss: 1.6771 0.4088 sec/batch\n",
      "Epoch 18/20  Iteration 3162/3560 Training loss: 1.6771 0.4291 sec/batch\n",
      "Epoch 18/20  Iteration 3163/3560 Training loss: 1.6773 0.3823 sec/batch\n",
      "Epoch 18/20  Iteration 3164/3560 Training loss: 1.6774 0.4446 sec/batch\n",
      "Epoch 18/20  Iteration 3165/3560 Training loss: 1.6775 0.4127 sec/batch\n",
      "Epoch 18/20  Iteration 3166/3560 Training loss: 1.6774 0.4621 sec/batch\n",
      "Epoch 18/20  Iteration 3167/3560 Training loss: 1.6777 0.4201 sec/batch\n",
      "Epoch 18/20  Iteration 3168/3560 Training loss: 1.6776 0.4285 sec/batch\n",
      "Epoch 18/20  Iteration 3169/3560 Training loss: 1.6775 0.3838 sec/batch\n",
      "Epoch 18/20  Iteration 3170/3560 Training loss: 1.6777 0.3796 sec/batch\n",
      "Epoch 18/20  Iteration 3171/3560 Training loss: 1.6775 0.3949 sec/batch\n",
      "Epoch 18/20  Iteration 3172/3560 Training loss: 1.6776 0.3875 sec/batch\n",
      "Epoch 18/20  Iteration 3173/3560 Training loss: 1.6777 0.3602 sec/batch\n",
      "Epoch 18/20  Iteration 3174/3560 Training loss: 1.6779 0.4645 sec/batch\n",
      "Epoch 18/20  Iteration 3175/3560 Training loss: 1.6780 0.4274 sec/batch\n",
      "Epoch 18/20  Iteration 3176/3560 Training loss: 1.6779 0.4406 sec/batch\n",
      "Epoch 18/20  Iteration 3177/3560 Training loss: 1.6776 0.3977 sec/batch\n",
      "Epoch 18/20  Iteration 3178/3560 Training loss: 1.6776 0.4051 sec/batch\n",
      "Epoch 18/20  Iteration 3179/3560 Training loss: 1.6777 0.4555 sec/batch\n",
      "Epoch 18/20  Iteration 3180/3560 Training loss: 1.6778 0.3784 sec/batch\n",
      "Epoch 18/20  Iteration 3181/3560 Training loss: 1.6778 0.4112 sec/batch\n",
      "Epoch 18/20  Iteration 3182/3560 Training loss: 1.6778 0.3836 sec/batch\n",
      "Epoch 18/20  Iteration 3183/3560 Training loss: 1.6779 0.4287 sec/batch\n",
      "Epoch 18/20  Iteration 3184/3560 Training loss: 1.6779 0.4143 sec/batch\n",
      "Epoch 18/20  Iteration 3185/3560 Training loss: 1.6778 0.4305 sec/batch\n",
      "Epoch 18/20  Iteration 3186/3560 Training loss: 1.6780 0.4427 sec/batch\n",
      "Epoch 18/20  Iteration 3187/3560 Training loss: 1.6782 0.4016 sec/batch\n",
      "Epoch 18/20  Iteration 3188/3560 Training loss: 1.6782 0.4166 sec/batch\n",
      "Epoch 18/20  Iteration 3189/3560 Training loss: 1.6783 0.4104 sec/batch\n",
      "Epoch 18/20  Iteration 3190/3560 Training loss: 1.6783 0.3709 sec/batch\n",
      "Epoch 18/20  Iteration 3191/3560 Training loss: 1.6783 0.4065 sec/batch\n",
      "Epoch 18/20  Iteration 3192/3560 Training loss: 1.6783 0.3580 sec/batch\n",
      "Epoch 18/20  Iteration 3193/3560 Training loss: 1.6785 0.4154 sec/batch\n",
      "Epoch 18/20  Iteration 3194/3560 Training loss: 1.6789 0.4238 sec/batch\n",
      "Epoch 18/20  Iteration 3195/3560 Training loss: 1.6788 0.4364 sec/batch\n",
      "Epoch 18/20  Iteration 3196/3560 Training loss: 1.6788 0.4318 sec/batch\n",
      "Epoch 18/20  Iteration 3197/3560 Training loss: 1.6788 0.4236 sec/batch\n",
      "Epoch 18/20  Iteration 3198/3560 Training loss: 1.6786 0.4105 sec/batch\n",
      "Epoch 18/20  Iteration 3199/3560 Training loss: 1.6787 0.4363 sec/batch\n",
      "Epoch 18/20  Iteration 3200/3560 Training loss: 1.6787 0.4240 sec/batch\n",
      "Epoch 18/20  Iteration 3201/3560 Training loss: 1.6788 0.3892 sec/batch\n",
      "Epoch 18/20  Iteration 3202/3560 Training loss: 1.6787 0.4625 sec/batch\n",
      "Epoch 18/20  Iteration 3203/3560 Training loss: 1.6786 0.4201 sec/batch\n",
      "Epoch 18/20  Iteration 3204/3560 Training loss: 1.6787 0.4698 sec/batch\n",
      "Epoch 19/20  Iteration 3205/3560 Training loss: 1.7561 0.4839 sec/batch\n",
      "Epoch 19/20  Iteration 3206/3560 Training loss: 1.7202 0.4477 sec/batch\n",
      "Epoch 19/20  Iteration 3207/3560 Training loss: 1.6996 0.3799 sec/batch\n",
      "Epoch 19/20  Iteration 3208/3560 Training loss: 1.6960 0.3758 sec/batch\n",
      "Epoch 19/20  Iteration 3209/3560 Training loss: 1.6896 0.3854 sec/batch\n",
      "Epoch 19/20  Iteration 3210/3560 Training loss: 1.6818 0.4080 sec/batch\n",
      "Epoch 19/20  Iteration 3211/3560 Training loss: 1.6831 0.3891 sec/batch\n",
      "Epoch 19/20  Iteration 3212/3560 Training loss: 1.6820 0.3825 sec/batch\n",
      "Epoch 19/20  Iteration 3213/3560 Training loss: 1.6833 0.4208 sec/batch\n",
      "Epoch 19/20  Iteration 3214/3560 Training loss: 1.6806 0.3931 sec/batch\n",
      "Epoch 19/20  Iteration 3215/3560 Training loss: 1.6764 0.4799 sec/batch\n",
      "Epoch 19/20  Iteration 3216/3560 Training loss: 1.6755 0.4907 sec/batch\n",
      "Epoch 19/20  Iteration 3217/3560 Training loss: 1.6755 0.4825 sec/batch\n",
      "Epoch 19/20  Iteration 3218/3560 Training loss: 1.6781 0.5257 sec/batch\n",
      "Epoch 19/20  Iteration 3219/3560 Training loss: 1.6775 0.4889 sec/batch\n",
      "Epoch 19/20  Iteration 3220/3560 Training loss: 1.6756 0.5788 sec/batch\n",
      "Epoch 19/20  Iteration 3221/3560 Training loss: 1.6759 0.5193 sec/batch\n",
      "Epoch 19/20  Iteration 3222/3560 Training loss: 1.6775 0.5647 sec/batch\n",
      "Epoch 19/20  Iteration 3223/3560 Training loss: 1.6779 0.6268 sec/batch\n",
      "Epoch 19/20  Iteration 3224/3560 Training loss: 1.6785 0.5088 sec/batch\n",
      "Epoch 19/20  Iteration 3225/3560 Training loss: 1.6781 0.5169 sec/batch\n",
      "Epoch 19/20  Iteration 3226/3560 Training loss: 1.6783 0.4539 sec/batch\n",
      "Epoch 19/20  Iteration 3227/3560 Training loss: 1.6775 0.3801 sec/batch\n",
      "Epoch 19/20  Iteration 3228/3560 Training loss: 1.6772 0.4094 sec/batch\n",
      "Epoch 19/20  Iteration 3229/3560 Training loss: 1.6769 0.4229 sec/batch\n",
      "Epoch 19/20  Iteration 3230/3560 Training loss: 1.6757 0.3897 sec/batch\n",
      "Epoch 19/20  Iteration 3231/3560 Training loss: 1.6748 0.4359 sec/batch\n",
      "Epoch 19/20  Iteration 3232/3560 Training loss: 1.6750 0.4063 sec/batch\n",
      "Epoch 19/20  Iteration 3233/3560 Training loss: 1.6761 0.4508 sec/batch\n",
      "Epoch 19/20  Iteration 3234/3560 Training loss: 1.6766 0.4804 sec/batch\n",
      "Epoch 19/20  Iteration 3235/3560 Training loss: 1.6766 0.3802 sec/batch\n",
      "Epoch 19/20  Iteration 3236/3560 Training loss: 1.6760 0.3925 sec/batch\n",
      "Epoch 19/20  Iteration 3237/3560 Training loss: 1.6764 0.3995 sec/batch\n",
      "Epoch 19/20  Iteration 3238/3560 Training loss: 1.6770 0.4554 sec/batch\n",
      "Epoch 19/20  Iteration 3239/3560 Training loss: 1.6766 0.4166 sec/batch\n",
      "Epoch 19/20  Iteration 3240/3560 Training loss: 1.6762 0.3894 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3241/3560 Training loss: 1.6756 0.4243 sec/batch\n",
      "Epoch 19/20  Iteration 3242/3560 Training loss: 1.6747 0.3944 sec/batch\n",
      "Epoch 19/20  Iteration 3243/3560 Training loss: 1.6732 0.4390 sec/batch\n",
      "Epoch 19/20  Iteration 3244/3560 Training loss: 1.6726 0.4246 sec/batch\n",
      "Epoch 19/20  Iteration 3245/3560 Training loss: 1.6723 0.4266 sec/batch\n",
      "Epoch 19/20  Iteration 3246/3560 Training loss: 1.6728 0.4256 sec/batch\n",
      "Epoch 19/20  Iteration 3247/3560 Training loss: 1.6724 0.3906 sec/batch\n",
      "Epoch 19/20  Iteration 3248/3560 Training loss: 1.6718 0.3907 sec/batch\n",
      "Epoch 19/20  Iteration 3249/3560 Training loss: 1.6720 0.4051 sec/batch\n",
      "Epoch 19/20  Iteration 3250/3560 Training loss: 1.6710 0.4339 sec/batch\n",
      "Epoch 19/20  Iteration 3251/3560 Training loss: 1.6706 0.4156 sec/batch\n",
      "Epoch 19/20  Iteration 3252/3560 Training loss: 1.6701 0.4348 sec/batch\n",
      "Epoch 19/20  Iteration 3253/3560 Training loss: 1.6701 0.4281 sec/batch\n",
      "Epoch 19/20  Iteration 3254/3560 Training loss: 1.6708 0.4069 sec/batch\n",
      "Epoch 19/20  Iteration 3255/3560 Training loss: 1.6703 0.4306 sec/batch\n",
      "Epoch 19/20  Iteration 3256/3560 Training loss: 1.6709 0.3781 sec/batch\n",
      "Epoch 19/20  Iteration 3257/3560 Training loss: 1.6710 0.4109 sec/batch\n",
      "Epoch 19/20  Iteration 3258/3560 Training loss: 1.6711 0.3828 sec/batch\n",
      "Epoch 19/20  Iteration 3259/3560 Training loss: 1.6707 0.4268 sec/batch\n",
      "Epoch 19/20  Iteration 3260/3560 Training loss: 1.6708 0.4589 sec/batch\n",
      "Epoch 19/20  Iteration 3261/3560 Training loss: 1.6712 0.4037 sec/batch\n",
      "Epoch 19/20  Iteration 3262/3560 Training loss: 1.6707 0.4481 sec/batch\n",
      "Epoch 19/20  Iteration 3263/3560 Training loss: 1.6702 0.5145 sec/batch\n",
      "Epoch 19/20  Iteration 3264/3560 Training loss: 1.6709 0.3495 sec/batch\n",
      "Epoch 19/20  Iteration 3265/3560 Training loss: 1.6707 0.3962 sec/batch\n",
      "Epoch 19/20  Iteration 3266/3560 Training loss: 1.6717 0.3924 sec/batch\n",
      "Epoch 19/20  Iteration 3267/3560 Training loss: 1.6721 0.4122 sec/batch\n",
      "Epoch 19/20  Iteration 3268/3560 Training loss: 1.6722 0.4372 sec/batch\n",
      "Epoch 19/20  Iteration 3269/3560 Training loss: 1.6723 0.4361 sec/batch\n",
      "Epoch 19/20  Iteration 3270/3560 Training loss: 1.6727 0.5130 sec/batch\n",
      "Epoch 19/20  Iteration 3271/3560 Training loss: 1.6729 0.4452 sec/batch\n",
      "Epoch 19/20  Iteration 3272/3560 Training loss: 1.6726 0.3954 sec/batch\n",
      "Epoch 19/20  Iteration 3273/3560 Training loss: 1.6725 0.4716 sec/batch\n",
      "Epoch 19/20  Iteration 3274/3560 Training loss: 1.6726 0.4050 sec/batch\n",
      "Epoch 19/20  Iteration 3275/3560 Training loss: 1.6732 0.4033 sec/batch\n",
      "Epoch 19/20  Iteration 3276/3560 Training loss: 1.6735 0.4189 sec/batch\n",
      "Epoch 19/20  Iteration 3277/3560 Training loss: 1.6739 0.4146 sec/batch\n",
      "Epoch 19/20  Iteration 3278/3560 Training loss: 1.6737 0.3838 sec/batch\n",
      "Epoch 19/20  Iteration 3279/3560 Training loss: 1.6737 0.4197 sec/batch\n",
      "Epoch 19/20  Iteration 3280/3560 Training loss: 1.6740 0.4305 sec/batch\n",
      "Epoch 19/20  Iteration 3281/3560 Training loss: 1.6738 0.4183 sec/batch\n",
      "Epoch 19/20  Iteration 3282/3560 Training loss: 1.6739 0.4217 sec/batch\n",
      "Epoch 19/20  Iteration 3283/3560 Training loss: 1.6735 0.4212 sec/batch\n",
      "Epoch 19/20  Iteration 3284/3560 Training loss: 1.6733 0.3661 sec/batch\n",
      "Epoch 19/20  Iteration 3285/3560 Training loss: 1.6728 0.4390 sec/batch\n",
      "Epoch 19/20  Iteration 3286/3560 Training loss: 1.6728 0.3623 sec/batch\n",
      "Epoch 19/20  Iteration 3287/3560 Training loss: 1.6724 0.4250 sec/batch\n",
      "Epoch 19/20  Iteration 3288/3560 Training loss: 1.6724 0.3852 sec/batch\n",
      "Epoch 19/20  Iteration 3289/3560 Training loss: 1.6720 0.4476 sec/batch\n",
      "Epoch 19/20  Iteration 3290/3560 Training loss: 1.6716 0.4542 sec/batch\n",
      "Epoch 19/20  Iteration 3291/3560 Training loss: 1.6714 0.4606 sec/batch\n",
      "Epoch 19/20  Iteration 3292/3560 Training loss: 1.6711 0.4332 sec/batch\n",
      "Epoch 19/20  Iteration 3293/3560 Training loss: 1.6707 0.3681 sec/batch\n",
      "Epoch 19/20  Iteration 3294/3560 Training loss: 1.6707 0.3862 sec/batch\n",
      "Epoch 19/20  Iteration 3295/3560 Training loss: 1.6704 0.4543 sec/batch\n",
      "Epoch 19/20  Iteration 3296/3560 Training loss: 1.6702 0.4101 sec/batch\n",
      "Epoch 19/20  Iteration 3297/3560 Training loss: 1.6698 0.4397 sec/batch\n",
      "Epoch 19/20  Iteration 3298/3560 Training loss: 1.6695 0.4609 sec/batch\n",
      "Epoch 19/20  Iteration 3299/3560 Training loss: 1.6692 0.4232 sec/batch\n",
      "Epoch 19/20  Iteration 3300/3560 Training loss: 1.6692 0.4058 sec/batch\n",
      "Epoch 19/20  Iteration 3301/3560 Training loss: 1.6691 0.4364 sec/batch\n",
      "Epoch 19/20  Iteration 3302/3560 Training loss: 1.6687 0.3728 sec/batch\n",
      "Epoch 19/20  Iteration 3303/3560 Training loss: 1.6683 0.3715 sec/batch\n",
      "Epoch 19/20  Iteration 3304/3560 Training loss: 1.6680 0.3994 sec/batch\n",
      "Epoch 19/20  Iteration 3305/3560 Training loss: 1.6680 0.3989 sec/batch\n",
      "Epoch 19/20  Iteration 3306/3560 Training loss: 1.6679 0.3736 sec/batch\n",
      "Epoch 19/20  Iteration 3307/3560 Training loss: 1.6678 0.3990 sec/batch\n",
      "Epoch 19/20  Iteration 3308/3560 Training loss: 1.6676 0.4003 sec/batch\n",
      "Epoch 19/20  Iteration 3309/3560 Training loss: 1.6674 0.3977 sec/batch\n",
      "Epoch 19/20  Iteration 3310/3560 Training loss: 1.6672 0.4620 sec/batch\n",
      "Epoch 19/20  Iteration 3311/3560 Training loss: 1.6672 0.4517 sec/batch\n",
      "Epoch 19/20  Iteration 3312/3560 Training loss: 1.6672 0.3974 sec/batch\n",
      "Epoch 19/20  Iteration 3313/3560 Training loss: 1.6672 0.4035 sec/batch\n",
      "Epoch 19/20  Iteration 3314/3560 Training loss: 1.6672 0.4071 sec/batch\n",
      "Epoch 19/20  Iteration 3315/3560 Training loss: 1.6671 0.4110 sec/batch\n",
      "Epoch 19/20  Iteration 3316/3560 Training loss: 1.6669 0.3883 sec/batch\n",
      "Epoch 19/20  Iteration 3317/3560 Training loss: 1.6669 0.4157 sec/batch\n",
      "Epoch 19/20  Iteration 3318/3560 Training loss: 1.6668 0.4885 sec/batch\n",
      "Epoch 19/20  Iteration 3319/3560 Training loss: 1.6665 0.4406 sec/batch\n",
      "Epoch 19/20  Iteration 3320/3560 Training loss: 1.6661 0.4875 sec/batch\n",
      "Epoch 19/20  Iteration 3321/3560 Training loss: 1.6662 0.4268 sec/batch\n",
      "Epoch 19/20  Iteration 3322/3560 Training loss: 1.6662 0.3760 sec/batch\n",
      "Epoch 19/20  Iteration 3323/3560 Training loss: 1.6662 0.3583 sec/batch\n",
      "Epoch 19/20  Iteration 3324/3560 Training loss: 1.6662 0.3926 sec/batch\n",
      "Epoch 19/20  Iteration 3325/3560 Training loss: 1.6662 0.4323 sec/batch\n",
      "Epoch 19/20  Iteration 3326/3560 Training loss: 1.6658 0.4180 sec/batch\n",
      "Epoch 19/20  Iteration 3327/3560 Training loss: 1.6655 0.4193 sec/batch\n",
      "Epoch 19/20  Iteration 3328/3560 Training loss: 1.6655 0.4131 sec/batch\n",
      "Epoch 19/20  Iteration 3329/3560 Training loss: 1.6656 0.4497 sec/batch\n",
      "Epoch 19/20  Iteration 3330/3560 Training loss: 1.6652 0.3908 sec/batch\n",
      "Epoch 19/20  Iteration 3331/3560 Training loss: 1.6654 0.4105 sec/batch\n",
      "Epoch 19/20  Iteration 3332/3560 Training loss: 1.6655 0.4253 sec/batch\n",
      "Epoch 19/20  Iteration 3333/3560 Training loss: 1.6654 0.3886 sec/batch\n",
      "Epoch 19/20  Iteration 3334/3560 Training loss: 1.6651 0.4056 sec/batch\n",
      "Epoch 19/20  Iteration 3335/3560 Training loss: 1.6648 0.3893 sec/batch\n",
      "Epoch 19/20  Iteration 3336/3560 Training loss: 1.6646 0.4219 sec/batch\n",
      "Epoch 19/20  Iteration 3337/3560 Training loss: 1.6647 0.4494 sec/batch\n",
      "Epoch 19/20  Iteration 3338/3560 Training loss: 1.6648 0.4028 sec/batch\n",
      "Epoch 19/20  Iteration 3339/3560 Training loss: 1.6649 0.4209 sec/batch\n",
      "Epoch 19/20  Iteration 3340/3560 Training loss: 1.6650 0.4316 sec/batch\n",
      "Epoch 19/20  Iteration 3341/3560 Training loss: 1.6651 0.3976 sec/batch\n",
      "Epoch 19/20  Iteration 3342/3560 Training loss: 1.6652 0.4249 sec/batch\n",
      "Epoch 19/20  Iteration 3343/3560 Training loss: 1.6653 0.4142 sec/batch\n",
      "Epoch 19/20  Iteration 3344/3560 Training loss: 1.6651 0.4144 sec/batch\n",
      "Epoch 19/20  Iteration 3345/3560 Training loss: 1.6654 0.4133 sec/batch\n",
      "Epoch 19/20  Iteration 3346/3560 Training loss: 1.6654 0.4365 sec/batch\n",
      "Epoch 19/20  Iteration 3347/3560 Training loss: 1.6653 0.4091 sec/batch\n",
      "Epoch 19/20  Iteration 3348/3560 Training loss: 1.6655 0.4150 sec/batch\n",
      "Epoch 19/20  Iteration 3349/3560 Training loss: 1.6654 0.4045 sec/batch\n",
      "Epoch 19/20  Iteration 3350/3560 Training loss: 1.6655 0.4391 sec/batch\n",
      "Epoch 19/20  Iteration 3351/3560 Training loss: 1.6656 0.3700 sec/batch\n",
      "Epoch 19/20  Iteration 3352/3560 Training loss: 1.6657 0.3710 sec/batch\n",
      "Epoch 19/20  Iteration 3353/3560 Training loss: 1.6659 0.3798 sec/batch\n",
      "Epoch 19/20  Iteration 3354/3560 Training loss: 1.6657 0.4150 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3355/3560 Training loss: 1.6656 0.4798 sec/batch\n",
      "Epoch 19/20  Iteration 3356/3560 Training loss: 1.6656 0.4951 sec/batch\n",
      "Epoch 19/20  Iteration 3357/3560 Training loss: 1.6658 0.4545 sec/batch\n",
      "Epoch 19/20  Iteration 3358/3560 Training loss: 1.6659 0.4488 sec/batch\n",
      "Epoch 19/20  Iteration 3359/3560 Training loss: 1.6659 0.4875 sec/batch\n",
      "Epoch 19/20  Iteration 3360/3560 Training loss: 1.6659 0.4086 sec/batch\n",
      "Epoch 19/20  Iteration 3361/3560 Training loss: 1.6660 0.3569 sec/batch\n",
      "Epoch 19/20  Iteration 3362/3560 Training loss: 1.6660 0.4242 sec/batch\n",
      "Epoch 19/20  Iteration 3363/3560 Training loss: 1.6658 0.3917 sec/batch\n",
      "Epoch 19/20  Iteration 3364/3560 Training loss: 1.6660 0.3861 sec/batch\n",
      "Epoch 19/20  Iteration 3365/3560 Training loss: 1.6662 0.4671 sec/batch\n",
      "Epoch 19/20  Iteration 3366/3560 Training loss: 1.6661 0.3960 sec/batch\n",
      "Epoch 19/20  Iteration 3367/3560 Training loss: 1.6663 0.4197 sec/batch\n",
      "Epoch 19/20  Iteration 3368/3560 Training loss: 1.6663 0.5404 sec/batch\n",
      "Epoch 19/20  Iteration 3369/3560 Training loss: 1.6663 0.4514 sec/batch\n",
      "Epoch 19/20  Iteration 3370/3560 Training loss: 1.6663 0.3788 sec/batch\n",
      "Epoch 19/20  Iteration 3371/3560 Training loss: 1.6664 0.3765 sec/batch\n",
      "Epoch 19/20  Iteration 3372/3560 Training loss: 1.6669 0.3904 sec/batch\n",
      "Epoch 19/20  Iteration 3373/3560 Training loss: 1.6668 0.4140 sec/batch\n",
      "Epoch 19/20  Iteration 3374/3560 Training loss: 1.6668 0.4413 sec/batch\n",
      "Epoch 19/20  Iteration 3375/3560 Training loss: 1.6667 0.4014 sec/batch\n",
      "Epoch 19/20  Iteration 3376/3560 Training loss: 1.6666 0.3971 sec/batch\n",
      "Epoch 19/20  Iteration 3377/3560 Training loss: 1.6667 0.4885 sec/batch\n",
      "Epoch 19/20  Iteration 3378/3560 Training loss: 1.6667 0.3943 sec/batch\n",
      "Epoch 19/20  Iteration 3379/3560 Training loss: 1.6668 0.4159 sec/batch\n",
      "Epoch 19/20  Iteration 3380/3560 Training loss: 1.6667 0.3749 sec/batch\n",
      "Epoch 19/20  Iteration 3381/3560 Training loss: 1.6666 0.3937 sec/batch\n",
      "Epoch 19/20  Iteration 3382/3560 Training loss: 1.6667 0.3836 sec/batch\n",
      "Epoch 20/20  Iteration 3383/3560 Training loss: 1.7432 0.3984 sec/batch\n",
      "Epoch 20/20  Iteration 3384/3560 Training loss: 1.7053 0.4500 sec/batch\n",
      "Epoch 20/20  Iteration 3385/3560 Training loss: 1.6912 0.4799 sec/batch\n",
      "Epoch 20/20  Iteration 3386/3560 Training loss: 1.6864 0.3606 sec/batch\n",
      "Epoch 20/20  Iteration 3387/3560 Training loss: 1.6780 0.4546 sec/batch\n",
      "Epoch 20/20  Iteration 3388/3560 Training loss: 1.6699 0.3874 sec/batch\n",
      "Epoch 20/20  Iteration 3389/3560 Training loss: 1.6707 0.4583 sec/batch\n",
      "Epoch 20/20  Iteration 3390/3560 Training loss: 1.6683 0.4004 sec/batch\n",
      "Epoch 20/20  Iteration 3391/3560 Training loss: 1.6690 0.3846 sec/batch\n",
      "Epoch 20/20  Iteration 3392/3560 Training loss: 1.6679 0.4518 sec/batch\n",
      "Epoch 20/20  Iteration 3393/3560 Training loss: 1.6647 0.3899 sec/batch\n",
      "Epoch 20/20  Iteration 3394/3560 Training loss: 1.6640 0.4171 sec/batch\n",
      "Epoch 20/20  Iteration 3395/3560 Training loss: 1.6641 0.4383 sec/batch\n",
      "Epoch 20/20  Iteration 3396/3560 Training loss: 1.6661 0.4238 sec/batch\n",
      "Epoch 20/20  Iteration 3397/3560 Training loss: 1.6653 0.3886 sec/batch\n",
      "Epoch 20/20  Iteration 3398/3560 Training loss: 1.6629 0.4295 sec/batch\n",
      "Epoch 20/20  Iteration 3399/3560 Training loss: 1.6632 0.3928 sec/batch\n",
      "Epoch 20/20  Iteration 3400/3560 Training loss: 1.6649 0.4301 sec/batch\n",
      "Epoch 20/20  Iteration 3401/3560 Training loss: 1.6654 0.3375 sec/batch\n",
      "Epoch 20/20  Iteration 3402/3560 Training loss: 1.6664 0.3843 sec/batch\n",
      "Epoch 20/20  Iteration 3403/3560 Training loss: 1.6659 0.4985 sec/batch\n",
      "Epoch 20/20  Iteration 3404/3560 Training loss: 1.6660 0.4339 sec/batch\n",
      "Epoch 20/20  Iteration 3405/3560 Training loss: 1.6653 0.4457 sec/batch\n",
      "Epoch 20/20  Iteration 3406/3560 Training loss: 1.6648 0.3755 sec/batch\n",
      "Epoch 20/20  Iteration 3407/3560 Training loss: 1.6648 0.4555 sec/batch\n",
      "Epoch 20/20  Iteration 3408/3560 Training loss: 1.6633 0.3980 sec/batch\n",
      "Epoch 20/20  Iteration 3409/3560 Training loss: 1.6620 0.4248 sec/batch\n",
      "Epoch 20/20  Iteration 3410/3560 Training loss: 1.6628 0.3517 sec/batch\n",
      "Epoch 20/20  Iteration 3411/3560 Training loss: 1.6633 0.3856 sec/batch\n",
      "Epoch 20/20  Iteration 3412/3560 Training loss: 1.6638 0.3824 sec/batch\n",
      "Epoch 20/20  Iteration 3413/3560 Training loss: 1.6634 0.4721 sec/batch\n",
      "Epoch 20/20  Iteration 3414/3560 Training loss: 1.6630 0.4110 sec/batch\n",
      "Epoch 20/20  Iteration 3415/3560 Training loss: 1.6636 0.4338 sec/batch\n",
      "Epoch 20/20  Iteration 3416/3560 Training loss: 1.6641 0.4222 sec/batch\n",
      "Epoch 20/20  Iteration 3417/3560 Training loss: 1.6641 0.4170 sec/batch\n",
      "Epoch 20/20  Iteration 3418/3560 Training loss: 1.6637 0.4252 sec/batch\n",
      "Epoch 20/20  Iteration 3419/3560 Training loss: 1.6628 0.3972 sec/batch\n",
      "Epoch 20/20  Iteration 3420/3560 Training loss: 1.6619 0.3938 sec/batch\n",
      "Epoch 20/20  Iteration 3421/3560 Training loss: 1.6608 0.4266 sec/batch\n",
      "Epoch 20/20  Iteration 3422/3560 Training loss: 1.6603 0.3957 sec/batch\n",
      "Epoch 20/20  Iteration 3423/3560 Training loss: 1.6600 0.4275 sec/batch\n",
      "Epoch 20/20  Iteration 3424/3560 Training loss: 1.6606 0.4495 sec/batch\n",
      "Epoch 20/20  Iteration 3425/3560 Training loss: 1.6602 0.4086 sec/batch\n",
      "Epoch 20/20  Iteration 3426/3560 Training loss: 1.6596 0.4856 sec/batch\n",
      "Epoch 20/20  Iteration 3427/3560 Training loss: 1.6599 0.3846 sec/batch\n",
      "Epoch 20/20  Iteration 3428/3560 Training loss: 1.6589 0.4383 sec/batch\n",
      "Epoch 20/20  Iteration 3429/3560 Training loss: 1.6587 0.4049 sec/batch\n",
      "Epoch 20/20  Iteration 3430/3560 Training loss: 1.6582 0.4686 sec/batch\n",
      "Epoch 20/20  Iteration 3431/3560 Training loss: 1.6581 0.4080 sec/batch\n",
      "Epoch 20/20  Iteration 3432/3560 Training loss: 1.6588 0.4377 sec/batch\n",
      "Epoch 20/20  Iteration 3433/3560 Training loss: 1.6583 0.3732 sec/batch\n",
      "Epoch 20/20  Iteration 3434/3560 Training loss: 1.6592 0.4932 sec/batch\n",
      "Epoch 20/20  Iteration 3435/3560 Training loss: 1.6591 0.3995 sec/batch\n",
      "Epoch 20/20  Iteration 3436/3560 Training loss: 1.6591 0.4160 sec/batch\n",
      "Epoch 20/20  Iteration 3437/3560 Training loss: 1.6587 0.4229 sec/batch\n",
      "Epoch 20/20  Iteration 3438/3560 Training loss: 1.6590 0.3906 sec/batch\n",
      "Epoch 20/20  Iteration 3439/3560 Training loss: 1.6594 0.4571 sec/batch\n",
      "Epoch 20/20  Iteration 3440/3560 Training loss: 1.6590 0.4044 sec/batch\n",
      "Epoch 20/20  Iteration 3441/3560 Training loss: 1.6586 0.4315 sec/batch\n",
      "Epoch 20/20  Iteration 3442/3560 Training loss: 1.6590 0.4356 sec/batch\n",
      "Epoch 20/20  Iteration 3443/3560 Training loss: 1.6588 0.4027 sec/batch\n",
      "Epoch 20/20  Iteration 3444/3560 Training loss: 1.6597 0.4790 sec/batch\n",
      "Epoch 20/20  Iteration 3445/3560 Training loss: 1.6600 0.4311 sec/batch\n",
      "Epoch 20/20  Iteration 3446/3560 Training loss: 1.6601 0.4121 sec/batch\n",
      "Epoch 20/20  Iteration 3447/3560 Training loss: 1.6599 0.4219 sec/batch\n",
      "Epoch 20/20  Iteration 3448/3560 Training loss: 1.6600 0.4060 sec/batch\n",
      "Epoch 20/20  Iteration 3449/3560 Training loss: 1.6602 0.4606 sec/batch\n",
      "Epoch 20/20  Iteration 3450/3560 Training loss: 1.6600 0.4071 sec/batch\n",
      "Epoch 20/20  Iteration 3451/3560 Training loss: 1.6602 0.3799 sec/batch\n",
      "Epoch 20/20  Iteration 3452/3560 Training loss: 1.6600 0.5306 sec/batch\n",
      "Epoch 20/20  Iteration 3453/3560 Training loss: 1.6606 0.4930 sec/batch\n",
      "Epoch 20/20  Iteration 3454/3560 Training loss: 1.6609 0.3831 sec/batch\n",
      "Epoch 20/20  Iteration 3455/3560 Training loss: 1.6614 0.5307 sec/batch\n",
      "Epoch 20/20  Iteration 3456/3560 Training loss: 1.6612 0.4213 sec/batch\n",
      "Epoch 20/20  Iteration 3457/3560 Training loss: 1.6612 0.3753 sec/batch\n",
      "Epoch 20/20  Iteration 3458/3560 Training loss: 1.6614 0.3978 sec/batch\n",
      "Epoch 20/20  Iteration 3459/3560 Training loss: 1.6612 0.3817 sec/batch\n",
      "Epoch 20/20  Iteration 3460/3560 Training loss: 1.6612 0.4702 sec/batch\n",
      "Epoch 20/20  Iteration 3461/3560 Training loss: 1.6608 0.4302 sec/batch\n",
      "Epoch 20/20  Iteration 3462/3560 Training loss: 1.6608 0.4191 sec/batch\n",
      "Epoch 20/20  Iteration 3463/3560 Training loss: 1.6603 0.4551 sec/batch\n",
      "Epoch 20/20  Iteration 3464/3560 Training loss: 1.6603 0.4611 sec/batch\n",
      "Epoch 20/20  Iteration 3465/3560 Training loss: 1.6599 0.4348 sec/batch\n",
      "Epoch 20/20  Iteration 3466/3560 Training loss: 1.6599 0.4230 sec/batch\n",
      "Epoch 20/20  Iteration 3467/3560 Training loss: 1.6595 0.4147 sec/batch\n",
      "Epoch 20/20  Iteration 3468/3560 Training loss: 1.6593 0.4080 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3469/3560 Training loss: 1.6591 0.3942 sec/batch\n",
      "Epoch 20/20  Iteration 3470/3560 Training loss: 1.6589 0.3973 sec/batch\n",
      "Epoch 20/20  Iteration 3471/3560 Training loss: 1.6584 0.4432 sec/batch\n",
      "Epoch 20/20  Iteration 3472/3560 Training loss: 1.6585 0.4591 sec/batch\n",
      "Epoch 20/20  Iteration 3473/3560 Training loss: 1.6582 0.4498 sec/batch\n",
      "Epoch 20/20  Iteration 3474/3560 Training loss: 1.6581 0.4303 sec/batch\n",
      "Epoch 20/20  Iteration 3475/3560 Training loss: 1.6577 0.4059 sec/batch\n",
      "Epoch 20/20  Iteration 3476/3560 Training loss: 1.6574 0.3929 sec/batch\n",
      "Epoch 20/20  Iteration 3477/3560 Training loss: 1.6571 0.4395 sec/batch\n",
      "Epoch 20/20  Iteration 3478/3560 Training loss: 1.6570 0.3637 sec/batch\n",
      "Epoch 20/20  Iteration 3479/3560 Training loss: 1.6571 0.4695 sec/batch\n",
      "Epoch 20/20  Iteration 3480/3560 Training loss: 1.6566 0.3808 sec/batch\n",
      "Epoch 20/20  Iteration 3481/3560 Training loss: 1.6563 0.4522 sec/batch\n",
      "Epoch 20/20  Iteration 3482/3560 Training loss: 1.6559 0.3985 sec/batch\n",
      "Epoch 20/20  Iteration 3483/3560 Training loss: 1.6558 0.4174 sec/batch\n",
      "Epoch 20/20  Iteration 3484/3560 Training loss: 1.6558 0.4090 sec/batch\n",
      "Epoch 20/20  Iteration 3485/3560 Training loss: 1.6557 0.4345 sec/batch\n",
      "Epoch 20/20  Iteration 3486/3560 Training loss: 1.6556 0.3779 sec/batch\n",
      "Epoch 20/20  Iteration 3487/3560 Training loss: 1.6554 0.4314 sec/batch\n",
      "Epoch 20/20  Iteration 3488/3560 Training loss: 1.6553 0.4513 sec/batch\n",
      "Epoch 20/20  Iteration 3489/3560 Training loss: 1.6553 0.4150 sec/batch\n",
      "Epoch 20/20  Iteration 3490/3560 Training loss: 1.6554 0.4092 sec/batch\n",
      "Epoch 20/20  Iteration 3491/3560 Training loss: 1.6554 0.4749 sec/batch\n",
      "Epoch 20/20  Iteration 3492/3560 Training loss: 1.6555 0.4218 sec/batch\n",
      "Epoch 20/20  Iteration 3493/3560 Training loss: 1.6554 0.4713 sec/batch\n",
      "Epoch 20/20  Iteration 3494/3560 Training loss: 1.6553 0.3929 sec/batch\n",
      "Epoch 20/20  Iteration 3495/3560 Training loss: 1.6552 0.4579 sec/batch\n",
      "Epoch 20/20  Iteration 3496/3560 Training loss: 1.6551 0.3866 sec/batch\n",
      "Epoch 20/20  Iteration 3497/3560 Training loss: 1.6548 0.3926 sec/batch\n",
      "Epoch 20/20  Iteration 3498/3560 Training loss: 1.6544 0.4241 sec/batch\n",
      "Epoch 20/20  Iteration 3499/3560 Training loss: 1.6544 0.4280 sec/batch\n",
      "Epoch 20/20  Iteration 3500/3560 Training loss: 1.6544 0.4059 sec/batch\n",
      "Epoch 20/20  Iteration 3501/3560 Training loss: 1.6544 0.4147 sec/batch\n",
      "Epoch 20/20  Iteration 3502/3560 Training loss: 1.6544 0.4261 sec/batch\n",
      "Epoch 20/20  Iteration 3503/3560 Training loss: 1.6544 0.3978 sec/batch\n",
      "Epoch 20/20  Iteration 3504/3560 Training loss: 1.6540 0.3945 sec/batch\n",
      "Epoch 20/20  Iteration 3505/3560 Training loss: 1.6537 0.4488 sec/batch\n",
      "Epoch 20/20  Iteration 3506/3560 Training loss: 1.6538 0.3807 sec/batch\n",
      "Epoch 20/20  Iteration 3507/3560 Training loss: 1.6538 0.3961 sec/batch\n",
      "Epoch 20/20  Iteration 3508/3560 Training loss: 1.6534 0.3844 sec/batch\n",
      "Epoch 20/20  Iteration 3509/3560 Training loss: 1.6536 0.4291 sec/batch\n",
      "Epoch 20/20  Iteration 3510/3560 Training loss: 1.6536 0.4823 sec/batch\n",
      "Epoch 20/20  Iteration 3511/3560 Training loss: 1.6536 0.5450 sec/batch\n",
      "Epoch 20/20  Iteration 3512/3560 Training loss: 1.6533 0.3965 sec/batch\n",
      "Epoch 20/20  Iteration 3513/3560 Training loss: 1.6530 0.4365 sec/batch\n",
      "Epoch 20/20  Iteration 3514/3560 Training loss: 1.6528 0.3974 sec/batch\n",
      "Epoch 20/20  Iteration 3515/3560 Training loss: 1.6530 0.4076 sec/batch\n",
      "Epoch 20/20  Iteration 3516/3560 Training loss: 1.6531 0.4186 sec/batch\n",
      "Epoch 20/20  Iteration 3517/3560 Training loss: 1.6530 0.4068 sec/batch\n",
      "Epoch 20/20  Iteration 3518/3560 Training loss: 1.6531 0.4537 sec/batch\n",
      "Epoch 20/20  Iteration 3519/3560 Training loss: 1.6532 0.4391 sec/batch\n",
      "Epoch 20/20  Iteration 3520/3560 Training loss: 1.6533 0.4233 sec/batch\n",
      "Epoch 20/20  Iteration 3521/3560 Training loss: 1.6534 0.4345 sec/batch\n",
      "Epoch 20/20  Iteration 3522/3560 Training loss: 1.6533 0.4228 sec/batch\n",
      "Epoch 20/20  Iteration 3523/3560 Training loss: 1.6536 0.4117 sec/batch\n",
      "Epoch 20/20  Iteration 3524/3560 Training loss: 1.6536 0.4310 sec/batch\n",
      "Epoch 20/20  Iteration 3525/3560 Training loss: 1.6535 0.4119 sec/batch\n",
      "Epoch 20/20  Iteration 3526/3560 Training loss: 1.6537 0.4007 sec/batch\n",
      "Epoch 20/20  Iteration 3527/3560 Training loss: 1.6536 0.3874 sec/batch\n",
      "Epoch 20/20  Iteration 3528/3560 Training loss: 1.6537 0.4341 sec/batch\n",
      "Epoch 20/20  Iteration 3529/3560 Training loss: 1.6537 0.4069 sec/batch\n",
      "Epoch 20/20  Iteration 3530/3560 Training loss: 1.6539 0.4454 sec/batch\n",
      "Epoch 20/20  Iteration 3531/3560 Training loss: 1.6541 0.4416 sec/batch\n",
      "Epoch 20/20  Iteration 3532/3560 Training loss: 1.6540 0.4549 sec/batch\n",
      "Epoch 20/20  Iteration 3533/3560 Training loss: 1.6538 0.4533 sec/batch\n",
      "Epoch 20/20  Iteration 3534/3560 Training loss: 1.6539 0.3928 sec/batch\n",
      "Epoch 20/20  Iteration 3535/3560 Training loss: 1.6540 0.3570 sec/batch\n",
      "Epoch 20/20  Iteration 3536/3560 Training loss: 1.6541 0.4604 sec/batch\n",
      "Epoch 20/20  Iteration 3537/3560 Training loss: 1.6541 0.4056 sec/batch\n",
      "Epoch 20/20  Iteration 3538/3560 Training loss: 1.6541 0.4567 sec/batch\n",
      "Epoch 20/20  Iteration 3539/3560 Training loss: 1.6542 0.4253 sec/batch\n",
      "Epoch 20/20  Iteration 3540/3560 Training loss: 1.6543 0.4150 sec/batch\n",
      "Epoch 20/20  Iteration 3541/3560 Training loss: 1.6541 0.4264 sec/batch\n",
      "Epoch 20/20  Iteration 3542/3560 Training loss: 1.6543 0.3861 sec/batch\n",
      "Epoch 20/20  Iteration 3543/3560 Training loss: 1.6545 0.4168 sec/batch\n",
      "Epoch 20/20  Iteration 3544/3560 Training loss: 1.6545 0.3916 sec/batch\n",
      "Epoch 20/20  Iteration 3545/3560 Training loss: 1.6546 0.4026 sec/batch\n",
      "Epoch 20/20  Iteration 3546/3560 Training loss: 1.6546 0.4283 sec/batch\n",
      "Epoch 20/20  Iteration 3547/3560 Training loss: 1.6546 0.3988 sec/batch\n",
      "Epoch 20/20  Iteration 3548/3560 Training loss: 1.6546 0.4600 sec/batch\n",
      "Epoch 20/20  Iteration 3549/3560 Training loss: 1.6547 0.4249 sec/batch\n",
      "Epoch 20/20  Iteration 3550/3560 Training loss: 1.6551 0.4290 sec/batch\n",
      "Epoch 20/20  Iteration 3551/3560 Training loss: 1.6550 0.4308 sec/batch\n",
      "Epoch 20/20  Iteration 3552/3560 Training loss: 1.6550 0.4401 sec/batch\n",
      "Epoch 20/20  Iteration 3553/3560 Training loss: 1.6549 0.3652 sec/batch\n",
      "Epoch 20/20  Iteration 3554/3560 Training loss: 1.6548 0.4164 sec/batch\n",
      "Epoch 20/20  Iteration 3555/3560 Training loss: 1.6549 0.3860 sec/batch\n",
      "Epoch 20/20  Iteration 3556/3560 Training loss: 1.6549 0.4095 sec/batch\n",
      "Epoch 20/20  Iteration 3557/3560 Training loss: 1.6550 0.4406 sec/batch\n",
      "Epoch 20/20  Iteration 3558/3560 Training loss: 1.6549 0.4451 sec/batch\n",
      "Epoch 20/20  Iteration 3559/3560 Training loss: 1.6548 0.4633 sec/batch\n",
      "Epoch 20/20  Iteration 3560/3560 Training loss: 1.6550 0.4061 sec/batch\n",
      "Epoch 1/20  Iteration 1/3560 Training loss: 4.4187 0.3130 sec/batch\n",
      "Epoch 1/20  Iteration 2/3560 Training loss: 4.4133 0.4232 sec/batch\n",
      "Epoch 1/20  Iteration 3/3560 Training loss: 4.4078 0.3905 sec/batch\n",
      "Epoch 1/20  Iteration 4/3560 Training loss: 4.4020 0.4318 sec/batch\n",
      "Epoch 1/20  Iteration 5/3560 Training loss: 4.3958 0.3772 sec/batch\n",
      "Epoch 1/20  Iteration 6/3560 Training loss: 4.3885 0.4051 sec/batch\n",
      "Epoch 1/20  Iteration 7/3560 Training loss: 4.3795 0.4228 sec/batch\n",
      "Epoch 1/20  Iteration 8/3560 Training loss: 4.3675 0.4435 sec/batch\n",
      "Epoch 1/20  Iteration 9/3560 Training loss: 4.3491 0.4231 sec/batch\n",
      "Epoch 1/20  Iteration 10/3560 Training loss: 4.3171 0.4316 sec/batch\n",
      "Epoch 1/20  Iteration 11/3560 Training loss: 4.2717 0.4896 sec/batch\n",
      "Epoch 1/20  Iteration 12/3560 Training loss: 4.2244 0.3637 sec/batch\n",
      "Epoch 1/20  Iteration 13/3560 Training loss: 4.1788 0.4104 sec/batch\n",
      "Epoch 1/20  Iteration 14/3560 Training loss: 4.1365 0.3915 sec/batch\n",
      "Epoch 1/20  Iteration 15/3560 Training loss: 4.0963 0.4008 sec/batch\n",
      "Epoch 1/20  Iteration 16/3560 Training loss: 4.0590 0.4010 sec/batch\n",
      "Epoch 1/20  Iteration 17/3560 Training loss: 4.0236 0.4248 sec/batch\n",
      "Epoch 1/20  Iteration 18/3560 Training loss: 3.9925 0.4536 sec/batch\n",
      "Epoch 1/20  Iteration 19/3560 Training loss: 3.9627 0.4042 sec/batch\n",
      "Epoch 1/20  Iteration 20/3560 Training loss: 3.9340 0.4441 sec/batch\n",
      "Epoch 1/20  Iteration 21/3560 Training loss: 3.9072 0.4145 sec/batch\n",
      "Epoch 1/20  Iteration 22/3560 Training loss: 3.8817 0.4536 sec/batch\n",
      "Epoch 1/20  Iteration 23/3560 Training loss: 3.8575 0.3866 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 24/3560 Training loss: 3.8350 0.4065 sec/batch\n",
      "Epoch 1/20  Iteration 25/3560 Training loss: 3.8136 0.3780 sec/batch\n",
      "Epoch 1/20  Iteration 26/3560 Training loss: 3.7943 0.3990 sec/batch\n",
      "Epoch 1/20  Iteration 27/3560 Training loss: 3.7768 0.4113 sec/batch\n",
      "Epoch 1/20  Iteration 28/3560 Training loss: 3.7588 0.3898 sec/batch\n",
      "Epoch 1/20  Iteration 29/3560 Training loss: 3.7425 0.4595 sec/batch\n",
      "Epoch 1/20  Iteration 30/3560 Training loss: 3.7274 0.4591 sec/batch\n",
      "Epoch 1/20  Iteration 31/3560 Training loss: 3.7135 0.4890 sec/batch\n",
      "Epoch 1/20  Iteration 32/3560 Training loss: 3.6994 0.5604 sec/batch\n",
      "Epoch 1/20  Iteration 33/3560 Training loss: 3.6856 0.6098 sec/batch\n",
      "Epoch 1/20  Iteration 34/3560 Training loss: 3.6732 0.4493 sec/batch\n",
      "Epoch 1/20  Iteration 35/3560 Training loss: 3.6606 0.4625 sec/batch\n",
      "Epoch 1/20  Iteration 36/3560 Training loss: 3.6495 0.4243 sec/batch\n",
      "Epoch 1/20  Iteration 37/3560 Training loss: 3.6380 0.4135 sec/batch\n",
      "Epoch 1/20  Iteration 38/3560 Training loss: 3.6269 0.5568 sec/batch\n",
      "Epoch 1/20  Iteration 39/3560 Training loss: 3.6164 0.4844 sec/batch\n",
      "Epoch 1/20  Iteration 40/3560 Training loss: 3.6065 0.4246 sec/batch\n",
      "Epoch 1/20  Iteration 41/3560 Training loss: 3.5968 0.4534 sec/batch\n",
      "Epoch 1/20  Iteration 42/3560 Training loss: 3.5875 0.3795 sec/batch\n",
      "Epoch 1/20  Iteration 43/3560 Training loss: 3.5783 0.3921 sec/batch\n",
      "Epoch 1/20  Iteration 44/3560 Training loss: 3.5695 0.4418 sec/batch\n",
      "Epoch 1/20  Iteration 45/3560 Training loss: 3.5611 0.4016 sec/batch\n",
      "Epoch 1/20  Iteration 46/3560 Training loss: 3.5533 0.3992 sec/batch\n",
      "Epoch 1/20  Iteration 47/3560 Training loss: 3.5457 0.4238 sec/batch\n",
      "Epoch 1/20  Iteration 48/3560 Training loss: 3.5386 0.3726 sec/batch\n",
      "Epoch 1/20  Iteration 49/3560 Training loss: 3.5317 0.3708 sec/batch\n",
      "Epoch 1/20  Iteration 50/3560 Training loss: 3.5249 0.3847 sec/batch\n",
      "Epoch 1/20  Iteration 51/3560 Training loss: 3.5183 0.4221 sec/batch\n",
      "Epoch 1/20  Iteration 52/3560 Training loss: 3.5119 0.4380 sec/batch\n",
      "Epoch 1/20  Iteration 53/3560 Training loss: 3.5058 0.4274 sec/batch\n",
      "Epoch 1/20  Iteration 54/3560 Training loss: 3.4995 0.3847 sec/batch\n",
      "Epoch 1/20  Iteration 55/3560 Training loss: 3.4938 0.3999 sec/batch\n",
      "Epoch 1/20  Iteration 56/3560 Training loss: 3.4876 0.4078 sec/batch\n",
      "Epoch 1/20  Iteration 57/3560 Training loss: 3.4820 0.5176 sec/batch\n",
      "Epoch 1/20  Iteration 58/3560 Training loss: 3.4766 0.4270 sec/batch\n",
      "Epoch 1/20  Iteration 59/3560 Training loss: 3.4712 0.4923 sec/batch\n",
      "Epoch 1/20  Iteration 60/3560 Training loss: 3.4662 0.3681 sec/batch\n",
      "Epoch 1/20  Iteration 61/3560 Training loss: 3.4613 0.4082 sec/batch\n",
      "Epoch 1/20  Iteration 62/3560 Training loss: 3.4567 0.4071 sec/batch\n",
      "Epoch 1/20  Iteration 63/3560 Training loss: 3.4524 0.4030 sec/batch\n",
      "Epoch 1/20  Iteration 64/3560 Training loss: 3.4475 0.4171 sec/batch\n",
      "Epoch 1/20  Iteration 65/3560 Training loss: 3.4429 0.4139 sec/batch\n",
      "Epoch 1/20  Iteration 66/3560 Training loss: 3.4388 0.4190 sec/batch\n",
      "Epoch 1/20  Iteration 67/3560 Training loss: 3.4347 0.4110 sec/batch\n",
      "Epoch 1/20  Iteration 68/3560 Training loss: 3.4301 0.4386 sec/batch\n",
      "Epoch 1/20  Iteration 69/3560 Training loss: 3.4257 0.4087 sec/batch\n",
      "Epoch 1/20  Iteration 70/3560 Training loss: 3.4220 0.3861 sec/batch\n",
      "Epoch 1/20  Iteration 71/3560 Training loss: 3.4181 0.3843 sec/batch\n",
      "Epoch 1/20  Iteration 72/3560 Training loss: 3.4147 0.4512 sec/batch\n",
      "Epoch 1/20  Iteration 73/3560 Training loss: 3.4110 0.4125 sec/batch\n",
      "Epoch 1/20  Iteration 74/3560 Training loss: 3.4075 0.3736 sec/batch\n",
      "Epoch 1/20  Iteration 75/3560 Training loss: 3.4041 0.4259 sec/batch\n",
      "Epoch 1/20  Iteration 76/3560 Training loss: 3.4008 0.4714 sec/batch\n",
      "Epoch 1/20  Iteration 77/3560 Training loss: 3.3975 0.4447 sec/batch\n",
      "Epoch 1/20  Iteration 78/3560 Training loss: 3.3943 0.3946 sec/batch\n",
      "Epoch 1/20  Iteration 79/3560 Training loss: 3.3910 0.3806 sec/batch\n",
      "Epoch 1/20  Iteration 80/3560 Training loss: 3.3876 0.4061 sec/batch\n",
      "Epoch 1/20  Iteration 81/3560 Training loss: 3.3844 0.4231 sec/batch\n",
      "Epoch 1/20  Iteration 82/3560 Training loss: 3.3814 0.4043 sec/batch\n",
      "Epoch 1/20  Iteration 83/3560 Training loss: 3.3785 0.4408 sec/batch\n",
      "Epoch 1/20  Iteration 84/3560 Training loss: 3.3756 0.4167 sec/batch\n",
      "Epoch 1/20  Iteration 85/3560 Training loss: 3.3725 0.4280 sec/batch\n",
      "Epoch 1/20  Iteration 86/3560 Training loss: 3.3696 0.4022 sec/batch\n",
      "Epoch 1/20  Iteration 87/3560 Training loss: 3.3668 0.4219 sec/batch\n",
      "Epoch 1/20  Iteration 88/3560 Training loss: 3.3639 0.4819 sec/batch\n",
      "Epoch 1/20  Iteration 89/3560 Training loss: 3.3614 0.4020 sec/batch\n",
      "Epoch 1/20  Iteration 90/3560 Training loss: 3.3589 0.3835 sec/batch\n",
      "Epoch 1/20  Iteration 91/3560 Training loss: 3.3564 0.4036 sec/batch\n",
      "Epoch 1/20  Iteration 92/3560 Training loss: 3.3539 0.3802 sec/batch\n",
      "Epoch 1/20  Iteration 93/3560 Training loss: 3.3514 0.4046 sec/batch\n",
      "Epoch 1/20  Iteration 94/3560 Training loss: 3.3490 0.3953 sec/batch\n",
      "Epoch 1/20  Iteration 95/3560 Training loss: 3.3465 0.3891 sec/batch\n",
      "Epoch 1/20  Iteration 96/3560 Training loss: 3.3441 0.4125 sec/batch\n",
      "Epoch 1/20  Iteration 97/3560 Training loss: 3.3417 0.4729 sec/batch\n",
      "Epoch 1/20  Iteration 98/3560 Training loss: 3.3393 0.4440 sec/batch\n",
      "Epoch 1/20  Iteration 99/3560 Training loss: 3.3371 0.4926 sec/batch\n",
      "Epoch 1/20  Iteration 100/3560 Training loss: 3.3348 0.3720 sec/batch\n",
      "Epoch 1/20  Iteration 101/3560 Training loss: 3.3326 0.4271 sec/batch\n",
      "Epoch 1/20  Iteration 102/3560 Training loss: 3.3305 0.4082 sec/batch\n",
      "Epoch 1/20  Iteration 103/3560 Training loss: 3.3283 0.3840 sec/batch\n",
      "Epoch 1/20  Iteration 104/3560 Training loss: 3.3262 0.3806 sec/batch\n",
      "Epoch 1/20  Iteration 105/3560 Training loss: 3.3242 0.4325 sec/batch\n",
      "Epoch 1/20  Iteration 106/3560 Training loss: 3.3221 0.4032 sec/batch\n",
      "Epoch 1/20  Iteration 107/3560 Training loss: 3.3199 0.4438 sec/batch\n",
      "Epoch 1/20  Iteration 108/3560 Training loss: 3.3176 0.4411 sec/batch\n",
      "Epoch 1/20  Iteration 109/3560 Training loss: 3.3157 0.3746 sec/batch\n",
      "Epoch 1/20  Iteration 110/3560 Training loss: 3.3134 0.4069 sec/batch\n",
      "Epoch 1/20  Iteration 111/3560 Training loss: 3.3114 0.4049 sec/batch\n",
      "Epoch 1/20  Iteration 112/3560 Training loss: 3.3095 0.3989 sec/batch\n",
      "Epoch 1/20  Iteration 113/3560 Training loss: 3.3075 0.4018 sec/batch\n",
      "Epoch 1/20  Iteration 114/3560 Training loss: 3.3054 0.4588 sec/batch\n",
      "Epoch 1/20  Iteration 115/3560 Training loss: 3.3034 0.4452 sec/batch\n",
      "Epoch 1/20  Iteration 116/3560 Training loss: 3.3014 0.4619 sec/batch\n",
      "Epoch 1/20  Iteration 117/3560 Training loss: 3.2995 0.4482 sec/batch\n",
      "Epoch 1/20  Iteration 118/3560 Training loss: 3.2978 0.4088 sec/batch\n",
      "Epoch 1/20  Iteration 119/3560 Training loss: 3.2961 0.4247 sec/batch\n",
      "Epoch 1/20  Iteration 120/3560 Training loss: 3.2942 0.3512 sec/batch\n",
      "Epoch 1/20  Iteration 121/3560 Training loss: 3.2926 0.4595 sec/batch\n",
      "Epoch 1/20  Iteration 122/3560 Training loss: 3.2909 0.4234 sec/batch\n",
      "Epoch 1/20  Iteration 123/3560 Training loss: 3.2892 0.3789 sec/batch\n",
      "Epoch 1/20  Iteration 124/3560 Training loss: 3.2875 0.4578 sec/batch\n",
      "Epoch 1/20  Iteration 125/3560 Training loss: 3.2858 0.4186 sec/batch\n",
      "Epoch 1/20  Iteration 126/3560 Training loss: 3.2839 0.3975 sec/batch\n",
      "Epoch 1/20  Iteration 127/3560 Training loss: 3.2822 0.4087 sec/batch\n",
      "Epoch 1/20  Iteration 128/3560 Training loss: 3.2805 0.4044 sec/batch\n",
      "Epoch 1/20  Iteration 129/3560 Training loss: 3.2788 0.4232 sec/batch\n",
      "Epoch 1/20  Iteration 130/3560 Training loss: 3.2771 0.3917 sec/batch\n",
      "Epoch 1/20  Iteration 131/3560 Training loss: 3.2755 0.4159 sec/batch\n",
      "Epoch 1/20  Iteration 132/3560 Training loss: 3.2738 0.4299 sec/batch\n",
      "Epoch 1/20  Iteration 133/3560 Training loss: 3.2722 0.4061 sec/batch\n",
      "Epoch 1/20  Iteration 134/3560 Training loss: 3.2705 0.4092 sec/batch\n",
      "Epoch 1/20  Iteration 135/3560 Training loss: 3.2686 0.4716 sec/batch\n",
      "Epoch 1/20  Iteration 136/3560 Training loss: 3.2669 0.4026 sec/batch\n",
      "Epoch 1/20  Iteration 137/3560 Training loss: 3.2652 0.4416 sec/batch\n",
      "Epoch 1/20  Iteration 138/3560 Training loss: 3.2634 0.4175 sec/batch\n",
      "Epoch 1/20  Iteration 139/3560 Training loss: 3.2619 0.4134 sec/batch\n",
      "Epoch 1/20  Iteration 140/3560 Training loss: 3.2603 0.4255 sec/batch\n",
      "Epoch 1/20  Iteration 141/3560 Training loss: 3.2587 0.4166 sec/batch\n",
      "Epoch 1/20  Iteration 142/3560 Training loss: 3.2569 0.4004 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 143/3560 Training loss: 3.2552 0.3973 sec/batch\n",
      "Epoch 1/20  Iteration 144/3560 Training loss: 3.2535 0.4062 sec/batch\n",
      "Epoch 1/20  Iteration 145/3560 Training loss: 3.2519 0.3909 sec/batch\n",
      "Epoch 1/20  Iteration 146/3560 Training loss: 3.2503 0.3627 sec/batch\n",
      "Epoch 1/20  Iteration 147/3560 Training loss: 3.2488 0.4586 sec/batch\n",
      "Epoch 1/20  Iteration 148/3560 Training loss: 3.2474 0.4542 sec/batch\n",
      "Epoch 1/20  Iteration 149/3560 Training loss: 3.2457 0.3620 sec/batch\n",
      "Epoch 1/20  Iteration 150/3560 Training loss: 3.2441 0.4489 sec/batch\n",
      "Epoch 1/20  Iteration 151/3560 Training loss: 3.2427 0.4413 sec/batch\n",
      "Epoch 1/20  Iteration 152/3560 Training loss: 3.2413 0.3858 sec/batch\n",
      "Epoch 1/20  Iteration 153/3560 Training loss: 3.2398 0.4015 sec/batch\n",
      "Epoch 1/20  Iteration 154/3560 Training loss: 3.2382 0.4669 sec/batch\n",
      "Epoch 1/20  Iteration 155/3560 Training loss: 3.2366 0.5144 sec/batch\n",
      "Epoch 1/20  Iteration 156/3560 Training loss: 3.2350 0.5153 sec/batch\n",
      "Epoch 1/20  Iteration 157/3560 Training loss: 3.2333 0.4785 sec/batch\n",
      "Epoch 1/20  Iteration 158/3560 Training loss: 3.2317 0.4806 sec/batch\n",
      "Epoch 1/20  Iteration 159/3560 Training loss: 3.2299 0.3956 sec/batch\n",
      "Epoch 1/20  Iteration 160/3560 Training loss: 3.2282 0.4000 sec/batch\n",
      "Epoch 1/20  Iteration 161/3560 Training loss: 3.2266 0.4699 sec/batch\n",
      "Epoch 1/20  Iteration 162/3560 Training loss: 3.2248 0.3951 sec/batch\n",
      "Epoch 1/20  Iteration 163/3560 Training loss: 3.2230 0.4610 sec/batch\n",
      "Epoch 1/20  Iteration 164/3560 Training loss: 3.2214 0.3910 sec/batch\n",
      "Epoch 1/20  Iteration 165/3560 Training loss: 3.2198 0.4121 sec/batch\n",
      "Epoch 1/20  Iteration 166/3560 Training loss: 3.2180 0.4524 sec/batch\n",
      "Epoch 1/20  Iteration 167/3560 Training loss: 3.2164 0.4694 sec/batch\n",
      "Epoch 1/20  Iteration 168/3560 Training loss: 3.2148 0.4422 sec/batch\n",
      "Epoch 1/20  Iteration 169/3560 Training loss: 3.2132 0.4073 sec/batch\n",
      "Epoch 1/20  Iteration 170/3560 Training loss: 3.2115 0.3598 sec/batch\n",
      "Epoch 1/20  Iteration 171/3560 Training loss: 3.2099 0.4069 sec/batch\n",
      "Epoch 1/20  Iteration 172/3560 Training loss: 3.2084 0.3815 sec/batch\n",
      "Epoch 1/20  Iteration 173/3560 Training loss: 3.2070 0.4370 sec/batch\n",
      "Epoch 1/20  Iteration 174/3560 Training loss: 3.2056 0.4373 sec/batch\n",
      "Epoch 1/20  Iteration 175/3560 Training loss: 3.2041 0.4434 sec/batch\n",
      "Epoch 1/20  Iteration 176/3560 Training loss: 3.2025 0.3937 sec/batch\n",
      "Epoch 1/20  Iteration 177/3560 Training loss: 3.2008 0.4500 sec/batch\n",
      "Epoch 1/20  Iteration 178/3560 Training loss: 3.1990 0.3749 sec/batch\n",
      "Epoch 2/20  Iteration 179/3560 Training loss: 2.9642 0.4831 sec/batch\n",
      "Epoch 2/20  Iteration 180/3560 Training loss: 2.9169 0.3265 sec/batch\n",
      "Epoch 2/20  Iteration 181/3560 Training loss: 2.9021 0.4060 sec/batch\n",
      "Epoch 2/20  Iteration 182/3560 Training loss: 2.8956 0.4823 sec/batch\n",
      "Epoch 2/20  Iteration 183/3560 Training loss: 2.8928 0.3863 sec/batch\n",
      "Epoch 2/20  Iteration 184/3560 Training loss: 2.8911 0.4195 sec/batch\n",
      "Epoch 2/20  Iteration 185/3560 Training loss: 2.8890 0.3891 sec/batch\n",
      "Epoch 2/20  Iteration 186/3560 Training loss: 2.8876 0.3838 sec/batch\n",
      "Epoch 2/20  Iteration 187/3560 Training loss: 2.8851 0.4420 sec/batch\n",
      "Epoch 2/20  Iteration 188/3560 Training loss: 2.8821 0.4184 sec/batch\n",
      "Epoch 2/20  Iteration 189/3560 Training loss: 2.8779 0.4670 sec/batch\n",
      "Epoch 2/20  Iteration 190/3560 Training loss: 2.8754 0.4007 sec/batch\n",
      "Epoch 2/20  Iteration 191/3560 Training loss: 2.8730 0.4138 sec/batch\n",
      "Epoch 2/20  Iteration 192/3560 Training loss: 2.8725 0.3899 sec/batch\n",
      "Epoch 2/20  Iteration 193/3560 Training loss: 2.8707 0.4318 sec/batch\n",
      "Epoch 2/20  Iteration 194/3560 Training loss: 2.8691 0.4219 sec/batch\n",
      "Epoch 2/20  Iteration 195/3560 Training loss: 2.8667 0.4107 sec/batch\n",
      "Epoch 2/20  Iteration 196/3560 Training loss: 2.8671 0.4065 sec/batch\n",
      "Epoch 2/20  Iteration 197/3560 Training loss: 2.8654 0.4093 sec/batch\n",
      "Epoch 2/20  Iteration 198/3560 Training loss: 2.8622 0.3750 sec/batch\n",
      "Epoch 2/20  Iteration 199/3560 Training loss: 2.8600 0.3979 sec/batch\n",
      "Epoch 2/20  Iteration 200/3560 Training loss: 2.8585 0.4169 sec/batch\n",
      "Epoch 2/20  Iteration 201/3560 Training loss: 2.8563 0.4107 sec/batch\n",
      "Epoch 2/20  Iteration 202/3560 Training loss: 2.8542 0.4255 sec/batch\n",
      "Epoch 2/20  Iteration 203/3560 Training loss: 2.8516 0.3583 sec/batch\n",
      "Epoch 2/20  Iteration 204/3560 Training loss: 2.8500 0.3865 sec/batch\n",
      "Epoch 2/20  Iteration 205/3560 Training loss: 2.8482 0.4341 sec/batch\n",
      "Epoch 2/20  Iteration 206/3560 Training loss: 2.8459 0.4429 sec/batch\n",
      "Epoch 2/20  Iteration 207/3560 Training loss: 2.8438 0.3808 sec/batch\n",
      "Epoch 2/20  Iteration 208/3560 Training loss: 2.8419 0.4266 sec/batch\n",
      "Epoch 2/20  Iteration 209/3560 Training loss: 2.8406 0.4289 sec/batch\n",
      "Epoch 2/20  Iteration 210/3560 Training loss: 2.8384 0.3921 sec/batch\n",
      "Epoch 2/20  Iteration 211/3560 Training loss: 2.8359 0.4824 sec/batch\n",
      "Epoch 2/20  Iteration 212/3560 Training loss: 2.8337 0.4550 sec/batch\n",
      "Epoch 2/20  Iteration 213/3560 Training loss: 2.8314 0.4529 sec/batch\n",
      "Epoch 2/20  Iteration 214/3560 Training loss: 2.8298 0.3905 sec/batch\n",
      "Epoch 2/20  Iteration 215/3560 Training loss: 2.8275 0.4215 sec/batch\n",
      "Epoch 2/20  Iteration 216/3560 Training loss: 2.8251 0.4150 sec/batch\n",
      "Epoch 2/20  Iteration 217/3560 Training loss: 2.8228 0.3840 sec/batch\n",
      "Epoch 2/20  Iteration 218/3560 Training loss: 2.8206 0.4192 sec/batch\n",
      "Epoch 2/20  Iteration 219/3560 Training loss: 2.8181 0.3939 sec/batch\n",
      "Epoch 2/20  Iteration 220/3560 Training loss: 2.8157 0.3702 sec/batch\n",
      "Epoch 2/20  Iteration 221/3560 Training loss: 2.8134 0.4285 sec/batch\n",
      "Epoch 2/20  Iteration 222/3560 Training loss: 2.8113 0.3937 sec/batch\n",
      "Epoch 2/20  Iteration 223/3560 Training loss: 2.8091 0.4041 sec/batch\n",
      "Epoch 2/20  Iteration 224/3560 Training loss: 2.8068 0.3919 sec/batch\n",
      "Epoch 2/20  Iteration 225/3560 Training loss: 2.8052 0.4487 sec/batch\n",
      "Epoch 2/20  Iteration 226/3560 Training loss: 2.8033 0.4249 sec/batch\n",
      "Epoch 2/20  Iteration 227/3560 Training loss: 2.8013 0.4440 sec/batch\n",
      "Epoch 2/20  Iteration 228/3560 Training loss: 2.7996 0.3932 sec/batch\n",
      "Epoch 2/20  Iteration 229/3560 Training loss: 2.7977 0.4039 sec/batch\n",
      "Epoch 2/20  Iteration 230/3560 Training loss: 2.7958 0.4826 sec/batch\n",
      "Epoch 2/20  Iteration 231/3560 Training loss: 2.7936 0.3908 sec/batch\n",
      "Epoch 2/20  Iteration 232/3560 Training loss: 2.7915 0.4357 sec/batch\n",
      "Epoch 2/20  Iteration 233/3560 Training loss: 2.7896 0.4114 sec/batch\n",
      "Epoch 2/20  Iteration 234/3560 Training loss: 2.7876 0.3974 sec/batch\n",
      "Epoch 2/20  Iteration 235/3560 Training loss: 2.7858 0.3864 sec/batch\n",
      "Epoch 2/20  Iteration 236/3560 Training loss: 2.7838 0.4428 sec/batch\n",
      "Epoch 2/20  Iteration 237/3560 Training loss: 2.7818 0.3722 sec/batch\n",
      "Epoch 2/20  Iteration 238/3560 Training loss: 2.7800 0.4331 sec/batch\n",
      "Epoch 2/20  Iteration 239/3560 Training loss: 2.7782 0.3807 sec/batch\n",
      "Epoch 2/20  Iteration 240/3560 Training loss: 2.7767 0.3806 sec/batch\n",
      "Epoch 2/20  Iteration 241/3560 Training loss: 2.7754 0.3841 sec/batch\n",
      "Epoch 2/20  Iteration 242/3560 Training loss: 2.7735 0.3781 sec/batch\n",
      "Epoch 2/20  Iteration 243/3560 Training loss: 2.7715 0.4083 sec/batch\n",
      "Epoch 2/20  Iteration 244/3560 Training loss: 2.7701 0.4142 sec/batch\n",
      "Epoch 2/20  Iteration 245/3560 Training loss: 2.7685 0.4572 sec/batch\n",
      "Epoch 2/20  Iteration 246/3560 Training loss: 2.7663 0.3861 sec/batch\n",
      "Epoch 2/20  Iteration 247/3560 Training loss: 2.7643 0.4199 sec/batch\n",
      "Epoch 2/20  Iteration 248/3560 Training loss: 2.7627 0.3999 sec/batch\n",
      "Epoch 2/20  Iteration 249/3560 Training loss: 2.7611 0.4635 sec/batch\n",
      "Epoch 2/20  Iteration 250/3560 Training loss: 2.7596 0.3953 sec/batch\n",
      "Epoch 2/20  Iteration 251/3560 Training loss: 2.7579 0.4151 sec/batch\n",
      "Epoch 2/20  Iteration 252/3560 Training loss: 2.7562 0.4449 sec/batch\n",
      "Epoch 2/20  Iteration 253/3560 Training loss: 2.7547 0.4330 sec/batch\n",
      "Epoch 2/20  Iteration 254/3560 Training loss: 2.7535 0.4365 sec/batch\n",
      "Epoch 2/20  Iteration 255/3560 Training loss: 2.7518 0.4230 sec/batch\n",
      "Epoch 2/20  Iteration 256/3560 Training loss: 2.7504 0.4320 sec/batch\n",
      "Epoch 2/20  Iteration 257/3560 Training loss: 2.7487 0.3897 sec/batch\n",
      "Epoch 2/20  Iteration 258/3560 Training loss: 2.7472 0.3949 sec/batch\n",
      "Epoch 2/20  Iteration 259/3560 Training loss: 2.7456 0.4399 sec/batch\n",
      "Epoch 2/20  Iteration 260/3560 Training loss: 2.7442 0.3836 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 261/3560 Training loss: 2.7427 0.3902 sec/batch\n",
      "Epoch 2/20  Iteration 262/3560 Training loss: 2.7411 0.3815 sec/batch\n",
      "Epoch 2/20  Iteration 263/3560 Training loss: 2.7392 0.4059 sec/batch\n",
      "Epoch 2/20  Iteration 264/3560 Training loss: 2.7375 0.4016 sec/batch\n",
      "Epoch 2/20  Iteration 265/3560 Training loss: 2.7360 0.4319 sec/batch\n",
      "Epoch 2/20  Iteration 266/3560 Training loss: 2.7344 0.4127 sec/batch\n",
      "Epoch 2/20  Iteration 267/3560 Training loss: 2.7330 0.4151 sec/batch\n",
      "Epoch 2/20  Iteration 268/3560 Training loss: 2.7316 0.4391 sec/batch\n",
      "Epoch 2/20  Iteration 269/3560 Training loss: 2.7302 0.4155 sec/batch\n",
      "Epoch 2/20  Iteration 270/3560 Training loss: 2.7288 0.3943 sec/batch\n",
      "Epoch 2/20  Iteration 271/3560 Training loss: 2.7273 0.4549 sec/batch\n",
      "Epoch 2/20  Iteration 272/3560 Training loss: 2.7257 0.3913 sec/batch\n",
      "Epoch 2/20  Iteration 273/3560 Training loss: 2.7240 0.3917 sec/batch\n",
      "Epoch 2/20  Iteration 274/3560 Training loss: 2.7224 0.4052 sec/batch\n",
      "Epoch 2/20  Iteration 275/3560 Training loss: 2.7210 0.4553 sec/batch\n",
      "Epoch 2/20  Iteration 276/3560 Training loss: 2.7196 0.3984 sec/batch\n",
      "Epoch 2/20  Iteration 277/3560 Training loss: 2.7182 0.3623 sec/batch\n",
      "Epoch 2/20  Iteration 278/3560 Training loss: 2.7168 0.3536 sec/batch\n",
      "Epoch 2/20  Iteration 279/3560 Training loss: 2.7155 0.4217 sec/batch\n",
      "Epoch 2/20  Iteration 280/3560 Training loss: 2.7142 0.4518 sec/batch\n",
      "Epoch 2/20  Iteration 281/3560 Training loss: 2.7126 0.3977 sec/batch\n",
      "Epoch 2/20  Iteration 282/3560 Training loss: 2.7112 0.4325 sec/batch\n",
      "Epoch 2/20  Iteration 283/3560 Training loss: 2.7099 0.4608 sec/batch\n",
      "Epoch 2/20  Iteration 284/3560 Training loss: 2.7086 0.3803 sec/batch\n",
      "Epoch 2/20  Iteration 285/3560 Training loss: 2.7071 0.4580 sec/batch\n",
      "Epoch 2/20  Iteration 286/3560 Training loss: 2.7060 0.4178 sec/batch\n",
      "Epoch 2/20  Iteration 287/3560 Training loss: 2.7048 0.4578 sec/batch\n",
      "Epoch 2/20  Iteration 288/3560 Training loss: 2.7032 0.4214 sec/batch\n",
      "Epoch 2/20  Iteration 289/3560 Training loss: 2.7019 0.3927 sec/batch\n",
      "Epoch 2/20  Iteration 290/3560 Training loss: 2.7007 0.4371 sec/batch\n",
      "Epoch 2/20  Iteration 291/3560 Training loss: 2.6993 0.3779 sec/batch\n",
      "Epoch 2/20  Iteration 292/3560 Training loss: 2.6979 0.4393 sec/batch\n",
      "Epoch 2/20  Iteration 293/3560 Training loss: 2.6966 0.4230 sec/batch\n",
      "Epoch 2/20  Iteration 294/3560 Training loss: 2.6950 0.4211 sec/batch\n",
      "Epoch 2/20  Iteration 295/3560 Training loss: 2.6937 0.4001 sec/batch\n",
      "Epoch 2/20  Iteration 296/3560 Training loss: 2.6925 0.3811 sec/batch\n",
      "Epoch 2/20  Iteration 297/3560 Training loss: 2.6914 0.4016 sec/batch\n",
      "Epoch 2/20  Iteration 298/3560 Training loss: 2.6902 0.4094 sec/batch\n",
      "Epoch 2/20  Iteration 299/3560 Training loss: 2.6892 0.4085 sec/batch\n",
      "Epoch 2/20  Iteration 300/3560 Training loss: 2.6880 0.3885 sec/batch\n",
      "Epoch 2/20  Iteration 301/3560 Training loss: 2.6867 0.3797 sec/batch\n",
      "Epoch 2/20  Iteration 302/3560 Training loss: 2.6856 0.4204 sec/batch\n",
      "Epoch 2/20  Iteration 303/3560 Training loss: 2.6844 0.4644 sec/batch\n",
      "Epoch 2/20  Iteration 304/3560 Training loss: 2.6831 0.4364 sec/batch\n",
      "Epoch 2/20  Iteration 305/3560 Training loss: 2.6820 0.4174 sec/batch\n",
      "Epoch 2/20  Iteration 306/3560 Training loss: 2.6810 0.4103 sec/batch\n",
      "Epoch 2/20  Iteration 307/3560 Training loss: 2.6799 0.3782 sec/batch\n",
      "Epoch 2/20  Iteration 308/3560 Training loss: 2.6788 0.4588 sec/batch\n",
      "Epoch 2/20  Iteration 309/3560 Training loss: 2.6777 0.4886 sec/batch\n",
      "Epoch 2/20  Iteration 310/3560 Training loss: 2.6765 0.3776 sec/batch\n",
      "Epoch 2/20  Iteration 311/3560 Training loss: 2.6754 0.3924 sec/batch\n",
      "Epoch 2/20  Iteration 312/3560 Training loss: 2.6744 0.4259 sec/batch\n",
      "Epoch 2/20  Iteration 313/3560 Training loss: 2.6731 0.4002 sec/batch\n",
      "Epoch 2/20  Iteration 314/3560 Training loss: 2.6719 0.3905 sec/batch\n",
      "Epoch 2/20  Iteration 315/3560 Training loss: 2.6707 0.3889 sec/batch\n",
      "Epoch 2/20  Iteration 316/3560 Training loss: 2.6696 0.4200 sec/batch\n",
      "Epoch 2/20  Iteration 317/3560 Training loss: 2.6687 0.3597 sec/batch\n",
      "Epoch 2/20  Iteration 318/3560 Training loss: 2.6675 0.3878 sec/batch\n",
      "Epoch 2/20  Iteration 319/3560 Training loss: 2.6666 0.4634 sec/batch\n",
      "Epoch 2/20  Iteration 320/3560 Training loss: 2.6655 0.3303 sec/batch\n",
      "Epoch 2/20  Iteration 321/3560 Training loss: 2.6644 0.4188 sec/batch\n",
      "Epoch 2/20  Iteration 322/3560 Training loss: 2.6634 0.4152 sec/batch\n",
      "Epoch 2/20  Iteration 323/3560 Training loss: 2.6623 0.4419 sec/batch\n",
      "Epoch 2/20  Iteration 324/3560 Training loss: 2.6614 0.4154 sec/batch\n",
      "Epoch 2/20  Iteration 325/3560 Training loss: 2.6604 0.4544 sec/batch\n",
      "Epoch 2/20  Iteration 326/3560 Training loss: 2.6595 0.3948 sec/batch\n",
      "Epoch 2/20  Iteration 327/3560 Training loss: 2.6584 0.4206 sec/batch\n",
      "Epoch 2/20  Iteration 328/3560 Training loss: 2.6573 0.4261 sec/batch\n",
      "Epoch 2/20  Iteration 329/3560 Training loss: 2.6565 0.4283 sec/batch\n",
      "Epoch 2/20  Iteration 330/3560 Training loss: 2.6558 0.4106 sec/batch\n",
      "Epoch 2/20  Iteration 331/3560 Training loss: 2.6549 0.4345 sec/batch\n",
      "Epoch 2/20  Iteration 332/3560 Training loss: 2.6540 0.4569 sec/batch\n",
      "Epoch 2/20  Iteration 333/3560 Training loss: 2.6530 0.3972 sec/batch\n",
      "Epoch 2/20  Iteration 334/3560 Training loss: 2.6519 0.3864 sec/batch\n",
      "Epoch 2/20  Iteration 335/3560 Training loss: 2.6509 0.3925 sec/batch\n",
      "Epoch 2/20  Iteration 336/3560 Training loss: 2.6499 0.4429 sec/batch\n",
      "Epoch 2/20  Iteration 337/3560 Training loss: 2.6488 0.4063 sec/batch\n",
      "Epoch 2/20  Iteration 338/3560 Training loss: 2.6480 0.3866 sec/batch\n",
      "Epoch 2/20  Iteration 339/3560 Training loss: 2.6470 0.3903 sec/batch\n",
      "Epoch 2/20  Iteration 340/3560 Training loss: 2.6459 0.3492 sec/batch\n",
      "Epoch 2/20  Iteration 341/3560 Training loss: 2.6448 0.4657 sec/batch\n",
      "Epoch 2/20  Iteration 342/3560 Training loss: 2.6439 0.4649 sec/batch\n",
      "Epoch 2/20  Iteration 343/3560 Training loss: 2.6430 0.3859 sec/batch\n",
      "Epoch 2/20  Iteration 344/3560 Training loss: 2.6420 0.3975 sec/batch\n",
      "Epoch 2/20  Iteration 345/3560 Training loss: 2.6411 0.4421 sec/batch\n",
      "Epoch 2/20  Iteration 346/3560 Training loss: 2.6402 0.4210 sec/batch\n",
      "Epoch 2/20  Iteration 347/3560 Training loss: 2.6393 0.4433 sec/batch\n",
      "Epoch 2/20  Iteration 348/3560 Training loss: 2.6382 0.4792 sec/batch\n",
      "Epoch 2/20  Iteration 349/3560 Training loss: 2.6374 0.4339 sec/batch\n",
      "Epoch 2/20  Iteration 350/3560 Training loss: 2.6366 0.3904 sec/batch\n",
      "Epoch 2/20  Iteration 351/3560 Training loss: 2.6359 0.3934 sec/batch\n",
      "Epoch 2/20  Iteration 352/3560 Training loss: 2.6353 0.4737 sec/batch\n",
      "Epoch 2/20  Iteration 353/3560 Training loss: 2.6346 0.4074 sec/batch\n",
      "Epoch 2/20  Iteration 354/3560 Training loss: 2.6337 0.4020 sec/batch\n",
      "Epoch 2/20  Iteration 355/3560 Training loss: 2.6327 0.4120 sec/batch\n",
      "Epoch 2/20  Iteration 356/3560 Training loss: 2.6317 0.4068 sec/batch\n",
      "Epoch 3/20  Iteration 357/3560 Training loss: 2.5415 0.3994 sec/batch\n",
      "Epoch 3/20  Iteration 358/3560 Training loss: 2.4920 0.4245 sec/batch\n",
      "Epoch 3/20  Iteration 359/3560 Training loss: 2.4768 0.3420 sec/batch\n",
      "Epoch 3/20  Iteration 360/3560 Training loss: 2.4730 0.4385 sec/batch\n",
      "Epoch 3/20  Iteration 361/3560 Training loss: 2.4709 0.3790 sec/batch\n",
      "Epoch 3/20  Iteration 362/3560 Training loss: 2.4674 0.4763 sec/batch\n",
      "Epoch 3/20  Iteration 363/3560 Training loss: 2.4676 0.3933 sec/batch\n",
      "Epoch 3/20  Iteration 364/3560 Training loss: 2.4684 0.4250 sec/batch\n",
      "Epoch 3/20  Iteration 365/3560 Training loss: 2.4702 0.4702 sec/batch\n",
      "Epoch 3/20  Iteration 366/3560 Training loss: 2.4691 0.4474 sec/batch\n",
      "Epoch 3/20  Iteration 367/3560 Training loss: 2.4670 0.5059 sec/batch\n",
      "Epoch 3/20  Iteration 368/3560 Training loss: 2.4666 0.3633 sec/batch\n",
      "Epoch 3/20  Iteration 369/3560 Training loss: 2.4656 0.4196 sec/batch\n",
      "Epoch 3/20  Iteration 370/3560 Training loss: 2.4675 0.3893 sec/batch\n",
      "Epoch 3/20  Iteration 371/3560 Training loss: 2.4674 0.3926 sec/batch\n",
      "Epoch 3/20  Iteration 372/3560 Training loss: 2.4670 0.4110 sec/batch\n",
      "Epoch 3/20  Iteration 373/3560 Training loss: 2.4664 0.4066 sec/batch\n",
      "Epoch 3/20  Iteration 374/3560 Training loss: 2.4679 0.3886 sec/batch\n",
      "Epoch 3/20  Iteration 375/3560 Training loss: 2.4677 0.3847 sec/batch\n",
      "Epoch 3/20  Iteration 376/3560 Training loss: 2.4661 0.4204 sec/batch\n",
      "Epoch 3/20  Iteration 377/3560 Training loss: 2.4649 0.3996 sec/batch\n",
      "Epoch 3/20  Iteration 378/3560 Training loss: 2.4653 0.4087 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 379/3560 Training loss: 2.4643 0.4111 sec/batch\n",
      "Epoch 3/20  Iteration 380/3560 Training loss: 2.4633 0.4942 sec/batch\n",
      "Epoch 3/20  Iteration 381/3560 Training loss: 2.4620 0.4163 sec/batch\n",
      "Epoch 3/20  Iteration 382/3560 Training loss: 2.4616 0.4533 sec/batch\n",
      "Epoch 3/20  Iteration 383/3560 Training loss: 2.4608 0.4109 sec/batch\n",
      "Epoch 3/20  Iteration 384/3560 Training loss: 2.4603 0.4449 sec/batch\n",
      "Epoch 3/20  Iteration 385/3560 Training loss: 2.4603 0.4783 sec/batch\n",
      "Epoch 3/20  Iteration 386/3560 Training loss: 2.4597 0.4123 sec/batch\n",
      "Epoch 3/20  Iteration 387/3560 Training loss: 2.4601 0.4682 sec/batch\n",
      "Epoch 3/20  Iteration 388/3560 Training loss: 2.4591 0.3919 sec/batch\n",
      "Epoch 3/20  Iteration 389/3560 Training loss: 2.4583 0.3939 sec/batch\n",
      "Epoch 3/20  Iteration 390/3560 Training loss: 2.4577 0.4049 sec/batch\n",
      "Epoch 3/20  Iteration 391/3560 Training loss: 2.4568 0.3792 sec/batch\n",
      "Epoch 3/20  Iteration 392/3560 Training loss: 2.4566 0.3639 sec/batch\n",
      "Epoch 3/20  Iteration 393/3560 Training loss: 2.4558 0.4121 sec/batch\n",
      "Epoch 3/20  Iteration 394/3560 Training loss: 2.4545 0.3971 sec/batch\n",
      "Epoch 3/20  Iteration 395/3560 Training loss: 2.4535 0.3905 sec/batch\n",
      "Epoch 3/20  Iteration 396/3560 Training loss: 2.4527 0.4755 sec/batch\n",
      "Epoch 3/20  Iteration 397/3560 Training loss: 2.4518 0.3683 sec/batch\n",
      "Epoch 3/20  Iteration 398/3560 Training loss: 2.4509 0.4091 sec/batch\n",
      "Epoch 3/20  Iteration 399/3560 Training loss: 2.4498 0.4506 sec/batch\n",
      "Epoch 3/20  Iteration 400/3560 Training loss: 2.4489 0.3743 sec/batch\n",
      "Epoch 3/20  Iteration 401/3560 Training loss: 2.4479 0.4357 sec/batch\n",
      "Epoch 3/20  Iteration 402/3560 Training loss: 2.4465 0.4824 sec/batch\n",
      "Epoch 3/20  Iteration 403/3560 Training loss: 2.4463 0.4232 sec/batch\n",
      "Epoch 3/20  Iteration 404/3560 Training loss: 2.4456 0.4337 sec/batch\n",
      "Epoch 3/20  Iteration 405/3560 Training loss: 2.4450 0.4393 sec/batch\n",
      "Epoch 3/20  Iteration 406/3560 Training loss: 2.4448 0.4308 sec/batch\n",
      "Epoch 3/20  Iteration 407/3560 Training loss: 2.4438 0.4526 sec/batch\n",
      "Epoch 3/20  Iteration 408/3560 Training loss: 2.4436 0.4180 sec/batch\n",
      "Epoch 3/20  Iteration 409/3560 Training loss: 2.4429 0.3917 sec/batch\n",
      "Epoch 3/20  Iteration 410/3560 Training loss: 2.4423 0.4243 sec/batch\n",
      "Epoch 3/20  Iteration 411/3560 Training loss: 2.4418 0.4241 sec/batch\n",
      "Epoch 3/20  Iteration 412/3560 Training loss: 2.4412 0.4868 sec/batch\n",
      "Epoch 3/20  Iteration 413/3560 Training loss: 2.4406 0.4856 sec/batch\n",
      "Epoch 3/20  Iteration 414/3560 Training loss: 2.4399 0.4899 sec/batch\n",
      "Epoch 3/20  Iteration 415/3560 Training loss: 2.4393 0.4534 sec/batch\n",
      "Epoch 3/20  Iteration 416/3560 Training loss: 2.4390 0.5003 sec/batch\n",
      "Epoch 3/20  Iteration 417/3560 Training loss: 2.4382 0.4368 sec/batch\n",
      "Epoch 3/20  Iteration 418/3560 Training loss: 2.4378 0.4867 sec/batch\n",
      "Epoch 3/20  Iteration 419/3560 Training loss: 2.4377 0.4107 sec/batch\n",
      "Epoch 3/20  Iteration 420/3560 Training loss: 2.4372 0.5110 sec/batch\n",
      "Epoch 3/20  Iteration 421/3560 Training loss: 2.4365 0.4453 sec/batch\n",
      "Epoch 3/20  Iteration 422/3560 Training loss: 2.4363 0.4333 sec/batch\n",
      "Epoch 3/20  Iteration 423/3560 Training loss: 2.4359 0.4876 sec/batch\n",
      "Epoch 3/20  Iteration 424/3560 Training loss: 2.4350 0.5322 sec/batch\n",
      "Epoch 3/20  Iteration 425/3560 Training loss: 2.4343 0.4523 sec/batch\n",
      "Epoch 3/20  Iteration 426/3560 Training loss: 2.4339 0.4242 sec/batch\n",
      "Epoch 3/20  Iteration 427/3560 Training loss: 2.4335 0.4347 sec/batch\n",
      "Epoch 3/20  Iteration 428/3560 Training loss: 2.4333 0.4101 sec/batch\n",
      "Epoch 3/20  Iteration 429/3560 Training loss: 2.4328 0.4000 sec/batch\n",
      "Epoch 3/20  Iteration 430/3560 Training loss: 2.4323 0.4544 sec/batch\n",
      "Epoch 3/20  Iteration 431/3560 Training loss: 2.4318 0.3814 sec/batch\n",
      "Epoch 3/20  Iteration 432/3560 Training loss: 2.4318 0.3882 sec/batch\n",
      "Epoch 3/20  Iteration 433/3560 Training loss: 2.4312 0.3987 sec/batch\n",
      "Epoch 3/20  Iteration 434/3560 Training loss: 2.4310 0.3817 sec/batch\n",
      "Epoch 3/20  Iteration 435/3560 Training loss: 2.4303 0.4484 sec/batch\n",
      "Epoch 3/20  Iteration 436/3560 Training loss: 2.4298 0.3990 sec/batch\n",
      "Epoch 3/20  Iteration 437/3560 Training loss: 2.4293 0.3987 sec/batch\n",
      "Epoch 3/20  Iteration 438/3560 Training loss: 2.4290 0.4328 sec/batch\n",
      "Epoch 3/20  Iteration 439/3560 Training loss: 2.4285 0.4329 sec/batch\n",
      "Epoch 3/20  Iteration 440/3560 Training loss: 2.4278 0.3807 sec/batch\n",
      "Epoch 3/20  Iteration 441/3560 Training loss: 2.4269 0.5225 sec/batch\n",
      "Epoch 3/20  Iteration 442/3560 Training loss: 2.4262 0.4068 sec/batch\n",
      "Epoch 3/20  Iteration 443/3560 Training loss: 2.4257 0.4614 sec/batch\n",
      "Epoch 3/20  Iteration 444/3560 Training loss: 2.4253 0.3983 sec/batch\n",
      "Epoch 3/20  Iteration 445/3560 Training loss: 2.4247 0.4172 sec/batch\n",
      "Epoch 3/20  Iteration 446/3560 Training loss: 2.4244 0.4219 sec/batch\n",
      "Epoch 3/20  Iteration 447/3560 Training loss: 2.4238 0.4693 sec/batch\n",
      "Epoch 3/20  Iteration 448/3560 Training loss: 2.4234 0.3858 sec/batch\n",
      "Epoch 3/20  Iteration 449/3560 Training loss: 2.4230 0.3896 sec/batch\n",
      "Epoch 3/20  Iteration 450/3560 Training loss: 2.4223 0.4510 sec/batch\n",
      "Epoch 3/20  Iteration 451/3560 Training loss: 2.4216 0.3824 sec/batch\n",
      "Epoch 3/20  Iteration 452/3560 Training loss: 2.4210 0.4075 sec/batch\n",
      "Epoch 3/20  Iteration 453/3560 Training loss: 2.4206 0.3709 sec/batch\n",
      "Epoch 3/20  Iteration 454/3560 Training loss: 2.4201 0.4327 sec/batch\n",
      "Epoch 3/20  Iteration 455/3560 Training loss: 2.4196 0.3498 sec/batch\n",
      "Epoch 3/20  Iteration 456/3560 Training loss: 2.4191 0.3879 sec/batch\n",
      "Epoch 3/20  Iteration 457/3560 Training loss: 2.4188 0.3920 sec/batch\n",
      "Epoch 3/20  Iteration 458/3560 Training loss: 2.4184 0.4545 sec/batch\n",
      "Epoch 3/20  Iteration 459/3560 Training loss: 2.4178 0.4151 sec/batch\n",
      "Epoch 3/20  Iteration 460/3560 Training loss: 2.4173 0.4359 sec/batch\n",
      "Epoch 3/20  Iteration 461/3560 Training loss: 2.4168 0.4564 sec/batch\n",
      "Epoch 3/20  Iteration 462/3560 Training loss: 2.4164 0.4605 sec/batch\n",
      "Epoch 3/20  Iteration 463/3560 Training loss: 2.4159 0.4235 sec/batch\n",
      "Epoch 3/20  Iteration 464/3560 Training loss: 2.4157 0.4744 sec/batch\n",
      "Epoch 3/20  Iteration 465/3560 Training loss: 2.4154 0.3741 sec/batch\n",
      "Epoch 3/20  Iteration 466/3560 Training loss: 2.4148 0.4334 sec/batch\n",
      "Epoch 3/20  Iteration 467/3560 Training loss: 2.4144 0.4079 sec/batch\n",
      "Epoch 3/20  Iteration 468/3560 Training loss: 2.4142 0.3964 sec/batch\n",
      "Epoch 3/20  Iteration 469/3560 Training loss: 2.4137 0.3724 sec/batch\n",
      "Epoch 3/20  Iteration 470/3560 Training loss: 2.4133 0.3757 sec/batch\n",
      "Epoch 3/20  Iteration 471/3560 Training loss: 2.4128 0.4442 sec/batch\n",
      "Epoch 3/20  Iteration 472/3560 Training loss: 2.4121 0.3865 sec/batch\n",
      "Epoch 3/20  Iteration 473/3560 Training loss: 2.4117 0.3935 sec/batch\n",
      "Epoch 3/20  Iteration 474/3560 Training loss: 2.4114 0.3970 sec/batch\n",
      "Epoch 3/20  Iteration 475/3560 Training loss: 2.4111 0.4142 sec/batch\n",
      "Epoch 3/20  Iteration 476/3560 Training loss: 2.4107 0.4806 sec/batch\n",
      "Epoch 3/20  Iteration 477/3560 Training loss: 2.4105 0.4063 sec/batch\n",
      "Epoch 3/20  Iteration 478/3560 Training loss: 2.4100 0.4433 sec/batch\n",
      "Epoch 3/20  Iteration 479/3560 Training loss: 2.4096 0.4973 sec/batch\n",
      "Epoch 3/20  Iteration 480/3560 Training loss: 2.4094 0.4604 sec/batch\n",
      "Epoch 3/20  Iteration 481/3560 Training loss: 2.4090 0.4904 sec/batch\n",
      "Epoch 3/20  Iteration 482/3560 Training loss: 2.4085 0.3422 sec/batch\n",
      "Epoch 3/20  Iteration 483/3560 Training loss: 2.4082 0.4560 sec/batch\n",
      "Epoch 3/20  Iteration 484/3560 Training loss: 2.4080 0.3951 sec/batch\n",
      "Epoch 3/20  Iteration 485/3560 Training loss: 2.4077 0.3805 sec/batch\n",
      "Epoch 3/20  Iteration 486/3560 Training loss: 2.4073 0.4035 sec/batch\n",
      "Epoch 3/20  Iteration 487/3560 Training loss: 2.4070 0.3921 sec/batch\n",
      "Epoch 3/20  Iteration 488/3560 Training loss: 2.4065 0.3987 sec/batch\n",
      "Epoch 3/20  Iteration 489/3560 Training loss: 2.4063 0.4388 sec/batch\n",
      "Epoch 3/20  Iteration 490/3560 Training loss: 2.4060 0.3869 sec/batch\n",
      "Epoch 3/20  Iteration 491/3560 Training loss: 2.4056 0.4235 sec/batch\n",
      "Epoch 3/20  Iteration 492/3560 Training loss: 2.4053 0.4421 sec/batch\n",
      "Epoch 3/20  Iteration 493/3560 Training loss: 2.4050 0.3755 sec/batch\n",
      "Epoch 3/20  Iteration 494/3560 Training loss: 2.4047 0.3835 sec/batch\n",
      "Epoch 3/20  Iteration 495/3560 Training loss: 2.4046 0.3869 sec/batch\n",
      "Epoch 3/20  Iteration 496/3560 Training loss: 2.4042 0.4713 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 497/3560 Training loss: 2.4039 0.4020 sec/batch\n",
      "Epoch 3/20  Iteration 498/3560 Training loss: 2.4036 0.4306 sec/batch\n",
      "Epoch 3/20  Iteration 499/3560 Training loss: 2.4033 0.4434 sec/batch\n",
      "Epoch 3/20  Iteration 500/3560 Training loss: 2.4029 0.4848 sec/batch\n",
      "Epoch 3/20  Iteration 501/3560 Training loss: 2.4026 0.4969 sec/batch\n",
      "Epoch 3/20  Iteration 502/3560 Training loss: 2.4025 0.4697 sec/batch\n",
      "Epoch 3/20  Iteration 503/3560 Training loss: 2.4021 0.3944 sec/batch\n",
      "Epoch 3/20  Iteration 504/3560 Training loss: 2.4020 0.3946 sec/batch\n",
      "Epoch 3/20  Iteration 505/3560 Training loss: 2.4016 0.3898 sec/batch\n",
      "Epoch 3/20  Iteration 506/3560 Training loss: 2.4012 0.4009 sec/batch\n",
      "Epoch 3/20  Iteration 507/3560 Training loss: 2.4010 0.3944 sec/batch\n",
      "Epoch 3/20  Iteration 508/3560 Training loss: 2.4009 0.3981 sec/batch\n",
      "Epoch 3/20  Iteration 509/3560 Training loss: 2.4007 0.4040 sec/batch\n",
      "Epoch 3/20  Iteration 510/3560 Training loss: 2.4005 0.4215 sec/batch\n",
      "Epoch 3/20  Iteration 511/3560 Training loss: 2.4001 0.3854 sec/batch\n",
      "Epoch 3/20  Iteration 512/3560 Training loss: 2.3998 0.4017 sec/batch\n",
      "Epoch 3/20  Iteration 513/3560 Training loss: 2.3995 0.3755 sec/batch\n",
      "Epoch 3/20  Iteration 514/3560 Training loss: 2.3992 0.4116 sec/batch\n",
      "Epoch 3/20  Iteration 515/3560 Training loss: 2.3988 0.3873 sec/batch\n",
      "Epoch 3/20  Iteration 516/3560 Training loss: 2.3985 0.4766 sec/batch\n",
      "Epoch 3/20  Iteration 517/3560 Training loss: 2.3983 0.4506 sec/batch\n",
      "Epoch 3/20  Iteration 518/3560 Training loss: 2.3979 0.4540 sec/batch\n",
      "Epoch 3/20  Iteration 519/3560 Training loss: 2.3975 0.4611 sec/batch\n",
      "Epoch 3/20  Iteration 520/3560 Training loss: 2.3972 0.4402 sec/batch\n",
      "Epoch 3/20  Iteration 521/3560 Training loss: 2.3970 0.4862 sec/batch\n",
      "Epoch 3/20  Iteration 522/3560 Training loss: 2.3967 0.4277 sec/batch\n",
      "Epoch 3/20  Iteration 523/3560 Training loss: 2.3964 0.3792 sec/batch\n",
      "Epoch 3/20  Iteration 524/3560 Training loss: 2.3962 0.4381 sec/batch\n",
      "Epoch 3/20  Iteration 525/3560 Training loss: 2.3959 0.3901 sec/batch\n",
      "Epoch 3/20  Iteration 526/3560 Training loss: 2.3955 0.4245 sec/batch\n",
      "Epoch 3/20  Iteration 527/3560 Training loss: 2.3953 0.4131 sec/batch\n",
      "Epoch 3/20  Iteration 528/3560 Training loss: 2.3951 0.4140 sec/batch\n",
      "Epoch 3/20  Iteration 529/3560 Training loss: 2.3950 0.3853 sec/batch\n",
      "Epoch 3/20  Iteration 530/3560 Training loss: 2.3948 0.4167 sec/batch\n",
      "Epoch 3/20  Iteration 531/3560 Training loss: 2.3946 0.4155 sec/batch\n",
      "Epoch 3/20  Iteration 532/3560 Training loss: 2.3946 0.4010 sec/batch\n",
      "Epoch 3/20  Iteration 533/3560 Training loss: 2.3944 0.4162 sec/batch\n",
      "Epoch 3/20  Iteration 534/3560 Training loss: 2.3943 0.4135 sec/batch\n",
      "Epoch 4/20  Iteration 535/3560 Training loss: 2.4057 0.4279 sec/batch\n",
      "Epoch 4/20  Iteration 536/3560 Training loss: 2.3589 0.4330 sec/batch\n",
      "Epoch 4/20  Iteration 537/3560 Training loss: 2.3454 0.4354 sec/batch\n",
      "Epoch 4/20  Iteration 538/3560 Training loss: 2.3414 0.4279 sec/batch\n",
      "Epoch 4/20  Iteration 539/3560 Training loss: 2.3396 0.4514 sec/batch\n",
      "Epoch 4/20  Iteration 540/3560 Training loss: 2.3368 0.3879 sec/batch\n",
      "Epoch 4/20  Iteration 541/3560 Training loss: 2.3360 0.4478 sec/batch\n",
      "Epoch 4/20  Iteration 542/3560 Training loss: 2.3367 0.3834 sec/batch\n",
      "Epoch 4/20  Iteration 543/3560 Training loss: 2.3381 0.3979 sec/batch\n",
      "Epoch 4/20  Iteration 544/3560 Training loss: 2.3374 0.4613 sec/batch\n",
      "Epoch 4/20  Iteration 545/3560 Training loss: 2.3365 0.4009 sec/batch\n",
      "Epoch 4/20  Iteration 546/3560 Training loss: 2.3352 0.3995 sec/batch\n",
      "Epoch 4/20  Iteration 547/3560 Training loss: 2.3359 0.4748 sec/batch\n",
      "Epoch 4/20  Iteration 548/3560 Training loss: 2.3387 0.4306 sec/batch\n",
      "Epoch 4/20  Iteration 549/3560 Training loss: 2.3382 0.4986 sec/batch\n",
      "Epoch 4/20  Iteration 550/3560 Training loss: 2.3381 0.4543 sec/batch\n",
      "Epoch 4/20  Iteration 551/3560 Training loss: 2.3387 0.4068 sec/batch\n",
      "Epoch 4/20  Iteration 552/3560 Training loss: 2.3403 0.4470 sec/batch\n",
      "Epoch 4/20  Iteration 553/3560 Training loss: 2.3407 0.4913 sec/batch\n",
      "Epoch 4/20  Iteration 554/3560 Training loss: 2.3395 0.4461 sec/batch\n",
      "Epoch 4/20  Iteration 555/3560 Training loss: 2.3385 0.4848 sec/batch\n",
      "Epoch 4/20  Iteration 556/3560 Training loss: 2.3390 0.4428 sec/batch\n",
      "Epoch 4/20  Iteration 557/3560 Training loss: 2.3383 0.4717 sec/batch\n",
      "Epoch 4/20  Iteration 558/3560 Training loss: 2.3375 0.5163 sec/batch\n",
      "Epoch 4/20  Iteration 559/3560 Training loss: 2.3366 0.5507 sec/batch\n",
      "Epoch 4/20  Iteration 560/3560 Training loss: 2.3360 0.3962 sec/batch\n",
      "Epoch 4/20  Iteration 561/3560 Training loss: 2.3354 0.3769 sec/batch\n",
      "Epoch 4/20  Iteration 562/3560 Training loss: 2.3351 0.4542 sec/batch\n",
      "Epoch 4/20  Iteration 563/3560 Training loss: 2.3352 0.4109 sec/batch\n",
      "Epoch 4/20  Iteration 564/3560 Training loss: 2.3349 0.4529 sec/batch\n",
      "Epoch 4/20  Iteration 565/3560 Training loss: 2.3352 0.4338 sec/batch\n",
      "Epoch 4/20  Iteration 566/3560 Training loss: 2.3344 0.4784 sec/batch\n",
      "Epoch 4/20  Iteration 567/3560 Training loss: 2.3339 0.4038 sec/batch\n",
      "Epoch 4/20  Iteration 568/3560 Training loss: 2.3341 0.3728 sec/batch\n",
      "Epoch 4/20  Iteration 569/3560 Training loss: 2.3337 0.3719 sec/batch\n",
      "Epoch 4/20  Iteration 570/3560 Training loss: 2.3334 0.3947 sec/batch\n",
      "Epoch 4/20  Iteration 571/3560 Training loss: 2.3330 0.4420 sec/batch\n",
      "Epoch 4/20  Iteration 572/3560 Training loss: 2.3319 0.4764 sec/batch\n",
      "Epoch 4/20  Iteration 573/3560 Training loss: 2.3314 0.4092 sec/batch\n",
      "Epoch 4/20  Iteration 574/3560 Training loss: 2.3305 0.4698 sec/batch\n",
      "Epoch 4/20  Iteration 575/3560 Training loss: 2.3297 0.4782 sec/batch\n",
      "Epoch 4/20  Iteration 576/3560 Training loss: 2.3292 0.4285 sec/batch\n",
      "Epoch 4/20  Iteration 577/3560 Training loss: 2.3284 0.4373 sec/batch\n",
      "Epoch 4/20  Iteration 578/3560 Training loss: 2.3276 0.4209 sec/batch\n",
      "Epoch 4/20  Iteration 579/3560 Training loss: 2.3271 0.4862 sec/batch\n",
      "Epoch 4/20  Iteration 580/3560 Training loss: 2.3260 0.3852 sec/batch\n",
      "Epoch 4/20  Iteration 581/3560 Training loss: 2.3260 0.4048 sec/batch\n",
      "Epoch 4/20  Iteration 582/3560 Training loss: 2.3256 0.4042 sec/batch\n",
      "Epoch 4/20  Iteration 583/3560 Training loss: 2.3252 0.4619 sec/batch\n",
      "Epoch 4/20  Iteration 584/3560 Training loss: 2.3253 0.4150 sec/batch\n",
      "Epoch 4/20  Iteration 585/3560 Training loss: 2.3246 0.4926 sec/batch\n",
      "Epoch 4/20  Iteration 586/3560 Training loss: 2.3246 0.4045 sec/batch\n",
      "Epoch 4/20  Iteration 587/3560 Training loss: 2.3241 0.4417 sec/batch\n",
      "Epoch 4/20  Iteration 588/3560 Training loss: 2.3236 0.3902 sec/batch\n",
      "Epoch 4/20  Iteration 589/3560 Training loss: 2.3231 0.3975 sec/batch\n",
      "Epoch 4/20  Iteration 590/3560 Training loss: 2.3228 0.4086 sec/batch\n",
      "Epoch 4/20  Iteration 591/3560 Training loss: 2.3225 0.3529 sec/batch\n",
      "Epoch 4/20  Iteration 592/3560 Training loss: 2.3218 0.4648 sec/batch\n",
      "Epoch 4/20  Iteration 593/3560 Training loss: 2.3214 0.4294 sec/batch\n",
      "Epoch 4/20  Iteration 594/3560 Training loss: 2.3213 0.4357 sec/batch\n",
      "Epoch 4/20  Iteration 595/3560 Training loss: 2.3208 0.4644 sec/batch\n",
      "Epoch 4/20  Iteration 596/3560 Training loss: 2.3209 0.4715 sec/batch\n",
      "Epoch 4/20  Iteration 597/3560 Training loss: 2.3209 0.4124 sec/batch\n",
      "Epoch 4/20  Iteration 598/3560 Training loss: 2.3206 0.4159 sec/batch\n",
      "Epoch 4/20  Iteration 599/3560 Training loss: 2.3202 0.4114 sec/batch\n",
      "Epoch 4/20  Iteration 600/3560 Training loss: 2.3202 0.4304 sec/batch\n",
      "Epoch 4/20  Iteration 601/3560 Training loss: 2.3200 0.3500 sec/batch\n",
      "Epoch 4/20  Iteration 602/3560 Training loss: 2.3193 0.3966 sec/batch\n",
      "Epoch 4/20  Iteration 603/3560 Training loss: 2.3187 0.4027 sec/batch\n",
      "Epoch 4/20  Iteration 604/3560 Training loss: 2.3185 0.4904 sec/batch\n",
      "Epoch 4/20  Iteration 605/3560 Training loss: 2.3184 0.2925 sec/batch\n",
      "Epoch 4/20  Iteration 606/3560 Training loss: 2.3182 0.4304 sec/batch\n",
      "Epoch 4/20  Iteration 607/3560 Training loss: 2.3181 0.3892 sec/batch\n",
      "Epoch 4/20  Iteration 608/3560 Training loss: 2.3176 0.3749 sec/batch\n",
      "Epoch 4/20  Iteration 609/3560 Training loss: 2.3172 0.4176 sec/batch\n",
      "Epoch 4/20  Iteration 610/3560 Training loss: 2.3174 0.3862 sec/batch\n",
      "Epoch 4/20  Iteration 611/3560 Training loss: 2.3169 0.4485 sec/batch\n",
      "Epoch 4/20  Iteration 612/3560 Training loss: 2.3168 0.4127 sec/batch\n",
      "Epoch 4/20  Iteration 613/3560 Training loss: 2.3162 0.4767 sec/batch\n",
      "Epoch 4/20  Iteration 614/3560 Training loss: 2.3159 0.4043 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 615/3560 Training loss: 2.3154 0.4035 sec/batch\n",
      "Epoch 4/20  Iteration 616/3560 Training loss: 2.3153 0.4896 sec/batch\n",
      "Epoch 4/20  Iteration 617/3560 Training loss: 2.3148 0.4015 sec/batch\n",
      "Epoch 4/20  Iteration 618/3560 Training loss: 2.3143 0.4608 sec/batch\n",
      "Epoch 4/20  Iteration 619/3560 Training loss: 2.3135 0.3992 sec/batch\n",
      "Epoch 4/20  Iteration 620/3560 Training loss: 2.3131 0.4004 sec/batch\n",
      "Epoch 4/20  Iteration 621/3560 Training loss: 2.3129 0.4225 sec/batch\n",
      "Epoch 4/20  Iteration 622/3560 Training loss: 2.3125 0.4186 sec/batch\n",
      "Epoch 4/20  Iteration 623/3560 Training loss: 2.3121 0.4156 sec/batch\n",
      "Epoch 4/20  Iteration 624/3560 Training loss: 2.3120 0.3922 sec/batch\n",
      "Epoch 4/20  Iteration 625/3560 Training loss: 2.3117 0.4036 sec/batch\n",
      "Epoch 4/20  Iteration 626/3560 Training loss: 2.3114 0.3905 sec/batch\n",
      "Epoch 4/20  Iteration 627/3560 Training loss: 2.3109 0.4014 sec/batch\n",
      "Epoch 4/20  Iteration 628/3560 Training loss: 2.3105 0.3912 sec/batch\n",
      "Epoch 4/20  Iteration 629/3560 Training loss: 2.3100 0.4269 sec/batch\n",
      "Epoch 4/20  Iteration 630/3560 Training loss: 2.3096 0.3942 sec/batch\n",
      "Epoch 4/20  Iteration 631/3560 Training loss: 2.3093 0.3820 sec/batch\n",
      "Epoch 4/20  Iteration 632/3560 Training loss: 2.3090 0.4162 sec/batch\n",
      "Epoch 4/20  Iteration 633/3560 Training loss: 2.3086 0.4647 sec/batch\n",
      "Epoch 4/20  Iteration 634/3560 Training loss: 2.3082 0.4297 sec/batch\n",
      "Epoch 4/20  Iteration 635/3560 Training loss: 2.3080 0.4436 sec/batch\n",
      "Epoch 4/20  Iteration 636/3560 Training loss: 2.3078 0.4510 sec/batch\n",
      "Epoch 4/20  Iteration 637/3560 Training loss: 2.3073 0.4607 sec/batch\n",
      "Epoch 4/20  Iteration 638/3560 Training loss: 2.3070 0.4747 sec/batch\n",
      "Epoch 4/20  Iteration 639/3560 Training loss: 2.3066 0.3770 sec/batch\n",
      "Epoch 4/20  Iteration 640/3560 Training loss: 2.3064 0.5402 sec/batch\n",
      "Epoch 4/20  Iteration 641/3560 Training loss: 2.3061 0.4215 sec/batch\n",
      "Epoch 4/20  Iteration 642/3560 Training loss: 2.3058 0.4105 sec/batch\n",
      "Epoch 4/20  Iteration 643/3560 Training loss: 2.3057 0.4269 sec/batch\n",
      "Epoch 4/20  Iteration 644/3560 Training loss: 2.3053 0.4472 sec/batch\n",
      "Epoch 4/20  Iteration 645/3560 Training loss: 2.3051 0.4201 sec/batch\n",
      "Epoch 4/20  Iteration 646/3560 Training loss: 2.3049 0.3952 sec/batch\n",
      "Epoch 4/20  Iteration 647/3560 Training loss: 2.3047 0.3865 sec/batch\n",
      "Epoch 4/20  Iteration 648/3560 Training loss: 2.3044 0.3921 sec/batch\n",
      "Epoch 4/20  Iteration 649/3560 Training loss: 2.3040 0.4548 sec/batch\n",
      "Epoch 4/20  Iteration 650/3560 Training loss: 2.3035 0.4349 sec/batch\n",
      "Epoch 4/20  Iteration 651/3560 Training loss: 2.3033 0.4427 sec/batch\n",
      "Epoch 4/20  Iteration 652/3560 Training loss: 2.3030 0.4502 sec/batch\n",
      "Epoch 4/20  Iteration 653/3560 Training loss: 2.3030 0.5169 sec/batch\n",
      "Epoch 4/20  Iteration 654/3560 Training loss: 2.3027 0.4595 sec/batch\n",
      "Epoch 4/20  Iteration 655/3560 Training loss: 2.3027 0.4870 sec/batch\n",
      "Epoch 4/20  Iteration 656/3560 Training loss: 2.3024 0.4690 sec/batch\n",
      "Epoch 4/20  Iteration 657/3560 Training loss: 2.3021 0.4405 sec/batch\n",
      "Epoch 4/20  Iteration 658/3560 Training loss: 2.3020 0.3842 sec/batch\n",
      "Epoch 4/20  Iteration 659/3560 Training loss: 2.3018 0.4437 sec/batch\n",
      "Epoch 4/20  Iteration 660/3560 Training loss: 2.3014 0.3559 sec/batch\n",
      "Epoch 4/20  Iteration 661/3560 Training loss: 2.3012 0.4152 sec/batch\n",
      "Epoch 4/20  Iteration 662/3560 Training loss: 2.3011 0.4290 sec/batch\n",
      "Epoch 4/20  Iteration 663/3560 Training loss: 2.3010 0.4119 sec/batch\n",
      "Epoch 4/20  Iteration 664/3560 Training loss: 2.3007 0.3874 sec/batch\n",
      "Epoch 4/20  Iteration 665/3560 Training loss: 2.3004 0.4180 sec/batch\n",
      "Epoch 4/20  Iteration 666/3560 Training loss: 2.3000 0.3822 sec/batch\n",
      "Epoch 4/20  Iteration 667/3560 Training loss: 2.2998 0.3859 sec/batch\n",
      "Epoch 4/20  Iteration 668/3560 Training loss: 2.2997 0.4062 sec/batch\n",
      "Epoch 4/20  Iteration 669/3560 Training loss: 2.2994 0.3696 sec/batch\n",
      "Epoch 4/20  Iteration 670/3560 Training loss: 2.2992 0.4560 sec/batch\n",
      "Epoch 4/20  Iteration 671/3560 Training loss: 2.2991 0.3925 sec/batch\n",
      "Epoch 4/20  Iteration 672/3560 Training loss: 2.2989 0.4508 sec/batch\n",
      "Epoch 4/20  Iteration 673/3560 Training loss: 2.2989 0.4668 sec/batch\n",
      "Epoch 4/20  Iteration 674/3560 Training loss: 2.2986 0.4635 sec/batch\n",
      "Epoch 4/20  Iteration 675/3560 Training loss: 2.2986 0.4777 sec/batch\n",
      "Epoch 4/20  Iteration 676/3560 Training loss: 2.2983 0.4485 sec/batch\n",
      "Epoch 4/20  Iteration 677/3560 Training loss: 2.2982 0.3880 sec/batch\n",
      "Epoch 4/20  Iteration 678/3560 Training loss: 2.2980 0.3878 sec/batch\n",
      "Epoch 4/20  Iteration 679/3560 Training loss: 2.2977 0.4009 sec/batch\n",
      "Epoch 4/20  Iteration 680/3560 Training loss: 2.2977 0.3985 sec/batch\n",
      "Epoch 4/20  Iteration 681/3560 Training loss: 2.2975 0.4312 sec/batch\n",
      "Epoch 4/20  Iteration 682/3560 Training loss: 2.2975 0.4674 sec/batch\n",
      "Epoch 4/20  Iteration 683/3560 Training loss: 2.2972 0.3634 sec/batch\n",
      "Epoch 4/20  Iteration 684/3560 Training loss: 2.2969 0.3895 sec/batch\n",
      "Epoch 4/20  Iteration 685/3560 Training loss: 2.2968 0.3734 sec/batch\n",
      "Epoch 4/20  Iteration 686/3560 Training loss: 2.2968 0.3844 sec/batch\n",
      "Epoch 4/20  Iteration 687/3560 Training loss: 2.2967 0.3989 sec/batch\n",
      "Epoch 4/20  Iteration 688/3560 Training loss: 2.2967 0.4168 sec/batch\n",
      "Epoch 4/20  Iteration 689/3560 Training loss: 2.2964 0.4079 sec/batch\n",
      "Epoch 4/20  Iteration 690/3560 Training loss: 2.2963 0.4315 sec/batch\n",
      "Epoch 4/20  Iteration 691/3560 Training loss: 2.2961 0.4974 sec/batch\n",
      "Epoch 4/20  Iteration 692/3560 Training loss: 2.2958 0.4714 sec/batch\n",
      "Epoch 4/20  Iteration 693/3560 Training loss: 2.2955 0.3780 sec/batch\n",
      "Epoch 4/20  Iteration 694/3560 Training loss: 2.2954 0.4685 sec/batch\n",
      "Epoch 4/20  Iteration 695/3560 Training loss: 2.2954 0.3977 sec/batch\n",
      "Epoch 4/20  Iteration 696/3560 Training loss: 2.2951 0.4411 sec/batch\n",
      "Epoch 4/20  Iteration 697/3560 Training loss: 2.2948 0.4188 sec/batch\n",
      "Epoch 4/20  Iteration 698/3560 Training loss: 2.2946 0.4028 sec/batch\n",
      "Epoch 4/20  Iteration 699/3560 Training loss: 2.2945 0.4481 sec/batch\n",
      "Epoch 4/20  Iteration 700/3560 Training loss: 2.2943 0.4510 sec/batch\n",
      "Epoch 4/20  Iteration 701/3560 Training loss: 2.2942 0.4339 sec/batch\n",
      "Epoch 4/20  Iteration 702/3560 Training loss: 2.2942 0.3945 sec/batch\n",
      "Epoch 4/20  Iteration 703/3560 Training loss: 2.2940 0.3740 sec/batch\n",
      "Epoch 4/20  Iteration 704/3560 Training loss: 2.2937 0.4413 sec/batch\n",
      "Epoch 4/20  Iteration 705/3560 Training loss: 2.2936 0.4281 sec/batch\n",
      "Epoch 4/20  Iteration 706/3560 Training loss: 2.2934 0.4228 sec/batch\n",
      "Epoch 4/20  Iteration 707/3560 Training loss: 2.2932 0.4648 sec/batch\n",
      "Epoch 4/20  Iteration 708/3560 Training loss: 2.2931 0.3840 sec/batch\n",
      "Epoch 4/20  Iteration 709/3560 Training loss: 2.2930 0.4640 sec/batch\n",
      "Epoch 4/20  Iteration 710/3560 Training loss: 2.2929 0.5564 sec/batch\n",
      "Epoch 4/20  Iteration 711/3560 Training loss: 2.2927 0.4692 sec/batch\n",
      "Epoch 4/20  Iteration 712/3560 Training loss: 2.2924 0.5024 sec/batch\n",
      "Epoch 5/20  Iteration 713/3560 Training loss: 2.3293 0.5494 sec/batch\n",
      "Epoch 5/20  Iteration 714/3560 Training loss: 2.2757 0.5219 sec/batch\n",
      "Epoch 5/20  Iteration 715/3560 Training loss: 2.2634 0.4914 sec/batch\n",
      "Epoch 5/20  Iteration 716/3560 Training loss: 2.2610 0.4319 sec/batch\n",
      "Epoch 5/20  Iteration 717/3560 Training loss: 2.2586 0.4025 sec/batch\n",
      "Epoch 5/20  Iteration 718/3560 Training loss: 2.2558 0.4591 sec/batch\n",
      "Epoch 5/20  Iteration 719/3560 Training loss: 2.2570 0.4106 sec/batch\n",
      "Epoch 5/20  Iteration 720/3560 Training loss: 2.2584 0.4365 sec/batch\n",
      "Epoch 5/20  Iteration 721/3560 Training loss: 2.2599 0.3826 sec/batch\n",
      "Epoch 5/20  Iteration 722/3560 Training loss: 2.2595 0.3722 sec/batch\n",
      "Epoch 5/20  Iteration 723/3560 Training loss: 2.2575 0.4061 sec/batch\n",
      "Epoch 5/20  Iteration 724/3560 Training loss: 2.2565 0.4068 sec/batch\n",
      "Epoch 5/20  Iteration 725/3560 Training loss: 2.2565 0.4591 sec/batch\n",
      "Epoch 5/20  Iteration 726/3560 Training loss: 2.2586 0.3896 sec/batch\n",
      "Epoch 5/20  Iteration 727/3560 Training loss: 2.2580 0.3495 sec/batch\n",
      "Epoch 5/20  Iteration 728/3560 Training loss: 2.2574 0.4787 sec/batch\n",
      "Epoch 5/20  Iteration 729/3560 Training loss: 2.2572 0.4978 sec/batch\n",
      "Epoch 5/20  Iteration 730/3560 Training loss: 2.2587 0.4493 sec/batch\n",
      "Epoch 5/20  Iteration 731/3560 Training loss: 2.2588 0.4199 sec/batch\n",
      "Epoch 5/20  Iteration 732/3560 Training loss: 2.2575 0.4429 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 733/3560 Training loss: 2.2568 0.5010 sec/batch\n",
      "Epoch 5/20  Iteration 734/3560 Training loss: 2.2575 0.4331 sec/batch\n",
      "Epoch 5/20  Iteration 735/3560 Training loss: 2.2570 0.3235 sec/batch\n",
      "Epoch 5/20  Iteration 736/3560 Training loss: 2.2560 0.4877 sec/batch\n",
      "Epoch 5/20  Iteration 737/3560 Training loss: 2.2555 0.4212 sec/batch\n",
      "Epoch 5/20  Iteration 738/3560 Training loss: 2.2548 0.3685 sec/batch\n",
      "Epoch 5/20  Iteration 739/3560 Training loss: 2.2542 0.3830 sec/batch\n",
      "Epoch 5/20  Iteration 740/3560 Training loss: 2.2544 0.4113 sec/batch\n",
      "Epoch 5/20  Iteration 741/3560 Training loss: 2.2548 0.4132 sec/batch\n",
      "Epoch 5/20  Iteration 742/3560 Training loss: 2.2549 0.3835 sec/batch\n",
      "Epoch 5/20  Iteration 743/3560 Training loss: 2.2553 0.3985 sec/batch\n",
      "Epoch 5/20  Iteration 744/3560 Training loss: 2.2547 0.3865 sec/batch\n",
      "Epoch 5/20  Iteration 745/3560 Training loss: 2.2546 0.3944 sec/batch\n",
      "Epoch 5/20  Iteration 746/3560 Training loss: 2.2547 0.3559 sec/batch\n",
      "Epoch 5/20  Iteration 747/3560 Training loss: 2.2543 0.3775 sec/batch\n",
      "Epoch 5/20  Iteration 748/3560 Training loss: 2.2541 0.4894 sec/batch\n",
      "Epoch 5/20  Iteration 749/3560 Training loss: 2.2538 0.4090 sec/batch\n",
      "Epoch 5/20  Iteration 750/3560 Training loss: 2.2527 0.4674 sec/batch\n",
      "Epoch 5/20  Iteration 751/3560 Training loss: 2.2518 0.4537 sec/batch\n",
      "Epoch 5/20  Iteration 752/3560 Training loss: 2.2510 0.4799 sec/batch\n",
      "Epoch 5/20  Iteration 753/3560 Training loss: 2.2506 0.3984 sec/batch\n",
      "Epoch 5/20  Iteration 754/3560 Training loss: 2.2500 0.3753 sec/batch\n",
      "Epoch 5/20  Iteration 755/3560 Training loss: 2.2494 0.4077 sec/batch\n",
      "Epoch 5/20  Iteration 756/3560 Training loss: 2.2487 0.3809 sec/batch\n",
      "Epoch 5/20  Iteration 757/3560 Training loss: 2.2483 0.3717 sec/batch\n",
      "Epoch 5/20  Iteration 758/3560 Training loss: 2.2473 0.4397 sec/batch\n",
      "Epoch 5/20  Iteration 759/3560 Training loss: 2.2473 0.4455 sec/batch\n",
      "Epoch 5/20  Iteration 760/3560 Training loss: 2.2467 0.3788 sec/batch\n",
      "Epoch 5/20  Iteration 761/3560 Training loss: 2.2463 0.4030 sec/batch\n",
      "Epoch 5/20  Iteration 762/3560 Training loss: 2.2466 0.4038 sec/batch\n",
      "Epoch 5/20  Iteration 763/3560 Training loss: 2.2459 0.3692 sec/batch\n",
      "Epoch 5/20  Iteration 764/3560 Training loss: 2.2464 0.3715 sec/batch\n",
      "Epoch 5/20  Iteration 765/3560 Training loss: 2.2461 0.3975 sec/batch\n",
      "Epoch 5/20  Iteration 766/3560 Training loss: 2.2456 0.4121 sec/batch\n",
      "Epoch 5/20  Iteration 767/3560 Training loss: 2.2452 0.4662 sec/batch\n",
      "Epoch 5/20  Iteration 768/3560 Training loss: 2.2451 0.5139 sec/batch\n",
      "Epoch 5/20  Iteration 769/3560 Training loss: 2.2450 0.4716 sec/batch\n",
      "Epoch 5/20  Iteration 770/3560 Training loss: 2.2447 0.4220 sec/batch\n",
      "Epoch 5/20  Iteration 771/3560 Training loss: 2.2442 0.4445 sec/batch\n",
      "Epoch 5/20  Iteration 772/3560 Training loss: 2.2444 0.4355 sec/batch\n",
      "Epoch 5/20  Iteration 773/3560 Training loss: 2.2441 0.4097 sec/batch\n",
      "Epoch 5/20  Iteration 774/3560 Training loss: 2.2444 0.4020 sec/batch\n",
      "Epoch 5/20  Iteration 775/3560 Training loss: 2.2445 0.4198 sec/batch\n",
      "Epoch 5/20  Iteration 776/3560 Training loss: 2.2442 0.3919 sec/batch\n",
      "Epoch 5/20  Iteration 777/3560 Training loss: 2.2439 0.3951 sec/batch\n",
      "Epoch 5/20  Iteration 778/3560 Training loss: 2.2439 0.3934 sec/batch\n",
      "Epoch 5/20  Iteration 779/3560 Training loss: 2.2437 0.4591 sec/batch\n",
      "Epoch 5/20  Iteration 780/3560 Training loss: 2.2430 0.4149 sec/batch\n",
      "Epoch 5/20  Iteration 781/3560 Training loss: 2.2425 0.3745 sec/batch\n",
      "Epoch 5/20  Iteration 782/3560 Training loss: 2.2423 0.4607 sec/batch\n",
      "Epoch 5/20  Iteration 783/3560 Training loss: 2.2422 0.4705 sec/batch\n",
      "Epoch 5/20  Iteration 784/3560 Training loss: 2.2421 0.4064 sec/batch\n",
      "Epoch 5/20  Iteration 785/3560 Training loss: 2.2422 0.3685 sec/batch\n",
      "Epoch 5/20  Iteration 786/3560 Training loss: 2.2418 0.4514 sec/batch\n",
      "Epoch 5/20  Iteration 787/3560 Training loss: 2.2415 0.4793 sec/batch\n",
      "Epoch 5/20  Iteration 788/3560 Training loss: 2.2418 0.4705 sec/batch\n",
      "Epoch 5/20  Iteration 789/3560 Training loss: 2.2416 0.4746 sec/batch\n",
      "Epoch 5/20  Iteration 790/3560 Training loss: 2.2417 0.4296 sec/batch\n",
      "Epoch 5/20  Iteration 791/3560 Training loss: 2.2412 0.4194 sec/batch\n",
      "Epoch 5/20  Iteration 792/3560 Training loss: 2.2410 0.4978 sec/batch\n",
      "Epoch 5/20  Iteration 793/3560 Training loss: 2.2406 0.5291 sec/batch\n",
      "Epoch 5/20  Iteration 794/3560 Training loss: 2.2406 0.4997 sec/batch\n",
      "Epoch 5/20  Iteration 795/3560 Training loss: 2.2402 0.4974 sec/batch\n",
      "Epoch 5/20  Iteration 796/3560 Training loss: 2.2399 0.4852 sec/batch\n",
      "Epoch 5/20  Iteration 797/3560 Training loss: 2.2391 0.4384 sec/batch\n",
      "Epoch 5/20  Iteration 798/3560 Training loss: 2.2388 0.4028 sec/batch\n",
      "Epoch 5/20  Iteration 799/3560 Training loss: 2.2387 0.5222 sec/batch\n",
      "Epoch 5/20  Iteration 800/3560 Training loss: 2.2384 0.4497 sec/batch\n",
      "Epoch 5/20  Iteration 801/3560 Training loss: 2.2380 0.4396 sec/batch\n",
      "Epoch 5/20  Iteration 802/3560 Training loss: 2.2379 0.4004 sec/batch\n",
      "Epoch 5/20  Iteration 803/3560 Training loss: 2.2377 0.3820 sec/batch\n",
      "Epoch 5/20  Iteration 804/3560 Training loss: 2.2376 0.4633 sec/batch\n",
      "Epoch 5/20  Iteration 805/3560 Training loss: 2.2371 0.4262 sec/batch\n",
      "Epoch 5/20  Iteration 806/3560 Training loss: 2.2367 0.4414 sec/batch\n",
      "Epoch 5/20  Iteration 807/3560 Training loss: 2.2362 0.4561 sec/batch\n",
      "Epoch 5/20  Iteration 808/3560 Training loss: 2.2358 0.4669 sec/batch\n",
      "Epoch 5/20  Iteration 809/3560 Training loss: 2.2355 0.4046 sec/batch\n",
      "Epoch 5/20  Iteration 810/3560 Training loss: 2.2352 0.3872 sec/batch\n",
      "Epoch 5/20  Iteration 811/3560 Training loss: 2.2349 0.4788 sec/batch\n",
      "Epoch 5/20  Iteration 812/3560 Training loss: 2.2346 0.3802 sec/batch\n",
      "Epoch 5/20  Iteration 813/3560 Training loss: 2.2345 0.3974 sec/batch\n",
      "Epoch 5/20  Iteration 814/3560 Training loss: 2.2343 0.3683 sec/batch\n",
      "Epoch 5/20  Iteration 815/3560 Training loss: 2.2339 0.3913 sec/batch\n",
      "Epoch 5/20  Iteration 816/3560 Training loss: 2.2337 0.4213 sec/batch\n",
      "Epoch 5/20  Iteration 817/3560 Training loss: 2.2334 0.4814 sec/batch\n",
      "Epoch 5/20  Iteration 818/3560 Training loss: 2.2332 0.3930 sec/batch\n",
      "Epoch 5/20  Iteration 819/3560 Training loss: 2.2329 0.4204 sec/batch\n",
      "Epoch 5/20  Iteration 820/3560 Training loss: 2.2327 0.3788 sec/batch\n",
      "Epoch 5/20  Iteration 821/3560 Training loss: 2.2326 0.4022 sec/batch\n",
      "Epoch 5/20  Iteration 822/3560 Training loss: 2.2323 0.4223 sec/batch\n",
      "Epoch 5/20  Iteration 823/3560 Training loss: 2.2322 0.4205 sec/batch\n",
      "Epoch 5/20  Iteration 824/3560 Training loss: 2.2320 0.4277 sec/batch\n",
      "Epoch 5/20  Iteration 825/3560 Training loss: 2.2318 0.4469 sec/batch\n",
      "Epoch 5/20  Iteration 826/3560 Training loss: 2.2316 0.5583 sec/batch\n",
      "Epoch 5/20  Iteration 827/3560 Training loss: 2.2313 0.3676 sec/batch\n",
      "Epoch 5/20  Iteration 828/3560 Training loss: 2.2309 0.4082 sec/batch\n",
      "Epoch 5/20  Iteration 829/3560 Training loss: 2.2307 0.4749 sec/batch\n",
      "Epoch 5/20  Iteration 830/3560 Training loss: 2.2305 0.4826 sec/batch\n",
      "Epoch 5/20  Iteration 831/3560 Training loss: 2.2305 0.4332 sec/batch\n",
      "Epoch 5/20  Iteration 832/3560 Training loss: 2.2303 0.4009 sec/batch\n",
      "Epoch 5/20  Iteration 833/3560 Training loss: 2.2303 0.4382 sec/batch\n",
      "Epoch 5/20  Iteration 834/3560 Training loss: 2.2301 0.3749 sec/batch\n",
      "Epoch 5/20  Iteration 835/3560 Training loss: 2.2298 0.4011 sec/batch\n",
      "Epoch 5/20  Iteration 836/3560 Training loss: 2.2298 0.3876 sec/batch\n",
      "Epoch 5/20  Iteration 837/3560 Training loss: 2.2296 0.3946 sec/batch\n",
      "Epoch 5/20  Iteration 838/3560 Training loss: 2.2292 0.4078 sec/batch\n",
      "Epoch 5/20  Iteration 839/3560 Training loss: 2.2291 0.4048 sec/batch\n",
      "Epoch 5/20  Iteration 840/3560 Training loss: 2.2290 0.4095 sec/batch\n",
      "Epoch 5/20  Iteration 841/3560 Training loss: 2.2289 0.3888 sec/batch\n",
      "Epoch 5/20  Iteration 842/3560 Training loss: 2.2288 0.3847 sec/batch\n",
      "Epoch 5/20  Iteration 843/3560 Training loss: 2.2286 0.4184 sec/batch\n",
      "Epoch 5/20  Iteration 844/3560 Training loss: 2.2282 0.4724 sec/batch\n",
      "Epoch 5/20  Iteration 845/3560 Training loss: 2.2281 0.4399 sec/batch\n",
      "Epoch 5/20  Iteration 846/3560 Training loss: 2.2280 0.5221 sec/batch\n",
      "Epoch 5/20  Iteration 847/3560 Training loss: 2.2279 0.4462 sec/batch\n",
      "Epoch 5/20  Iteration 848/3560 Training loss: 2.2279 0.4416 sec/batch\n",
      "Epoch 5/20  Iteration 849/3560 Training loss: 2.2278 0.3862 sec/batch\n",
      "Epoch 5/20  Iteration 850/3560 Training loss: 2.2277 0.4016 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 851/3560 Training loss: 2.2278 0.3981 sec/batch\n",
      "Epoch 5/20  Iteration 852/3560 Training loss: 2.2276 0.4164 sec/batch\n",
      "Epoch 5/20  Iteration 853/3560 Training loss: 2.2276 0.4274 sec/batch\n",
      "Epoch 5/20  Iteration 854/3560 Training loss: 2.2274 0.4025 sec/batch\n",
      "Epoch 5/20  Iteration 855/3560 Training loss: 2.2274 0.4014 sec/batch\n",
      "Epoch 5/20  Iteration 856/3560 Training loss: 2.2272 0.4180 sec/batch\n",
      "Epoch 5/20  Iteration 857/3560 Training loss: 2.2271 0.4178 sec/batch\n",
      "Epoch 5/20  Iteration 858/3560 Training loss: 2.2271 0.3788 sec/batch\n",
      "Epoch 5/20  Iteration 859/3560 Training loss: 2.2270 0.3888 sec/batch\n",
      "Epoch 5/20  Iteration 860/3560 Training loss: 2.2270 0.4036 sec/batch\n",
      "Epoch 5/20  Iteration 861/3560 Training loss: 2.2269 0.4016 sec/batch\n",
      "Epoch 5/20  Iteration 862/3560 Training loss: 2.2265 0.4924 sec/batch\n",
      "Epoch 5/20  Iteration 863/3560 Training loss: 2.2264 0.3778 sec/batch\n",
      "Epoch 5/20  Iteration 864/3560 Training loss: 2.2264 0.4962 sec/batch\n",
      "Epoch 5/20  Iteration 865/3560 Training loss: 2.2264 0.4654 sec/batch\n",
      "Epoch 5/20  Iteration 866/3560 Training loss: 2.2264 0.4033 sec/batch\n",
      "Epoch 5/20  Iteration 867/3560 Training loss: 2.2262 0.3740 sec/batch\n",
      "Epoch 5/20  Iteration 868/3560 Training loss: 2.2261 0.4425 sec/batch\n",
      "Epoch 5/20  Iteration 869/3560 Training loss: 2.2259 0.4061 sec/batch\n",
      "Epoch 5/20  Iteration 870/3560 Training loss: 2.2257 0.3922 sec/batch\n",
      "Epoch 5/20  Iteration 871/3560 Training loss: 2.2255 0.4092 sec/batch\n",
      "Epoch 5/20  Iteration 872/3560 Training loss: 2.2255 0.3938 sec/batch\n",
      "Epoch 5/20  Iteration 873/3560 Training loss: 2.2254 0.3865 sec/batch\n",
      "Epoch 5/20  Iteration 874/3560 Training loss: 2.2252 0.3891 sec/batch\n",
      "Epoch 5/20  Iteration 875/3560 Training loss: 2.2251 0.4048 sec/batch\n",
      "Epoch 5/20  Iteration 876/3560 Training loss: 2.2250 0.4194 sec/batch\n",
      "Epoch 5/20  Iteration 877/3560 Training loss: 2.2250 0.3927 sec/batch\n",
      "Epoch 5/20  Iteration 878/3560 Training loss: 2.2249 0.3670 sec/batch\n",
      "Epoch 5/20  Iteration 879/3560 Training loss: 2.2249 0.4014 sec/batch\n",
      "Epoch 5/20  Iteration 880/3560 Training loss: 2.2249 0.4368 sec/batch\n",
      "Epoch 5/20  Iteration 881/3560 Training loss: 2.2248 0.4249 sec/batch\n",
      "Epoch 5/20  Iteration 882/3560 Training loss: 2.2247 0.3980 sec/batch\n",
      "Epoch 5/20  Iteration 883/3560 Training loss: 2.2246 0.4659 sec/batch\n",
      "Epoch 5/20  Iteration 884/3560 Training loss: 2.2244 0.4463 sec/batch\n",
      "Epoch 5/20  Iteration 885/3560 Training loss: 2.2243 0.4420 sec/batch\n",
      "Epoch 5/20  Iteration 886/3560 Training loss: 2.2243 0.4243 sec/batch\n",
      "Epoch 5/20  Iteration 887/3560 Training loss: 2.2243 0.4001 sec/batch\n",
      "Epoch 5/20  Iteration 888/3560 Training loss: 2.2242 0.3674 sec/batch\n",
      "Epoch 5/20  Iteration 889/3560 Training loss: 2.2240 0.4205 sec/batch\n",
      "Epoch 5/20  Iteration 890/3560 Training loss: 2.2238 0.4365 sec/batch\n",
      "Epoch 6/20  Iteration 891/3560 Training loss: 2.2763 0.4006 sec/batch\n",
      "Epoch 6/20  Iteration 892/3560 Training loss: 2.2193 0.3761 sec/batch\n",
      "Epoch 6/20  Iteration 893/3560 Training loss: 2.2036 0.3957 sec/batch\n",
      "Epoch 6/20  Iteration 894/3560 Training loss: 2.1993 0.4409 sec/batch\n",
      "Epoch 6/20  Iteration 895/3560 Training loss: 2.1974 0.3867 sec/batch\n",
      "Epoch 6/20  Iteration 896/3560 Training loss: 2.1942 0.4174 sec/batch\n",
      "Epoch 6/20  Iteration 897/3560 Training loss: 2.1957 0.4093 sec/batch\n",
      "Epoch 6/20  Iteration 898/3560 Training loss: 2.1969 0.4507 sec/batch\n",
      "Epoch 6/20  Iteration 899/3560 Training loss: 2.1988 0.3542 sec/batch\n",
      "Epoch 6/20  Iteration 900/3560 Training loss: 2.1985 0.4199 sec/batch\n",
      "Epoch 6/20  Iteration 901/3560 Training loss: 2.1970 0.4462 sec/batch\n",
      "Epoch 6/20  Iteration 902/3560 Training loss: 2.1956 0.4025 sec/batch\n",
      "Epoch 6/20  Iteration 903/3560 Training loss: 2.1960 0.4847 sec/batch\n",
      "Epoch 6/20  Iteration 904/3560 Training loss: 2.1987 0.4758 sec/batch\n",
      "Epoch 6/20  Iteration 905/3560 Training loss: 2.1978 0.4116 sec/batch\n",
      "Epoch 6/20  Iteration 906/3560 Training loss: 2.1972 0.4472 sec/batch\n",
      "Epoch 6/20  Iteration 907/3560 Training loss: 2.1972 0.4161 sec/batch\n",
      "Epoch 6/20  Iteration 908/3560 Training loss: 2.1995 0.3972 sec/batch\n",
      "Epoch 6/20  Iteration 909/3560 Training loss: 2.1993 0.4099 sec/batch\n",
      "Epoch 6/20  Iteration 910/3560 Training loss: 2.1983 0.3583 sec/batch\n",
      "Epoch 6/20  Iteration 911/3560 Training loss: 2.1971 0.3810 sec/batch\n",
      "Epoch 6/20  Iteration 912/3560 Training loss: 2.1976 0.4441 sec/batch\n",
      "Epoch 6/20  Iteration 913/3560 Training loss: 2.1973 0.3816 sec/batch\n",
      "Epoch 6/20  Iteration 914/3560 Training loss: 2.1969 0.3981 sec/batch\n",
      "Epoch 6/20  Iteration 915/3560 Training loss: 2.1962 0.4055 sec/batch\n",
      "Epoch 6/20  Iteration 916/3560 Training loss: 2.1956 0.4111 sec/batch\n",
      "Epoch 6/20  Iteration 917/3560 Training loss: 2.1952 0.4110 sec/batch\n",
      "Epoch 6/20  Iteration 918/3560 Training loss: 2.1956 0.3963 sec/batch\n",
      "Epoch 6/20  Iteration 919/3560 Training loss: 2.1962 0.3967 sec/batch\n",
      "Epoch 6/20  Iteration 920/3560 Training loss: 2.1962 0.4207 sec/batch\n",
      "Epoch 6/20  Iteration 921/3560 Training loss: 2.1963 0.4622 sec/batch\n",
      "Epoch 6/20  Iteration 922/3560 Training loss: 2.1961 0.4496 sec/batch\n",
      "Epoch 6/20  Iteration 923/3560 Training loss: 2.1959 0.5219 sec/batch\n",
      "Epoch 6/20  Iteration 924/3560 Training loss: 2.1966 0.4578 sec/batch\n",
      "Epoch 6/20  Iteration 925/3560 Training loss: 2.1963 0.3858 sec/batch\n",
      "Epoch 6/20  Iteration 926/3560 Training loss: 2.1963 0.4457 sec/batch\n",
      "Epoch 6/20  Iteration 927/3560 Training loss: 2.1960 0.4480 sec/batch\n",
      "Epoch 6/20  Iteration 928/3560 Training loss: 2.1950 0.4648 sec/batch\n",
      "Epoch 6/20  Iteration 929/3560 Training loss: 2.1942 0.4169 sec/batch\n",
      "Epoch 6/20  Iteration 930/3560 Training loss: 2.1934 0.3787 sec/batch\n",
      "Epoch 6/20  Iteration 931/3560 Training loss: 2.1929 0.3806 sec/batch\n",
      "Epoch 6/20  Iteration 932/3560 Training loss: 2.1925 0.4322 sec/batch\n",
      "Epoch 6/20  Iteration 933/3560 Training loss: 2.1920 0.4110 sec/batch\n",
      "Epoch 6/20  Iteration 934/3560 Training loss: 2.1914 0.3758 sec/batch\n",
      "Epoch 6/20  Iteration 935/3560 Training loss: 2.1911 0.3919 sec/batch\n",
      "Epoch 6/20  Iteration 936/3560 Training loss: 2.1899 0.3746 sec/batch\n",
      "Epoch 6/20  Iteration 937/3560 Training loss: 2.1898 0.4308 sec/batch\n",
      "Epoch 6/20  Iteration 938/3560 Training loss: 2.1892 0.3947 sec/batch\n",
      "Epoch 6/20  Iteration 939/3560 Training loss: 2.1890 0.4175 sec/batch\n",
      "Epoch 6/20  Iteration 940/3560 Training loss: 2.1894 0.4171 sec/batch\n",
      "Epoch 6/20  Iteration 941/3560 Training loss: 2.1889 0.5082 sec/batch\n",
      "Epoch 6/20  Iteration 942/3560 Training loss: 2.1891 0.4180 sec/batch\n",
      "Epoch 6/20  Iteration 943/3560 Training loss: 2.1887 0.4677 sec/batch\n",
      "Epoch 6/20  Iteration 944/3560 Training loss: 2.1883 0.4382 sec/batch\n",
      "Epoch 6/20  Iteration 945/3560 Training loss: 2.1879 0.4214 sec/batch\n",
      "Epoch 6/20  Iteration 946/3560 Training loss: 2.1878 0.3929 sec/batch\n",
      "Epoch 6/20  Iteration 947/3560 Training loss: 2.1879 0.4574 sec/batch\n",
      "Epoch 6/20  Iteration 948/3560 Training loss: 2.1874 0.3725 sec/batch\n",
      "Epoch 6/20  Iteration 949/3560 Training loss: 2.1870 0.4254 sec/batch\n",
      "Epoch 6/20  Iteration 950/3560 Training loss: 2.1872 0.4375 sec/batch\n",
      "Epoch 6/20  Iteration 951/3560 Training loss: 2.1869 0.4096 sec/batch\n",
      "Epoch 6/20  Iteration 952/3560 Training loss: 2.1871 0.3985 sec/batch\n",
      "Epoch 6/20  Iteration 953/3560 Training loss: 2.1872 0.3654 sec/batch\n",
      "Epoch 6/20  Iteration 954/3560 Training loss: 2.1871 0.3946 sec/batch\n",
      "Epoch 6/20  Iteration 955/3560 Training loss: 2.1867 0.3796 sec/batch\n",
      "Epoch 6/20  Iteration 956/3560 Training loss: 2.1868 0.3850 sec/batch\n",
      "Epoch 6/20  Iteration 957/3560 Training loss: 2.1867 0.3848 sec/batch\n",
      "Epoch 6/20  Iteration 958/3560 Training loss: 2.1861 0.4642 sec/batch\n",
      "Epoch 6/20  Iteration 959/3560 Training loss: 2.1859 0.3833 sec/batch\n",
      "Epoch 6/20  Iteration 960/3560 Training loss: 2.1857 0.4590 sec/batch\n",
      "Epoch 6/20  Iteration 961/3560 Training loss: 2.1857 0.4344 sec/batch\n",
      "Epoch 6/20  Iteration 962/3560 Training loss: 2.1858 0.4999 sec/batch\n",
      "Epoch 6/20  Iteration 963/3560 Training loss: 2.1859 0.3660 sec/batch\n",
      "Epoch 6/20  Iteration 964/3560 Training loss: 2.1856 0.4622 sec/batch\n",
      "Epoch 6/20  Iteration 965/3560 Training loss: 2.1854 0.3853 sec/batch\n",
      "Epoch 6/20  Iteration 966/3560 Training loss: 2.1856 0.3710 sec/batch\n",
      "Epoch 6/20  Iteration 967/3560 Training loss: 2.1853 0.4840 sec/batch\n",
      "Epoch 6/20  Iteration 968/3560 Training loss: 2.1855 0.3345 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 969/3560 Training loss: 2.1850 0.3831 sec/batch\n",
      "Epoch 6/20  Iteration 970/3560 Training loss: 2.1848 0.3782 sec/batch\n",
      "Epoch 6/20  Iteration 971/3560 Training loss: 2.1843 0.3993 sec/batch\n",
      "Epoch 6/20  Iteration 972/3560 Training loss: 2.1844 0.4165 sec/batch\n",
      "Epoch 6/20  Iteration 973/3560 Training loss: 2.1839 0.4079 sec/batch\n",
      "Epoch 6/20  Iteration 974/3560 Training loss: 2.1837 0.4323 sec/batch\n",
      "Epoch 6/20  Iteration 975/3560 Training loss: 2.1830 0.3871 sec/batch\n",
      "Epoch 6/20  Iteration 976/3560 Training loss: 2.1826 0.4281 sec/batch\n",
      "Epoch 6/20  Iteration 977/3560 Training loss: 2.1825 0.4131 sec/batch\n",
      "Epoch 6/20  Iteration 978/3560 Training loss: 2.1822 0.4150 sec/batch\n",
      "Epoch 6/20  Iteration 979/3560 Training loss: 2.1819 0.4614 sec/batch\n",
      "Epoch 6/20  Iteration 980/3560 Training loss: 2.1818 0.4534 sec/batch\n",
      "Epoch 6/20  Iteration 981/3560 Training loss: 2.1816 0.4680 sec/batch\n",
      "Epoch 6/20  Iteration 982/3560 Training loss: 2.1815 0.3889 sec/batch\n",
      "Epoch 6/20  Iteration 983/3560 Training loss: 2.1811 0.5085 sec/batch\n",
      "Epoch 6/20  Iteration 984/3560 Training loss: 2.1807 0.3863 sec/batch\n",
      "Epoch 6/20  Iteration 985/3560 Training loss: 2.1803 0.3966 sec/batch\n",
      "Epoch 6/20  Iteration 986/3560 Training loss: 2.1801 0.3893 sec/batch\n",
      "Epoch 6/20  Iteration 987/3560 Training loss: 2.1799 0.3778 sec/batch\n",
      "Epoch 6/20  Iteration 988/3560 Training loss: 2.1796 0.4062 sec/batch\n",
      "Epoch 6/20  Iteration 989/3560 Training loss: 2.1792 0.3972 sec/batch\n",
      "Epoch 6/20  Iteration 990/3560 Training loss: 2.1788 0.3977 sec/batch\n",
      "Epoch 6/20  Iteration 991/3560 Training loss: 2.1788 0.3966 sec/batch\n",
      "Epoch 6/20  Iteration 992/3560 Training loss: 2.1788 0.4139 sec/batch\n",
      "Epoch 6/20  Iteration 993/3560 Training loss: 2.1784 0.3782 sec/batch\n",
      "Epoch 6/20  Iteration 994/3560 Training loss: 2.1782 0.4386 sec/batch\n",
      "Epoch 6/20  Iteration 995/3560 Training loss: 2.1779 0.4188 sec/batch\n",
      "Epoch 6/20  Iteration 996/3560 Training loss: 2.1778 0.4367 sec/batch\n",
      "Epoch 6/20  Iteration 997/3560 Training loss: 2.1776 0.4607 sec/batch\n",
      "Epoch 6/20  Iteration 998/3560 Training loss: 2.1774 0.4223 sec/batch\n",
      "Epoch 6/20  Iteration 999/3560 Training loss: 2.1773 0.4551 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3560 Training loss: 2.1771 0.4829 sec/batch\n",
      "Epoch 6/20  Iteration 1001/3560 Training loss: 2.1770 0.4082 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3560 Training loss: 2.1769 0.3863 sec/batch\n",
      "Epoch 6/20  Iteration 1003/3560 Training loss: 2.1768 0.4316 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3560 Training loss: 2.1766 0.3820 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3560 Training loss: 2.1764 0.4234 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3560 Training loss: 2.1761 0.4146 sec/batch\n",
      "Epoch 6/20  Iteration 1007/3560 Training loss: 2.1760 0.3948 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3560 Training loss: 2.1758 0.3579 sec/batch\n",
      "Epoch 6/20  Iteration 1009/3560 Training loss: 2.1758 0.4253 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3560 Training loss: 2.1756 0.3815 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3560 Training loss: 2.1755 0.3821 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3560 Training loss: 2.1752 0.3839 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3560 Training loss: 2.1750 0.3986 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3560 Training loss: 2.1750 0.4011 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3560 Training loss: 2.1748 0.4053 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3560 Training loss: 2.1745 0.3926 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3560 Training loss: 2.1744 0.4487 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3560 Training loss: 2.1743 0.5670 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3560 Training loss: 2.1742 0.4518 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3560 Training loss: 2.1742 0.4557 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3560 Training loss: 2.1740 0.4081 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3560 Training loss: 2.1736 0.4256 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3560 Training loss: 2.1735 0.4357 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3560 Training loss: 2.1734 0.3820 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3560 Training loss: 2.1732 0.4969 sec/batch\n",
      "Epoch 6/20  Iteration 1026/3560 Training loss: 2.1732 0.3150 sec/batch\n",
      "Epoch 6/20  Iteration 1027/3560 Training loss: 2.1732 0.3853 sec/batch\n",
      "Epoch 6/20  Iteration 1028/3560 Training loss: 2.1731 0.3518 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3560 Training loss: 2.1733 0.4150 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3560 Training loss: 2.1730 0.4098 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3560 Training loss: 2.1731 0.4271 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3560 Training loss: 2.1730 0.3916 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3560 Training loss: 2.1729 0.3862 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3560 Training loss: 2.1727 0.4096 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3560 Training loss: 2.1726 0.4108 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3560 Training loss: 2.1727 0.4524 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3560 Training loss: 2.1727 0.3778 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3560 Training loss: 2.1728 0.5428 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3560 Training loss: 2.1727 0.3843 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3560 Training loss: 2.1724 0.4311 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3560 Training loss: 2.1722 0.4078 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3560 Training loss: 2.1724 0.4269 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3560 Training loss: 2.1724 0.4045 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3560 Training loss: 2.1724 0.3938 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3560 Training loss: 2.1723 0.4405 sec/batch\n",
      "Epoch 6/20  Iteration 1046/3560 Training loss: 2.1721 0.3785 sec/batch\n",
      "Epoch 6/20  Iteration 1047/3560 Training loss: 2.1719 0.4323 sec/batch\n",
      "Epoch 6/20  Iteration 1048/3560 Training loss: 2.1719 0.3907 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3560 Training loss: 2.1717 0.3934 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3560 Training loss: 2.1717 0.4610 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3560 Training loss: 2.1718 0.3849 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3560 Training loss: 2.1716 0.4299 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3560 Training loss: 2.1715 0.3744 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3560 Training loss: 2.1714 0.4159 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3560 Training loss: 2.1714 0.4490 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3560 Training loss: 2.1712 0.4784 sec/batch\n",
      "Epoch 6/20  Iteration 1057/3560 Training loss: 2.1712 0.4031 sec/batch\n",
      "Epoch 6/20  Iteration 1058/3560 Training loss: 2.1713 0.4838 sec/batch\n",
      "Epoch 6/20  Iteration 1059/3560 Training loss: 2.1712 0.4103 sec/batch\n",
      "Epoch 6/20  Iteration 1060/3560 Training loss: 2.1711 0.4100 sec/batch\n",
      "Epoch 6/20  Iteration 1061/3560 Training loss: 2.1710 0.3759 sec/batch\n",
      "Epoch 6/20  Iteration 1062/3560 Training loss: 2.1708 0.4459 sec/batch\n",
      "Epoch 6/20  Iteration 1063/3560 Training loss: 2.1708 0.3778 sec/batch\n",
      "Epoch 6/20  Iteration 1064/3560 Training loss: 2.1708 0.4058 sec/batch\n",
      "Epoch 6/20  Iteration 1065/3560 Training loss: 2.1708 0.4020 sec/batch\n",
      "Epoch 6/20  Iteration 1066/3560 Training loss: 2.1707 0.4493 sec/batch\n",
      "Epoch 6/20  Iteration 1067/3560 Training loss: 2.1705 0.3900 sec/batch\n",
      "Epoch 6/20  Iteration 1068/3560 Training loss: 2.1704 0.3690 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3560 Training loss: 2.2166 0.4214 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3560 Training loss: 2.1698 0.4142 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3560 Training loss: 2.1558 0.3776 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3560 Training loss: 2.1511 0.3965 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3560 Training loss: 2.1483 0.5089 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3560 Training loss: 2.1439 0.3698 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3560 Training loss: 2.1454 0.4284 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3560 Training loss: 2.1479 0.4867 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3560 Training loss: 2.1501 0.4384 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3560 Training loss: 2.1495 0.4085 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3560 Training loss: 2.1479 0.4613 sec/batch\n",
      "Epoch 7/20  Iteration 1080/3560 Training loss: 2.1465 0.3916 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3560 Training loss: 2.1463 0.4070 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3560 Training loss: 2.1489 0.3775 sec/batch\n",
      "Epoch 7/20  Iteration 1083/3560 Training loss: 2.1487 0.3872 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3560 Training loss: 2.1481 0.4049 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1085/3560 Training loss: 2.1474 0.3840 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3560 Training loss: 2.1497 0.4343 sec/batch\n",
      "Epoch 7/20  Iteration 1087/3560 Training loss: 2.1499 0.3559 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3560 Training loss: 2.1497 0.4043 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3560 Training loss: 2.1489 0.4046 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3560 Training loss: 2.1495 0.4168 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3560 Training loss: 2.1492 0.4146 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3560 Training loss: 2.1487 0.4294 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3560 Training loss: 2.1483 0.4607 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3560 Training loss: 2.1474 0.3984 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3560 Training loss: 2.1466 0.4898 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3560 Training loss: 2.1469 0.5045 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3560 Training loss: 2.1477 0.4060 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3560 Training loss: 2.1477 0.4460 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3560 Training loss: 2.1478 0.4812 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3560 Training loss: 2.1473 0.3623 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3560 Training loss: 2.1470 0.3816 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3560 Training loss: 2.1475 0.3989 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3560 Training loss: 2.1473 0.4206 sec/batch\n",
      "Epoch 7/20  Iteration 1104/3560 Training loss: 2.1473 0.4155 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3560 Training loss: 2.1470 0.4315 sec/batch\n",
      "Epoch 7/20  Iteration 1106/3560 Training loss: 2.1457 0.3869 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3560 Training loss: 2.1449 0.4007 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3560 Training loss: 2.1443 0.4090 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3560 Training loss: 2.1437 0.3941 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3560 Training loss: 2.1434 0.4073 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3560 Training loss: 2.1428 0.4098 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3560 Training loss: 2.1420 0.4156 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3560 Training loss: 2.1419 0.4476 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3560 Training loss: 2.1405 0.4121 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3560 Training loss: 2.1405 0.4507 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3560 Training loss: 2.1400 0.5298 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3560 Training loss: 2.1398 0.4036 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3560 Training loss: 2.1402 0.4227 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3560 Training loss: 2.1395 0.4102 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3560 Training loss: 2.1400 0.4446 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3560 Training loss: 2.1396 0.3763 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3560 Training loss: 2.1393 0.3894 sec/batch\n",
      "Epoch 7/20  Iteration 1123/3560 Training loss: 2.1390 0.3813 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3560 Training loss: 2.1389 0.3860 sec/batch\n",
      "Epoch 7/20  Iteration 1125/3560 Training loss: 2.1390 0.4274 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3560 Training loss: 2.1387 0.3526 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3560 Training loss: 2.1382 0.4357 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3560 Training loss: 2.1386 0.3810 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3560 Training loss: 2.1384 0.3746 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3560 Training loss: 2.1387 0.3955 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3560 Training loss: 2.1391 0.3991 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3560 Training loss: 2.1390 0.4355 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3560 Training loss: 2.1389 0.5169 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3560 Training loss: 2.1389 0.4204 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3560 Training loss: 2.1390 0.4349 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3560 Training loss: 2.1385 0.4093 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3560 Training loss: 2.1382 0.4533 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3560 Training loss: 2.1382 0.3495 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3560 Training loss: 2.1384 0.4633 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3560 Training loss: 2.1385 0.3824 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3560 Training loss: 2.1388 0.4353 sec/batch\n",
      "Epoch 7/20  Iteration 1142/3560 Training loss: 2.1385 0.3519 sec/batch\n",
      "Epoch 7/20  Iteration 1143/3560 Training loss: 2.1382 0.4373 sec/batch\n",
      "Epoch 7/20  Iteration 1144/3560 Training loss: 2.1385 0.4016 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3560 Training loss: 2.1384 0.4244 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3560 Training loss: 2.1385 0.3496 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3560 Training loss: 2.1380 0.3877 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3560 Training loss: 2.1379 0.3677 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3560 Training loss: 2.1375 0.4512 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3560 Training loss: 2.1375 0.3991 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3560 Training loss: 2.1369 0.4238 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3560 Training loss: 2.1367 0.4420 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3560 Training loss: 2.1360 0.4724 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3560 Training loss: 2.1357 0.4453 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3560 Training loss: 2.1356 0.4623 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3560 Training loss: 2.1354 0.3989 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3560 Training loss: 2.1349 0.4046 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3560 Training loss: 2.1348 0.3840 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3560 Training loss: 2.1346 0.4196 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3560 Training loss: 2.1345 0.4435 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3560 Training loss: 2.1340 0.3758 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3560 Training loss: 2.1337 0.4320 sec/batch\n",
      "Epoch 7/20  Iteration 1163/3560 Training loss: 2.1333 0.4633 sec/batch\n",
      "Epoch 7/20  Iteration 1164/3560 Training loss: 2.1331 0.3745 sec/batch\n",
      "Epoch 7/20  Iteration 1165/3560 Training loss: 2.1329 0.4144 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3560 Training loss: 2.1326 0.4146 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3560 Training loss: 2.1322 0.4263 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3560 Training loss: 2.1318 0.4490 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3560 Training loss: 2.1317 0.4432 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3560 Training loss: 2.1316 0.6213 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3560 Training loss: 2.1312 0.5888 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3560 Training loss: 2.1310 0.5289 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3560 Training loss: 2.1307 0.5534 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3560 Training loss: 2.1305 0.5280 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3560 Training loss: 2.1304 0.4864 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3560 Training loss: 2.1303 0.5081 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3560 Training loss: 2.1303 0.4664 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3560 Training loss: 2.1301 0.4178 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3560 Training loss: 2.1301 0.4525 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3560 Training loss: 2.1300 0.3697 sec/batch\n",
      "Epoch 7/20  Iteration 1181/3560 Training loss: 2.1297 0.4058 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3560 Training loss: 2.1295 0.3787 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3560 Training loss: 2.1293 0.4231 sec/batch\n",
      "Epoch 7/20  Iteration 1184/3560 Training loss: 2.1288 0.4061 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3560 Training loss: 2.1286 0.4005 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3560 Training loss: 2.1285 0.4129 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3560 Training loss: 2.1285 0.4273 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3560 Training loss: 2.1282 0.4195 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3560 Training loss: 2.1282 0.5229 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3560 Training loss: 2.1279 0.4741 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3560 Training loss: 2.1278 0.5390 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3560 Training loss: 2.1279 0.4569 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3560 Training loss: 2.1277 0.4207 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3560 Training loss: 2.1274 0.5357 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3560 Training loss: 2.1274 0.3642 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3560 Training loss: 2.1274 0.3701 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3560 Training loss: 2.1273 0.4236 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3560 Training loss: 2.1272 0.4057 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3560 Training loss: 2.1269 0.4180 sec/batch\n",
      "Epoch 7/20  Iteration 1200/3560 Training loss: 2.1267 0.4079 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1201/3560 Training loss: 2.1266 0.4092 sec/batch\n",
      "Epoch 7/20  Iteration 1202/3560 Training loss: 2.1265 0.4967 sec/batch\n",
      "Epoch 7/20  Iteration 1203/3560 Training loss: 2.1264 0.3972 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3560 Training loss: 2.1264 0.4233 sec/batch\n",
      "Epoch 7/20  Iteration 1205/3560 Training loss: 2.1264 0.4023 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3560 Training loss: 2.1263 0.4386 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3560 Training loss: 2.1265 0.4520 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3560 Training loss: 2.1263 0.3513 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3560 Training loss: 2.1264 0.4874 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3560 Training loss: 2.1262 0.4628 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3560 Training loss: 2.1262 0.5103 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3560 Training loss: 2.1260 0.4068 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3560 Training loss: 2.1259 0.4383 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3560 Training loss: 2.1259 0.4203 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3560 Training loss: 2.1259 0.5230 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3560 Training loss: 2.1260 0.3693 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3560 Training loss: 2.1259 0.4142 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3560 Training loss: 2.1257 0.6004 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3560 Training loss: 2.1255 0.4469 sec/batch\n",
      "Epoch 7/20  Iteration 1220/3560 Training loss: 2.1256 0.5573 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3560 Training loss: 2.1256 0.3878 sec/batch\n",
      "Epoch 7/20  Iteration 1222/3560 Training loss: 2.1256 0.4556 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3560 Training loss: 2.1255 0.4033 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3560 Training loss: 2.1254 0.4035 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3560 Training loss: 2.1253 0.4887 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3560 Training loss: 2.1252 0.4260 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3560 Training loss: 2.1249 0.4375 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3560 Training loss: 2.1250 0.4911 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3560 Training loss: 2.1251 0.4656 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3560 Training loss: 2.1249 0.4047 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3560 Training loss: 2.1249 0.5399 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3560 Training loss: 2.1248 0.4763 sec/batch\n",
      "Epoch 7/20  Iteration 1233/3560 Training loss: 2.1248 0.5610 sec/batch\n",
      "Epoch 7/20  Iteration 1234/3560 Training loss: 2.1247 0.4080 sec/batch\n",
      "Epoch 7/20  Iteration 1235/3560 Training loss: 2.1247 0.4287 sec/batch\n",
      "Epoch 7/20  Iteration 1236/3560 Training loss: 2.1247 0.5031 sec/batch\n",
      "Epoch 7/20  Iteration 1237/3560 Training loss: 2.1247 0.5166 sec/batch\n",
      "Epoch 7/20  Iteration 1238/3560 Training loss: 2.1245 0.4279 sec/batch\n",
      "Epoch 7/20  Iteration 1239/3560 Training loss: 2.1244 0.5197 sec/batch\n",
      "Epoch 7/20  Iteration 1240/3560 Training loss: 2.1243 0.4434 sec/batch\n",
      "Epoch 7/20  Iteration 1241/3560 Training loss: 2.1244 0.5563 sec/batch\n",
      "Epoch 7/20  Iteration 1242/3560 Training loss: 2.1243 0.4875 sec/batch\n",
      "Epoch 7/20  Iteration 1243/3560 Training loss: 2.1243 0.4565 sec/batch\n",
      "Epoch 7/20  Iteration 1244/3560 Training loss: 2.1243 0.4075 sec/batch\n",
      "Epoch 7/20  Iteration 1245/3560 Training loss: 2.1241 0.4166 sec/batch\n",
      "Epoch 7/20  Iteration 1246/3560 Training loss: 2.1240 0.4669 sec/batch\n",
      "Epoch 8/20  Iteration 1247/3560 Training loss: 2.1715 0.5041 sec/batch\n",
      "Epoch 8/20  Iteration 1248/3560 Training loss: 2.1251 0.4792 sec/batch\n",
      "Epoch 8/20  Iteration 1249/3560 Training loss: 2.1114 0.4722 sec/batch\n",
      "Epoch 8/20  Iteration 1250/3560 Training loss: 2.1074 0.4287 sec/batch\n",
      "Epoch 8/20  Iteration 1251/3560 Training loss: 2.1049 0.5079 sec/batch\n",
      "Epoch 8/20  Iteration 1252/3560 Training loss: 2.0989 0.5805 sec/batch\n",
      "Epoch 8/20  Iteration 1253/3560 Training loss: 2.0985 0.5400 sec/batch\n",
      "Epoch 8/20  Iteration 1254/3560 Training loss: 2.0997 0.4599 sec/batch\n",
      "Epoch 8/20  Iteration 1255/3560 Training loss: 2.1019 0.3863 sec/batch\n",
      "Epoch 8/20  Iteration 1256/3560 Training loss: 2.1010 0.4366 sec/batch\n",
      "Epoch 8/20  Iteration 1257/3560 Training loss: 2.0996 0.4393 sec/batch\n",
      "Epoch 8/20  Iteration 1258/3560 Training loss: 2.0983 0.3995 sec/batch\n",
      "Epoch 8/20  Iteration 1259/3560 Training loss: 2.0987 0.3793 sec/batch\n",
      "Epoch 8/20  Iteration 1260/3560 Training loss: 2.1012 0.4418 sec/batch\n",
      "Epoch 8/20  Iteration 1261/3560 Training loss: 2.1006 0.3800 sec/batch\n",
      "Epoch 8/20  Iteration 1262/3560 Training loss: 2.1001 0.3893 sec/batch\n",
      "Epoch 8/20  Iteration 1263/3560 Training loss: 2.1006 0.4359 sec/batch\n",
      "Epoch 8/20  Iteration 1264/3560 Training loss: 2.1030 0.3763 sec/batch\n",
      "Epoch 8/20  Iteration 1265/3560 Training loss: 2.1035 0.4896 sec/batch\n",
      "Epoch 8/20  Iteration 1266/3560 Training loss: 2.1035 0.4838 sec/batch\n",
      "Epoch 8/20  Iteration 1267/3560 Training loss: 2.1027 0.5576 sec/batch\n",
      "Epoch 8/20  Iteration 1268/3560 Training loss: 2.1038 0.4687 sec/batch\n",
      "Epoch 8/20  Iteration 1269/3560 Training loss: 2.1035 0.5689 sec/batch\n",
      "Epoch 8/20  Iteration 1270/3560 Training loss: 2.1027 0.5833 sec/batch\n",
      "Epoch 8/20  Iteration 1271/3560 Training loss: 2.1024 0.5114 sec/batch\n",
      "Epoch 8/20  Iteration 1272/3560 Training loss: 2.1016 0.4457 sec/batch\n",
      "Epoch 8/20  Iteration 1273/3560 Training loss: 2.1009 0.4656 sec/batch\n",
      "Epoch 8/20  Iteration 1274/3560 Training loss: 2.1010 0.5000 sec/batch\n",
      "Epoch 8/20  Iteration 1275/3560 Training loss: 2.1019 0.4116 sec/batch\n",
      "Epoch 8/20  Iteration 1276/3560 Training loss: 2.1021 0.4750 sec/batch\n",
      "Epoch 8/20  Iteration 1277/3560 Training loss: 2.1021 0.5609 sec/batch\n",
      "Epoch 8/20  Iteration 1278/3560 Training loss: 2.1016 0.4339 sec/batch\n",
      "Epoch 8/20  Iteration 1279/3560 Training loss: 2.1013 0.4628 sec/batch\n",
      "Epoch 8/20  Iteration 1280/3560 Training loss: 2.1023 0.4214 sec/batch\n",
      "Epoch 8/20  Iteration 1281/3560 Training loss: 2.1018 0.5113 sec/batch\n",
      "Epoch 8/20  Iteration 1282/3560 Training loss: 2.1018 0.3477 sec/batch\n",
      "Epoch 8/20  Iteration 1283/3560 Training loss: 2.1017 0.4440 sec/batch\n",
      "Epoch 8/20  Iteration 1284/3560 Training loss: 2.1004 0.4728 sec/batch\n",
      "Epoch 8/20  Iteration 1285/3560 Training loss: 2.0994 0.5847 sec/batch\n",
      "Epoch 8/20  Iteration 1286/3560 Training loss: 2.0986 0.5390 sec/batch\n",
      "Epoch 8/20  Iteration 1287/3560 Training loss: 2.0982 0.4082 sec/batch\n",
      "Epoch 8/20  Iteration 1288/3560 Training loss: 2.0980 0.4615 sec/batch\n",
      "Epoch 8/20  Iteration 1289/3560 Training loss: 2.0976 0.4341 sec/batch\n",
      "Epoch 8/20  Iteration 1290/3560 Training loss: 2.0967 0.4598 sec/batch\n",
      "Epoch 8/20  Iteration 1291/3560 Training loss: 2.0968 0.5083 sec/batch\n",
      "Epoch 8/20  Iteration 1292/3560 Training loss: 2.0956 0.5466 sec/batch\n",
      "Epoch 8/20  Iteration 1293/3560 Training loss: 2.0955 0.4198 sec/batch\n",
      "Epoch 8/20  Iteration 1294/3560 Training loss: 2.0951 0.4149 sec/batch\n",
      "Epoch 8/20  Iteration 1295/3560 Training loss: 2.0949 0.3644 sec/batch\n",
      "Epoch 8/20  Iteration 1296/3560 Training loss: 2.0954 0.4161 sec/batch\n",
      "Epoch 8/20  Iteration 1297/3560 Training loss: 2.0948 0.4978 sec/batch\n",
      "Epoch 8/20  Iteration 1298/3560 Training loss: 2.0953 0.3948 sec/batch\n",
      "Epoch 8/20  Iteration 1299/3560 Training loss: 2.0951 0.4868 sec/batch\n",
      "Epoch 8/20  Iteration 1300/3560 Training loss: 2.0949 0.4152 sec/batch\n",
      "Epoch 8/20  Iteration 1301/3560 Training loss: 2.0946 0.4179 sec/batch\n",
      "Epoch 8/20  Iteration 1302/3560 Training loss: 2.0947 0.4037 sec/batch\n",
      "Epoch 8/20  Iteration 1303/3560 Training loss: 2.0947 0.4280 sec/batch\n",
      "Epoch 8/20  Iteration 1304/3560 Training loss: 2.0942 0.4191 sec/batch\n",
      "Epoch 8/20  Iteration 1305/3560 Training loss: 2.0940 0.5785 sec/batch\n",
      "Epoch 8/20  Iteration 1306/3560 Training loss: 2.0945 0.5042 sec/batch\n",
      "Epoch 8/20  Iteration 1307/3560 Training loss: 2.0943 0.4832 sec/batch\n",
      "Epoch 8/20  Iteration 1308/3560 Training loss: 2.0948 0.5875 sec/batch\n",
      "Epoch 8/20  Iteration 1309/3560 Training loss: 2.0951 0.4514 sec/batch\n",
      "Epoch 8/20  Iteration 1310/3560 Training loss: 2.0951 0.5413 sec/batch\n",
      "Epoch 8/20  Iteration 1311/3560 Training loss: 2.0950 0.5569 sec/batch\n",
      "Epoch 8/20  Iteration 1312/3560 Training loss: 2.0952 0.6279 sec/batch\n",
      "Epoch 8/20  Iteration 1313/3560 Training loss: 2.0952 0.4863 sec/batch\n",
      "Epoch 8/20  Iteration 1314/3560 Training loss: 2.0946 0.4316 sec/batch\n",
      "Epoch 8/20  Iteration 1315/3560 Training loss: 2.0944 0.4125 sec/batch\n",
      "Epoch 8/20  Iteration 1316/3560 Training loss: 2.0944 0.3937 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1317/3560 Training loss: 2.0946 0.4495 sec/batch\n",
      "Epoch 8/20  Iteration 1318/3560 Training loss: 2.0947 0.4391 sec/batch\n",
      "Epoch 8/20  Iteration 1319/3560 Training loss: 2.0949 0.7102 sec/batch\n",
      "Epoch 8/20  Iteration 1320/3560 Training loss: 2.0946 0.4972 sec/batch\n",
      "Epoch 8/20  Iteration 1321/3560 Training loss: 2.0943 0.5859 sec/batch\n",
      "Epoch 8/20  Iteration 1322/3560 Training loss: 2.0945 0.4140 sec/batch\n",
      "Epoch 8/20  Iteration 1323/3560 Training loss: 2.0944 0.5202 sec/batch\n",
      "Epoch 8/20  Iteration 1324/3560 Training loss: 2.0946 0.4183 sec/batch\n",
      "Epoch 8/20  Iteration 1325/3560 Training loss: 2.0942 0.4572 sec/batch\n",
      "Epoch 8/20  Iteration 1326/3560 Training loss: 2.0941 0.4776 sec/batch\n",
      "Epoch 8/20  Iteration 1327/3560 Training loss: 2.0936 0.4719 sec/batch\n",
      "Epoch 8/20  Iteration 1328/3560 Training loss: 2.0937 0.3665 sec/batch\n",
      "Epoch 8/20  Iteration 1329/3560 Training loss: 2.0933 0.4992 sec/batch\n",
      "Epoch 8/20  Iteration 1330/3560 Training loss: 2.0932 0.4345 sec/batch\n",
      "Epoch 8/20  Iteration 1331/3560 Training loss: 2.0926 0.4099 sec/batch\n",
      "Epoch 8/20  Iteration 1332/3560 Training loss: 2.0923 0.4129 sec/batch\n",
      "Epoch 8/20  Iteration 1333/3560 Training loss: 2.0922 0.4104 sec/batch\n",
      "Epoch 8/20  Iteration 1334/3560 Training loss: 2.0918 0.3880 sec/batch\n",
      "Epoch 8/20  Iteration 1335/3560 Training loss: 2.0913 0.3961 sec/batch\n",
      "Epoch 8/20  Iteration 1336/3560 Training loss: 2.0912 0.3901 sec/batch\n",
      "Epoch 8/20  Iteration 1337/3560 Training loss: 2.0910 0.4536 sec/batch\n",
      "Epoch 8/20  Iteration 1338/3560 Training loss: 2.0908 0.3599 sec/batch\n",
      "Epoch 8/20  Iteration 1339/3560 Training loss: 2.0904 0.4598 sec/batch\n",
      "Epoch 8/20  Iteration 1340/3560 Training loss: 2.0901 0.4118 sec/batch\n",
      "Epoch 8/20  Iteration 1341/3560 Training loss: 2.0898 0.4649 sec/batch\n",
      "Epoch 8/20  Iteration 1342/3560 Training loss: 2.0896 0.4204 sec/batch\n",
      "Epoch 8/20  Iteration 1343/3560 Training loss: 2.0895 0.4719 sec/batch\n",
      "Epoch 8/20  Iteration 1344/3560 Training loss: 2.0891 0.4793 sec/batch\n",
      "Epoch 8/20  Iteration 1345/3560 Training loss: 2.0888 0.4208 sec/batch\n",
      "Epoch 8/20  Iteration 1346/3560 Training loss: 2.0884 0.4001 sec/batch\n",
      "Epoch 8/20  Iteration 1347/3560 Training loss: 2.0883 0.3801 sec/batch\n",
      "Epoch 8/20  Iteration 1348/3560 Training loss: 2.0882 0.4596 sec/batch\n",
      "Epoch 8/20  Iteration 1349/3560 Training loss: 2.0879 0.5681 sec/batch\n",
      "Epoch 8/20  Iteration 1350/3560 Training loss: 2.0877 0.4580 sec/batch\n",
      "Epoch 8/20  Iteration 1351/3560 Training loss: 2.0874 0.3869 sec/batch\n",
      "Epoch 8/20  Iteration 1352/3560 Training loss: 2.0871 0.4267 sec/batch\n",
      "Epoch 8/20  Iteration 1353/3560 Training loss: 2.0870 0.4058 sec/batch\n",
      "Epoch 8/20  Iteration 1354/3560 Training loss: 2.0870 0.4060 sec/batch\n",
      "Epoch 8/20  Iteration 1355/3560 Training loss: 2.0870 0.3709 sec/batch\n",
      "Epoch 8/20  Iteration 1356/3560 Training loss: 2.0869 0.4009 sec/batch\n",
      "Epoch 8/20  Iteration 1357/3560 Training loss: 2.0868 0.3614 sec/batch\n",
      "Epoch 8/20  Iteration 1358/3560 Training loss: 2.0868 0.4263 sec/batch\n",
      "Epoch 8/20  Iteration 1359/3560 Training loss: 2.0866 0.3981 sec/batch\n",
      "Epoch 8/20  Iteration 1360/3560 Training loss: 2.0865 0.4393 sec/batch\n",
      "Epoch 8/20  Iteration 1361/3560 Training loss: 2.0862 0.4042 sec/batch\n",
      "Epoch 8/20  Iteration 1362/3560 Training loss: 2.0858 0.4157 sec/batch\n",
      "Epoch 8/20  Iteration 1363/3560 Training loss: 2.0857 0.4853 sec/batch\n",
      "Epoch 8/20  Iteration 1364/3560 Training loss: 2.0856 0.4170 sec/batch\n",
      "Epoch 8/20  Iteration 1365/3560 Training loss: 2.0856 0.5384 sec/batch\n",
      "Epoch 8/20  Iteration 1366/3560 Training loss: 2.0854 0.4347 sec/batch\n",
      "Epoch 8/20  Iteration 1367/3560 Training loss: 2.0854 0.4162 sec/batch\n",
      "Epoch 8/20  Iteration 1368/3560 Training loss: 2.0852 0.4552 sec/batch\n",
      "Epoch 8/20  Iteration 1369/3560 Training loss: 2.0851 0.4781 sec/batch\n",
      "Epoch 8/20  Iteration 1370/3560 Training loss: 2.0852 0.4089 sec/batch\n",
      "Epoch 8/20  Iteration 1371/3560 Training loss: 2.0851 0.3778 sec/batch\n",
      "Epoch 8/20  Iteration 1372/3560 Training loss: 2.0848 0.4028 sec/batch\n",
      "Epoch 8/20  Iteration 1373/3560 Training loss: 2.0847 0.3889 sec/batch\n",
      "Epoch 8/20  Iteration 1374/3560 Training loss: 2.0847 0.3887 sec/batch\n",
      "Epoch 8/20  Iteration 1375/3560 Training loss: 2.0847 0.4138 sec/batch\n",
      "Epoch 8/20  Iteration 1376/3560 Training loss: 2.0846 0.3885 sec/batch\n",
      "Epoch 8/20  Iteration 1377/3560 Training loss: 2.0844 0.4168 sec/batch\n",
      "Epoch 8/20  Iteration 1378/3560 Training loss: 2.0841 0.3810 sec/batch\n",
      "Epoch 8/20  Iteration 1379/3560 Training loss: 2.0841 0.4431 sec/batch\n",
      "Epoch 8/20  Iteration 1380/3560 Training loss: 2.0841 0.4231 sec/batch\n",
      "Epoch 8/20  Iteration 1381/3560 Training loss: 2.0840 0.4580 sec/batch\n",
      "Epoch 8/20  Iteration 1382/3560 Training loss: 2.0840 0.4346 sec/batch\n",
      "Epoch 8/20  Iteration 1383/3560 Training loss: 2.0839 0.4366 sec/batch\n",
      "Epoch 8/20  Iteration 1384/3560 Training loss: 2.0839 0.4120 sec/batch\n",
      "Epoch 8/20  Iteration 1385/3560 Training loss: 2.0841 0.4092 sec/batch\n",
      "Epoch 8/20  Iteration 1386/3560 Training loss: 2.0838 0.4454 sec/batch\n",
      "Epoch 8/20  Iteration 1387/3560 Training loss: 2.0840 0.4333 sec/batch\n",
      "Epoch 8/20  Iteration 1388/3560 Training loss: 2.0839 0.4296 sec/batch\n",
      "Epoch 8/20  Iteration 1389/3560 Training loss: 2.0838 0.4826 sec/batch\n",
      "Epoch 8/20  Iteration 1390/3560 Training loss: 2.0838 0.3928 sec/batch\n",
      "Epoch 8/20  Iteration 1391/3560 Training loss: 2.0836 0.4295 sec/batch\n",
      "Epoch 8/20  Iteration 1392/3560 Training loss: 2.0837 0.4471 sec/batch\n",
      "Epoch 8/20  Iteration 1393/3560 Training loss: 2.0837 0.3666 sec/batch\n",
      "Epoch 8/20  Iteration 1394/3560 Training loss: 2.0838 0.4229 sec/batch\n",
      "Epoch 8/20  Iteration 1395/3560 Training loss: 2.0836 0.3696 sec/batch\n",
      "Epoch 8/20  Iteration 1396/3560 Training loss: 2.0834 0.4107 sec/batch\n",
      "Epoch 8/20  Iteration 1397/3560 Training loss: 2.0832 0.3937 sec/batch\n",
      "Epoch 8/20  Iteration 1398/3560 Training loss: 2.0833 0.4532 sec/batch\n",
      "Epoch 8/20  Iteration 1399/3560 Training loss: 2.0834 0.4064 sec/batch\n",
      "Epoch 8/20  Iteration 1400/3560 Training loss: 2.0833 0.4520 sec/batch\n",
      "Epoch 8/20  Iteration 1401/3560 Training loss: 2.0832 0.4717 sec/batch\n",
      "Epoch 8/20  Iteration 1402/3560 Training loss: 2.0831 0.3720 sec/batch\n",
      "Epoch 8/20  Iteration 1403/3560 Training loss: 2.0830 0.4176 sec/batch\n",
      "Epoch 8/20  Iteration 1404/3560 Training loss: 2.0829 0.4149 sec/batch\n",
      "Epoch 8/20  Iteration 1405/3560 Training loss: 2.0827 0.4369 sec/batch\n",
      "Epoch 8/20  Iteration 1406/3560 Training loss: 2.0828 0.4547 sec/batch\n",
      "Epoch 8/20  Iteration 1407/3560 Training loss: 2.0829 0.3713 sec/batch\n",
      "Epoch 8/20  Iteration 1408/3560 Training loss: 2.0827 0.4459 sec/batch\n",
      "Epoch 8/20  Iteration 1409/3560 Training loss: 2.0827 0.4317 sec/batch\n",
      "Epoch 8/20  Iteration 1410/3560 Training loss: 2.0826 0.3798 sec/batch\n",
      "Epoch 8/20  Iteration 1411/3560 Training loss: 2.0826 0.3740 sec/batch\n",
      "Epoch 8/20  Iteration 1412/3560 Training loss: 2.0826 0.4171 sec/batch\n",
      "Epoch 8/20  Iteration 1413/3560 Training loss: 2.0826 0.3970 sec/batch\n",
      "Epoch 8/20  Iteration 1414/3560 Training loss: 2.0827 0.3749 sec/batch\n",
      "Epoch 8/20  Iteration 1415/3560 Training loss: 2.0826 0.3980 sec/batch\n",
      "Epoch 8/20  Iteration 1416/3560 Training loss: 2.0826 0.4350 sec/batch\n",
      "Epoch 8/20  Iteration 1417/3560 Training loss: 2.0824 0.4553 sec/batch\n",
      "Epoch 8/20  Iteration 1418/3560 Training loss: 2.0823 0.4024 sec/batch\n",
      "Epoch 8/20  Iteration 1419/3560 Training loss: 2.0824 0.4611 sec/batch\n",
      "Epoch 8/20  Iteration 1420/3560 Training loss: 2.0824 0.3861 sec/batch\n",
      "Epoch 8/20  Iteration 1421/3560 Training loss: 2.0824 0.4231 sec/batch\n",
      "Epoch 8/20  Iteration 1422/3560 Training loss: 2.0823 0.3681 sec/batch\n",
      "Epoch 8/20  Iteration 1423/3560 Training loss: 2.0822 0.4764 sec/batch\n",
      "Epoch 8/20  Iteration 1424/3560 Training loss: 2.0821 0.3746 sec/batch\n",
      "Epoch 9/20  Iteration 1425/3560 Training loss: 2.1340 0.4557 sec/batch\n",
      "Epoch 9/20  Iteration 1426/3560 Training loss: 2.0869 0.3735 sec/batch\n",
      "Epoch 9/20  Iteration 1427/3560 Training loss: 2.0750 0.4322 sec/batch\n",
      "Epoch 9/20  Iteration 1428/3560 Training loss: 2.0691 0.3980 sec/batch\n",
      "Epoch 9/20  Iteration 1429/3560 Training loss: 2.0674 0.3985 sec/batch\n",
      "Epoch 9/20  Iteration 1430/3560 Training loss: 2.0617 0.4065 sec/batch\n",
      "Epoch 9/20  Iteration 1431/3560 Training loss: 2.0605 0.3741 sec/batch\n",
      "Epoch 9/20  Iteration 1432/3560 Training loss: 2.0624 0.3939 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1433/3560 Training loss: 2.0656 0.3900 sec/batch\n",
      "Epoch 9/20  Iteration 1434/3560 Training loss: 2.0657 0.3681 sec/batch\n",
      "Epoch 9/20  Iteration 1435/3560 Training loss: 2.0641 0.3823 sec/batch\n",
      "Epoch 9/20  Iteration 1436/3560 Training loss: 2.0616 0.4243 sec/batch\n",
      "Epoch 9/20  Iteration 1437/3560 Training loss: 2.0619 0.4218 sec/batch\n",
      "Epoch 9/20  Iteration 1438/3560 Training loss: 2.0639 0.4187 sec/batch\n",
      "Epoch 9/20  Iteration 1439/3560 Training loss: 2.0630 0.4638 sec/batch\n",
      "Epoch 9/20  Iteration 1440/3560 Training loss: 2.0623 0.4158 sec/batch\n",
      "Epoch 9/20  Iteration 1441/3560 Training loss: 2.0624 0.3651 sec/batch\n",
      "Epoch 9/20  Iteration 1442/3560 Training loss: 2.0644 0.4348 sec/batch\n",
      "Epoch 9/20  Iteration 1443/3560 Training loss: 2.0641 0.4649 sec/batch\n",
      "Epoch 9/20  Iteration 1444/3560 Training loss: 2.0638 0.3923 sec/batch\n",
      "Epoch 9/20  Iteration 1445/3560 Training loss: 2.0627 0.4286 sec/batch\n",
      "Epoch 9/20  Iteration 1446/3560 Training loss: 2.0638 0.3960 sec/batch\n",
      "Epoch 9/20  Iteration 1447/3560 Training loss: 2.0633 0.4777 sec/batch\n",
      "Epoch 9/20  Iteration 1448/3560 Training loss: 2.0627 0.3510 sec/batch\n",
      "Epoch 9/20  Iteration 1449/3560 Training loss: 2.0624 0.3653 sec/batch\n",
      "Epoch 9/20  Iteration 1450/3560 Training loss: 2.0615 0.4109 sec/batch\n",
      "Epoch 9/20  Iteration 1451/3560 Training loss: 2.0608 0.3982 sec/batch\n",
      "Epoch 9/20  Iteration 1452/3560 Training loss: 2.0613 0.4003 sec/batch\n",
      "Epoch 9/20  Iteration 1453/3560 Training loss: 2.0619 0.3660 sec/batch\n",
      "Epoch 9/20  Iteration 1454/3560 Training loss: 2.0620 0.4433 sec/batch\n",
      "Epoch 9/20  Iteration 1455/3560 Training loss: 2.0622 0.4026 sec/batch\n",
      "Epoch 9/20  Iteration 1456/3560 Training loss: 2.0616 0.4605 sec/batch\n",
      "Epoch 9/20  Iteration 1457/3560 Training loss: 2.0615 0.4209 sec/batch\n",
      "Epoch 9/20  Iteration 1458/3560 Training loss: 2.0623 0.3823 sec/batch\n",
      "Epoch 9/20  Iteration 1459/3560 Training loss: 2.0620 0.3879 sec/batch\n",
      "Epoch 9/20  Iteration 1460/3560 Training loss: 2.0620 0.4349 sec/batch\n",
      "Epoch 9/20  Iteration 1461/3560 Training loss: 2.0616 0.4395 sec/batch\n",
      "Epoch 9/20  Iteration 1462/3560 Training loss: 2.0606 0.4399 sec/batch\n",
      "Epoch 9/20  Iteration 1463/3560 Training loss: 2.0596 0.4529 sec/batch\n",
      "Epoch 9/20  Iteration 1464/3560 Training loss: 2.0589 0.4483 sec/batch\n",
      "Epoch 9/20  Iteration 1465/3560 Training loss: 2.0584 0.4145 sec/batch\n",
      "Epoch 9/20  Iteration 1466/3560 Training loss: 2.0584 0.4569 sec/batch\n",
      "Epoch 9/20  Iteration 1467/3560 Training loss: 2.0580 0.3832 sec/batch\n",
      "Epoch 9/20  Iteration 1468/3560 Training loss: 2.0570 0.3948 sec/batch\n",
      "Epoch 9/20  Iteration 1469/3560 Training loss: 2.0570 0.3963 sec/batch\n",
      "Epoch 9/20  Iteration 1470/3560 Training loss: 2.0556 0.4092 sec/batch\n",
      "Epoch 9/20  Iteration 1471/3560 Training loss: 2.0556 0.4274 sec/batch\n",
      "Epoch 9/20  Iteration 1472/3560 Training loss: 2.0550 0.4293 sec/batch\n",
      "Epoch 9/20  Iteration 1473/3560 Training loss: 2.0549 0.3780 sec/batch\n",
      "Epoch 9/20  Iteration 1474/3560 Training loss: 2.0554 0.4844 sec/batch\n",
      "Epoch 9/20  Iteration 1475/3560 Training loss: 2.0549 0.3828 sec/batch\n",
      "Epoch 9/20  Iteration 1476/3560 Training loss: 2.0554 0.3992 sec/batch\n",
      "Epoch 9/20  Iteration 1477/3560 Training loss: 2.0553 0.4572 sec/batch\n",
      "Epoch 9/20  Iteration 1478/3560 Training loss: 2.0552 0.3804 sec/batch\n",
      "Epoch 9/20  Iteration 1479/3560 Training loss: 2.0548 0.4475 sec/batch\n",
      "Epoch 9/20  Iteration 1480/3560 Training loss: 2.0550 0.4430 sec/batch\n",
      "Epoch 9/20  Iteration 1481/3560 Training loss: 2.0551 0.4061 sec/batch\n",
      "Epoch 9/20  Iteration 1482/3560 Training loss: 2.0547 0.4324 sec/batch\n",
      "Epoch 9/20  Iteration 1483/3560 Training loss: 2.0543 0.4076 sec/batch\n",
      "Epoch 9/20  Iteration 1484/3560 Training loss: 2.0548 0.4752 sec/batch\n",
      "Epoch 9/20  Iteration 1485/3560 Training loss: 2.0547 0.4808 sec/batch\n",
      "Epoch 9/20  Iteration 1486/3560 Training loss: 2.0551 0.3993 sec/batch\n",
      "Epoch 9/20  Iteration 1487/3560 Training loss: 2.0555 0.4019 sec/batch\n",
      "Epoch 9/20  Iteration 1488/3560 Training loss: 2.0557 0.4470 sec/batch\n",
      "Epoch 9/20  Iteration 1489/3560 Training loss: 2.0554 0.3961 sec/batch\n",
      "Epoch 9/20  Iteration 1490/3560 Training loss: 2.0558 0.3851 sec/batch\n",
      "Epoch 9/20  Iteration 1491/3560 Training loss: 2.0556 0.4050 sec/batch\n",
      "Epoch 9/20  Iteration 1492/3560 Training loss: 2.0551 0.3924 sec/batch\n",
      "Epoch 9/20  Iteration 1493/3560 Training loss: 2.0549 0.4634 sec/batch\n",
      "Epoch 9/20  Iteration 1494/3560 Training loss: 2.0548 0.3913 sec/batch\n",
      "Epoch 9/20  Iteration 1495/3560 Training loss: 2.0551 0.4534 sec/batch\n",
      "Epoch 9/20  Iteration 1496/3560 Training loss: 2.0552 0.4139 sec/batch\n",
      "Epoch 9/20  Iteration 1497/3560 Training loss: 2.0554 0.4501 sec/batch\n",
      "Epoch 9/20  Iteration 1498/3560 Training loss: 2.0551 0.4256 sec/batch\n",
      "Epoch 9/20  Iteration 1499/3560 Training loss: 2.0550 0.3458 sec/batch\n",
      "Epoch 9/20  Iteration 1500/3560 Training loss: 2.0552 0.4291 sec/batch\n",
      "Epoch 9/20  Iteration 1501/3560 Training loss: 2.0551 0.4297 sec/batch\n",
      "Epoch 9/20  Iteration 1502/3560 Training loss: 2.0553 0.4419 sec/batch\n",
      "Epoch 9/20  Iteration 1503/3560 Training loss: 2.0548 0.4135 sec/batch\n",
      "Epoch 9/20  Iteration 1504/3560 Training loss: 2.0548 0.4284 sec/batch\n",
      "Epoch 9/20  Iteration 1505/3560 Training loss: 2.0543 0.4136 sec/batch\n",
      "Epoch 9/20  Iteration 1506/3560 Training loss: 2.0543 0.4582 sec/batch\n",
      "Epoch 9/20  Iteration 1507/3560 Training loss: 2.0538 0.3475 sec/batch\n",
      "Epoch 9/20  Iteration 1508/3560 Training loss: 2.0536 0.4191 sec/batch\n",
      "Epoch 9/20  Iteration 1509/3560 Training loss: 2.0530 0.3875 sec/batch\n",
      "Epoch 9/20  Iteration 1510/3560 Training loss: 2.0525 0.4278 sec/batch\n",
      "Epoch 9/20  Iteration 1511/3560 Training loss: 2.0524 0.4161 sec/batch\n",
      "Epoch 9/20  Iteration 1512/3560 Training loss: 2.0522 0.4098 sec/batch\n",
      "Epoch 9/20  Iteration 1513/3560 Training loss: 2.0518 0.4504 sec/batch\n",
      "Epoch 9/20  Iteration 1514/3560 Training loss: 2.0517 0.4356 sec/batch\n",
      "Epoch 9/20  Iteration 1515/3560 Training loss: 2.0515 0.3755 sec/batch\n",
      "Epoch 9/20  Iteration 1516/3560 Training loss: 2.0514 0.4163 sec/batch\n",
      "Epoch 9/20  Iteration 1517/3560 Training loss: 2.0509 0.3959 sec/batch\n",
      "Epoch 9/20  Iteration 1518/3560 Training loss: 2.0506 0.4091 sec/batch\n",
      "Epoch 9/20  Iteration 1519/3560 Training loss: 2.0502 0.4459 sec/batch\n",
      "Epoch 9/20  Iteration 1520/3560 Training loss: 2.0501 0.4011 sec/batch\n",
      "Epoch 9/20  Iteration 1521/3560 Training loss: 2.0500 0.4567 sec/batch\n",
      "Epoch 9/20  Iteration 1522/3560 Training loss: 2.0497 0.4163 sec/batch\n",
      "Epoch 9/20  Iteration 1523/3560 Training loss: 2.0494 0.4128 sec/batch\n",
      "Epoch 9/20  Iteration 1524/3560 Training loss: 2.0490 0.3944 sec/batch\n",
      "Epoch 9/20  Iteration 1525/3560 Training loss: 2.0490 0.3955 sec/batch\n",
      "Epoch 9/20  Iteration 1526/3560 Training loss: 2.0489 0.4902 sec/batch\n",
      "Epoch 9/20  Iteration 1527/3560 Training loss: 2.0486 0.3860 sec/batch\n",
      "Epoch 9/20  Iteration 1528/3560 Training loss: 2.0484 0.4308 sec/batch\n",
      "Epoch 9/20  Iteration 1529/3560 Training loss: 2.0481 0.3651 sec/batch\n",
      "Epoch 9/20  Iteration 1530/3560 Training loss: 2.0479 0.3894 sec/batch\n",
      "Epoch 9/20  Iteration 1531/3560 Training loss: 2.0479 0.3951 sec/batch\n",
      "Epoch 9/20  Iteration 1532/3560 Training loss: 2.0479 0.4207 sec/batch\n",
      "Epoch 9/20  Iteration 1533/3560 Training loss: 2.0479 0.4575 sec/batch\n",
      "Epoch 9/20  Iteration 1534/3560 Training loss: 2.0478 0.5309 sec/batch\n",
      "Epoch 9/20  Iteration 1535/3560 Training loss: 2.0478 0.3817 sec/batch\n",
      "Epoch 9/20  Iteration 1536/3560 Training loss: 2.0478 0.6542 sec/batch\n",
      "Epoch 9/20  Iteration 1537/3560 Training loss: 2.0477 0.5845 sec/batch\n",
      "Epoch 9/20  Iteration 1538/3560 Training loss: 2.0475 0.3919 sec/batch\n",
      "Epoch 9/20  Iteration 1539/3560 Training loss: 2.0473 0.5129 sec/batch\n",
      "Epoch 9/20  Iteration 1540/3560 Training loss: 2.0470 0.4759 sec/batch\n",
      "Epoch 9/20  Iteration 1541/3560 Training loss: 2.0469 0.4028 sec/batch\n",
      "Epoch 9/20  Iteration 1542/3560 Training loss: 2.0468 0.5576 sec/batch\n",
      "Epoch 9/20  Iteration 1543/3560 Training loss: 2.0468 0.4946 sec/batch\n",
      "Epoch 9/20  Iteration 1544/3560 Training loss: 2.0467 0.4826 sec/batch\n",
      "Epoch 9/20  Iteration 1545/3560 Training loss: 2.0467 0.4257 sec/batch\n",
      "Epoch 9/20  Iteration 1546/3560 Training loss: 2.0463 0.4144 sec/batch\n",
      "Epoch 9/20  Iteration 1547/3560 Training loss: 2.0461 0.4235 sec/batch\n",
      "Epoch 9/20  Iteration 1548/3560 Training loss: 2.0462 0.4215 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1549/3560 Training loss: 2.0461 0.3967 sec/batch\n",
      "Epoch 9/20  Iteration 1550/3560 Training loss: 2.0458 0.3700 sec/batch\n",
      "Epoch 9/20  Iteration 1551/3560 Training loss: 2.0459 0.4062 sec/batch\n",
      "Epoch 9/20  Iteration 1552/3560 Training loss: 2.0459 0.4489 sec/batch\n",
      "Epoch 9/20  Iteration 1553/3560 Training loss: 2.0459 0.4374 sec/batch\n",
      "Epoch 9/20  Iteration 1554/3560 Training loss: 2.0459 0.4941 sec/batch\n",
      "Epoch 9/20  Iteration 1555/3560 Training loss: 2.0456 0.4063 sec/batch\n",
      "Epoch 9/20  Iteration 1556/3560 Training loss: 2.0453 0.4045 sec/batch\n",
      "Epoch 9/20  Iteration 1557/3560 Training loss: 2.0452 0.3721 sec/batch\n",
      "Epoch 9/20  Iteration 1558/3560 Training loss: 2.0453 0.4144 sec/batch\n",
      "Epoch 9/20  Iteration 1559/3560 Training loss: 2.0452 0.4708 sec/batch\n",
      "Epoch 9/20  Iteration 1560/3560 Training loss: 2.0452 0.5332 sec/batch\n",
      "Epoch 9/20  Iteration 1561/3560 Training loss: 2.0452 0.4115 sec/batch\n",
      "Epoch 9/20  Iteration 1562/3560 Training loss: 2.0452 0.4233 sec/batch\n",
      "Epoch 9/20  Iteration 1563/3560 Training loss: 2.0454 0.4147 sec/batch\n",
      "Epoch 9/20  Iteration 1564/3560 Training loss: 2.0451 0.4292 sec/batch\n",
      "Epoch 9/20  Iteration 1565/3560 Training loss: 2.0453 0.4025 sec/batch\n",
      "Epoch 9/20  Iteration 1566/3560 Training loss: 2.0451 0.3880 sec/batch\n",
      "Epoch 9/20  Iteration 1567/3560 Training loss: 2.0451 0.4016 sec/batch\n",
      "Epoch 9/20  Iteration 1568/3560 Training loss: 2.0451 0.4240 sec/batch\n",
      "Epoch 9/20  Iteration 1569/3560 Training loss: 2.0449 0.4043 sec/batch\n",
      "Epoch 9/20  Iteration 1570/3560 Training loss: 2.0450 0.3661 sec/batch\n",
      "Epoch 9/20  Iteration 1571/3560 Training loss: 2.0449 0.4075 sec/batch\n",
      "Epoch 9/20  Iteration 1572/3560 Training loss: 2.0450 0.4499 sec/batch\n",
      "Epoch 9/20  Iteration 1573/3560 Training loss: 2.0449 0.4369 sec/batch\n",
      "Epoch 9/20  Iteration 1574/3560 Training loss: 2.0447 0.4160 sec/batch\n",
      "Epoch 9/20  Iteration 1575/3560 Training loss: 2.0445 0.4300 sec/batch\n",
      "Epoch 9/20  Iteration 1576/3560 Training loss: 2.0447 0.4275 sec/batch\n",
      "Epoch 9/20  Iteration 1577/3560 Training loss: 2.0446 0.4609 sec/batch\n",
      "Epoch 9/20  Iteration 1578/3560 Training loss: 2.0446 0.4247 sec/batch\n",
      "Epoch 9/20  Iteration 1579/3560 Training loss: 2.0445 0.3906 sec/batch\n",
      "Epoch 9/20  Iteration 1580/3560 Training loss: 2.0445 0.4309 sec/batch\n",
      "Epoch 9/20  Iteration 1581/3560 Training loss: 2.0444 0.4174 sec/batch\n",
      "Epoch 9/20  Iteration 1582/3560 Training loss: 2.0443 0.4463 sec/batch\n",
      "Epoch 9/20  Iteration 1583/3560 Training loss: 2.0441 0.4611 sec/batch\n",
      "Epoch 9/20  Iteration 1584/3560 Training loss: 2.0441 0.3805 sec/batch\n",
      "Epoch 9/20  Iteration 1585/3560 Training loss: 2.0442 0.4731 sec/batch\n",
      "Epoch 9/20  Iteration 1586/3560 Training loss: 2.0441 0.3479 sec/batch\n",
      "Epoch 9/20  Iteration 1587/3560 Training loss: 2.0441 0.4142 sec/batch\n",
      "Epoch 9/20  Iteration 1588/3560 Training loss: 2.0441 0.3888 sec/batch\n",
      "Epoch 9/20  Iteration 1589/3560 Training loss: 2.0441 0.4343 sec/batch\n",
      "Epoch 9/20  Iteration 1590/3560 Training loss: 2.0440 0.4716 sec/batch\n",
      "Epoch 9/20  Iteration 1591/3560 Training loss: 2.0440 0.4602 sec/batch\n",
      "Epoch 9/20  Iteration 1592/3560 Training loss: 2.0442 0.4078 sec/batch\n",
      "Epoch 9/20  Iteration 1593/3560 Training loss: 2.0442 0.4498 sec/batch\n",
      "Epoch 9/20  Iteration 1594/3560 Training loss: 2.0442 0.3914 sec/batch\n",
      "Epoch 9/20  Iteration 1595/3560 Training loss: 2.0441 0.3509 sec/batch\n",
      "Epoch 9/20  Iteration 1596/3560 Training loss: 2.0439 0.4523 sec/batch\n",
      "Epoch 9/20  Iteration 1597/3560 Training loss: 2.0440 0.4911 sec/batch\n",
      "Epoch 9/20  Iteration 1598/3560 Training loss: 2.0440 0.4800 sec/batch\n",
      "Epoch 9/20  Iteration 1599/3560 Training loss: 2.0441 0.5668 sec/batch\n",
      "Epoch 9/20  Iteration 1600/3560 Training loss: 2.0440 0.4502 sec/batch\n",
      "Epoch 9/20  Iteration 1601/3560 Training loss: 2.0438 0.5746 sec/batch\n",
      "Epoch 9/20  Iteration 1602/3560 Training loss: 2.0437 0.3736 sec/batch\n",
      "Epoch 10/20  Iteration 1603/3560 Training loss: 2.1007 0.4557 sec/batch\n",
      "Epoch 10/20  Iteration 1604/3560 Training loss: 2.0573 0.4337 sec/batch\n",
      "Epoch 10/20  Iteration 1605/3560 Training loss: 2.0446 0.4440 sec/batch\n",
      "Epoch 10/20  Iteration 1606/3560 Training loss: 2.0362 0.4223 sec/batch\n",
      "Epoch 10/20  Iteration 1607/3560 Training loss: 2.0337 0.4185 sec/batch\n",
      "Epoch 10/20  Iteration 1608/3560 Training loss: 2.0276 0.4162 sec/batch\n",
      "Epoch 10/20  Iteration 1609/3560 Training loss: 2.0285 0.5331 sec/batch\n",
      "Epoch 10/20  Iteration 1610/3560 Training loss: 2.0292 0.4012 sec/batch\n",
      "Epoch 10/20  Iteration 1611/3560 Training loss: 2.0309 0.4394 sec/batch\n",
      "Epoch 10/20  Iteration 1612/3560 Training loss: 2.0310 0.4969 sec/batch\n",
      "Epoch 10/20  Iteration 1613/3560 Training loss: 2.0285 0.4002 sec/batch\n",
      "Epoch 10/20  Iteration 1614/3560 Training loss: 2.0269 0.3812 sec/batch\n",
      "Epoch 10/20  Iteration 1615/3560 Training loss: 2.0270 0.4651 sec/batch\n",
      "Epoch 10/20  Iteration 1616/3560 Training loss: 2.0291 0.4136 sec/batch\n",
      "Epoch 10/20  Iteration 1617/3560 Training loss: 2.0284 0.4514 sec/batch\n",
      "Epoch 10/20  Iteration 1618/3560 Training loss: 2.0275 0.4738 sec/batch\n",
      "Epoch 10/20  Iteration 1619/3560 Training loss: 2.0276 0.4766 sec/batch\n",
      "Epoch 10/20  Iteration 1620/3560 Training loss: 2.0296 0.4764 sec/batch\n",
      "Epoch 10/20  Iteration 1621/3560 Training loss: 2.0297 0.4339 sec/batch\n",
      "Epoch 10/20  Iteration 1622/3560 Training loss: 2.0298 0.3527 sec/batch\n",
      "Epoch 10/20  Iteration 1623/3560 Training loss: 2.0290 0.4564 sec/batch\n",
      "Epoch 10/20  Iteration 1624/3560 Training loss: 2.0295 0.4598 sec/batch\n",
      "Epoch 10/20  Iteration 1625/3560 Training loss: 2.0287 0.3751 sec/batch\n",
      "Epoch 10/20  Iteration 1626/3560 Training loss: 2.0279 0.4746 sec/batch\n",
      "Epoch 10/20  Iteration 1627/3560 Training loss: 2.0275 0.4781 sec/batch\n",
      "Epoch 10/20  Iteration 1628/3560 Training loss: 2.0267 0.3788 sec/batch\n",
      "Epoch 10/20  Iteration 1629/3560 Training loss: 2.0260 0.4429 sec/batch\n",
      "Epoch 10/20  Iteration 1630/3560 Training loss: 2.0264 0.4216 sec/batch\n",
      "Epoch 10/20  Iteration 1631/3560 Training loss: 2.0278 0.3789 sec/batch\n",
      "Epoch 10/20  Iteration 1632/3560 Training loss: 2.0278 0.3769 sec/batch\n",
      "Epoch 10/20  Iteration 1633/3560 Training loss: 2.0279 0.3753 sec/batch\n",
      "Epoch 10/20  Iteration 1634/3560 Training loss: 2.0273 0.4819 sec/batch\n",
      "Epoch 10/20  Iteration 1635/3560 Training loss: 2.0275 0.4367 sec/batch\n",
      "Epoch 10/20  Iteration 1636/3560 Training loss: 2.0280 0.4341 sec/batch\n",
      "Epoch 10/20  Iteration 1637/3560 Training loss: 2.0278 0.4336 sec/batch\n",
      "Epoch 10/20  Iteration 1638/3560 Training loss: 2.0275 0.4785 sec/batch\n",
      "Epoch 10/20  Iteration 1639/3560 Training loss: 2.0271 0.4113 sec/batch\n",
      "Epoch 10/20  Iteration 1640/3560 Training loss: 2.0261 0.4375 sec/batch\n",
      "Epoch 10/20  Iteration 1641/3560 Training loss: 2.0251 0.4297 sec/batch\n",
      "Epoch 10/20  Iteration 1642/3560 Training loss: 2.0247 0.4210 sec/batch\n",
      "Epoch 10/20  Iteration 1643/3560 Training loss: 2.0241 0.4322 sec/batch\n",
      "Epoch 10/20  Iteration 1644/3560 Training loss: 2.0240 0.4785 sec/batch\n",
      "Epoch 10/20  Iteration 1645/3560 Training loss: 2.0235 0.4395 sec/batch\n",
      "Epoch 10/20  Iteration 1646/3560 Training loss: 2.0227 0.4357 sec/batch\n",
      "Epoch 10/20  Iteration 1647/3560 Training loss: 2.0227 0.4154 sec/batch\n",
      "Epoch 10/20  Iteration 1648/3560 Training loss: 2.0216 0.5537 sec/batch\n",
      "Epoch 10/20  Iteration 1649/3560 Training loss: 2.0214 0.5520 sec/batch\n",
      "Epoch 10/20  Iteration 1650/3560 Training loss: 2.0209 0.5836 sec/batch\n",
      "Epoch 10/20  Iteration 1651/3560 Training loss: 2.0210 0.4322 sec/batch\n",
      "Epoch 10/20  Iteration 1652/3560 Training loss: 2.0216 0.4741 sec/batch\n",
      "Epoch 10/20  Iteration 1653/3560 Training loss: 2.0211 0.5020 sec/batch\n",
      "Epoch 10/20  Iteration 1654/3560 Training loss: 2.0217 0.4477 sec/batch\n",
      "Epoch 10/20  Iteration 1655/3560 Training loss: 2.0217 0.4634 sec/batch\n",
      "Epoch 10/20  Iteration 1656/3560 Training loss: 2.0215 0.4473 sec/batch\n",
      "Epoch 10/20  Iteration 1657/3560 Training loss: 2.0211 0.6221 sec/batch\n",
      "Epoch 10/20  Iteration 1658/3560 Training loss: 2.0212 0.7168 sec/batch\n",
      "Epoch 10/20  Iteration 1659/3560 Training loss: 2.0214 0.4148 sec/batch\n",
      "Epoch 10/20  Iteration 1660/3560 Training loss: 2.0210 0.6029 sec/batch\n",
      "Epoch 10/20  Iteration 1661/3560 Training loss: 2.0205 0.4823 sec/batch\n",
      "Epoch 10/20  Iteration 1662/3560 Training loss: 2.0209 0.5862 sec/batch\n",
      "Epoch 10/20  Iteration 1663/3560 Training loss: 2.0206 0.3852 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1664/3560 Training loss: 2.0213 0.3981 sec/batch\n",
      "Epoch 10/20  Iteration 1665/3560 Training loss: 2.0215 0.4502 sec/batch\n",
      "Epoch 10/20  Iteration 1666/3560 Training loss: 2.0216 0.4261 sec/batch\n",
      "Epoch 10/20  Iteration 1667/3560 Training loss: 2.0215 0.4353 sec/batch\n",
      "Epoch 10/20  Iteration 1668/3560 Training loss: 2.0218 0.5208 sec/batch\n",
      "Epoch 10/20  Iteration 1669/3560 Training loss: 2.0216 0.6506 sec/batch\n",
      "Epoch 10/20  Iteration 1670/3560 Training loss: 2.0212 0.6658 sec/batch\n",
      "Epoch 10/20  Iteration 1671/3560 Training loss: 2.0210 0.5349 sec/batch\n",
      "Epoch 10/20  Iteration 1672/3560 Training loss: 2.0210 0.4926 sec/batch\n",
      "Epoch 10/20  Iteration 1673/3560 Training loss: 2.0212 0.4685 sec/batch\n",
      "Epoch 10/20  Iteration 1674/3560 Training loss: 2.0213 0.4341 sec/batch\n",
      "Epoch 10/20  Iteration 1675/3560 Training loss: 2.0215 0.5175 sec/batch\n",
      "Epoch 10/20  Iteration 1676/3560 Training loss: 2.0212 0.4189 sec/batch\n",
      "Epoch 10/20  Iteration 1677/3560 Training loss: 2.0211 0.6772 sec/batch\n",
      "Epoch 10/20  Iteration 1678/3560 Training loss: 2.0214 0.5828 sec/batch\n",
      "Epoch 10/20  Iteration 1679/3560 Training loss: 2.0211 0.6647 sec/batch\n",
      "Epoch 10/20  Iteration 1680/3560 Training loss: 2.0213 0.6032 sec/batch\n",
      "Epoch 10/20  Iteration 1681/3560 Training loss: 2.0209 0.4592 sec/batch\n",
      "Epoch 10/20  Iteration 1682/3560 Training loss: 2.0208 0.4384 sec/batch\n",
      "Epoch 10/20  Iteration 1683/3560 Training loss: 2.0203 0.4577 sec/batch\n",
      "Epoch 10/20  Iteration 1684/3560 Training loss: 2.0204 0.4241 sec/batch\n",
      "Epoch 10/20  Iteration 1685/3560 Training loss: 2.0200 0.4838 sec/batch\n",
      "Epoch 10/20  Iteration 1686/3560 Training loss: 2.0198 0.4107 sec/batch\n",
      "Epoch 10/20  Iteration 1687/3560 Training loss: 2.0192 0.4624 sec/batch\n",
      "Epoch 10/20  Iteration 1688/3560 Training loss: 2.0189 0.4049 sec/batch\n",
      "Epoch 10/20  Iteration 1689/3560 Training loss: 2.0187 0.4076 sec/batch\n",
      "Epoch 10/20  Iteration 1690/3560 Training loss: 2.0183 0.4855 sec/batch\n",
      "Epoch 10/20  Iteration 1691/3560 Training loss: 2.0179 0.3568 sec/batch\n",
      "Epoch 10/20  Iteration 1692/3560 Training loss: 2.0179 0.4095 sec/batch\n",
      "Epoch 10/20  Iteration 1693/3560 Training loss: 2.0177 0.4784 sec/batch\n",
      "Epoch 10/20  Iteration 1694/3560 Training loss: 2.0175 0.4847 sec/batch\n",
      "Epoch 10/20  Iteration 1695/3560 Training loss: 2.0170 0.4715 sec/batch\n",
      "Epoch 10/20  Iteration 1696/3560 Training loss: 2.0167 0.5224 sec/batch\n",
      "Epoch 10/20  Iteration 1697/3560 Training loss: 2.0164 0.4356 sec/batch\n",
      "Epoch 10/20  Iteration 1698/3560 Training loss: 2.0162 0.4412 sec/batch\n",
      "Epoch 10/20  Iteration 1699/3560 Training loss: 2.0160 0.5744 sec/batch\n",
      "Epoch 10/20  Iteration 1700/3560 Training loss: 2.0156 0.6080 sec/batch\n",
      "Epoch 10/20  Iteration 1701/3560 Training loss: 2.0151 0.4097 sec/batch\n",
      "Epoch 10/20  Iteration 1702/3560 Training loss: 2.0148 0.4498 sec/batch\n",
      "Epoch 10/20  Iteration 1703/3560 Training loss: 2.0147 0.4993 sec/batch\n",
      "Epoch 10/20  Iteration 1704/3560 Training loss: 2.0147 0.5283 sec/batch\n",
      "Epoch 10/20  Iteration 1705/3560 Training loss: 2.0144 0.4808 sec/batch\n",
      "Epoch 10/20  Iteration 1706/3560 Training loss: 2.0142 0.4264 sec/batch\n",
      "Epoch 10/20  Iteration 1707/3560 Training loss: 2.0140 0.4038 sec/batch\n",
      "Epoch 10/20  Iteration 1708/3560 Training loss: 2.0137 0.4900 sec/batch\n",
      "Epoch 10/20  Iteration 1709/3560 Training loss: 2.0137 0.4946 sec/batch\n",
      "Epoch 10/20  Iteration 1710/3560 Training loss: 2.0137 0.4949 sec/batch\n",
      "Epoch 10/20  Iteration 1711/3560 Training loss: 2.0137 0.4866 sec/batch\n",
      "Epoch 10/20  Iteration 1712/3560 Training loss: 2.0136 0.3952 sec/batch\n",
      "Epoch 10/20  Iteration 1713/3560 Training loss: 2.0135 0.4500 sec/batch\n",
      "Epoch 10/20  Iteration 1714/3560 Training loss: 2.0135 0.5189 sec/batch\n",
      "Epoch 10/20  Iteration 1715/3560 Training loss: 2.0133 0.5628 sec/batch\n",
      "Epoch 10/20  Iteration 1716/3560 Training loss: 2.0131 0.6178 sec/batch\n",
      "Epoch 10/20  Iteration 1717/3560 Training loss: 2.0128 0.5859 sec/batch\n",
      "Epoch 10/20  Iteration 1718/3560 Training loss: 2.0124 0.5374 sec/batch\n",
      "Epoch 10/20  Iteration 1719/3560 Training loss: 2.0122 0.4183 sec/batch\n",
      "Epoch 10/20  Iteration 1720/3560 Training loss: 2.0122 0.4388 sec/batch\n",
      "Epoch 10/20  Iteration 1721/3560 Training loss: 2.0121 0.4315 sec/batch\n",
      "Epoch 10/20  Iteration 1722/3560 Training loss: 2.0120 0.4470 sec/batch\n",
      "Epoch 10/20  Iteration 1723/3560 Training loss: 2.0120 0.4241 sec/batch\n",
      "Epoch 10/20  Iteration 1724/3560 Training loss: 2.0117 0.4280 sec/batch\n",
      "Epoch 10/20  Iteration 1725/3560 Training loss: 2.0115 0.5160 sec/batch\n",
      "Epoch 10/20  Iteration 1726/3560 Training loss: 2.0116 0.4971 sec/batch\n",
      "Epoch 10/20  Iteration 1727/3560 Training loss: 2.0116 0.4788 sec/batch\n",
      "Epoch 10/20  Iteration 1728/3560 Training loss: 2.0113 0.4114 sec/batch\n",
      "Epoch 10/20  Iteration 1729/3560 Training loss: 2.0112 0.5005 sec/batch\n",
      "Epoch 10/20  Iteration 1730/3560 Training loss: 2.0113 0.4521 sec/batch\n",
      "Epoch 10/20  Iteration 1731/3560 Training loss: 2.0112 0.4497 sec/batch\n",
      "Epoch 10/20  Iteration 1732/3560 Training loss: 2.0112 0.4816 sec/batch\n",
      "Epoch 10/20  Iteration 1733/3560 Training loss: 2.0109 0.5388 sec/batch\n",
      "Epoch 10/20  Iteration 1734/3560 Training loss: 2.0106 0.4115 sec/batch\n",
      "Epoch 10/20  Iteration 1735/3560 Training loss: 2.0106 0.4264 sec/batch\n",
      "Epoch 10/20  Iteration 1736/3560 Training loss: 2.0106 0.6116 sec/batch\n",
      "Epoch 10/20  Iteration 1737/3560 Training loss: 2.0105 0.5936 sec/batch\n",
      "Epoch 10/20  Iteration 1738/3560 Training loss: 2.0106 0.4262 sec/batch\n",
      "Epoch 10/20  Iteration 1739/3560 Training loss: 2.0106 0.3588 sec/batch\n",
      "Epoch 10/20  Iteration 1740/3560 Training loss: 2.0107 0.4256 sec/batch\n",
      "Epoch 10/20  Iteration 1741/3560 Training loss: 2.0109 0.4878 sec/batch\n",
      "Epoch 10/20  Iteration 1742/3560 Training loss: 2.0106 0.3665 sec/batch\n",
      "Epoch 10/20  Iteration 1743/3560 Training loss: 2.0108 0.4412 sec/batch\n",
      "Epoch 10/20  Iteration 1744/3560 Training loss: 2.0108 0.4690 sec/batch\n",
      "Epoch 10/20  Iteration 1745/3560 Training loss: 2.0107 0.3695 sec/batch\n",
      "Epoch 10/20  Iteration 1746/3560 Training loss: 2.0106 0.3787 sec/batch\n",
      "Epoch 10/20  Iteration 1747/3560 Training loss: 2.0104 0.3494 sec/batch\n",
      "Epoch 10/20  Iteration 1748/3560 Training loss: 2.0104 0.4610 sec/batch\n",
      "Epoch 10/20  Iteration 1749/3560 Training loss: 2.0104 0.4475 sec/batch\n",
      "Epoch 10/20  Iteration 1750/3560 Training loss: 2.0105 0.4568 sec/batch\n",
      "Epoch 10/20  Iteration 1751/3560 Training loss: 2.0104 0.4495 sec/batch\n",
      "Epoch 10/20  Iteration 1752/3560 Training loss: 2.0103 0.4883 sec/batch\n",
      "Epoch 10/20  Iteration 1753/3560 Training loss: 2.0101 0.5626 sec/batch\n",
      "Epoch 10/20  Iteration 1754/3560 Training loss: 2.0102 0.4847 sec/batch\n",
      "Epoch 10/20  Iteration 1755/3560 Training loss: 2.0103 0.5973 sec/batch\n",
      "Epoch 10/20  Iteration 1756/3560 Training loss: 2.0103 0.5347 sec/batch\n",
      "Epoch 10/20  Iteration 1757/3560 Training loss: 2.0102 0.6344 sec/batch\n",
      "Epoch 10/20  Iteration 1758/3560 Training loss: 2.0102 0.4465 sec/batch\n",
      "Epoch 10/20  Iteration 1759/3560 Training loss: 2.0101 0.4516 sec/batch\n",
      "Epoch 10/20  Iteration 1760/3560 Training loss: 2.0100 0.4696 sec/batch\n",
      "Epoch 10/20  Iteration 1761/3560 Training loss: 2.0098 0.3722 sec/batch\n",
      "Epoch 10/20  Iteration 1762/3560 Training loss: 2.0099 0.4699 sec/batch\n",
      "Epoch 10/20  Iteration 1763/3560 Training loss: 2.0100 0.3743 sec/batch\n",
      "Epoch 10/20  Iteration 1764/3560 Training loss: 2.0099 0.3958 sec/batch\n",
      "Epoch 10/20  Iteration 1765/3560 Training loss: 2.0099 0.4133 sec/batch\n",
      "Epoch 10/20  Iteration 1766/3560 Training loss: 2.0099 0.5117 sec/batch\n",
      "Epoch 10/20  Iteration 1767/3560 Training loss: 2.0098 0.3387 sec/batch\n",
      "Epoch 10/20  Iteration 1768/3560 Training loss: 2.0097 0.4443 sec/batch\n",
      "Epoch 10/20  Iteration 1769/3560 Training loss: 2.0097 0.4860 sec/batch\n",
      "Epoch 10/20  Iteration 1770/3560 Training loss: 2.0099 0.4220 sec/batch\n",
      "Epoch 10/20  Iteration 1771/3560 Training loss: 2.0099 0.4245 sec/batch\n",
      "Epoch 10/20  Iteration 1772/3560 Training loss: 2.0099 0.4546 sec/batch\n",
      "Epoch 10/20  Iteration 1773/3560 Training loss: 2.0097 0.3966 sec/batch\n",
      "Epoch 10/20  Iteration 1774/3560 Training loss: 2.0096 0.5333 sec/batch\n",
      "Epoch 10/20  Iteration 1775/3560 Training loss: 2.0097 0.5083 sec/batch\n",
      "Epoch 10/20  Iteration 1776/3560 Training loss: 2.0098 0.4016 sec/batch\n",
      "Epoch 10/20  Iteration 1777/3560 Training loss: 2.0097 0.4314 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1778/3560 Training loss: 2.0097 0.4154 sec/batch\n",
      "Epoch 10/20  Iteration 1779/3560 Training loss: 2.0095 0.3668 sec/batch\n",
      "Epoch 10/20  Iteration 1780/3560 Training loss: 2.0095 0.3701 sec/batch\n",
      "Epoch 11/20  Iteration 1781/3560 Training loss: 2.0608 0.4278 sec/batch\n",
      "Epoch 11/20  Iteration 1782/3560 Training loss: 2.0207 0.4028 sec/batch\n",
      "Epoch 11/20  Iteration 1783/3560 Training loss: 2.0097 0.4425 sec/batch\n",
      "Epoch 11/20  Iteration 1784/3560 Training loss: 2.0044 0.3927 sec/batch\n",
      "Epoch 11/20  Iteration 1785/3560 Training loss: 1.9994 0.4379 sec/batch\n",
      "Epoch 11/20  Iteration 1786/3560 Training loss: 1.9931 0.4204 sec/batch\n",
      "Epoch 11/20  Iteration 1787/3560 Training loss: 1.9927 0.4682 sec/batch\n",
      "Epoch 11/20  Iteration 1788/3560 Training loss: 1.9928 0.4360 sec/batch\n",
      "Epoch 11/20  Iteration 1789/3560 Training loss: 1.9957 0.4937 sec/batch\n",
      "Epoch 11/20  Iteration 1790/3560 Training loss: 1.9953 0.4215 sec/batch\n",
      "Epoch 11/20  Iteration 1791/3560 Training loss: 1.9933 0.4654 sec/batch\n",
      "Epoch 11/20  Iteration 1792/3560 Training loss: 1.9921 0.4302 sec/batch\n",
      "Epoch 11/20  Iteration 1793/3560 Training loss: 1.9923 0.5051 sec/batch\n",
      "Epoch 11/20  Iteration 1794/3560 Training loss: 1.9947 0.5245 sec/batch\n",
      "Epoch 11/20  Iteration 1795/3560 Training loss: 1.9939 0.5419 sec/batch\n",
      "Epoch 11/20  Iteration 1796/3560 Training loss: 1.9929 0.3975 sec/batch\n",
      "Epoch 11/20  Iteration 1797/3560 Training loss: 1.9927 0.4030 sec/batch\n",
      "Epoch 11/20  Iteration 1798/3560 Training loss: 1.9948 0.4096 sec/batch\n",
      "Epoch 11/20  Iteration 1799/3560 Training loss: 1.9950 0.4779 sec/batch\n",
      "Epoch 11/20  Iteration 1800/3560 Training loss: 1.9952 0.5596 sec/batch\n",
      "Epoch 11/20  Iteration 1801/3560 Training loss: 1.9946 0.3960 sec/batch\n",
      "Epoch 11/20  Iteration 1802/3560 Training loss: 1.9959 0.4443 sec/batch\n",
      "Epoch 11/20  Iteration 1803/3560 Training loss: 1.9956 0.4590 sec/batch\n",
      "Epoch 11/20  Iteration 1804/3560 Training loss: 1.9950 0.4076 sec/batch\n",
      "Epoch 11/20  Iteration 1805/3560 Training loss: 1.9949 0.3911 sec/batch\n",
      "Epoch 11/20  Iteration 1806/3560 Training loss: 1.9937 0.3807 sec/batch\n",
      "Epoch 11/20  Iteration 1807/3560 Training loss: 1.9929 0.4025 sec/batch\n",
      "Epoch 11/20  Iteration 1808/3560 Training loss: 1.9934 0.4467 sec/batch\n",
      "Epoch 11/20  Iteration 1809/3560 Training loss: 1.9943 0.4488 sec/batch\n",
      "Epoch 11/20  Iteration 1810/3560 Training loss: 1.9946 0.4262 sec/batch\n",
      "Epoch 11/20  Iteration 1811/3560 Training loss: 1.9944 0.4830 sec/batch\n",
      "Epoch 11/20  Iteration 1812/3560 Training loss: 1.9940 0.4381 sec/batch\n",
      "Epoch 11/20  Iteration 1813/3560 Training loss: 1.9941 0.4510 sec/batch\n",
      "Epoch 11/20  Iteration 1814/3560 Training loss: 1.9949 0.4145 sec/batch\n",
      "Epoch 11/20  Iteration 1815/3560 Training loss: 1.9947 0.4523 sec/batch\n",
      "Epoch 11/20  Iteration 1816/3560 Training loss: 1.9945 0.4409 sec/batch\n",
      "Epoch 11/20  Iteration 1817/3560 Training loss: 1.9938 0.4019 sec/batch\n",
      "Epoch 11/20  Iteration 1818/3560 Training loss: 1.9928 0.3843 sec/batch\n",
      "Epoch 11/20  Iteration 1819/3560 Training loss: 1.9917 0.3949 sec/batch\n",
      "Epoch 11/20  Iteration 1820/3560 Training loss: 1.9912 0.4124 sec/batch\n",
      "Epoch 11/20  Iteration 1821/3560 Training loss: 1.9907 0.4280 sec/batch\n",
      "Epoch 11/20  Iteration 1822/3560 Training loss: 1.9908 0.4299 sec/batch\n",
      "Epoch 11/20  Iteration 1823/3560 Training loss: 1.9905 0.5208 sec/batch\n",
      "Epoch 11/20  Iteration 1824/3560 Training loss: 1.9897 0.3745 sec/batch\n",
      "Epoch 11/20  Iteration 1825/3560 Training loss: 1.9897 0.4261 sec/batch\n",
      "Epoch 11/20  Iteration 1826/3560 Training loss: 1.9886 0.4258 sec/batch\n",
      "Epoch 11/20  Iteration 1827/3560 Training loss: 1.9884 0.4574 sec/batch\n",
      "Epoch 11/20  Iteration 1828/3560 Training loss: 1.9879 0.4357 sec/batch\n",
      "Epoch 11/20  Iteration 1829/3560 Training loss: 1.9878 0.5172 sec/batch\n",
      "Epoch 11/20  Iteration 1830/3560 Training loss: 1.9885 0.4825 sec/batch\n",
      "Epoch 11/20  Iteration 1831/3560 Training loss: 1.9880 0.4309 sec/batch\n",
      "Epoch 11/20  Iteration 1832/3560 Training loss: 1.9886 0.4447 sec/batch\n",
      "Epoch 11/20  Iteration 1833/3560 Training loss: 1.9886 0.4737 sec/batch\n",
      "Epoch 11/20  Iteration 1834/3560 Training loss: 1.9885 0.4237 sec/batch\n",
      "Epoch 11/20  Iteration 1835/3560 Training loss: 1.9880 0.4523 sec/batch\n",
      "Epoch 11/20  Iteration 1836/3560 Training loss: 1.9881 0.3749 sec/batch\n",
      "Epoch 11/20  Iteration 1837/3560 Training loss: 1.9883 0.4411 sec/batch\n",
      "Epoch 11/20  Iteration 1838/3560 Training loss: 1.9878 0.3639 sec/batch\n",
      "Epoch 11/20  Iteration 1839/3560 Training loss: 1.9874 0.4379 sec/batch\n",
      "Epoch 11/20  Iteration 1840/3560 Training loss: 1.9880 0.3819 sec/batch\n",
      "Epoch 11/20  Iteration 1841/3560 Training loss: 1.9878 0.3971 sec/batch\n",
      "Epoch 11/20  Iteration 1842/3560 Training loss: 1.9883 0.3959 sec/batch\n",
      "Epoch 11/20  Iteration 1843/3560 Training loss: 1.9886 0.4089 sec/batch\n",
      "Epoch 11/20  Iteration 1844/3560 Training loss: 1.9888 0.4319 sec/batch\n",
      "Epoch 11/20  Iteration 1845/3560 Training loss: 1.9887 0.3815 sec/batch\n",
      "Epoch 11/20  Iteration 1846/3560 Training loss: 1.9889 0.3911 sec/batch\n",
      "Epoch 11/20  Iteration 1847/3560 Training loss: 1.9889 0.4220 sec/batch\n",
      "Epoch 11/20  Iteration 1848/3560 Training loss: 1.9883 0.5756 sec/batch\n",
      "Epoch 11/20  Iteration 1849/3560 Training loss: 1.9882 0.4247 sec/batch\n",
      "Epoch 11/20  Iteration 1850/3560 Training loss: 1.9881 0.4428 sec/batch\n",
      "Epoch 11/20  Iteration 1851/3560 Training loss: 1.9884 0.5246 sec/batch\n",
      "Epoch 11/20  Iteration 1852/3560 Training loss: 1.9886 0.6837 sec/batch\n",
      "Epoch 11/20  Iteration 1853/3560 Training loss: 1.9889 0.4542 sec/batch\n",
      "Epoch 11/20  Iteration 1854/3560 Training loss: 1.9885 0.4167 sec/batch\n",
      "Epoch 11/20  Iteration 1855/3560 Training loss: 1.9883 0.4376 sec/batch\n",
      "Epoch 11/20  Iteration 1856/3560 Training loss: 1.9887 0.4057 sec/batch\n",
      "Epoch 11/20  Iteration 1857/3560 Training loss: 1.9888 0.4228 sec/batch\n",
      "Epoch 11/20  Iteration 1858/3560 Training loss: 1.9889 0.3817 sec/batch\n",
      "Epoch 11/20  Iteration 1859/3560 Training loss: 1.9885 0.3775 sec/batch\n",
      "Epoch 11/20  Iteration 1860/3560 Training loss: 1.9883 0.4190 sec/batch\n",
      "Epoch 11/20  Iteration 1861/3560 Training loss: 1.9878 0.4182 sec/batch\n",
      "Epoch 11/20  Iteration 1862/3560 Training loss: 1.9879 0.4514 sec/batch\n",
      "Epoch 11/20  Iteration 1863/3560 Training loss: 1.9874 0.3873 sec/batch\n",
      "Epoch 11/20  Iteration 1864/3560 Training loss: 1.9873 0.4516 sec/batch\n",
      "Epoch 11/20  Iteration 1865/3560 Training loss: 1.9866 0.4316 sec/batch\n",
      "Epoch 11/20  Iteration 1866/3560 Training loss: 1.9862 0.4878 sec/batch\n",
      "Epoch 11/20  Iteration 1867/3560 Training loss: 1.9860 0.4437 sec/batch\n",
      "Epoch 11/20  Iteration 1868/3560 Training loss: 1.9857 0.4310 sec/batch\n",
      "Epoch 11/20  Iteration 1869/3560 Training loss: 1.9853 0.4704 sec/batch\n",
      "Epoch 11/20  Iteration 1870/3560 Training loss: 1.9853 0.4745 sec/batch\n",
      "Epoch 11/20  Iteration 1871/3560 Training loss: 1.9851 0.4528 sec/batch\n",
      "Epoch 11/20  Iteration 1872/3560 Training loss: 1.9849 0.4784 sec/batch\n",
      "Epoch 11/20  Iteration 1873/3560 Training loss: 1.9845 0.4472 sec/batch\n",
      "Epoch 11/20  Iteration 1874/3560 Training loss: 1.9841 0.4323 sec/batch\n",
      "Epoch 11/20  Iteration 1875/3560 Training loss: 1.9838 0.4006 sec/batch\n",
      "Epoch 11/20  Iteration 1876/3560 Training loss: 1.9837 0.4133 sec/batch\n",
      "Epoch 11/20  Iteration 1877/3560 Training loss: 1.9836 0.4051 sec/batch\n",
      "Epoch 11/20  Iteration 1878/3560 Training loss: 1.9832 0.4707 sec/batch\n",
      "Epoch 11/20  Iteration 1879/3560 Training loss: 1.9829 0.4190 sec/batch\n",
      "Epoch 11/20  Iteration 1880/3560 Training loss: 1.9826 0.3665 sec/batch\n",
      "Epoch 11/20  Iteration 1881/3560 Training loss: 1.9826 0.4089 sec/batch\n",
      "Epoch 11/20  Iteration 1882/3560 Training loss: 1.9824 0.4338 sec/batch\n",
      "Epoch 11/20  Iteration 1883/3560 Training loss: 1.9821 0.4186 sec/batch\n",
      "Epoch 11/20  Iteration 1884/3560 Training loss: 1.9820 0.3725 sec/batch\n",
      "Epoch 11/20  Iteration 1885/3560 Training loss: 1.9817 0.5016 sec/batch\n",
      "Epoch 11/20  Iteration 1886/3560 Training loss: 1.9815 0.4343 sec/batch\n",
      "Epoch 11/20  Iteration 1887/3560 Training loss: 1.9815 0.5833 sec/batch\n",
      "Epoch 11/20  Iteration 1888/3560 Training loss: 1.9815 0.4836 sec/batch\n",
      "Epoch 11/20  Iteration 1889/3560 Training loss: 1.9815 0.4352 sec/batch\n",
      "Epoch 11/20  Iteration 1890/3560 Training loss: 1.9815 0.4764 sec/batch\n",
      "Epoch 11/20  Iteration 1891/3560 Training loss: 1.9815 0.4614 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1892/3560 Training loss: 1.9814 0.3996 sec/batch\n",
      "Epoch 11/20  Iteration 1893/3560 Training loss: 1.9813 0.4524 sec/batch\n",
      "Epoch 11/20  Iteration 1894/3560 Training loss: 1.9812 0.3978 sec/batch\n",
      "Epoch 11/20  Iteration 1895/3560 Training loss: 1.9810 0.4692 sec/batch\n",
      "Epoch 11/20  Iteration 1896/3560 Training loss: 1.9806 0.4032 sec/batch\n",
      "Epoch 11/20  Iteration 1897/3560 Training loss: 1.9805 0.4185 sec/batch\n",
      "Epoch 11/20  Iteration 1898/3560 Training loss: 1.9804 0.3986 sec/batch\n",
      "Epoch 11/20  Iteration 1899/3560 Training loss: 1.9805 0.4063 sec/batch\n",
      "Epoch 11/20  Iteration 1900/3560 Training loss: 1.9804 0.4209 sec/batch\n",
      "Epoch 11/20  Iteration 1901/3560 Training loss: 1.9803 0.4698 sec/batch\n",
      "Epoch 11/20  Iteration 1902/3560 Training loss: 1.9800 0.4267 sec/batch\n",
      "Epoch 11/20  Iteration 1903/3560 Training loss: 1.9798 0.5118 sec/batch\n",
      "Epoch 11/20  Iteration 1904/3560 Training loss: 1.9798 0.3247 sec/batch\n",
      "Epoch 11/20  Iteration 1905/3560 Training loss: 1.9797 0.4180 sec/batch\n",
      "Epoch 11/20  Iteration 1906/3560 Training loss: 1.9794 0.4645 sec/batch\n",
      "Epoch 11/20  Iteration 1907/3560 Training loss: 1.9795 0.4751 sec/batch\n",
      "Epoch 11/20  Iteration 1908/3560 Training loss: 1.9795 0.4608 sec/batch\n",
      "Epoch 11/20  Iteration 1909/3560 Training loss: 1.9795 0.5011 sec/batch\n",
      "Epoch 11/20  Iteration 1910/3560 Training loss: 1.9795 0.4274 sec/batch\n",
      "Epoch 11/20  Iteration 1911/3560 Training loss: 1.9792 0.3920 sec/batch\n",
      "Epoch 11/20  Iteration 1912/3560 Training loss: 1.9789 0.3940 sec/batch\n",
      "Epoch 11/20  Iteration 1913/3560 Training loss: 1.9789 0.4488 sec/batch\n",
      "Epoch 11/20  Iteration 1914/3560 Training loss: 1.9788 0.4328 sec/batch\n",
      "Epoch 11/20  Iteration 1915/3560 Training loss: 1.9788 0.4304 sec/batch\n",
      "Epoch 11/20  Iteration 1916/3560 Training loss: 1.9789 0.3971 sec/batch\n",
      "Epoch 11/20  Iteration 1917/3560 Training loss: 1.9789 0.3898 sec/batch\n",
      "Epoch 11/20  Iteration 1918/3560 Training loss: 1.9789 0.3787 sec/batch\n",
      "Epoch 11/20  Iteration 1919/3560 Training loss: 1.9791 0.4187 sec/batch\n",
      "Epoch 11/20  Iteration 1920/3560 Training loss: 1.9789 0.3953 sec/batch\n",
      "Epoch 11/20  Iteration 1921/3560 Training loss: 1.9791 0.4291 sec/batch\n",
      "Epoch 11/20  Iteration 1922/3560 Training loss: 1.9790 0.4433 sec/batch\n",
      "Epoch 11/20  Iteration 1923/3560 Training loss: 1.9790 0.3619 sec/batch\n",
      "Epoch 11/20  Iteration 1924/3560 Training loss: 1.9789 0.4078 sec/batch\n",
      "Epoch 11/20  Iteration 1925/3560 Training loss: 1.9788 0.5189 sec/batch\n",
      "Epoch 11/20  Iteration 1926/3560 Training loss: 1.9789 0.4653 sec/batch\n",
      "Epoch 11/20  Iteration 1927/3560 Training loss: 1.9789 0.4166 sec/batch\n",
      "Epoch 11/20  Iteration 1928/3560 Training loss: 1.9790 0.4835 sec/batch\n",
      "Epoch 11/20  Iteration 1929/3560 Training loss: 1.9790 0.4027 sec/batch\n",
      "Epoch 11/20  Iteration 1930/3560 Training loss: 1.9787 0.4269 sec/batch\n",
      "Epoch 11/20  Iteration 1931/3560 Training loss: 1.9785 0.4133 sec/batch\n",
      "Epoch 11/20  Iteration 1932/3560 Training loss: 1.9787 0.4223 sec/batch\n",
      "Epoch 11/20  Iteration 1933/3560 Training loss: 1.9787 0.3982 sec/batch\n",
      "Epoch 11/20  Iteration 1934/3560 Training loss: 1.9787 0.4460 sec/batch\n",
      "Epoch 11/20  Iteration 1935/3560 Training loss: 1.9787 0.4043 sec/batch\n",
      "Epoch 11/20  Iteration 1936/3560 Training loss: 1.9785 0.3854 sec/batch\n",
      "Epoch 11/20  Iteration 1937/3560 Training loss: 1.9786 0.4014 sec/batch\n",
      "Epoch 11/20  Iteration 1938/3560 Training loss: 1.9785 0.4627 sec/batch\n",
      "Epoch 11/20  Iteration 1939/3560 Training loss: 1.9783 0.3625 sec/batch\n",
      "Epoch 11/20  Iteration 1940/3560 Training loss: 1.9784 0.3862 sec/batch\n",
      "Epoch 11/20  Iteration 1941/3560 Training loss: 1.9785 0.4705 sec/batch\n",
      "Epoch 11/20  Iteration 1942/3560 Training loss: 1.9784 0.4756 sec/batch\n",
      "Epoch 11/20  Iteration 1943/3560 Training loss: 1.9784 0.4737 sec/batch\n",
      "Epoch 11/20  Iteration 1944/3560 Training loss: 1.9783 0.5242 sec/batch\n",
      "Epoch 11/20  Iteration 1945/3560 Training loss: 1.9783 0.4863 sec/batch\n",
      "Epoch 11/20  Iteration 1946/3560 Training loss: 1.9782 0.6656 sec/batch\n",
      "Epoch 11/20  Iteration 1947/3560 Training loss: 1.9782 0.4814 sec/batch\n",
      "Epoch 11/20  Iteration 1948/3560 Training loss: 1.9785 0.5836 sec/batch\n",
      "Epoch 11/20  Iteration 1949/3560 Training loss: 1.9784 0.5943 sec/batch\n",
      "Epoch 11/20  Iteration 1950/3560 Training loss: 1.9784 0.4104 sec/batch\n",
      "Epoch 11/20  Iteration 1951/3560 Training loss: 1.9783 0.4656 sec/batch\n",
      "Epoch 11/20  Iteration 1952/3560 Training loss: 1.9782 0.4086 sec/batch\n",
      "Epoch 11/20  Iteration 1953/3560 Training loss: 1.9783 0.4833 sec/batch\n",
      "Epoch 11/20  Iteration 1954/3560 Training loss: 1.9783 0.4331 sec/batch\n",
      "Epoch 11/20  Iteration 1955/3560 Training loss: 1.9784 0.4485 sec/batch\n",
      "Epoch 11/20  Iteration 1956/3560 Training loss: 1.9783 0.4750 sec/batch\n",
      "Epoch 11/20  Iteration 1957/3560 Training loss: 1.9782 0.3973 sec/batch\n",
      "Epoch 11/20  Iteration 1958/3560 Training loss: 1.9782 0.4046 sec/batch\n",
      "Epoch 12/20  Iteration 1959/3560 Training loss: 2.0273 0.4267 sec/batch\n",
      "Epoch 12/20  Iteration 1960/3560 Training loss: 1.9923 0.4210 sec/batch\n",
      "Epoch 12/20  Iteration 1961/3560 Training loss: 1.9788 0.4155 sec/batch\n",
      "Epoch 12/20  Iteration 1962/3560 Training loss: 1.9727 0.4120 sec/batch\n",
      "Epoch 12/20  Iteration 1963/3560 Training loss: 1.9698 0.4261 sec/batch\n",
      "Epoch 12/20  Iteration 1964/3560 Training loss: 1.9636 0.3933 sec/batch\n",
      "Epoch 12/20  Iteration 1965/3560 Training loss: 1.9642 0.4211 sec/batch\n",
      "Epoch 12/20  Iteration 1966/3560 Training loss: 1.9644 0.4449 sec/batch\n",
      "Epoch 12/20  Iteration 1967/3560 Training loss: 1.9660 0.4834 sec/batch\n",
      "Epoch 12/20  Iteration 1968/3560 Training loss: 1.9662 0.4349 sec/batch\n",
      "Epoch 12/20  Iteration 1969/3560 Training loss: 1.9628 0.3740 sec/batch\n",
      "Epoch 12/20  Iteration 1970/3560 Training loss: 1.9618 0.3750 sec/batch\n",
      "Epoch 12/20  Iteration 1971/3560 Training loss: 1.9622 0.4396 sec/batch\n",
      "Epoch 12/20  Iteration 1972/3560 Training loss: 1.9643 0.3541 sec/batch\n",
      "Epoch 12/20  Iteration 1973/3560 Training loss: 1.9641 0.4087 sec/batch\n",
      "Epoch 12/20  Iteration 1974/3560 Training loss: 1.9634 0.3817 sec/batch\n",
      "Epoch 12/20  Iteration 1975/3560 Training loss: 1.9635 0.4029 sec/batch\n",
      "Epoch 12/20  Iteration 1976/3560 Training loss: 1.9654 0.3731 sec/batch\n",
      "Epoch 12/20  Iteration 1977/3560 Training loss: 1.9658 0.3878 sec/batch\n",
      "Epoch 12/20  Iteration 1978/3560 Training loss: 1.9657 0.4145 sec/batch\n",
      "Epoch 12/20  Iteration 1979/3560 Training loss: 1.9650 0.3814 sec/batch\n",
      "Epoch 12/20  Iteration 1980/3560 Training loss: 1.9659 0.3951 sec/batch\n",
      "Epoch 12/20  Iteration 1981/3560 Training loss: 1.9655 0.4138 sec/batch\n",
      "Epoch 12/20  Iteration 1982/3560 Training loss: 1.9650 0.4614 sec/batch\n",
      "Epoch 12/20  Iteration 1983/3560 Training loss: 1.9650 0.4278 sec/batch\n",
      "Epoch 12/20  Iteration 1984/3560 Training loss: 1.9640 0.4704 sec/batch\n",
      "Epoch 12/20  Iteration 1985/3560 Training loss: 1.9633 0.4107 sec/batch\n",
      "Epoch 12/20  Iteration 1986/3560 Training loss: 1.9639 0.4399 sec/batch\n",
      "Epoch 12/20  Iteration 1987/3560 Training loss: 1.9647 0.4435 sec/batch\n",
      "Epoch 12/20  Iteration 1988/3560 Training loss: 1.9650 0.4329 sec/batch\n",
      "Epoch 12/20  Iteration 1989/3560 Training loss: 1.9650 0.4099 sec/batch\n",
      "Epoch 12/20  Iteration 1990/3560 Training loss: 1.9647 0.3512 sec/batch\n",
      "Epoch 12/20  Iteration 1991/3560 Training loss: 1.9648 0.3913 sec/batch\n",
      "Epoch 12/20  Iteration 1992/3560 Training loss: 1.9657 0.3925 sec/batch\n",
      "Epoch 12/20  Iteration 1993/3560 Training loss: 1.9655 0.3957 sec/batch\n",
      "Epoch 12/20  Iteration 1994/3560 Training loss: 1.9651 0.4042 sec/batch\n",
      "Epoch 12/20  Iteration 1995/3560 Training loss: 1.9647 0.3760 sec/batch\n",
      "Epoch 12/20  Iteration 1996/3560 Training loss: 1.9636 0.4171 sec/batch\n",
      "Epoch 12/20  Iteration 1997/3560 Training loss: 1.9625 0.4177 sec/batch\n",
      "Epoch 12/20  Iteration 1998/3560 Training loss: 1.9622 0.3480 sec/batch\n",
      "Epoch 12/20  Iteration 1999/3560 Training loss: 1.9616 0.4007 sec/batch\n",
      "Epoch 12/20  Iteration 2000/3560 Training loss: 1.9617 0.3933 sec/batch\n",
      "Epoch 12/20  Iteration 2001/3560 Training loss: 1.9611 0.4555 sec/batch\n",
      "Epoch 12/20  Iteration 2002/3560 Training loss: 1.9602 0.4439 sec/batch\n",
      "Epoch 12/20  Iteration 2003/3560 Training loss: 1.9603 0.4510 sec/batch\n",
      "Epoch 12/20  Iteration 2004/3560 Training loss: 1.9591 0.4601 sec/batch\n",
      "Epoch 12/20  Iteration 2005/3560 Training loss: 1.9593 0.4139 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 2006/3560 Training loss: 1.9590 0.4669 sec/batch\n",
      "Epoch 12/20  Iteration 2007/3560 Training loss: 1.9589 0.3978 sec/batch\n",
      "Epoch 12/20  Iteration 2008/3560 Training loss: 1.9596 0.3936 sec/batch\n",
      "Epoch 12/20  Iteration 2009/3560 Training loss: 1.9590 0.4285 sec/batch\n",
      "Epoch 12/20  Iteration 2010/3560 Training loss: 1.9596 0.4266 sec/batch\n",
      "Epoch 12/20  Iteration 2011/3560 Training loss: 1.9593 0.4214 sec/batch\n",
      "Epoch 12/20  Iteration 2012/3560 Training loss: 1.9592 0.4568 sec/batch\n",
      "Epoch 12/20  Iteration 2013/3560 Training loss: 1.9589 0.3854 sec/batch\n",
      "Epoch 12/20  Iteration 2014/3560 Training loss: 1.9590 0.3999 sec/batch\n",
      "Epoch 12/20  Iteration 2015/3560 Training loss: 1.9590 0.4589 sec/batch\n",
      "Epoch 12/20  Iteration 2016/3560 Training loss: 1.9588 0.3679 sec/batch\n",
      "Epoch 12/20  Iteration 2017/3560 Training loss: 1.9584 0.4159 sec/batch\n",
      "Epoch 12/20  Iteration 2018/3560 Training loss: 1.9590 0.3675 sec/batch\n",
      "Epoch 12/20  Iteration 2019/3560 Training loss: 1.9589 0.4148 sec/batch\n",
      "Epoch 12/20  Iteration 2020/3560 Training loss: 1.9596 0.4272 sec/batch\n",
      "Epoch 12/20  Iteration 2021/3560 Training loss: 1.9598 0.3940 sec/batch\n",
      "Epoch 12/20  Iteration 2022/3560 Training loss: 1.9601 0.4120 sec/batch\n",
      "Epoch 12/20  Iteration 2023/3560 Training loss: 1.9601 0.4613 sec/batch\n",
      "Epoch 12/20  Iteration 2024/3560 Training loss: 1.9603 0.4430 sec/batch\n",
      "Epoch 12/20  Iteration 2025/3560 Training loss: 1.9605 0.4394 sec/batch\n",
      "Epoch 12/20  Iteration 2026/3560 Training loss: 1.9600 0.4395 sec/batch\n",
      "Epoch 12/20  Iteration 2027/3560 Training loss: 1.9598 0.3736 sec/batch\n",
      "Epoch 12/20  Iteration 2028/3560 Training loss: 1.9597 0.4120 sec/batch\n",
      "Epoch 12/20  Iteration 2029/3560 Training loss: 1.9601 0.4411 sec/batch\n",
      "Epoch 12/20  Iteration 2030/3560 Training loss: 1.9602 0.4235 sec/batch\n",
      "Epoch 12/20  Iteration 2031/3560 Training loss: 1.9607 0.3869 sec/batch\n",
      "Epoch 12/20  Iteration 2032/3560 Training loss: 1.9604 0.3954 sec/batch\n",
      "Epoch 12/20  Iteration 2033/3560 Training loss: 1.9603 0.4349 sec/batch\n",
      "Epoch 12/20  Iteration 2034/3560 Training loss: 1.9605 0.4184 sec/batch\n",
      "Epoch 12/20  Iteration 2035/3560 Training loss: 1.9604 0.4119 sec/batch\n",
      "Epoch 12/20  Iteration 2036/3560 Training loss: 1.9605 0.4667 sec/batch\n",
      "Epoch 12/20  Iteration 2037/3560 Training loss: 1.9601 0.3909 sec/batch\n",
      "Epoch 12/20  Iteration 2038/3560 Training loss: 1.9599 0.3924 sec/batch\n",
      "Epoch 12/20  Iteration 2039/3560 Training loss: 1.9593 0.3817 sec/batch\n",
      "Epoch 12/20  Iteration 2040/3560 Training loss: 1.9593 0.4839 sec/batch\n",
      "Epoch 12/20  Iteration 2041/3560 Training loss: 1.9588 0.4129 sec/batch\n",
      "Epoch 12/20  Iteration 2042/3560 Training loss: 1.9587 0.5083 sec/batch\n",
      "Epoch 12/20  Iteration 2043/3560 Training loss: 1.9582 0.4656 sec/batch\n",
      "Epoch 12/20  Iteration 2044/3560 Training loss: 1.9578 0.3927 sec/batch\n",
      "Epoch 12/20  Iteration 2045/3560 Training loss: 1.9577 0.3962 sec/batch\n",
      "Epoch 12/20  Iteration 2046/3560 Training loss: 1.9575 0.3726 sec/batch\n",
      "Epoch 12/20  Iteration 2047/3560 Training loss: 1.9570 0.4371 sec/batch\n",
      "Epoch 12/20  Iteration 2048/3560 Training loss: 1.9571 0.4211 sec/batch\n",
      "Epoch 12/20  Iteration 2049/3560 Training loss: 1.9569 0.4418 sec/batch\n",
      "Epoch 12/20  Iteration 2050/3560 Training loss: 1.9567 0.4397 sec/batch\n",
      "Epoch 12/20  Iteration 2051/3560 Training loss: 1.9563 0.3896 sec/batch\n",
      "Epoch 12/20  Iteration 2052/3560 Training loss: 1.9560 0.3849 sec/batch\n",
      "Epoch 12/20  Iteration 2053/3560 Training loss: 1.9559 0.4374 sec/batch\n",
      "Epoch 12/20  Iteration 2054/3560 Training loss: 1.9558 0.4007 sec/batch\n",
      "Epoch 12/20  Iteration 2055/3560 Training loss: 1.9557 0.3981 sec/batch\n",
      "Epoch 12/20  Iteration 2056/3560 Training loss: 1.9553 0.3685 sec/batch\n",
      "Epoch 12/20  Iteration 2057/3560 Training loss: 1.9549 0.4035 sec/batch\n",
      "Epoch 12/20  Iteration 2058/3560 Training loss: 1.9546 0.4497 sec/batch\n",
      "Epoch 12/20  Iteration 2059/3560 Training loss: 1.9546 0.4452 sec/batch\n",
      "Epoch 12/20  Iteration 2060/3560 Training loss: 1.9546 0.4783 sec/batch\n",
      "Epoch 12/20  Iteration 2061/3560 Training loss: 1.9542 0.4990 sec/batch\n",
      "Epoch 12/20  Iteration 2062/3560 Training loss: 1.9541 0.4359 sec/batch\n",
      "Epoch 12/20  Iteration 2063/3560 Training loss: 1.9539 0.4504 sec/batch\n",
      "Epoch 12/20  Iteration 2064/3560 Training loss: 1.9538 0.4128 sec/batch\n",
      "Epoch 12/20  Iteration 2065/3560 Training loss: 1.9538 0.3956 sec/batch\n",
      "Epoch 12/20  Iteration 2066/3560 Training loss: 1.9538 0.4099 sec/batch\n",
      "Epoch 12/20  Iteration 2067/3560 Training loss: 1.9538 0.4134 sec/batch\n",
      "Epoch 12/20  Iteration 2068/3560 Training loss: 1.9538 0.3946 sec/batch\n",
      "Epoch 12/20  Iteration 2069/3560 Training loss: 1.9536 0.3757 sec/batch\n",
      "Epoch 12/20  Iteration 2070/3560 Training loss: 1.9535 0.4138 sec/batch\n",
      "Epoch 12/20  Iteration 2071/3560 Training loss: 1.9534 0.3847 sec/batch\n",
      "Epoch 12/20  Iteration 2072/3560 Training loss: 1.9532 0.3955 sec/batch\n",
      "Epoch 12/20  Iteration 2073/3560 Training loss: 1.9530 0.4539 sec/batch\n",
      "Epoch 12/20  Iteration 2074/3560 Training loss: 1.9526 0.3630 sec/batch\n",
      "Epoch 12/20  Iteration 2075/3560 Training loss: 1.9525 0.3660 sec/batch\n",
      "Epoch 12/20  Iteration 2076/3560 Training loss: 1.9525 0.4040 sec/batch\n",
      "Epoch 12/20  Iteration 2077/3560 Training loss: 1.9525 0.4162 sec/batch\n",
      "Epoch 12/20  Iteration 2078/3560 Training loss: 1.9523 0.4357 sec/batch\n",
      "Epoch 12/20  Iteration 2079/3560 Training loss: 1.9524 0.4900 sec/batch\n",
      "Epoch 12/20  Iteration 2080/3560 Training loss: 1.9520 0.5161 sec/batch\n",
      "Epoch 12/20  Iteration 2081/3560 Training loss: 1.9518 0.4028 sec/batch\n",
      "Epoch 12/20  Iteration 2082/3560 Training loss: 1.9518 0.4307 sec/batch\n",
      "Epoch 12/20  Iteration 2083/3560 Training loss: 1.9518 0.4493 sec/batch\n",
      "Epoch 12/20  Iteration 2084/3560 Training loss: 1.9515 0.3994 sec/batch\n",
      "Epoch 12/20  Iteration 2085/3560 Training loss: 1.9516 0.3718 sec/batch\n",
      "Epoch 12/20  Iteration 2086/3560 Training loss: 1.9516 0.4265 sec/batch\n",
      "Epoch 12/20  Iteration 2087/3560 Training loss: 1.9516 0.3514 sec/batch\n",
      "Epoch 12/20  Iteration 2088/3560 Training loss: 1.9515 0.4166 sec/batch\n",
      "Epoch 12/20  Iteration 2089/3560 Training loss: 1.9513 0.3918 sec/batch\n",
      "Epoch 12/20  Iteration 2090/3560 Training loss: 1.9510 0.4338 sec/batch\n",
      "Epoch 12/20  Iteration 2091/3560 Training loss: 1.9509 0.4318 sec/batch\n",
      "Epoch 12/20  Iteration 2092/3560 Training loss: 1.9510 0.4056 sec/batch\n",
      "Epoch 12/20  Iteration 2093/3560 Training loss: 1.9509 0.4005 sec/batch\n",
      "Epoch 12/20  Iteration 2094/3560 Training loss: 1.9509 0.3991 sec/batch\n",
      "Epoch 12/20  Iteration 2095/3560 Training loss: 1.9509 0.3934 sec/batch\n",
      "Epoch 12/20  Iteration 2096/3560 Training loss: 1.9509 0.4421 sec/batch\n",
      "Epoch 12/20  Iteration 2097/3560 Training loss: 1.9511 0.4753 sec/batch\n",
      "Epoch 12/20  Iteration 2098/3560 Training loss: 1.9509 0.4230 sec/batch\n",
      "Epoch 12/20  Iteration 2099/3560 Training loss: 1.9511 0.5145 sec/batch\n",
      "Epoch 12/20  Iteration 2100/3560 Training loss: 1.9510 0.3527 sec/batch\n",
      "Epoch 12/20  Iteration 2101/3560 Training loss: 1.9510 0.4078 sec/batch\n",
      "Epoch 12/20  Iteration 2102/3560 Training loss: 1.9510 0.4623 sec/batch\n",
      "Epoch 12/20  Iteration 2103/3560 Training loss: 1.9509 0.4043 sec/batch\n",
      "Epoch 12/20  Iteration 2104/3560 Training loss: 1.9509 0.4970 sec/batch\n",
      "Epoch 12/20  Iteration 2105/3560 Training loss: 1.9510 0.4762 sec/batch\n",
      "Epoch 12/20  Iteration 2106/3560 Training loss: 1.9511 0.3760 sec/batch\n",
      "Epoch 12/20  Iteration 2107/3560 Training loss: 1.9510 0.5014 sec/batch\n",
      "Epoch 12/20  Iteration 2108/3560 Training loss: 1.9509 0.3600 sec/batch\n",
      "Epoch 12/20  Iteration 2109/3560 Training loss: 1.9507 0.4033 sec/batch\n",
      "Epoch 12/20  Iteration 2110/3560 Training loss: 1.9508 0.4508 sec/batch\n",
      "Epoch 12/20  Iteration 2111/3560 Training loss: 1.9508 0.4307 sec/batch\n",
      "Epoch 12/20  Iteration 2112/3560 Training loss: 1.9508 0.5019 sec/batch\n",
      "Epoch 12/20  Iteration 2113/3560 Training loss: 1.9507 0.3986 sec/batch\n",
      "Epoch 12/20  Iteration 2114/3560 Training loss: 1.9507 0.3896 sec/batch\n",
      "Epoch 12/20  Iteration 2115/3560 Training loss: 1.9506 0.4917 sec/batch\n",
      "Epoch 12/20  Iteration 2116/3560 Training loss: 1.9506 0.4200 sec/batch\n",
      "Epoch 12/20  Iteration 2117/3560 Training loss: 1.9504 0.4131 sec/batch\n",
      "Epoch 12/20  Iteration 2118/3560 Training loss: 1.9506 0.4767 sec/batch\n",
      "Epoch 12/20  Iteration 2119/3560 Training loss: 1.9507 0.4019 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 2120/3560 Training loss: 1.9507 0.3784 sec/batch\n",
      "Epoch 12/20  Iteration 2121/3560 Training loss: 1.9507 0.4245 sec/batch\n",
      "Epoch 12/20  Iteration 2122/3560 Training loss: 1.9507 0.5030 sec/batch\n",
      "Epoch 12/20  Iteration 2123/3560 Training loss: 1.9508 0.4559 sec/batch\n",
      "Epoch 12/20  Iteration 2124/3560 Training loss: 1.9507 0.3934 sec/batch\n",
      "Epoch 12/20  Iteration 2125/3560 Training loss: 1.9507 0.3615 sec/batch\n",
      "Epoch 12/20  Iteration 2126/3560 Training loss: 1.9510 0.4037 sec/batch\n",
      "Epoch 12/20  Iteration 2127/3560 Training loss: 1.9510 0.4408 sec/batch\n",
      "Epoch 12/20  Iteration 2128/3560 Training loss: 1.9510 0.4316 sec/batch\n",
      "Epoch 12/20  Iteration 2129/3560 Training loss: 1.9509 0.4213 sec/batch\n",
      "Epoch 12/20  Iteration 2130/3560 Training loss: 1.9507 0.4073 sec/batch\n",
      "Epoch 12/20  Iteration 2131/3560 Training loss: 1.9508 0.3839 sec/batch\n",
      "Epoch 12/20  Iteration 2132/3560 Training loss: 1.9509 0.4093 sec/batch\n",
      "Epoch 12/20  Iteration 2133/3560 Training loss: 1.9510 0.4189 sec/batch\n",
      "Epoch 12/20  Iteration 2134/3560 Training loss: 1.9510 0.3441 sec/batch\n",
      "Epoch 12/20  Iteration 2135/3560 Training loss: 1.9508 0.4491 sec/batch\n",
      "Epoch 12/20  Iteration 2136/3560 Training loss: 1.9509 0.4538 sec/batch\n",
      "Epoch 13/20  Iteration 2137/3560 Training loss: 2.0080 0.4014 sec/batch\n",
      "Epoch 13/20  Iteration 2138/3560 Training loss: 1.9699 0.4248 sec/batch\n",
      "Epoch 13/20  Iteration 2139/3560 Training loss: 1.9560 0.4371 sec/batch\n",
      "Epoch 13/20  Iteration 2140/3560 Training loss: 1.9485 0.3888 sec/batch\n",
      "Epoch 13/20  Iteration 2141/3560 Training loss: 1.9465 0.4505 sec/batch\n",
      "Epoch 13/20  Iteration 2142/3560 Training loss: 1.9389 0.4022 sec/batch\n",
      "Epoch 13/20  Iteration 2143/3560 Training loss: 1.9389 0.3931 sec/batch\n",
      "Epoch 13/20  Iteration 2144/3560 Training loss: 1.9389 0.3840 sec/batch\n",
      "Epoch 13/20  Iteration 2145/3560 Training loss: 1.9411 0.4598 sec/batch\n",
      "Epoch 13/20  Iteration 2146/3560 Training loss: 1.9408 0.3679 sec/batch\n",
      "Epoch 13/20  Iteration 2147/3560 Training loss: 1.9381 0.4318 sec/batch\n",
      "Epoch 13/20  Iteration 2148/3560 Training loss: 1.9362 0.3964 sec/batch\n",
      "Epoch 13/20  Iteration 2149/3560 Training loss: 1.9362 0.4059 sec/batch\n",
      "Epoch 13/20  Iteration 2150/3560 Training loss: 1.9380 0.3999 sec/batch\n",
      "Epoch 13/20  Iteration 2151/3560 Training loss: 1.9377 0.4568 sec/batch\n",
      "Epoch 13/20  Iteration 2152/3560 Training loss: 1.9365 0.3750 sec/batch\n",
      "Epoch 13/20  Iteration 2153/3560 Training loss: 1.9366 0.3761 sec/batch\n",
      "Epoch 13/20  Iteration 2154/3560 Training loss: 1.9387 0.4978 sec/batch\n",
      "Epoch 13/20  Iteration 2155/3560 Training loss: 1.9389 0.4514 sec/batch\n",
      "Epoch 13/20  Iteration 2156/3560 Training loss: 1.9388 0.4120 sec/batch\n",
      "Epoch 13/20  Iteration 2157/3560 Training loss: 1.9384 0.4259 sec/batch\n",
      "Epoch 13/20  Iteration 2158/3560 Training loss: 1.9396 0.4289 sec/batch\n",
      "Epoch 13/20  Iteration 2159/3560 Training loss: 1.9389 0.4225 sec/batch\n",
      "Epoch 13/20  Iteration 2160/3560 Training loss: 1.9382 0.4685 sec/batch\n",
      "Epoch 13/20  Iteration 2161/3560 Training loss: 1.9384 0.4280 sec/batch\n",
      "Epoch 13/20  Iteration 2162/3560 Training loss: 1.9374 0.3567 sec/batch\n",
      "Epoch 13/20  Iteration 2163/3560 Training loss: 1.9366 0.4824 sec/batch\n",
      "Epoch 13/20  Iteration 2164/3560 Training loss: 1.9373 0.3448 sec/batch\n",
      "Epoch 13/20  Iteration 2165/3560 Training loss: 1.9385 0.4196 sec/batch\n",
      "Epoch 13/20  Iteration 2166/3560 Training loss: 1.9385 0.4495 sec/batch\n",
      "Epoch 13/20  Iteration 2167/3560 Training loss: 1.9387 0.3814 sec/batch\n",
      "Epoch 13/20  Iteration 2168/3560 Training loss: 1.9380 0.4039 sec/batch\n",
      "Epoch 13/20  Iteration 2169/3560 Training loss: 1.9380 0.4889 sec/batch\n",
      "Epoch 13/20  Iteration 2170/3560 Training loss: 1.9392 0.4139 sec/batch\n",
      "Epoch 13/20  Iteration 2171/3560 Training loss: 1.9389 0.3710 sec/batch\n",
      "Epoch 13/20  Iteration 2172/3560 Training loss: 1.9390 0.4095 sec/batch\n",
      "Epoch 13/20  Iteration 2173/3560 Training loss: 1.9385 0.4215 sec/batch\n",
      "Epoch 13/20  Iteration 2174/3560 Training loss: 1.9373 0.4221 sec/batch\n",
      "Epoch 13/20  Iteration 2175/3560 Training loss: 1.9361 0.4789 sec/batch\n",
      "Epoch 13/20  Iteration 2176/3560 Training loss: 1.9353 0.4686 sec/batch\n",
      "Epoch 13/20  Iteration 2177/3560 Training loss: 1.9348 0.4175 sec/batch\n",
      "Epoch 13/20  Iteration 2178/3560 Training loss: 1.9347 0.3943 sec/batch\n",
      "Epoch 13/20  Iteration 2179/3560 Training loss: 1.9342 0.3850 sec/batch\n",
      "Epoch 13/20  Iteration 2180/3560 Training loss: 1.9335 0.4079 sec/batch\n",
      "Epoch 13/20  Iteration 2181/3560 Training loss: 1.9336 0.4018 sec/batch\n",
      "Epoch 13/20  Iteration 2182/3560 Training loss: 1.9325 0.4205 sec/batch\n",
      "Epoch 13/20  Iteration 2183/3560 Training loss: 1.9325 0.4253 sec/batch\n",
      "Epoch 13/20  Iteration 2184/3560 Training loss: 1.9321 0.4100 sec/batch\n",
      "Epoch 13/20  Iteration 2185/3560 Training loss: 1.9321 0.3879 sec/batch\n",
      "Epoch 13/20  Iteration 2186/3560 Training loss: 1.9330 0.4012 sec/batch\n",
      "Epoch 13/20  Iteration 2187/3560 Training loss: 1.9324 0.4280 sec/batch\n",
      "Epoch 13/20  Iteration 2188/3560 Training loss: 1.9330 0.4028 sec/batch\n",
      "Epoch 13/20  Iteration 2189/3560 Training loss: 1.9330 0.3568 sec/batch\n",
      "Epoch 13/20  Iteration 2190/3560 Training loss: 1.9330 0.4140 sec/batch\n",
      "Epoch 13/20  Iteration 2191/3560 Training loss: 1.9328 0.4019 sec/batch\n",
      "Epoch 13/20  Iteration 2192/3560 Training loss: 1.9330 0.4037 sec/batch\n",
      "Epoch 13/20  Iteration 2193/3560 Training loss: 1.9333 0.4512 sec/batch\n",
      "Epoch 13/20  Iteration 2194/3560 Training loss: 1.9330 0.3937 sec/batch\n",
      "Epoch 13/20  Iteration 2195/3560 Training loss: 1.9326 0.4081 sec/batch\n",
      "Epoch 13/20  Iteration 2196/3560 Training loss: 1.9332 0.4530 sec/batch\n",
      "Epoch 13/20  Iteration 2197/3560 Training loss: 1.9331 0.4012 sec/batch\n",
      "Epoch 13/20  Iteration 2198/3560 Training loss: 1.9337 0.4097 sec/batch\n",
      "Epoch 13/20  Iteration 2199/3560 Training loss: 1.9339 0.4141 sec/batch\n",
      "Epoch 13/20  Iteration 2200/3560 Training loss: 1.9341 0.4081 sec/batch\n",
      "Epoch 13/20  Iteration 2201/3560 Training loss: 1.9339 0.4517 sec/batch\n",
      "Epoch 13/20  Iteration 2202/3560 Training loss: 1.9343 0.4146 sec/batch\n",
      "Epoch 13/20  Iteration 2203/3560 Training loss: 1.9343 0.4554 sec/batch\n",
      "Epoch 13/20  Iteration 2204/3560 Training loss: 1.9339 0.4319 sec/batch\n",
      "Epoch 13/20  Iteration 2205/3560 Training loss: 1.9339 0.4087 sec/batch\n",
      "Epoch 13/20  Iteration 2206/3560 Training loss: 1.9339 0.3744 sec/batch\n",
      "Epoch 13/20  Iteration 2207/3560 Training loss: 1.9343 0.4066 sec/batch\n",
      "Epoch 13/20  Iteration 2208/3560 Training loss: 1.9344 0.5067 sec/batch\n",
      "Epoch 13/20  Iteration 2209/3560 Training loss: 1.9346 0.3402 sec/batch\n",
      "Epoch 13/20  Iteration 2210/3560 Training loss: 1.9343 0.4034 sec/batch\n",
      "Epoch 13/20  Iteration 2211/3560 Training loss: 1.9342 0.4375 sec/batch\n",
      "Epoch 13/20  Iteration 2212/3560 Training loss: 1.9345 0.3947 sec/batch\n",
      "Epoch 13/20  Iteration 2213/3560 Training loss: 1.9344 0.4568 sec/batch\n",
      "Epoch 13/20  Iteration 2214/3560 Training loss: 1.9345 0.4523 sec/batch\n",
      "Epoch 13/20  Iteration 2215/3560 Training loss: 1.9341 0.4413 sec/batch\n",
      "Epoch 13/20  Iteration 2216/3560 Training loss: 1.9340 0.4311 sec/batch\n",
      "Epoch 13/20  Iteration 2217/3560 Training loss: 1.9335 0.4291 sec/batch\n",
      "Epoch 13/20  Iteration 2218/3560 Training loss: 1.9336 0.3859 sec/batch\n",
      "Epoch 13/20  Iteration 2219/3560 Training loss: 1.9331 0.4528 sec/batch\n",
      "Epoch 13/20  Iteration 2220/3560 Training loss: 1.9330 0.4063 sec/batch\n",
      "Epoch 13/20  Iteration 2221/3560 Training loss: 1.9324 0.3900 sec/batch\n",
      "Epoch 13/20  Iteration 2222/3560 Training loss: 1.9321 0.4234 sec/batch\n",
      "Epoch 13/20  Iteration 2223/3560 Training loss: 1.9319 0.4686 sec/batch\n",
      "Epoch 13/20  Iteration 2224/3560 Training loss: 1.9316 0.4089 sec/batch\n",
      "Epoch 13/20  Iteration 2225/3560 Training loss: 1.9310 0.3877 sec/batch\n",
      "Epoch 13/20  Iteration 2226/3560 Training loss: 1.9311 0.4650 sec/batch\n",
      "Epoch 13/20  Iteration 2227/3560 Training loss: 1.9309 0.3738 sec/batch\n",
      "Epoch 13/20  Iteration 2228/3560 Training loss: 1.9308 0.4204 sec/batch\n",
      "Epoch 13/20  Iteration 2229/3560 Training loss: 1.9303 0.4093 sec/batch\n",
      "Epoch 13/20  Iteration 2230/3560 Training loss: 1.9299 0.4778 sec/batch\n",
      "Epoch 13/20  Iteration 2231/3560 Training loss: 1.9296 0.4210 sec/batch\n",
      "Epoch 13/20  Iteration 2232/3560 Training loss: 1.9296 0.4447 sec/batch\n",
      "Epoch 13/20  Iteration 2233/3560 Training loss: 1.9296 0.4486 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2234/3560 Training loss: 1.9291 0.4041 sec/batch\n",
      "Epoch 13/20  Iteration 2235/3560 Training loss: 1.9289 0.3737 sec/batch\n",
      "Epoch 13/20  Iteration 2236/3560 Training loss: 1.9285 0.4070 sec/batch\n",
      "Epoch 13/20  Iteration 2237/3560 Training loss: 1.9285 0.3806 sec/batch\n",
      "Epoch 13/20  Iteration 2238/3560 Training loss: 1.9284 0.4350 sec/batch\n",
      "Epoch 13/20  Iteration 2239/3560 Training loss: 1.9282 0.4302 sec/batch\n",
      "Epoch 13/20  Iteration 2240/3560 Training loss: 1.9281 0.4196 sec/batch\n",
      "Epoch 13/20  Iteration 2241/3560 Training loss: 1.9280 0.5075 sec/batch\n",
      "Epoch 13/20  Iteration 2242/3560 Training loss: 1.9278 0.4122 sec/batch\n",
      "Epoch 13/20  Iteration 2243/3560 Training loss: 1.9278 0.4384 sec/batch\n",
      "Epoch 13/20  Iteration 2244/3560 Training loss: 1.9278 0.3846 sec/batch\n",
      "Epoch 13/20  Iteration 2245/3560 Training loss: 1.9279 0.3907 sec/batch\n",
      "Epoch 13/20  Iteration 2246/3560 Training loss: 1.9278 0.3922 sec/batch\n",
      "Epoch 13/20  Iteration 2247/3560 Training loss: 1.9278 0.4410 sec/batch\n",
      "Epoch 13/20  Iteration 2248/3560 Training loss: 1.9277 0.4814 sec/batch\n",
      "Epoch 13/20  Iteration 2249/3560 Training loss: 1.9276 0.3966 sec/batch\n",
      "Epoch 13/20  Iteration 2250/3560 Training loss: 1.9274 0.4285 sec/batch\n",
      "Epoch 13/20  Iteration 2251/3560 Training loss: 1.9272 0.3950 sec/batch\n",
      "Epoch 13/20  Iteration 2252/3560 Training loss: 1.9270 0.4596 sec/batch\n",
      "Epoch 13/20  Iteration 2253/3560 Training loss: 1.9269 0.4598 sec/batch\n",
      "Epoch 13/20  Iteration 2254/3560 Training loss: 1.9269 0.4724 sec/batch\n",
      "Epoch 13/20  Iteration 2255/3560 Training loss: 1.9268 0.3463 sec/batch\n",
      "Epoch 13/20  Iteration 2256/3560 Training loss: 1.9267 0.4443 sec/batch\n",
      "Epoch 13/20  Iteration 2257/3560 Training loss: 1.9267 0.4084 sec/batch\n",
      "Epoch 13/20  Iteration 2258/3560 Training loss: 1.9264 0.4277 sec/batch\n",
      "Epoch 13/20  Iteration 2259/3560 Training loss: 1.9262 0.4626 sec/batch\n",
      "Epoch 13/20  Iteration 2260/3560 Training loss: 1.9263 0.3879 sec/batch\n",
      "Epoch 13/20  Iteration 2261/3560 Training loss: 1.9263 0.4702 sec/batch\n",
      "Epoch 13/20  Iteration 2262/3560 Training loss: 1.9260 0.4163 sec/batch\n",
      "Epoch 13/20  Iteration 2263/3560 Training loss: 1.9260 0.4133 sec/batch\n",
      "Epoch 13/20  Iteration 2264/3560 Training loss: 1.9260 0.4491 sec/batch\n",
      "Epoch 13/20  Iteration 2265/3560 Training loss: 1.9261 0.4761 sec/batch\n",
      "Epoch 13/20  Iteration 2266/3560 Training loss: 1.9260 0.5123 sec/batch\n",
      "Epoch 13/20  Iteration 2267/3560 Training loss: 1.9257 0.5612 sec/batch\n",
      "Epoch 13/20  Iteration 2268/3560 Training loss: 1.9255 0.5355 sec/batch\n",
      "Epoch 13/20  Iteration 2269/3560 Training loss: 1.9255 0.5159 sec/batch\n",
      "Epoch 13/20  Iteration 2270/3560 Training loss: 1.9256 0.4894 sec/batch\n",
      "Epoch 13/20  Iteration 2271/3560 Training loss: 1.9254 0.4258 sec/batch\n",
      "Epoch 13/20  Iteration 2272/3560 Training loss: 1.9254 0.5643 sec/batch\n",
      "Epoch 13/20  Iteration 2273/3560 Training loss: 1.9254 0.4594 sec/batch\n",
      "Epoch 13/20  Iteration 2274/3560 Training loss: 1.9254 0.4143 sec/batch\n",
      "Epoch 13/20  Iteration 2275/3560 Training loss: 1.9257 0.4181 sec/batch\n",
      "Epoch 13/20  Iteration 2276/3560 Training loss: 1.9255 0.4218 sec/batch\n",
      "Epoch 13/20  Iteration 2277/3560 Training loss: 1.9257 0.4469 sec/batch\n",
      "Epoch 13/20  Iteration 2278/3560 Training loss: 1.9257 0.3894 sec/batch\n",
      "Epoch 13/20  Iteration 2279/3560 Training loss: 1.9257 0.4333 sec/batch\n",
      "Epoch 13/20  Iteration 2280/3560 Training loss: 1.9257 0.4253 sec/batch\n",
      "Epoch 13/20  Iteration 2281/3560 Training loss: 1.9255 0.4198 sec/batch\n",
      "Epoch 13/20  Iteration 2282/3560 Training loss: 1.9255 0.3896 sec/batch\n",
      "Epoch 13/20  Iteration 2283/3560 Training loss: 1.9255 0.4358 sec/batch\n",
      "Epoch 13/20  Iteration 2284/3560 Training loss: 1.9257 0.3769 sec/batch\n",
      "Epoch 13/20  Iteration 2285/3560 Training loss: 1.9256 0.4021 sec/batch\n",
      "Epoch 13/20  Iteration 2286/3560 Training loss: 1.9255 0.4528 sec/batch\n",
      "Epoch 13/20  Iteration 2287/3560 Training loss: 1.9252 0.3932 sec/batch\n",
      "Epoch 13/20  Iteration 2288/3560 Training loss: 1.9253 0.4805 sec/batch\n",
      "Epoch 13/20  Iteration 2289/3560 Training loss: 1.9254 0.4368 sec/batch\n",
      "Epoch 13/20  Iteration 2290/3560 Training loss: 1.9255 0.4115 sec/batch\n",
      "Epoch 13/20  Iteration 2291/3560 Training loss: 1.9253 0.4584 sec/batch\n",
      "Epoch 13/20  Iteration 2292/3560 Training loss: 1.9253 0.4218 sec/batch\n",
      "Epoch 13/20  Iteration 2293/3560 Training loss: 1.9253 0.4302 sec/batch\n",
      "Epoch 13/20  Iteration 2294/3560 Training loss: 1.9253 0.3775 sec/batch\n",
      "Epoch 13/20  Iteration 2295/3560 Training loss: 1.9251 0.4940 sec/batch\n",
      "Epoch 13/20  Iteration 2296/3560 Training loss: 1.9253 0.4435 sec/batch\n",
      "Epoch 13/20  Iteration 2297/3560 Training loss: 1.9254 0.4247 sec/batch\n",
      "Epoch 13/20  Iteration 2298/3560 Training loss: 1.9254 0.4267 sec/batch\n",
      "Epoch 13/20  Iteration 2299/3560 Training loss: 1.9255 0.4293 sec/batch\n",
      "Epoch 13/20  Iteration 2300/3560 Training loss: 1.9255 0.3838 sec/batch\n",
      "Epoch 13/20  Iteration 2301/3560 Training loss: 1.9255 0.4412 sec/batch\n",
      "Epoch 13/20  Iteration 2302/3560 Training loss: 1.9254 0.3763 sec/batch\n",
      "Epoch 13/20  Iteration 2303/3560 Training loss: 1.9254 0.4257 sec/batch\n",
      "Epoch 13/20  Iteration 2304/3560 Training loss: 1.9258 0.4069 sec/batch\n",
      "Epoch 13/20  Iteration 2305/3560 Training loss: 1.9257 0.4308 sec/batch\n",
      "Epoch 13/20  Iteration 2306/3560 Training loss: 1.9257 0.4458 sec/batch\n",
      "Epoch 13/20  Iteration 2307/3560 Training loss: 1.9255 0.4298 sec/batch\n",
      "Epoch 13/20  Iteration 2308/3560 Training loss: 1.9254 0.4405 sec/batch\n",
      "Epoch 13/20  Iteration 2309/3560 Training loss: 1.9255 0.3922 sec/batch\n",
      "Epoch 13/20  Iteration 2310/3560 Training loss: 1.9254 0.3797 sec/batch\n",
      "Epoch 13/20  Iteration 2311/3560 Training loss: 1.9255 0.4292 sec/batch\n",
      "Epoch 13/20  Iteration 2312/3560 Training loss: 1.9254 0.3906 sec/batch\n",
      "Epoch 13/20  Iteration 2313/3560 Training loss: 1.9254 0.3796 sec/batch\n",
      "Epoch 13/20  Iteration 2314/3560 Training loss: 1.9254 0.4188 sec/batch\n",
      "Epoch 14/20  Iteration 2315/3560 Training loss: 1.9858 0.5065 sec/batch\n",
      "Epoch 14/20  Iteration 2316/3560 Training loss: 1.9482 0.4525 sec/batch\n",
      "Epoch 14/20  Iteration 2317/3560 Training loss: 1.9341 0.4377 sec/batch\n",
      "Epoch 14/20  Iteration 2318/3560 Training loss: 1.9283 0.3974 sec/batch\n",
      "Epoch 14/20  Iteration 2319/3560 Training loss: 1.9247 0.4530 sec/batch\n",
      "Epoch 14/20  Iteration 2320/3560 Training loss: 1.9168 0.3904 sec/batch\n",
      "Epoch 14/20  Iteration 2321/3560 Training loss: 1.9156 0.4313 sec/batch\n",
      "Epoch 14/20  Iteration 2322/3560 Training loss: 1.9159 0.4151 sec/batch\n",
      "Epoch 14/20  Iteration 2323/3560 Training loss: 1.9189 0.3921 sec/batch\n",
      "Epoch 14/20  Iteration 2324/3560 Training loss: 1.9192 0.4154 sec/batch\n",
      "Epoch 14/20  Iteration 2325/3560 Training loss: 1.9159 0.4144 sec/batch\n",
      "Epoch 14/20  Iteration 2326/3560 Training loss: 1.9150 0.4089 sec/batch\n",
      "Epoch 14/20  Iteration 2327/3560 Training loss: 1.9154 0.4275 sec/batch\n",
      "Epoch 14/20  Iteration 2328/3560 Training loss: 1.9167 0.4328 sec/batch\n",
      "Epoch 14/20  Iteration 2329/3560 Training loss: 1.9163 0.4711 sec/batch\n",
      "Epoch 14/20  Iteration 2330/3560 Training loss: 1.9150 0.4186 sec/batch\n",
      "Epoch 14/20  Iteration 2331/3560 Training loss: 1.9146 0.3798 sec/batch\n",
      "Epoch 14/20  Iteration 2332/3560 Training loss: 1.9172 0.3920 sec/batch\n",
      "Epoch 14/20  Iteration 2333/3560 Training loss: 1.9171 0.4011 sec/batch\n",
      "Epoch 14/20  Iteration 2334/3560 Training loss: 1.9171 0.4306 sec/batch\n",
      "Epoch 14/20  Iteration 2335/3560 Training loss: 1.9167 0.4193 sec/batch\n",
      "Epoch 14/20  Iteration 2336/3560 Training loss: 1.9175 0.4298 sec/batch\n",
      "Epoch 14/20  Iteration 2337/3560 Training loss: 1.9168 0.4321 sec/batch\n",
      "Epoch 14/20  Iteration 2338/3560 Training loss: 1.9164 0.3916 sec/batch\n",
      "Epoch 14/20  Iteration 2339/3560 Training loss: 1.9163 0.4219 sec/batch\n",
      "Epoch 14/20  Iteration 2340/3560 Training loss: 1.9153 0.4091 sec/batch\n",
      "Epoch 14/20  Iteration 2341/3560 Training loss: 1.9144 0.4405 sec/batch\n",
      "Epoch 14/20  Iteration 2342/3560 Training loss: 1.9150 0.4977 sec/batch\n",
      "Epoch 14/20  Iteration 2343/3560 Training loss: 1.9160 0.3470 sec/batch\n",
      "Epoch 14/20  Iteration 2344/3560 Training loss: 1.9163 0.4547 sec/batch\n",
      "Epoch 14/20  Iteration 2345/3560 Training loss: 1.9161 0.3427 sec/batch\n",
      "Epoch 14/20  Iteration 2346/3560 Training loss: 1.9156 0.4802 sec/batch\n",
      "Epoch 14/20  Iteration 2347/3560 Training loss: 1.9157 0.4384 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2348/3560 Training loss: 1.9167 0.4323 sec/batch\n",
      "Epoch 14/20  Iteration 2349/3560 Training loss: 1.9163 0.4087 sec/batch\n",
      "Epoch 14/20  Iteration 2350/3560 Training loss: 1.9162 0.4142 sec/batch\n",
      "Epoch 14/20  Iteration 2351/3560 Training loss: 1.9158 0.3888 sec/batch\n",
      "Epoch 14/20  Iteration 2352/3560 Training loss: 1.9148 0.4023 sec/batch\n",
      "Epoch 14/20  Iteration 2353/3560 Training loss: 1.9135 0.4654 sec/batch\n",
      "Epoch 14/20  Iteration 2354/3560 Training loss: 1.9128 0.4186 sec/batch\n",
      "Epoch 14/20  Iteration 2355/3560 Training loss: 1.9125 0.4087 sec/batch\n",
      "Epoch 14/20  Iteration 2356/3560 Training loss: 1.9129 0.4243 sec/batch\n",
      "Epoch 14/20  Iteration 2357/3560 Training loss: 1.9126 0.4259 sec/batch\n",
      "Epoch 14/20  Iteration 2358/3560 Training loss: 1.9117 0.4102 sec/batch\n",
      "Epoch 14/20  Iteration 2359/3560 Training loss: 1.9118 0.3674 sec/batch\n",
      "Epoch 14/20  Iteration 2360/3560 Training loss: 1.9105 0.4280 sec/batch\n",
      "Epoch 14/20  Iteration 2361/3560 Training loss: 1.9105 0.3883 sec/batch\n",
      "Epoch 14/20  Iteration 2362/3560 Training loss: 1.9101 0.4555 sec/batch\n",
      "Epoch 14/20  Iteration 2363/3560 Training loss: 1.9101 0.4368 sec/batch\n",
      "Epoch 14/20  Iteration 2364/3560 Training loss: 1.9109 0.4619 sec/batch\n",
      "Epoch 14/20  Iteration 2365/3560 Training loss: 1.9102 0.4529 sec/batch\n",
      "Epoch 14/20  Iteration 2366/3560 Training loss: 1.9108 0.3914 sec/batch\n",
      "Epoch 14/20  Iteration 2367/3560 Training loss: 1.9105 0.4546 sec/batch\n",
      "Epoch 14/20  Iteration 2368/3560 Training loss: 1.9105 0.4325 sec/batch\n",
      "Epoch 14/20  Iteration 2369/3560 Training loss: 1.9101 0.3620 sec/batch\n",
      "Epoch 14/20  Iteration 2370/3560 Training loss: 1.9102 0.4553 sec/batch\n",
      "Epoch 14/20  Iteration 2371/3560 Training loss: 1.9105 0.4359 sec/batch\n",
      "Epoch 14/20  Iteration 2372/3560 Training loss: 1.9103 0.5750 sec/batch\n",
      "Epoch 14/20  Iteration 2373/3560 Training loss: 1.9099 0.4514 sec/batch\n",
      "Epoch 14/20  Iteration 2374/3560 Training loss: 1.9104 0.4431 sec/batch\n",
      "Epoch 14/20  Iteration 2375/3560 Training loss: 1.9103 0.4629 sec/batch\n",
      "Epoch 14/20  Iteration 2376/3560 Training loss: 1.9109 0.5592 sec/batch\n",
      "Epoch 14/20  Iteration 2377/3560 Training loss: 1.9112 0.4807 sec/batch\n",
      "Epoch 14/20  Iteration 2378/3560 Training loss: 1.9114 0.4085 sec/batch\n",
      "Epoch 14/20  Iteration 2379/3560 Training loss: 1.9111 0.4929 sec/batch\n",
      "Epoch 14/20  Iteration 2380/3560 Training loss: 1.9116 0.4451 sec/batch\n",
      "Epoch 14/20  Iteration 2381/3560 Training loss: 1.9116 0.5097 sec/batch\n",
      "Epoch 14/20  Iteration 2382/3560 Training loss: 1.9112 0.4568 sec/batch\n",
      "Epoch 14/20  Iteration 2383/3560 Training loss: 1.9112 0.5172 sec/batch\n",
      "Epoch 14/20  Iteration 2384/3560 Training loss: 1.9112 0.4915 sec/batch\n",
      "Epoch 14/20  Iteration 2385/3560 Training loss: 1.9116 0.5443 sec/batch\n",
      "Epoch 14/20  Iteration 2386/3560 Training loss: 1.9118 0.5488 sec/batch\n",
      "Epoch 14/20  Iteration 2387/3560 Training loss: 1.9121 0.4987 sec/batch\n",
      "Epoch 14/20  Iteration 2388/3560 Training loss: 1.9119 0.4400 sec/batch\n",
      "Epoch 14/20  Iteration 2389/3560 Training loss: 1.9119 0.4945 sec/batch\n",
      "Epoch 14/20  Iteration 2390/3560 Training loss: 1.9121 0.4006 sec/batch\n",
      "Epoch 14/20  Iteration 2391/3560 Training loss: 1.9120 0.4678 sec/batch\n",
      "Epoch 14/20  Iteration 2392/3560 Training loss: 1.9123 0.5192 sec/batch\n",
      "Epoch 14/20  Iteration 2393/3560 Training loss: 1.9118 0.3880 sec/batch\n",
      "Epoch 14/20  Iteration 2394/3560 Training loss: 1.9117 0.4612 sec/batch\n",
      "Epoch 14/20  Iteration 2395/3560 Training loss: 1.9111 0.4727 sec/batch\n",
      "Epoch 14/20  Iteration 2396/3560 Training loss: 1.9112 0.4989 sec/batch\n",
      "Epoch 14/20  Iteration 2397/3560 Training loss: 1.9107 0.4661 sec/batch\n",
      "Epoch 14/20  Iteration 2398/3560 Training loss: 1.9105 0.4828 sec/batch\n",
      "Epoch 14/20  Iteration 2399/3560 Training loss: 1.9099 0.4806 sec/batch\n",
      "Epoch 14/20  Iteration 2400/3560 Training loss: 1.9096 0.3672 sec/batch\n",
      "Epoch 14/20  Iteration 2401/3560 Training loss: 1.9094 0.4107 sec/batch\n",
      "Epoch 14/20  Iteration 2402/3560 Training loss: 1.9091 0.4724 sec/batch\n",
      "Epoch 14/20  Iteration 2403/3560 Training loss: 1.9086 0.3939 sec/batch\n",
      "Epoch 14/20  Iteration 2404/3560 Training loss: 1.9085 0.4044 sec/batch\n",
      "Epoch 14/20  Iteration 2405/3560 Training loss: 1.9082 0.4257 sec/batch\n",
      "Epoch 14/20  Iteration 2406/3560 Training loss: 1.9082 0.3894 sec/batch\n",
      "Epoch 14/20  Iteration 2407/3560 Training loss: 1.9076 0.3975 sec/batch\n",
      "Epoch 14/20  Iteration 2408/3560 Training loss: 1.9073 0.5077 sec/batch\n",
      "Epoch 14/20  Iteration 2409/3560 Training loss: 1.9070 0.3901 sec/batch\n",
      "Epoch 14/20  Iteration 2410/3560 Training loss: 1.9070 0.4884 sec/batch\n",
      "Epoch 14/20  Iteration 2411/3560 Training loss: 1.9069 0.5153 sec/batch\n",
      "Epoch 14/20  Iteration 2412/3560 Training loss: 1.9065 0.4401 sec/batch\n",
      "Epoch 14/20  Iteration 2413/3560 Training loss: 1.9062 0.3881 sec/batch\n",
      "Epoch 14/20  Iteration 2414/3560 Training loss: 1.9059 0.4241 sec/batch\n",
      "Epoch 14/20  Iteration 2415/3560 Training loss: 1.9059 0.4383 sec/batch\n",
      "Epoch 14/20  Iteration 2416/3560 Training loss: 1.9058 0.4117 sec/batch\n",
      "Epoch 14/20  Iteration 2417/3560 Training loss: 1.9056 0.4528 sec/batch\n",
      "Epoch 14/20  Iteration 2418/3560 Training loss: 1.9053 0.3605 sec/batch\n",
      "Epoch 14/20  Iteration 2419/3560 Training loss: 1.9052 0.4222 sec/batch\n",
      "Epoch 14/20  Iteration 2420/3560 Training loss: 1.9051 0.3803 sec/batch\n",
      "Epoch 14/20  Iteration 2421/3560 Training loss: 1.9051 0.4633 sec/batch\n",
      "Epoch 14/20  Iteration 2422/3560 Training loss: 1.9052 0.4486 sec/batch\n",
      "Epoch 14/20  Iteration 2423/3560 Training loss: 1.9052 0.3724 sec/batch\n",
      "Epoch 14/20  Iteration 2424/3560 Training loss: 1.9051 0.4819 sec/batch\n",
      "Epoch 14/20  Iteration 2425/3560 Training loss: 1.9051 0.4523 sec/batch\n",
      "Epoch 14/20  Iteration 2426/3560 Training loss: 1.9050 0.4233 sec/batch\n",
      "Epoch 14/20  Iteration 2427/3560 Training loss: 1.9049 0.4043 sec/batch\n",
      "Epoch 14/20  Iteration 2428/3560 Training loss: 1.9048 0.4830 sec/batch\n",
      "Epoch 14/20  Iteration 2429/3560 Training loss: 1.9045 0.3703 sec/batch\n",
      "Epoch 14/20  Iteration 2430/3560 Training loss: 1.9042 0.5001 sec/batch\n",
      "Epoch 14/20  Iteration 2431/3560 Training loss: 1.9041 0.3947 sec/batch\n",
      "Epoch 14/20  Iteration 2432/3560 Training loss: 1.9041 0.4695 sec/batch\n",
      "Epoch 14/20  Iteration 2433/3560 Training loss: 1.9041 0.5497 sec/batch\n",
      "Epoch 14/20  Iteration 2434/3560 Training loss: 1.9040 0.3675 sec/batch\n",
      "Epoch 14/20  Iteration 2435/3560 Training loss: 1.9040 0.5311 sec/batch\n",
      "Epoch 14/20  Iteration 2436/3560 Training loss: 1.9037 0.4254 sec/batch\n",
      "Epoch 14/20  Iteration 2437/3560 Training loss: 1.9034 0.4000 sec/batch\n",
      "Epoch 14/20  Iteration 2438/3560 Training loss: 1.9036 0.4350 sec/batch\n",
      "Epoch 14/20  Iteration 2439/3560 Training loss: 1.9035 0.4451 sec/batch\n",
      "Epoch 14/20  Iteration 2440/3560 Training loss: 1.9032 0.4351 sec/batch\n",
      "Epoch 14/20  Iteration 2441/3560 Training loss: 1.9033 0.4286 sec/batch\n",
      "Epoch 14/20  Iteration 2442/3560 Training loss: 1.9034 0.4384 sec/batch\n",
      "Epoch 14/20  Iteration 2443/3560 Training loss: 1.9033 0.4619 sec/batch\n",
      "Epoch 14/20  Iteration 2444/3560 Training loss: 1.9033 0.5519 sec/batch\n",
      "Epoch 14/20  Iteration 2445/3560 Training loss: 1.9030 0.6225 sec/batch\n",
      "Epoch 14/20  Iteration 2446/3560 Training loss: 1.9027 0.4039 sec/batch\n",
      "Epoch 14/20  Iteration 2447/3560 Training loss: 1.9028 0.4337 sec/batch\n",
      "Epoch 14/20  Iteration 2448/3560 Training loss: 1.9028 0.3858 sec/batch\n",
      "Epoch 14/20  Iteration 2449/3560 Training loss: 1.9027 0.4138 sec/batch\n",
      "Epoch 14/20  Iteration 2450/3560 Training loss: 1.9028 0.4496 sec/batch\n",
      "Epoch 14/20  Iteration 2451/3560 Training loss: 1.9028 0.4203 sec/batch\n",
      "Epoch 14/20  Iteration 2452/3560 Training loss: 1.9029 0.5002 sec/batch\n",
      "Epoch 14/20  Iteration 2453/3560 Training loss: 1.9031 0.4132 sec/batch\n",
      "Epoch 14/20  Iteration 2454/3560 Training loss: 1.9029 0.4292 sec/batch\n",
      "Epoch 14/20  Iteration 2455/3560 Training loss: 1.9031 0.4093 sec/batch\n",
      "Epoch 14/20  Iteration 2456/3560 Training loss: 1.9031 0.4158 sec/batch\n",
      "Epoch 14/20  Iteration 2457/3560 Training loss: 1.9031 0.4094 sec/batch\n",
      "Epoch 14/20  Iteration 2458/3560 Training loss: 1.9031 0.4472 sec/batch\n",
      "Epoch 14/20  Iteration 2459/3560 Training loss: 1.9029 0.4246 sec/batch\n",
      "Epoch 14/20  Iteration 2460/3560 Training loss: 1.9029 0.4555 sec/batch\n",
      "Epoch 14/20  Iteration 2461/3560 Training loss: 1.9030 0.4046 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2462/3560 Training loss: 1.9031 0.4215 sec/batch\n",
      "Epoch 14/20  Iteration 2463/3560 Training loss: 1.9031 0.4285 sec/batch\n",
      "Epoch 14/20  Iteration 2464/3560 Training loss: 1.9029 0.3663 sec/batch\n",
      "Epoch 14/20  Iteration 2465/3560 Training loss: 1.9027 0.4254 sec/batch\n",
      "Epoch 14/20  Iteration 2466/3560 Training loss: 1.9029 0.3931 sec/batch\n",
      "Epoch 14/20  Iteration 2467/3560 Training loss: 1.9029 0.3549 sec/batch\n",
      "Epoch 14/20  Iteration 2468/3560 Training loss: 1.9029 0.4706 sec/batch\n",
      "Epoch 14/20  Iteration 2469/3560 Training loss: 1.9028 0.4035 sec/batch\n",
      "Epoch 14/20  Iteration 2470/3560 Training loss: 1.9028 0.4793 sec/batch\n",
      "Epoch 14/20  Iteration 2471/3560 Training loss: 1.9028 0.4651 sec/batch\n",
      "Epoch 14/20  Iteration 2472/3560 Training loss: 1.9028 0.4579 sec/batch\n",
      "Epoch 14/20  Iteration 2473/3560 Training loss: 1.9025 0.3603 sec/batch\n",
      "Epoch 14/20  Iteration 2474/3560 Training loss: 1.9027 0.4196 sec/batch\n",
      "Epoch 14/20  Iteration 2475/3560 Training loss: 1.9028 0.3562 sec/batch\n",
      "Epoch 14/20  Iteration 2476/3560 Training loss: 1.9028 0.4358 sec/batch\n",
      "Epoch 14/20  Iteration 2477/3560 Training loss: 1.9028 0.3414 sec/batch\n",
      "Epoch 14/20  Iteration 2478/3560 Training loss: 1.9028 0.4046 sec/batch\n",
      "Epoch 14/20  Iteration 2479/3560 Training loss: 1.9028 0.4868 sec/batch\n",
      "Epoch 14/20  Iteration 2480/3560 Training loss: 1.9028 0.4888 sec/batch\n",
      "Epoch 14/20  Iteration 2481/3560 Training loss: 1.9028 0.4275 sec/batch\n",
      "Epoch 14/20  Iteration 2482/3560 Training loss: 1.9031 0.4596 sec/batch\n",
      "Epoch 14/20  Iteration 2483/3560 Training loss: 1.9030 0.3746 sec/batch\n",
      "Epoch 14/20  Iteration 2484/3560 Training loss: 1.9030 0.4090 sec/batch\n",
      "Epoch 14/20  Iteration 2485/3560 Training loss: 1.9029 0.3862 sec/batch\n",
      "Epoch 14/20  Iteration 2486/3560 Training loss: 1.9027 0.3747 sec/batch\n",
      "Epoch 14/20  Iteration 2487/3560 Training loss: 1.9028 0.4198 sec/batch\n",
      "Epoch 14/20  Iteration 2488/3560 Training loss: 1.9028 0.3642 sec/batch\n",
      "Epoch 14/20  Iteration 2489/3560 Training loss: 1.9029 0.4412 sec/batch\n",
      "Epoch 14/20  Iteration 2490/3560 Training loss: 1.9028 0.3894 sec/batch\n",
      "Epoch 14/20  Iteration 2491/3560 Training loss: 1.9027 0.4685 sec/batch\n",
      "Epoch 14/20  Iteration 2492/3560 Training loss: 1.9028 0.4137 sec/batch\n",
      "Epoch 15/20  Iteration 2493/3560 Training loss: 1.9613 0.4126 sec/batch\n",
      "Epoch 15/20  Iteration 2494/3560 Training loss: 1.9282 0.4218 sec/batch\n",
      "Epoch 15/20  Iteration 2495/3560 Training loss: 1.9131 0.3877 sec/batch\n",
      "Epoch 15/20  Iteration 2496/3560 Training loss: 1.9080 0.3976 sec/batch\n",
      "Epoch 15/20  Iteration 2497/3560 Training loss: 1.9052 0.4370 sec/batch\n",
      "Epoch 15/20  Iteration 2498/3560 Training loss: 1.8976 0.4100 sec/batch\n",
      "Epoch 15/20  Iteration 2499/3560 Training loss: 1.8966 0.4706 sec/batch\n",
      "Epoch 15/20  Iteration 2500/3560 Training loss: 1.8952 0.4914 sec/batch\n",
      "Epoch 15/20  Iteration 2501/3560 Training loss: 1.8978 0.4034 sec/batch\n",
      "Epoch 15/20  Iteration 2502/3560 Training loss: 1.8968 0.4211 sec/batch\n",
      "Epoch 15/20  Iteration 2503/3560 Training loss: 1.8931 0.3848 sec/batch\n",
      "Epoch 15/20  Iteration 2504/3560 Training loss: 1.8911 0.5084 sec/batch\n",
      "Epoch 15/20  Iteration 2505/3560 Training loss: 1.8913 0.3532 sec/batch\n",
      "Epoch 15/20  Iteration 2506/3560 Training loss: 1.8942 0.3818 sec/batch\n",
      "Epoch 15/20  Iteration 2507/3560 Training loss: 1.8931 0.4543 sec/batch\n",
      "Epoch 15/20  Iteration 2508/3560 Training loss: 1.8915 0.4097 sec/batch\n",
      "Epoch 15/20  Iteration 2509/3560 Training loss: 1.8912 0.3971 sec/batch\n",
      "Epoch 15/20  Iteration 2510/3560 Training loss: 1.8936 0.4369 sec/batch\n",
      "Epoch 15/20  Iteration 2511/3560 Training loss: 1.8940 0.5193 sec/batch\n",
      "Epoch 15/20  Iteration 2512/3560 Training loss: 1.8939 0.3673 sec/batch\n",
      "Epoch 15/20  Iteration 2513/3560 Training loss: 1.8931 0.4702 sec/batch\n",
      "Epoch 15/20  Iteration 2514/3560 Training loss: 1.8937 0.4014 sec/batch\n",
      "Epoch 15/20  Iteration 2515/3560 Training loss: 1.8934 0.3833 sec/batch\n",
      "Epoch 15/20  Iteration 2516/3560 Training loss: 1.8931 0.4137 sec/batch\n",
      "Epoch 15/20  Iteration 2517/3560 Training loss: 1.8933 0.4322 sec/batch\n",
      "Epoch 15/20  Iteration 2518/3560 Training loss: 1.8920 0.5095 sec/batch\n",
      "Epoch 15/20  Iteration 2519/3560 Training loss: 1.8914 0.4441 sec/batch\n",
      "Epoch 15/20  Iteration 2520/3560 Training loss: 1.8919 0.3430 sec/batch\n",
      "Epoch 15/20  Iteration 2521/3560 Training loss: 1.8927 0.4524 sec/batch\n",
      "Epoch 15/20  Iteration 2522/3560 Training loss: 1.8929 0.3332 sec/batch\n",
      "Epoch 15/20  Iteration 2523/3560 Training loss: 1.8928 0.4122 sec/batch\n",
      "Epoch 15/20  Iteration 2524/3560 Training loss: 1.8921 0.4337 sec/batch\n",
      "Epoch 15/20  Iteration 2525/3560 Training loss: 1.8923 0.4028 sec/batch\n",
      "Epoch 15/20  Iteration 2526/3560 Training loss: 1.8930 0.5199 sec/batch\n",
      "Epoch 15/20  Iteration 2527/3560 Training loss: 1.8929 0.3442 sec/batch\n",
      "Epoch 15/20  Iteration 2528/3560 Training loss: 1.8926 0.4238 sec/batch\n",
      "Epoch 15/20  Iteration 2529/3560 Training loss: 1.8920 0.4877 sec/batch\n",
      "Epoch 15/20  Iteration 2530/3560 Training loss: 1.8910 0.4269 sec/batch\n",
      "Epoch 15/20  Iteration 2531/3560 Training loss: 1.8899 0.4040 sec/batch\n",
      "Epoch 15/20  Iteration 2532/3560 Training loss: 1.8894 0.3894 sec/batch\n",
      "Epoch 15/20  Iteration 2533/3560 Training loss: 1.8890 0.4419 sec/batch\n",
      "Epoch 15/20  Iteration 2534/3560 Training loss: 1.8893 0.3846 sec/batch\n",
      "Epoch 15/20  Iteration 2535/3560 Training loss: 1.8889 0.4709 sec/batch\n",
      "Epoch 15/20  Iteration 2536/3560 Training loss: 1.8881 0.3880 sec/batch\n",
      "Epoch 15/20  Iteration 2537/3560 Training loss: 1.8883 0.4459 sec/batch\n",
      "Epoch 15/20  Iteration 2538/3560 Training loss: 1.8871 0.4072 sec/batch\n",
      "Epoch 15/20  Iteration 2539/3560 Training loss: 1.8870 0.4053 sec/batch\n",
      "Epoch 15/20  Iteration 2540/3560 Training loss: 1.8866 0.4491 sec/batch\n",
      "Epoch 15/20  Iteration 2541/3560 Training loss: 1.8866 0.3821 sec/batch\n",
      "Epoch 15/20  Iteration 2542/3560 Training loss: 1.8872 0.4229 sec/batch\n",
      "Epoch 15/20  Iteration 2543/3560 Training loss: 1.8867 0.4240 sec/batch\n",
      "Epoch 15/20  Iteration 2544/3560 Training loss: 1.8876 0.3622 sec/batch\n",
      "Epoch 15/20  Iteration 2545/3560 Training loss: 1.8875 0.4095 sec/batch\n",
      "Epoch 15/20  Iteration 2546/3560 Training loss: 1.8874 0.4155 sec/batch\n",
      "Epoch 15/20  Iteration 2547/3560 Training loss: 1.8871 0.4324 sec/batch\n",
      "Epoch 15/20  Iteration 2548/3560 Training loss: 1.8874 0.4602 sec/batch\n",
      "Epoch 15/20  Iteration 2549/3560 Training loss: 1.8877 0.4980 sec/batch\n",
      "Epoch 15/20  Iteration 2550/3560 Training loss: 1.8874 0.3795 sec/batch\n",
      "Epoch 15/20  Iteration 2551/3560 Training loss: 1.8869 0.4002 sec/batch\n",
      "Epoch 15/20  Iteration 2552/3560 Training loss: 1.8876 0.3887 sec/batch\n",
      "Epoch 15/20  Iteration 2553/3560 Training loss: 1.8875 0.4303 sec/batch\n",
      "Epoch 15/20  Iteration 2554/3560 Training loss: 1.8882 0.4441 sec/batch\n",
      "Epoch 15/20  Iteration 2555/3560 Training loss: 1.8886 0.5148 sec/batch\n",
      "Epoch 15/20  Iteration 2556/3560 Training loss: 1.8890 0.3856 sec/batch\n",
      "Epoch 15/20  Iteration 2557/3560 Training loss: 1.8888 0.4540 sec/batch\n",
      "Epoch 15/20  Iteration 2558/3560 Training loss: 1.8891 0.4246 sec/batch\n",
      "Epoch 15/20  Iteration 2559/3560 Training loss: 1.8890 0.4180 sec/batch\n",
      "Epoch 15/20  Iteration 2560/3560 Training loss: 1.8887 0.4255 sec/batch\n",
      "Epoch 15/20  Iteration 2561/3560 Training loss: 1.8886 0.3933 sec/batch\n",
      "Epoch 15/20  Iteration 2562/3560 Training loss: 1.8887 0.4100 sec/batch\n",
      "Epoch 15/20  Iteration 2563/3560 Training loss: 1.8892 0.4258 sec/batch\n",
      "Epoch 15/20  Iteration 2564/3560 Training loss: 1.8893 0.3782 sec/batch\n",
      "Epoch 15/20  Iteration 2565/3560 Training loss: 1.8897 0.4487 sec/batch\n",
      "Epoch 15/20  Iteration 2566/3560 Training loss: 1.8895 0.4420 sec/batch\n",
      "Epoch 15/20  Iteration 2567/3560 Training loss: 1.8895 0.4276 sec/batch\n",
      "Epoch 15/20  Iteration 2568/3560 Training loss: 1.8898 0.4446 sec/batch\n",
      "Epoch 15/20  Iteration 2569/3560 Training loss: 1.8897 0.4357 sec/batch\n",
      "Epoch 15/20  Iteration 2570/3560 Training loss: 1.8899 0.3853 sec/batch\n",
      "Epoch 15/20  Iteration 2571/3560 Training loss: 1.8895 0.3999 sec/batch\n",
      "Epoch 15/20  Iteration 2572/3560 Training loss: 1.8894 0.3503 sec/batch\n",
      "Epoch 15/20  Iteration 2573/3560 Training loss: 1.8889 0.4375 sec/batch\n",
      "Epoch 15/20  Iteration 2574/3560 Training loss: 1.8891 0.3975 sec/batch\n",
      "Epoch 15/20  Iteration 2575/3560 Training loss: 1.8886 0.4711 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2576/3560 Training loss: 1.8885 0.4141 sec/batch\n",
      "Epoch 15/20  Iteration 2577/3560 Training loss: 1.8880 0.4864 sec/batch\n",
      "Epoch 15/20  Iteration 2578/3560 Training loss: 1.8878 0.3959 sec/batch\n",
      "Epoch 15/20  Iteration 2579/3560 Training loss: 1.8875 0.4035 sec/batch\n",
      "Epoch 15/20  Iteration 2580/3560 Training loss: 1.8872 0.3773 sec/batch\n",
      "Epoch 15/20  Iteration 2581/3560 Training loss: 1.8868 0.3529 sec/batch\n",
      "Epoch 15/20  Iteration 2582/3560 Training loss: 1.8867 0.4258 sec/batch\n",
      "Epoch 15/20  Iteration 2583/3560 Training loss: 1.8866 0.3990 sec/batch\n",
      "Epoch 15/20  Iteration 2584/3560 Training loss: 1.8865 0.4509 sec/batch\n",
      "Epoch 15/20  Iteration 2585/3560 Training loss: 1.8859 0.4180 sec/batch\n",
      "Epoch 15/20  Iteration 2586/3560 Training loss: 1.8857 0.4226 sec/batch\n",
      "Epoch 15/20  Iteration 2587/3560 Training loss: 1.8854 0.4107 sec/batch\n",
      "Epoch 15/20  Iteration 2588/3560 Training loss: 1.8854 0.4373 sec/batch\n",
      "Epoch 15/20  Iteration 2589/3560 Training loss: 1.8853 0.4207 sec/batch\n",
      "Epoch 15/20  Iteration 2590/3560 Training loss: 1.8850 0.3782 sec/batch\n",
      "Epoch 15/20  Iteration 2591/3560 Training loss: 1.8847 0.3570 sec/batch\n",
      "Epoch 15/20  Iteration 2592/3560 Training loss: 1.8844 0.4149 sec/batch\n",
      "Epoch 15/20  Iteration 2593/3560 Training loss: 1.8844 0.4211 sec/batch\n",
      "Epoch 15/20  Iteration 2594/3560 Training loss: 1.8844 0.4513 sec/batch\n",
      "Epoch 15/20  Iteration 2595/3560 Training loss: 1.8842 0.4448 sec/batch\n",
      "Epoch 15/20  Iteration 2596/3560 Training loss: 1.8840 0.4331 sec/batch\n",
      "Epoch 15/20  Iteration 2597/3560 Training loss: 1.8838 0.4540 sec/batch\n",
      "Epoch 15/20  Iteration 2598/3560 Training loss: 1.8837 0.3690 sec/batch\n",
      "Epoch 15/20  Iteration 2599/3560 Training loss: 1.8837 0.4476 sec/batch\n",
      "Epoch 15/20  Iteration 2600/3560 Training loss: 1.8837 0.3635 sec/batch\n",
      "Epoch 15/20  Iteration 2601/3560 Training loss: 1.8837 0.4767 sec/batch\n",
      "Epoch 15/20  Iteration 2602/3560 Training loss: 1.8837 0.3645 sec/batch\n",
      "Epoch 15/20  Iteration 2603/3560 Training loss: 1.8836 0.4090 sec/batch\n",
      "Epoch 15/20  Iteration 2604/3560 Training loss: 1.8835 0.4630 sec/batch\n",
      "Epoch 15/20  Iteration 2605/3560 Training loss: 1.8834 0.3866 sec/batch\n",
      "Epoch 15/20  Iteration 2606/3560 Training loss: 1.8832 0.4217 sec/batch\n",
      "Epoch 15/20  Iteration 2607/3560 Training loss: 1.8830 0.4581 sec/batch\n",
      "Epoch 15/20  Iteration 2608/3560 Training loss: 1.8827 0.4641 sec/batch\n",
      "Epoch 15/20  Iteration 2609/3560 Training loss: 1.8827 0.3711 sec/batch\n",
      "Epoch 15/20  Iteration 2610/3560 Training loss: 1.8827 0.3962 sec/batch\n",
      "Epoch 15/20  Iteration 2611/3560 Training loss: 1.8827 0.3747 sec/batch\n",
      "Epoch 15/20  Iteration 2612/3560 Training loss: 1.8826 0.4247 sec/batch\n",
      "Epoch 15/20  Iteration 2613/3560 Training loss: 1.8826 0.4276 sec/batch\n",
      "Epoch 15/20  Iteration 2614/3560 Training loss: 1.8822 0.4128 sec/batch\n",
      "Epoch 15/20  Iteration 2615/3560 Training loss: 1.8820 0.4523 sec/batch\n",
      "Epoch 15/20  Iteration 2616/3560 Training loss: 1.8822 0.4005 sec/batch\n",
      "Epoch 15/20  Iteration 2617/3560 Training loss: 1.8822 0.4130 sec/batch\n",
      "Epoch 15/20  Iteration 2618/3560 Training loss: 1.8818 0.3785 sec/batch\n",
      "Epoch 15/20  Iteration 2619/3560 Training loss: 1.8821 0.4017 sec/batch\n",
      "Epoch 15/20  Iteration 2620/3560 Training loss: 1.8821 0.4047 sec/batch\n",
      "Epoch 15/20  Iteration 2621/3560 Training loss: 1.8821 0.4064 sec/batch\n",
      "Epoch 15/20  Iteration 2622/3560 Training loss: 1.8821 0.3669 sec/batch\n",
      "Epoch 15/20  Iteration 2623/3560 Training loss: 1.8818 0.4676 sec/batch\n",
      "Epoch 15/20  Iteration 2624/3560 Training loss: 1.8815 0.4245 sec/batch\n",
      "Epoch 15/20  Iteration 2625/3560 Training loss: 1.8816 0.4164 sec/batch\n",
      "Epoch 15/20  Iteration 2626/3560 Training loss: 1.8815 0.5218 sec/batch\n",
      "Epoch 15/20  Iteration 2627/3560 Training loss: 1.8816 0.3374 sec/batch\n",
      "Epoch 15/20  Iteration 2628/3560 Training loss: 1.8816 0.3457 sec/batch\n",
      "Epoch 15/20  Iteration 2629/3560 Training loss: 1.8816 0.4743 sec/batch\n",
      "Epoch 15/20  Iteration 2630/3560 Training loss: 1.8817 0.4055 sec/batch\n",
      "Epoch 15/20  Iteration 2631/3560 Training loss: 1.8819 0.4225 sec/batch\n",
      "Epoch 15/20  Iteration 2632/3560 Training loss: 1.8818 0.4528 sec/batch\n",
      "Epoch 15/20  Iteration 2633/3560 Training loss: 1.8820 0.3909 sec/batch\n",
      "Epoch 15/20  Iteration 2634/3560 Training loss: 1.8819 0.4422 sec/batch\n",
      "Epoch 15/20  Iteration 2635/3560 Training loss: 1.8819 0.4446 sec/batch\n",
      "Epoch 15/20  Iteration 2636/3560 Training loss: 1.8820 0.4434 sec/batch\n",
      "Epoch 15/20  Iteration 2637/3560 Training loss: 1.8819 0.3897 sec/batch\n",
      "Epoch 15/20  Iteration 2638/3560 Training loss: 1.8819 0.3985 sec/batch\n",
      "Epoch 15/20  Iteration 2639/3560 Training loss: 1.8819 0.3952 sec/batch\n",
      "Epoch 15/20  Iteration 2640/3560 Training loss: 1.8820 0.4734 sec/batch\n",
      "Epoch 15/20  Iteration 2641/3560 Training loss: 1.8820 0.5565 sec/batch\n",
      "Epoch 15/20  Iteration 2642/3560 Training loss: 1.8818 0.5372 sec/batch\n",
      "Epoch 15/20  Iteration 2643/3560 Training loss: 1.8816 0.5424 sec/batch\n",
      "Epoch 15/20  Iteration 2644/3560 Training loss: 1.8818 0.5047 sec/batch\n",
      "Epoch 15/20  Iteration 2645/3560 Training loss: 1.8818 0.4870 sec/batch\n",
      "Epoch 15/20  Iteration 2646/3560 Training loss: 1.8819 0.4018 sec/batch\n",
      "Epoch 15/20  Iteration 2647/3560 Training loss: 1.8819 0.4772 sec/batch\n",
      "Epoch 15/20  Iteration 2648/3560 Training loss: 1.8819 0.4460 sec/batch\n",
      "Epoch 15/20  Iteration 2649/3560 Training loss: 1.8819 0.4794 sec/batch\n",
      "Epoch 15/20  Iteration 2650/3560 Training loss: 1.8819 0.4236 sec/batch\n",
      "Epoch 15/20  Iteration 2651/3560 Training loss: 1.8816 0.4188 sec/batch\n",
      "Epoch 15/20  Iteration 2652/3560 Training loss: 1.8818 0.3858 sec/batch\n",
      "Epoch 15/20  Iteration 2653/3560 Training loss: 1.8819 0.4803 sec/batch\n",
      "Epoch 15/20  Iteration 2654/3560 Training loss: 1.8818 0.3920 sec/batch\n",
      "Epoch 15/20  Iteration 2655/3560 Training loss: 1.8819 0.4204 sec/batch\n",
      "Epoch 15/20  Iteration 2656/3560 Training loss: 1.8819 0.4336 sec/batch\n",
      "Epoch 15/20  Iteration 2657/3560 Training loss: 1.8819 0.3915 sec/batch\n",
      "Epoch 15/20  Iteration 2658/3560 Training loss: 1.8818 0.3758 sec/batch\n",
      "Epoch 15/20  Iteration 2659/3560 Training loss: 1.8819 0.4083 sec/batch\n",
      "Epoch 15/20  Iteration 2660/3560 Training loss: 1.8822 0.3713 sec/batch\n",
      "Epoch 15/20  Iteration 2661/3560 Training loss: 1.8821 0.4012 sec/batch\n",
      "Epoch 15/20  Iteration 2662/3560 Training loss: 1.8821 0.4060 sec/batch\n",
      "Epoch 15/20  Iteration 2663/3560 Training loss: 1.8820 0.4821 sec/batch\n",
      "Epoch 15/20  Iteration 2664/3560 Training loss: 1.8819 0.3779 sec/batch\n",
      "Epoch 15/20  Iteration 2665/3560 Training loss: 1.8820 0.4917 sec/batch\n",
      "Epoch 15/20  Iteration 2666/3560 Training loss: 1.8820 0.3939 sec/batch\n",
      "Epoch 15/20  Iteration 2667/3560 Training loss: 1.8821 0.4224 sec/batch\n",
      "Epoch 15/20  Iteration 2668/3560 Training loss: 1.8820 0.4023 sec/batch\n",
      "Epoch 15/20  Iteration 2669/3560 Training loss: 1.8819 0.4037 sec/batch\n",
      "Epoch 15/20  Iteration 2670/3560 Training loss: 1.8819 0.4541 sec/batch\n",
      "Epoch 16/20  Iteration 2671/3560 Training loss: 1.9419 0.4639 sec/batch\n",
      "Epoch 16/20  Iteration 2672/3560 Training loss: 1.8970 0.4392 sec/batch\n",
      "Epoch 16/20  Iteration 2673/3560 Training loss: 1.8871 0.4491 sec/batch\n",
      "Epoch 16/20  Iteration 2674/3560 Training loss: 1.8815 0.4232 sec/batch\n",
      "Epoch 16/20  Iteration 2675/3560 Training loss: 1.8807 0.4115 sec/batch\n",
      "Epoch 16/20  Iteration 2676/3560 Training loss: 1.8728 0.4041 sec/batch\n",
      "Epoch 16/20  Iteration 2677/3560 Training loss: 1.8732 0.3902 sec/batch\n",
      "Epoch 16/20  Iteration 2678/3560 Training loss: 1.8725 0.3912 sec/batch\n",
      "Epoch 16/20  Iteration 2679/3560 Training loss: 1.8748 0.4022 sec/batch\n",
      "Epoch 16/20  Iteration 2680/3560 Training loss: 1.8747 0.4050 sec/batch\n",
      "Epoch 16/20  Iteration 2681/3560 Training loss: 1.8702 0.4247 sec/batch\n",
      "Epoch 16/20  Iteration 2682/3560 Training loss: 1.8685 0.4348 sec/batch\n",
      "Epoch 16/20  Iteration 2683/3560 Training loss: 1.8682 0.4419 sec/batch\n",
      "Epoch 16/20  Iteration 2684/3560 Training loss: 1.8708 0.4080 sec/batch\n",
      "Epoch 16/20  Iteration 2685/3560 Training loss: 1.8706 0.4210 sec/batch\n",
      "Epoch 16/20  Iteration 2686/3560 Training loss: 1.8694 0.4194 sec/batch\n",
      "Epoch 16/20  Iteration 2687/3560 Training loss: 1.8699 0.3958 sec/batch\n",
      "Epoch 16/20  Iteration 2688/3560 Training loss: 1.8719 0.4009 sec/batch\n",
      "Epoch 16/20  Iteration 2689/3560 Training loss: 1.8720 0.4496 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2690/3560 Training loss: 1.8726 0.3973 sec/batch\n",
      "Epoch 16/20  Iteration 2691/3560 Training loss: 1.8724 0.4450 sec/batch\n",
      "Epoch 16/20  Iteration 2692/3560 Training loss: 1.8737 0.4324 sec/batch\n",
      "Epoch 16/20  Iteration 2693/3560 Training loss: 1.8735 0.4312 sec/batch\n",
      "Epoch 16/20  Iteration 2694/3560 Training loss: 1.8728 0.4898 sec/batch\n",
      "Epoch 16/20  Iteration 2695/3560 Training loss: 1.8726 0.4890 sec/batch\n",
      "Epoch 16/20  Iteration 2696/3560 Training loss: 1.8719 0.4474 sec/batch\n",
      "Epoch 16/20  Iteration 2697/3560 Training loss: 1.8712 0.4375 sec/batch\n",
      "Epoch 16/20  Iteration 2698/3560 Training loss: 1.8721 0.3833 sec/batch\n",
      "Epoch 16/20  Iteration 2699/3560 Training loss: 1.8729 0.4360 sec/batch\n",
      "Epoch 16/20  Iteration 2700/3560 Training loss: 1.8732 0.4024 sec/batch\n",
      "Epoch 16/20  Iteration 2701/3560 Training loss: 1.8733 0.4235 sec/batch\n",
      "Epoch 16/20  Iteration 2702/3560 Training loss: 1.8727 0.3810 sec/batch\n",
      "Epoch 16/20  Iteration 2703/3560 Training loss: 1.8730 0.4708 sec/batch\n",
      "Epoch 16/20  Iteration 2704/3560 Training loss: 1.8739 0.4512 sec/batch\n",
      "Epoch 16/20  Iteration 2705/3560 Training loss: 1.8737 0.3868 sec/batch\n",
      "Epoch 16/20  Iteration 2706/3560 Training loss: 1.8737 0.3774 sec/batch\n",
      "Epoch 16/20  Iteration 2707/3560 Training loss: 1.8733 0.3966 sec/batch\n",
      "Epoch 16/20  Iteration 2708/3560 Training loss: 1.8722 0.4425 sec/batch\n",
      "Epoch 16/20  Iteration 2709/3560 Training loss: 1.8709 0.4340 sec/batch\n",
      "Epoch 16/20  Iteration 2710/3560 Training loss: 1.8704 0.4465 sec/batch\n",
      "Epoch 16/20  Iteration 2711/3560 Training loss: 1.8703 0.4429 sec/batch\n",
      "Epoch 16/20  Iteration 2712/3560 Training loss: 1.8707 0.4005 sec/batch\n",
      "Epoch 16/20  Iteration 2713/3560 Training loss: 1.8704 0.4356 sec/batch\n",
      "Epoch 16/20  Iteration 2714/3560 Training loss: 1.8697 0.3852 sec/batch\n",
      "Epoch 16/20  Iteration 2715/3560 Training loss: 1.8700 0.4454 sec/batch\n",
      "Epoch 16/20  Iteration 2716/3560 Training loss: 1.8687 0.4250 sec/batch\n",
      "Epoch 16/20  Iteration 2717/3560 Training loss: 1.8686 0.4676 sec/batch\n",
      "Epoch 16/20  Iteration 2718/3560 Training loss: 1.8683 0.3889 sec/batch\n",
      "Epoch 16/20  Iteration 2719/3560 Training loss: 1.8682 0.4609 sec/batch\n",
      "Epoch 16/20  Iteration 2720/3560 Training loss: 1.8689 0.4939 sec/batch\n",
      "Epoch 16/20  Iteration 2721/3560 Training loss: 1.8686 0.3815 sec/batch\n",
      "Epoch 16/20  Iteration 2722/3560 Training loss: 1.8694 0.4268 sec/batch\n",
      "Epoch 16/20  Iteration 2723/3560 Training loss: 1.8693 0.4040 sec/batch\n",
      "Epoch 16/20  Iteration 2724/3560 Training loss: 1.8693 0.4388 sec/batch\n",
      "Epoch 16/20  Iteration 2725/3560 Training loss: 1.8687 0.3393 sec/batch\n",
      "Epoch 16/20  Iteration 2726/3560 Training loss: 1.8689 0.4480 sec/batch\n",
      "Epoch 16/20  Iteration 2727/3560 Training loss: 1.8694 0.4272 sec/batch\n",
      "Epoch 16/20  Iteration 2728/3560 Training loss: 1.8689 0.4426 sec/batch\n",
      "Epoch 16/20  Iteration 2729/3560 Training loss: 1.8684 0.3944 sec/batch\n",
      "Epoch 16/20  Iteration 2730/3560 Training loss: 1.8690 0.4159 sec/batch\n",
      "Epoch 16/20  Iteration 2731/3560 Training loss: 1.8689 0.4829 sec/batch\n",
      "Epoch 16/20  Iteration 2732/3560 Training loss: 1.8697 0.3943 sec/batch\n",
      "Epoch 16/20  Iteration 2733/3560 Training loss: 1.8702 0.4430 sec/batch\n",
      "Epoch 16/20  Iteration 2734/3560 Training loss: 1.8705 0.3828 sec/batch\n",
      "Epoch 16/20  Iteration 2735/3560 Training loss: 1.8704 0.4130 sec/batch\n",
      "Epoch 16/20  Iteration 2736/3560 Training loss: 1.8708 0.3620 sec/batch\n",
      "Epoch 16/20  Iteration 2737/3560 Training loss: 1.8708 0.4216 sec/batch\n",
      "Epoch 16/20  Iteration 2738/3560 Training loss: 1.8704 0.4765 sec/batch\n",
      "Epoch 16/20  Iteration 2739/3560 Training loss: 1.8705 0.3364 sec/batch\n",
      "Epoch 16/20  Iteration 2740/3560 Training loss: 1.8704 0.4346 sec/batch\n",
      "Epoch 16/20  Iteration 2741/3560 Training loss: 1.8709 0.4512 sec/batch\n",
      "Epoch 16/20  Iteration 2742/3560 Training loss: 1.8708 0.3985 sec/batch\n",
      "Epoch 16/20  Iteration 2743/3560 Training loss: 1.8713 0.3851 sec/batch\n",
      "Epoch 16/20  Iteration 2744/3560 Training loss: 1.8710 0.4622 sec/batch\n",
      "Epoch 16/20  Iteration 2745/3560 Training loss: 1.8709 0.4060 sec/batch\n",
      "Epoch 16/20  Iteration 2746/3560 Training loss: 1.8711 0.4150 sec/batch\n",
      "Epoch 16/20  Iteration 2747/3560 Training loss: 1.8712 0.4118 sec/batch\n",
      "Epoch 16/20  Iteration 2748/3560 Training loss: 1.8713 0.4542 sec/batch\n",
      "Epoch 16/20  Iteration 2749/3560 Training loss: 1.8710 0.4317 sec/batch\n",
      "Epoch 16/20  Iteration 2750/3560 Training loss: 1.8709 0.4267 sec/batch\n",
      "Epoch 16/20  Iteration 2751/3560 Training loss: 1.8705 0.4549 sec/batch\n",
      "Epoch 16/20  Iteration 2752/3560 Training loss: 1.8705 0.3960 sec/batch\n",
      "Epoch 16/20  Iteration 2753/3560 Training loss: 1.8701 0.3935 sec/batch\n",
      "Epoch 16/20  Iteration 2754/3560 Training loss: 1.8700 0.4103 sec/batch\n",
      "Epoch 16/20  Iteration 2755/3560 Training loss: 1.8694 0.4106 sec/batch\n",
      "Epoch 16/20  Iteration 2756/3560 Training loss: 1.8690 0.3874 sec/batch\n",
      "Epoch 16/20  Iteration 2757/3560 Training loss: 1.8689 0.4020 sec/batch\n",
      "Epoch 16/20  Iteration 2758/3560 Training loss: 1.8685 0.4300 sec/batch\n",
      "Epoch 16/20  Iteration 2759/3560 Training loss: 1.8681 0.4691 sec/batch\n",
      "Epoch 16/20  Iteration 2760/3560 Training loss: 1.8682 0.4173 sec/batch\n",
      "Epoch 16/20  Iteration 2761/3560 Training loss: 1.8679 0.3899 sec/batch\n",
      "Epoch 16/20  Iteration 2762/3560 Training loss: 1.8678 0.4238 sec/batch\n",
      "Epoch 16/20  Iteration 2763/3560 Training loss: 1.8675 0.4088 sec/batch\n",
      "Epoch 16/20  Iteration 2764/3560 Training loss: 1.8672 0.3714 sec/batch\n",
      "Epoch 16/20  Iteration 2765/3560 Training loss: 1.8669 0.4344 sec/batch\n",
      "Epoch 16/20  Iteration 2766/3560 Training loss: 1.8668 0.4656 sec/batch\n",
      "Epoch 16/20  Iteration 2767/3560 Training loss: 1.8668 0.4459 sec/batch\n",
      "Epoch 16/20  Iteration 2768/3560 Training loss: 1.8665 0.4701 sec/batch\n",
      "Epoch 16/20  Iteration 2769/3560 Training loss: 1.8661 0.4131 sec/batch\n",
      "Epoch 16/20  Iteration 2770/3560 Training loss: 1.8658 0.4123 sec/batch\n",
      "Epoch 16/20  Iteration 2771/3560 Training loss: 1.8658 0.4490 sec/batch\n",
      "Epoch 16/20  Iteration 2772/3560 Training loss: 1.8658 0.3268 sec/batch\n",
      "Epoch 16/20  Iteration 2773/3560 Training loss: 1.8655 0.3820 sec/batch\n",
      "Epoch 16/20  Iteration 2774/3560 Training loss: 1.8654 0.3646 sec/batch\n",
      "Epoch 16/20  Iteration 2775/3560 Training loss: 1.8651 0.4416 sec/batch\n",
      "Epoch 16/20  Iteration 2776/3560 Training loss: 1.8650 0.3935 sec/batch\n",
      "Epoch 16/20  Iteration 2777/3560 Training loss: 1.8651 0.4452 sec/batch\n",
      "Epoch 16/20  Iteration 2778/3560 Training loss: 1.8650 0.4597 sec/batch\n",
      "Epoch 16/20  Iteration 2779/3560 Training loss: 1.8652 0.4338 sec/batch\n",
      "Epoch 16/20  Iteration 2780/3560 Training loss: 1.8651 0.4044 sec/batch\n",
      "Epoch 16/20  Iteration 2781/3560 Training loss: 1.8650 0.4389 sec/batch\n",
      "Epoch 16/20  Iteration 2782/3560 Training loss: 1.8650 0.3480 sec/batch\n",
      "Epoch 16/20  Iteration 2783/3560 Training loss: 1.8649 0.4232 sec/batch\n",
      "Epoch 16/20  Iteration 2784/3560 Training loss: 1.8647 0.3874 sec/batch\n",
      "Epoch 16/20  Iteration 2785/3560 Training loss: 1.8646 0.4413 sec/batch\n",
      "Epoch 16/20  Iteration 2786/3560 Training loss: 1.8643 0.4269 sec/batch\n",
      "Epoch 16/20  Iteration 2787/3560 Training loss: 1.8642 0.4205 sec/batch\n",
      "Epoch 16/20  Iteration 2788/3560 Training loss: 1.8642 0.4586 sec/batch\n",
      "Epoch 16/20  Iteration 2789/3560 Training loss: 1.8641 0.4034 sec/batch\n",
      "Epoch 16/20  Iteration 2790/3560 Training loss: 1.8640 0.4271 sec/batch\n",
      "Epoch 16/20  Iteration 2791/3560 Training loss: 1.8641 0.4291 sec/batch\n",
      "Epoch 16/20  Iteration 2792/3560 Training loss: 1.8637 0.4417 sec/batch\n",
      "Epoch 16/20  Iteration 2793/3560 Training loss: 1.8634 0.3772 sec/batch\n",
      "Epoch 16/20  Iteration 2794/3560 Training loss: 1.8635 0.3856 sec/batch\n",
      "Epoch 16/20  Iteration 2795/3560 Training loss: 1.8635 0.4067 sec/batch\n",
      "Epoch 16/20  Iteration 2796/3560 Training loss: 1.8631 0.4278 sec/batch\n",
      "Epoch 16/20  Iteration 2797/3560 Training loss: 1.8632 0.4160 sec/batch\n",
      "Epoch 16/20  Iteration 2798/3560 Training loss: 1.8633 0.4453 sec/batch\n",
      "Epoch 16/20  Iteration 2799/3560 Training loss: 1.8633 0.4574 sec/batch\n",
      "Epoch 16/20  Iteration 2800/3560 Training loss: 1.8632 0.3865 sec/batch\n",
      "Epoch 16/20  Iteration 2801/3560 Training loss: 1.8630 0.4366 sec/batch\n",
      "Epoch 16/20  Iteration 2802/3560 Training loss: 1.8627 0.3903 sec/batch\n",
      "Epoch 16/20  Iteration 2803/3560 Training loss: 1.8627 0.4055 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2804/3560 Training loss: 1.8627 0.4027 sec/batch\n",
      "Epoch 16/20  Iteration 2805/3560 Training loss: 1.8626 0.4089 sec/batch\n",
      "Epoch 16/20  Iteration 2806/3560 Training loss: 1.8627 0.4611 sec/batch\n",
      "Epoch 16/20  Iteration 2807/3560 Training loss: 1.8627 0.4203 sec/batch\n",
      "Epoch 16/20  Iteration 2808/3560 Training loss: 1.8628 0.4563 sec/batch\n",
      "Epoch 16/20  Iteration 2809/3560 Training loss: 1.8630 0.4298 sec/batch\n",
      "Epoch 16/20  Iteration 2810/3560 Training loss: 1.8628 0.4018 sec/batch\n",
      "Epoch 16/20  Iteration 2811/3560 Training loss: 1.8631 0.3785 sec/batch\n",
      "Epoch 16/20  Iteration 2812/3560 Training loss: 1.8630 0.4199 sec/batch\n",
      "Epoch 16/20  Iteration 2813/3560 Training loss: 1.8629 0.3730 sec/batch\n",
      "Epoch 16/20  Iteration 2814/3560 Training loss: 1.8629 0.4366 sec/batch\n",
      "Epoch 16/20  Iteration 2815/3560 Training loss: 1.8628 0.4237 sec/batch\n",
      "Epoch 16/20  Iteration 2816/3560 Training loss: 1.8629 0.4459 sec/batch\n",
      "Epoch 16/20  Iteration 2817/3560 Training loss: 1.8629 0.4035 sec/batch\n",
      "Epoch 16/20  Iteration 2818/3560 Training loss: 1.8630 0.4271 sec/batch\n",
      "Epoch 16/20  Iteration 2819/3560 Training loss: 1.8630 0.4262 sec/batch\n",
      "Epoch 16/20  Iteration 2820/3560 Training loss: 1.8628 0.4317 sec/batch\n",
      "Epoch 16/20  Iteration 2821/3560 Training loss: 1.8626 0.4383 sec/batch\n",
      "Epoch 16/20  Iteration 2822/3560 Training loss: 1.8627 0.4282 sec/batch\n",
      "Epoch 16/20  Iteration 2823/3560 Training loss: 1.8627 0.4021 sec/batch\n",
      "Epoch 16/20  Iteration 2824/3560 Training loss: 1.8628 0.4527 sec/batch\n",
      "Epoch 16/20  Iteration 2825/3560 Training loss: 1.8627 0.4125 sec/batch\n",
      "Epoch 16/20  Iteration 2826/3560 Training loss: 1.8627 0.4147 sec/batch\n",
      "Epoch 16/20  Iteration 2827/3560 Training loss: 1.8628 0.4680 sec/batch\n",
      "Epoch 16/20  Iteration 2828/3560 Training loss: 1.8628 0.4267 sec/batch\n",
      "Epoch 16/20  Iteration 2829/3560 Training loss: 1.8626 0.4006 sec/batch\n",
      "Epoch 16/20  Iteration 2830/3560 Training loss: 1.8627 0.3722 sec/batch\n",
      "Epoch 16/20  Iteration 2831/3560 Training loss: 1.8628 0.3952 sec/batch\n",
      "Epoch 16/20  Iteration 2832/3560 Training loss: 1.8627 0.4337 sec/batch\n",
      "Epoch 16/20  Iteration 2833/3560 Training loss: 1.8628 0.3873 sec/batch\n",
      "Epoch 16/20  Iteration 2834/3560 Training loss: 1.8628 0.3877 sec/batch\n",
      "Epoch 16/20  Iteration 2835/3560 Training loss: 1.8628 0.4556 sec/batch\n",
      "Epoch 16/20  Iteration 2836/3560 Training loss: 1.8628 0.4241 sec/batch\n",
      "Epoch 16/20  Iteration 2837/3560 Training loss: 1.8628 0.4516 sec/batch\n",
      "Epoch 16/20  Iteration 2838/3560 Training loss: 1.8631 0.4229 sec/batch\n",
      "Epoch 16/20  Iteration 2839/3560 Training loss: 1.8630 0.3993 sec/batch\n",
      "Epoch 16/20  Iteration 2840/3560 Training loss: 1.8630 0.3376 sec/batch\n",
      "Epoch 16/20  Iteration 2841/3560 Training loss: 1.8629 0.4574 sec/batch\n",
      "Epoch 16/20  Iteration 2842/3560 Training loss: 1.8627 0.4067 sec/batch\n",
      "Epoch 16/20  Iteration 2843/3560 Training loss: 1.8628 0.4614 sec/batch\n",
      "Epoch 16/20  Iteration 2844/3560 Training loss: 1.8628 0.4222 sec/batch\n",
      "Epoch 16/20  Iteration 2845/3560 Training loss: 1.8629 0.4360 sec/batch\n",
      "Epoch 16/20  Iteration 2846/3560 Training loss: 1.8629 0.4472 sec/batch\n",
      "Epoch 16/20  Iteration 2847/3560 Training loss: 1.8628 0.4162 sec/batch\n",
      "Epoch 16/20  Iteration 2848/3560 Training loss: 1.8629 0.4023 sec/batch\n",
      "Epoch 17/20  Iteration 2849/3560 Training loss: 1.9142 0.3702 sec/batch\n",
      "Epoch 17/20  Iteration 2850/3560 Training loss: 1.8812 0.4061 sec/batch\n",
      "Epoch 17/20  Iteration 2851/3560 Training loss: 1.8692 0.3866 sec/batch\n",
      "Epoch 17/20  Iteration 2852/3560 Training loss: 1.8662 0.4047 sec/batch\n",
      "Epoch 17/20  Iteration 2853/3560 Training loss: 1.8640 0.3768 sec/batch\n",
      "Epoch 17/20  Iteration 2854/3560 Training loss: 1.8564 0.4792 sec/batch\n",
      "Epoch 17/20  Iteration 2855/3560 Training loss: 1.8559 0.4049 sec/batch\n",
      "Epoch 17/20  Iteration 2856/3560 Training loss: 1.8550 0.3964 sec/batch\n",
      "Epoch 17/20  Iteration 2857/3560 Training loss: 1.8571 0.4454 sec/batch\n",
      "Epoch 17/20  Iteration 2858/3560 Training loss: 1.8575 0.4456 sec/batch\n",
      "Epoch 17/20  Iteration 2859/3560 Training loss: 1.8535 0.3760 sec/batch\n",
      "Epoch 17/20  Iteration 2860/3560 Training loss: 1.8517 0.4582 sec/batch\n",
      "Epoch 17/20  Iteration 2861/3560 Training loss: 1.8522 0.3636 sec/batch\n",
      "Epoch 17/20  Iteration 2862/3560 Training loss: 1.8554 0.3935 sec/batch\n",
      "Epoch 17/20  Iteration 2863/3560 Training loss: 1.8551 0.4670 sec/batch\n",
      "Epoch 17/20  Iteration 2864/3560 Training loss: 1.8537 0.4419 sec/batch\n",
      "Epoch 17/20  Iteration 2865/3560 Training loss: 1.8541 0.4061 sec/batch\n",
      "Epoch 17/20  Iteration 2866/3560 Training loss: 1.8567 0.4956 sec/batch\n",
      "Epoch 17/20  Iteration 2867/3560 Training loss: 1.8564 0.3572 sec/batch\n",
      "Epoch 17/20  Iteration 2868/3560 Training loss: 1.8569 0.3858 sec/batch\n",
      "Epoch 17/20  Iteration 2869/3560 Training loss: 1.8566 0.3817 sec/batch\n",
      "Epoch 17/20  Iteration 2870/3560 Training loss: 1.8572 0.4125 sec/batch\n",
      "Epoch 17/20  Iteration 2871/3560 Training loss: 1.8573 0.3871 sec/batch\n",
      "Epoch 17/20  Iteration 2872/3560 Training loss: 1.8569 0.4011 sec/batch\n",
      "Epoch 17/20  Iteration 2873/3560 Training loss: 1.8567 0.4144 sec/batch\n",
      "Epoch 17/20  Iteration 2874/3560 Training loss: 1.8553 0.4330 sec/batch\n",
      "Epoch 17/20  Iteration 2875/3560 Training loss: 1.8547 0.4858 sec/batch\n",
      "Epoch 17/20  Iteration 2876/3560 Training loss: 1.8554 0.5003 sec/batch\n",
      "Epoch 17/20  Iteration 2877/3560 Training loss: 1.8561 0.3522 sec/batch\n",
      "Epoch 17/20  Iteration 2878/3560 Training loss: 1.8563 0.5248 sec/batch\n",
      "Epoch 17/20  Iteration 2879/3560 Training loss: 1.8561 0.3957 sec/batch\n",
      "Epoch 17/20  Iteration 2880/3560 Training loss: 1.8554 0.3530 sec/batch\n",
      "Epoch 17/20  Iteration 2881/3560 Training loss: 1.8556 0.4061 sec/batch\n",
      "Epoch 17/20  Iteration 2882/3560 Training loss: 1.8564 0.4473 sec/batch\n",
      "Epoch 17/20  Iteration 2883/3560 Training loss: 1.8562 0.5037 sec/batch\n",
      "Epoch 17/20  Iteration 2884/3560 Training loss: 1.8561 0.5264 sec/batch\n",
      "Epoch 17/20  Iteration 2885/3560 Training loss: 1.8556 0.3894 sec/batch\n",
      "Epoch 17/20  Iteration 2886/3560 Training loss: 1.8548 0.4430 sec/batch\n",
      "Epoch 17/20  Iteration 2887/3560 Training loss: 1.8538 0.3687 sec/batch\n",
      "Epoch 17/20  Iteration 2888/3560 Training loss: 1.8530 0.4635 sec/batch\n",
      "Epoch 17/20  Iteration 2889/3560 Training loss: 1.8525 0.3714 sec/batch\n",
      "Epoch 17/20  Iteration 2890/3560 Training loss: 1.8527 0.3753 sec/batch\n",
      "Epoch 17/20  Iteration 2891/3560 Training loss: 1.8523 0.3880 sec/batch\n",
      "Epoch 17/20  Iteration 2892/3560 Training loss: 1.8516 0.3974 sec/batch\n",
      "Epoch 17/20  Iteration 2893/3560 Training loss: 1.8518 0.4861 sec/batch\n",
      "Epoch 17/20  Iteration 2894/3560 Training loss: 1.8506 0.3916 sec/batch\n",
      "Epoch 17/20  Iteration 2895/3560 Training loss: 1.8504 0.4658 sec/batch\n",
      "Epoch 17/20  Iteration 2896/3560 Training loss: 1.8501 0.3815 sec/batch\n",
      "Epoch 17/20  Iteration 2897/3560 Training loss: 1.8500 0.4377 sec/batch\n",
      "Epoch 17/20  Iteration 2898/3560 Training loss: 1.8506 0.3749 sec/batch\n",
      "Epoch 17/20  Iteration 2899/3560 Training loss: 1.8500 0.4154 sec/batch\n",
      "Epoch 17/20  Iteration 2900/3560 Training loss: 1.8509 0.3761 sec/batch\n",
      "Epoch 17/20  Iteration 2901/3560 Training loss: 1.8507 0.4290 sec/batch\n",
      "Epoch 17/20  Iteration 2902/3560 Training loss: 1.8507 0.4245 sec/batch\n",
      "Epoch 17/20  Iteration 2903/3560 Training loss: 1.8501 0.4211 sec/batch\n",
      "Epoch 17/20  Iteration 2904/3560 Training loss: 1.8502 0.4135 sec/batch\n",
      "Epoch 17/20  Iteration 2905/3560 Training loss: 1.8506 0.4269 sec/batch\n",
      "Epoch 17/20  Iteration 2906/3560 Training loss: 1.8503 0.3891 sec/batch\n",
      "Epoch 17/20  Iteration 2907/3560 Training loss: 1.8498 0.4674 sec/batch\n",
      "Epoch 17/20  Iteration 2908/3560 Training loss: 1.8504 0.4037 sec/batch\n",
      "Epoch 17/20  Iteration 2909/3560 Training loss: 1.8503 0.4017 sec/batch\n",
      "Epoch 17/20  Iteration 2910/3560 Training loss: 1.8511 0.4011 sec/batch\n",
      "Epoch 17/20  Iteration 2911/3560 Training loss: 1.8515 0.4573 sec/batch\n",
      "Epoch 17/20  Iteration 2912/3560 Training loss: 1.8518 0.4449 sec/batch\n",
      "Epoch 17/20  Iteration 2913/3560 Training loss: 1.8518 0.3894 sec/batch\n",
      "Epoch 17/20  Iteration 2914/3560 Training loss: 1.8521 0.4045 sec/batch\n",
      "Epoch 17/20  Iteration 2915/3560 Training loss: 1.8521 0.4403 sec/batch\n",
      "Epoch 17/20  Iteration 2916/3560 Training loss: 1.8518 0.3853 sec/batch\n",
      "Epoch 17/20  Iteration 2917/3560 Training loss: 1.8519 0.3882 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 2918/3560 Training loss: 1.8520 0.3695 sec/batch\n",
      "Epoch 17/20  Iteration 2919/3560 Training loss: 1.8525 0.4260 sec/batch\n",
      "Epoch 17/20  Iteration 2920/3560 Training loss: 1.8526 0.4126 sec/batch\n",
      "Epoch 17/20  Iteration 2921/3560 Training loss: 1.8531 0.4432 sec/batch\n",
      "Epoch 17/20  Iteration 2922/3560 Training loss: 1.8529 0.4185 sec/batch\n",
      "Epoch 17/20  Iteration 2923/3560 Training loss: 1.8528 0.4309 sec/batch\n",
      "Epoch 17/20  Iteration 2924/3560 Training loss: 1.8530 0.4200 sec/batch\n",
      "Epoch 17/20  Iteration 2925/3560 Training loss: 1.8530 0.3723 sec/batch\n",
      "Epoch 17/20  Iteration 2926/3560 Training loss: 1.8531 0.4204 sec/batch\n",
      "Epoch 17/20  Iteration 2927/3560 Training loss: 1.8527 0.4025 sec/batch\n",
      "Epoch 17/20  Iteration 2928/3560 Training loss: 1.8526 0.3725 sec/batch\n",
      "Epoch 17/20  Iteration 2929/3560 Training loss: 1.8521 0.4202 sec/batch\n",
      "Epoch 17/20  Iteration 2930/3560 Training loss: 1.8522 0.3665 sec/batch\n",
      "Epoch 17/20  Iteration 2931/3560 Training loss: 1.8517 0.3946 sec/batch\n",
      "Epoch 17/20  Iteration 2932/3560 Training loss: 1.8518 0.5106 sec/batch\n",
      "Epoch 17/20  Iteration 2933/3560 Training loss: 1.8512 0.4646 sec/batch\n",
      "Epoch 17/20  Iteration 2934/3560 Training loss: 1.8509 0.4680 sec/batch\n",
      "Epoch 17/20  Iteration 2935/3560 Training loss: 1.8508 0.4251 sec/batch\n",
      "Epoch 17/20  Iteration 2936/3560 Training loss: 1.8505 0.4033 sec/batch\n",
      "Epoch 17/20  Iteration 2937/3560 Training loss: 1.8500 0.3813 sec/batch\n",
      "Epoch 17/20  Iteration 2938/3560 Training loss: 1.8501 0.3959 sec/batch\n",
      "Epoch 17/20  Iteration 2939/3560 Training loss: 1.8498 0.4313 sec/batch\n",
      "Epoch 17/20  Iteration 2940/3560 Training loss: 1.8496 0.3973 sec/batch\n",
      "Epoch 17/20  Iteration 2941/3560 Training loss: 1.8491 0.4605 sec/batch\n",
      "Epoch 17/20  Iteration 2942/3560 Training loss: 1.8488 0.4260 sec/batch\n",
      "Epoch 17/20  Iteration 2943/3560 Training loss: 1.8485 0.4442 sec/batch\n",
      "Epoch 17/20  Iteration 2944/3560 Training loss: 1.8485 0.4272 sec/batch\n",
      "Epoch 17/20  Iteration 2945/3560 Training loss: 1.8485 0.3794 sec/batch\n",
      "Epoch 17/20  Iteration 2946/3560 Training loss: 1.8481 0.4011 sec/batch\n",
      "Epoch 17/20  Iteration 2947/3560 Training loss: 1.8477 0.4279 sec/batch\n",
      "Epoch 17/20  Iteration 2948/3560 Training loss: 1.8474 0.4200 sec/batch\n",
      "Epoch 17/20  Iteration 2949/3560 Training loss: 1.8473 0.4589 sec/batch\n",
      "Epoch 17/20  Iteration 2950/3560 Training loss: 1.8473 0.4198 sec/batch\n",
      "Epoch 17/20  Iteration 2951/3560 Training loss: 1.8472 0.4583 sec/batch\n",
      "Epoch 17/20  Iteration 2952/3560 Training loss: 1.8469 0.4081 sec/batch\n",
      "Epoch 17/20  Iteration 2953/3560 Training loss: 1.8467 0.4862 sec/batch\n",
      "Epoch 17/20  Iteration 2954/3560 Training loss: 1.8466 0.3492 sec/batch\n",
      "Epoch 17/20  Iteration 2955/3560 Training loss: 1.8466 0.3761 sec/batch\n",
      "Epoch 17/20  Iteration 2956/3560 Training loss: 1.8466 0.4087 sec/batch\n",
      "Epoch 17/20  Iteration 2957/3560 Training loss: 1.8467 0.3959 sec/batch\n",
      "Epoch 17/20  Iteration 2958/3560 Training loss: 1.8466 0.5416 sec/batch\n",
      "Epoch 17/20  Iteration 2959/3560 Training loss: 1.8465 0.3702 sec/batch\n",
      "Epoch 17/20  Iteration 2960/3560 Training loss: 1.8464 0.4288 sec/batch\n",
      "Epoch 17/20  Iteration 2961/3560 Training loss: 1.8463 0.4486 sec/batch\n",
      "Epoch 17/20  Iteration 2962/3560 Training loss: 1.8461 0.4115 sec/batch\n",
      "Epoch 17/20  Iteration 2963/3560 Training loss: 1.8459 0.4070 sec/batch\n",
      "Epoch 17/20  Iteration 2964/3560 Training loss: 1.8456 0.3591 sec/batch\n",
      "Epoch 17/20  Iteration 2965/3560 Training loss: 1.8455 0.3979 sec/batch\n",
      "Epoch 17/20  Iteration 2966/3560 Training loss: 1.8455 0.4161 sec/batch\n",
      "Epoch 17/20  Iteration 2967/3560 Training loss: 1.8455 0.4080 sec/batch\n",
      "Epoch 17/20  Iteration 2968/3560 Training loss: 1.8454 0.3957 sec/batch\n",
      "Epoch 17/20  Iteration 2969/3560 Training loss: 1.8455 0.4263 sec/batch\n",
      "Epoch 17/20  Iteration 2970/3560 Training loss: 1.8451 0.4536 sec/batch\n",
      "Epoch 17/20  Iteration 2971/3560 Training loss: 1.8449 0.4371 sec/batch\n",
      "Epoch 17/20  Iteration 2972/3560 Training loss: 1.8450 0.4011 sec/batch\n",
      "Epoch 17/20  Iteration 2973/3560 Training loss: 1.8450 0.3976 sec/batch\n",
      "Epoch 17/20  Iteration 2974/3560 Training loss: 1.8447 0.4118 sec/batch\n",
      "Epoch 17/20  Iteration 2975/3560 Training loss: 1.8448 0.3786 sec/batch\n",
      "Epoch 17/20  Iteration 2976/3560 Training loss: 1.8448 0.3570 sec/batch\n",
      "Epoch 17/20  Iteration 2977/3560 Training loss: 1.8448 0.3961 sec/batch\n",
      "Epoch 17/20  Iteration 2978/3560 Training loss: 1.8448 0.4020 sec/batch\n",
      "Epoch 17/20  Iteration 2979/3560 Training loss: 1.8445 0.4950 sec/batch\n",
      "Epoch 17/20  Iteration 2980/3560 Training loss: 1.8443 0.4103 sec/batch\n",
      "Epoch 17/20  Iteration 2981/3560 Training loss: 1.8443 0.4617 sec/batch\n",
      "Epoch 17/20  Iteration 2982/3560 Training loss: 1.8443 0.4006 sec/batch\n",
      "Epoch 17/20  Iteration 2983/3560 Training loss: 1.8443 0.3970 sec/batch\n",
      "Epoch 17/20  Iteration 2984/3560 Training loss: 1.8444 0.4317 sec/batch\n",
      "Epoch 17/20  Iteration 2985/3560 Training loss: 1.8445 0.4022 sec/batch\n",
      "Epoch 17/20  Iteration 2986/3560 Training loss: 1.8445 0.4018 sec/batch\n",
      "Epoch 17/20  Iteration 2987/3560 Training loss: 1.8447 0.3833 sec/batch\n",
      "Epoch 17/20  Iteration 2988/3560 Training loss: 1.8445 0.4161 sec/batch\n",
      "Epoch 17/20  Iteration 2989/3560 Training loss: 1.8448 0.4544 sec/batch\n",
      "Epoch 17/20  Iteration 2990/3560 Training loss: 1.8447 0.4823 sec/batch\n",
      "Epoch 17/20  Iteration 2991/3560 Training loss: 1.8447 0.4330 sec/batch\n",
      "Epoch 17/20  Iteration 2992/3560 Training loss: 1.8448 0.5173 sec/batch\n",
      "Epoch 17/20  Iteration 2993/3560 Training loss: 1.8446 0.4525 sec/batch\n",
      "Epoch 17/20  Iteration 2994/3560 Training loss: 1.8447 0.4474 sec/batch\n",
      "Epoch 17/20  Iteration 2995/3560 Training loss: 1.8448 0.3923 sec/batch\n",
      "Epoch 17/20  Iteration 2996/3560 Training loss: 1.8449 0.4029 sec/batch\n",
      "Epoch 17/20  Iteration 2997/3560 Training loss: 1.8449 0.4061 sec/batch\n",
      "Epoch 17/20  Iteration 2998/3560 Training loss: 1.8448 0.4584 sec/batch\n",
      "Epoch 17/20  Iteration 2999/3560 Training loss: 1.8445 0.3875 sec/batch\n",
      "Epoch 17/20  Iteration 3000/3560 Training loss: 1.8447 0.4723 sec/batch\n",
      "Epoch 17/20  Iteration 3001/3560 Training loss: 1.8448 0.4152 sec/batch\n",
      "Epoch 17/20  Iteration 3002/3560 Training loss: 1.8448 0.4037 sec/batch\n",
      "Epoch 17/20  Iteration 3003/3560 Training loss: 1.8448 0.4425 sec/batch\n",
      "Epoch 17/20  Iteration 3004/3560 Training loss: 1.8447 0.3881 sec/batch\n",
      "Epoch 17/20  Iteration 3005/3560 Training loss: 1.8448 0.5203 sec/batch\n",
      "Epoch 17/20  Iteration 3006/3560 Training loss: 1.8448 0.4185 sec/batch\n",
      "Epoch 17/20  Iteration 3007/3560 Training loss: 1.8446 0.4036 sec/batch\n",
      "Epoch 17/20  Iteration 3008/3560 Training loss: 1.8448 0.5249 sec/batch\n",
      "Epoch 17/20  Iteration 3009/3560 Training loss: 1.8450 0.5134 sec/batch\n",
      "Epoch 17/20  Iteration 3010/3560 Training loss: 1.8449 0.3648 sec/batch\n",
      "Epoch 17/20  Iteration 3011/3560 Training loss: 1.8451 0.4705 sec/batch\n",
      "Epoch 17/20  Iteration 3012/3560 Training loss: 1.8452 0.4327 sec/batch\n",
      "Epoch 17/20  Iteration 3013/3560 Training loss: 1.8452 0.3971 sec/batch\n",
      "Epoch 17/20  Iteration 3014/3560 Training loss: 1.8452 0.4155 sec/batch\n",
      "Epoch 17/20  Iteration 3015/3560 Training loss: 1.8452 0.3736 sec/batch\n",
      "Epoch 17/20  Iteration 3016/3560 Training loss: 1.8456 0.4695 sec/batch\n",
      "Epoch 17/20  Iteration 3017/3560 Training loss: 1.8456 0.5289 sec/batch\n",
      "Epoch 17/20  Iteration 3018/3560 Training loss: 1.8456 0.5544 sec/batch\n",
      "Epoch 17/20  Iteration 3019/3560 Training loss: 1.8455 0.5478 sec/batch\n",
      "Epoch 17/20  Iteration 3020/3560 Training loss: 1.8453 0.5518 sec/batch\n",
      "Epoch 17/20  Iteration 3021/3560 Training loss: 1.8455 0.5094 sec/batch\n",
      "Epoch 17/20  Iteration 3022/3560 Training loss: 1.8455 0.3869 sec/batch\n",
      "Epoch 17/20  Iteration 3023/3560 Training loss: 1.8456 0.4444 sec/batch\n",
      "Epoch 17/20  Iteration 3024/3560 Training loss: 1.8455 0.5401 sec/batch\n",
      "Epoch 17/20  Iteration 3025/3560 Training loss: 1.8454 0.3901 sec/batch\n",
      "Epoch 17/20  Iteration 3026/3560 Training loss: 1.8455 0.4155 sec/batch\n",
      "Epoch 18/20  Iteration 3027/3560 Training loss: 1.9073 0.4539 sec/batch\n",
      "Epoch 18/20  Iteration 3028/3560 Training loss: 1.8717 0.4472 sec/batch\n",
      "Epoch 18/20  Iteration 3029/3560 Training loss: 1.8587 0.4224 sec/batch\n",
      "Epoch 18/20  Iteration 3030/3560 Training loss: 1.8534 0.4539 sec/batch\n",
      "Epoch 18/20  Iteration 3031/3560 Training loss: 1.8505 0.4080 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3032/3560 Training loss: 1.8415 0.4395 sec/batch\n",
      "Epoch 18/20  Iteration 3033/3560 Training loss: 1.8403 0.4025 sec/batch\n",
      "Epoch 18/20  Iteration 3034/3560 Training loss: 1.8387 0.4118 sec/batch\n",
      "Epoch 18/20  Iteration 3035/3560 Training loss: 1.8402 0.4597 sec/batch\n",
      "Epoch 18/20  Iteration 3036/3560 Training loss: 1.8399 0.4206 sec/batch\n",
      "Epoch 18/20  Iteration 3037/3560 Training loss: 1.8367 0.4155 sec/batch\n",
      "Epoch 18/20  Iteration 3038/3560 Training loss: 1.8357 0.4415 sec/batch\n",
      "Epoch 18/20  Iteration 3039/3560 Training loss: 1.8350 0.4551 sec/batch\n",
      "Epoch 18/20  Iteration 3040/3560 Training loss: 1.8375 0.3943 sec/batch\n",
      "Epoch 18/20  Iteration 3041/3560 Training loss: 1.8369 0.4139 sec/batch\n",
      "Epoch 18/20  Iteration 3042/3560 Training loss: 1.8359 0.3932 sec/batch\n",
      "Epoch 18/20  Iteration 3043/3560 Training loss: 1.8362 0.3961 sec/batch\n",
      "Epoch 18/20  Iteration 3044/3560 Training loss: 1.8384 0.4313 sec/batch\n",
      "Epoch 18/20  Iteration 3045/3560 Training loss: 1.8387 0.4208 sec/batch\n",
      "Epoch 18/20  Iteration 3046/3560 Training loss: 1.8389 0.4674 sec/batch\n",
      "Epoch 18/20  Iteration 3047/3560 Training loss: 1.8382 0.4168 sec/batch\n",
      "Epoch 18/20  Iteration 3048/3560 Training loss: 1.8392 0.4215 sec/batch\n",
      "Epoch 18/20  Iteration 3049/3560 Training loss: 1.8387 0.4331 sec/batch\n",
      "Epoch 18/20  Iteration 3050/3560 Training loss: 1.8386 0.3878 sec/batch\n",
      "Epoch 18/20  Iteration 3051/3560 Training loss: 1.8387 0.3952 sec/batch\n",
      "Epoch 18/20  Iteration 3052/3560 Training loss: 1.8379 0.3778 sec/batch\n",
      "Epoch 18/20  Iteration 3053/3560 Training loss: 1.8373 0.4772 sec/batch\n",
      "Epoch 18/20  Iteration 3054/3560 Training loss: 1.8378 0.4031 sec/batch\n",
      "Epoch 18/20  Iteration 3055/3560 Training loss: 1.8388 0.4507 sec/batch\n",
      "Epoch 18/20  Iteration 3056/3560 Training loss: 1.8394 0.3945 sec/batch\n",
      "Epoch 18/20  Iteration 3057/3560 Training loss: 1.8392 0.4292 sec/batch\n",
      "Epoch 18/20  Iteration 3058/3560 Training loss: 1.8386 0.4530 sec/batch\n",
      "Epoch 18/20  Iteration 3059/3560 Training loss: 1.8388 0.3948 sec/batch\n",
      "Epoch 18/20  Iteration 3060/3560 Training loss: 1.8397 0.3935 sec/batch\n",
      "Epoch 18/20  Iteration 3061/3560 Training loss: 1.8395 0.3639 sec/batch\n",
      "Epoch 18/20  Iteration 3062/3560 Training loss: 1.8393 0.3510 sec/batch\n",
      "Epoch 18/20  Iteration 3063/3560 Training loss: 1.8388 0.4109 sec/batch\n",
      "Epoch 18/20  Iteration 3064/3560 Training loss: 1.8376 0.4165 sec/batch\n",
      "Epoch 18/20  Iteration 3065/3560 Training loss: 1.8363 0.3975 sec/batch\n",
      "Epoch 18/20  Iteration 3066/3560 Training loss: 1.8359 0.4180 sec/batch\n",
      "Epoch 18/20  Iteration 3067/3560 Training loss: 1.8357 0.4400 sec/batch\n",
      "Epoch 18/20  Iteration 3068/3560 Training loss: 1.8360 0.4205 sec/batch\n",
      "Epoch 18/20  Iteration 3069/3560 Training loss: 1.8355 0.4427 sec/batch\n",
      "Epoch 18/20  Iteration 3070/3560 Training loss: 1.8348 0.4441 sec/batch\n",
      "Epoch 18/20  Iteration 3071/3560 Training loss: 1.8349 0.3703 sec/batch\n",
      "Epoch 18/20  Iteration 3072/3560 Training loss: 1.8338 0.3852 sec/batch\n",
      "Epoch 18/20  Iteration 3073/3560 Training loss: 1.8334 0.5072 sec/batch\n",
      "Epoch 18/20  Iteration 3074/3560 Training loss: 1.8330 0.4244 sec/batch\n",
      "Epoch 18/20  Iteration 3075/3560 Training loss: 1.8330 0.4335 sec/batch\n",
      "Epoch 18/20  Iteration 3076/3560 Training loss: 1.8339 0.4424 sec/batch\n",
      "Epoch 18/20  Iteration 3077/3560 Training loss: 1.8334 0.4212 sec/batch\n",
      "Epoch 18/20  Iteration 3078/3560 Training loss: 1.8344 0.4329 sec/batch\n",
      "Epoch 18/20  Iteration 3079/3560 Training loss: 1.8343 0.4048 sec/batch\n",
      "Epoch 18/20  Iteration 3080/3560 Training loss: 1.8343 0.4003 sec/batch\n",
      "Epoch 18/20  Iteration 3081/3560 Training loss: 1.8339 0.3939 sec/batch\n",
      "Epoch 18/20  Iteration 3082/3560 Training loss: 1.8340 0.4779 sec/batch\n",
      "Epoch 18/20  Iteration 3083/3560 Training loss: 1.8344 0.3991 sec/batch\n",
      "Epoch 18/20  Iteration 3084/3560 Training loss: 1.8340 0.4418 sec/batch\n",
      "Epoch 18/20  Iteration 3085/3560 Training loss: 1.8335 0.4562 sec/batch\n",
      "Epoch 18/20  Iteration 3086/3560 Training loss: 1.8342 0.5136 sec/batch\n",
      "Epoch 18/20  Iteration 3087/3560 Training loss: 1.8340 0.4310 sec/batch\n",
      "Epoch 18/20  Iteration 3088/3560 Training loss: 1.8348 0.5125 sec/batch\n",
      "Epoch 18/20  Iteration 3089/3560 Training loss: 1.8352 0.4134 sec/batch\n",
      "Epoch 18/20  Iteration 3090/3560 Training loss: 1.8355 0.3809 sec/batch\n",
      "Epoch 18/20  Iteration 3091/3560 Training loss: 1.8355 0.4586 sec/batch\n",
      "Epoch 18/20  Iteration 3092/3560 Training loss: 1.8359 0.4049 sec/batch\n",
      "Epoch 18/20  Iteration 3093/3560 Training loss: 1.8360 0.3812 sec/batch\n",
      "Epoch 18/20  Iteration 3094/3560 Training loss: 1.8356 0.5236 sec/batch\n",
      "Epoch 18/20  Iteration 3095/3560 Training loss: 1.8356 0.4057 sec/batch\n",
      "Epoch 18/20  Iteration 3096/3560 Training loss: 1.8356 0.4752 sec/batch\n",
      "Epoch 18/20  Iteration 3097/3560 Training loss: 1.8359 0.4299 sec/batch\n",
      "Epoch 18/20  Iteration 3098/3560 Training loss: 1.8361 0.3905 sec/batch\n",
      "Epoch 18/20  Iteration 3099/3560 Training loss: 1.8366 0.4698 sec/batch\n",
      "Epoch 18/20  Iteration 3100/3560 Training loss: 1.8363 0.4268 sec/batch\n",
      "Epoch 18/20  Iteration 3101/3560 Training loss: 1.8362 0.4944 sec/batch\n",
      "Epoch 18/20  Iteration 3102/3560 Training loss: 1.8364 0.4534 sec/batch\n",
      "Epoch 18/20  Iteration 3103/3560 Training loss: 1.8363 0.4629 sec/batch\n",
      "Epoch 18/20  Iteration 3104/3560 Training loss: 1.8365 0.4201 sec/batch\n",
      "Epoch 18/20  Iteration 3105/3560 Training loss: 1.8361 0.5287 sec/batch\n",
      "Epoch 18/20  Iteration 3106/3560 Training loss: 1.8361 0.5327 sec/batch\n",
      "Epoch 18/20  Iteration 3107/3560 Training loss: 1.8355 0.5202 sec/batch\n",
      "Epoch 18/20  Iteration 3108/3560 Training loss: 1.8358 0.4283 sec/batch\n",
      "Epoch 18/20  Iteration 3109/3560 Training loss: 1.8353 0.4986 sec/batch\n",
      "Epoch 18/20  Iteration 3110/3560 Training loss: 1.8352 0.4214 sec/batch\n",
      "Epoch 18/20  Iteration 3111/3560 Training loss: 1.8348 0.4524 sec/batch\n",
      "Epoch 18/20  Iteration 3112/3560 Training loss: 1.8345 0.4958 sec/batch\n",
      "Epoch 18/20  Iteration 3113/3560 Training loss: 1.8343 0.5746 sec/batch\n",
      "Epoch 18/20  Iteration 3114/3560 Training loss: 1.8339 0.3983 sec/batch\n",
      "Epoch 18/20  Iteration 3115/3560 Training loss: 1.8334 0.5048 sec/batch\n",
      "Epoch 18/20  Iteration 3116/3560 Training loss: 1.8336 0.4620 sec/batch\n",
      "Epoch 18/20  Iteration 3117/3560 Training loss: 1.8332 0.4909 sec/batch\n",
      "Epoch 18/20  Iteration 3118/3560 Training loss: 1.8330 0.4642 sec/batch\n",
      "Epoch 18/20  Iteration 3119/3560 Training loss: 1.8327 0.3995 sec/batch\n",
      "Epoch 18/20  Iteration 3120/3560 Training loss: 1.8324 0.3948 sec/batch\n",
      "Epoch 18/20  Iteration 3121/3560 Training loss: 1.8322 0.4374 sec/batch\n",
      "Epoch 18/20  Iteration 3122/3560 Training loss: 1.8321 0.4148 sec/batch\n",
      "Epoch 18/20  Iteration 3123/3560 Training loss: 1.8321 0.4500 sec/batch\n",
      "Epoch 18/20  Iteration 3124/3560 Training loss: 1.8316 0.4105 sec/batch\n",
      "Epoch 18/20  Iteration 3125/3560 Training loss: 1.8312 0.4279 sec/batch\n",
      "Epoch 18/20  Iteration 3126/3560 Training loss: 1.8308 0.4207 sec/batch\n",
      "Epoch 18/20  Iteration 3127/3560 Training loss: 1.8308 0.4078 sec/batch\n",
      "Epoch 18/20  Iteration 3128/3560 Training loss: 1.8308 0.3934 sec/batch\n",
      "Epoch 18/20  Iteration 3129/3560 Training loss: 1.8306 0.4822 sec/batch\n",
      "Epoch 18/20  Iteration 3130/3560 Training loss: 1.8305 0.4124 sec/batch\n",
      "Epoch 18/20  Iteration 3131/3560 Training loss: 1.8303 0.4449 sec/batch\n",
      "Epoch 18/20  Iteration 3132/3560 Training loss: 1.8302 0.4764 sec/batch\n",
      "Epoch 18/20  Iteration 3133/3560 Training loss: 1.8302 0.4294 sec/batch\n",
      "Epoch 18/20  Iteration 3134/3560 Training loss: 1.8302 0.4067 sec/batch\n",
      "Epoch 18/20  Iteration 3135/3560 Training loss: 1.8302 0.4306 sec/batch\n",
      "Epoch 18/20  Iteration 3136/3560 Training loss: 1.8302 0.4259 sec/batch\n",
      "Epoch 18/20  Iteration 3137/3560 Training loss: 1.8302 0.4075 sec/batch\n",
      "Epoch 18/20  Iteration 3138/3560 Training loss: 1.8302 0.3821 sec/batch\n",
      "Epoch 18/20  Iteration 3139/3560 Training loss: 1.8301 0.4180 sec/batch\n",
      "Epoch 18/20  Iteration 3140/3560 Training loss: 1.8300 0.4952 sec/batch\n",
      "Epoch 18/20  Iteration 3141/3560 Training loss: 1.8298 0.4582 sec/batch\n",
      "Epoch 18/20  Iteration 3142/3560 Training loss: 1.8295 0.4916 sec/batch\n",
      "Epoch 18/20  Iteration 3143/3560 Training loss: 1.8295 0.4862 sec/batch\n",
      "Epoch 18/20  Iteration 3144/3560 Training loss: 1.8295 0.5302 sec/batch\n",
      "Epoch 18/20  Iteration 3145/3560 Training loss: 1.8295 0.4094 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3146/3560 Training loss: 1.8294 0.4460 sec/batch\n",
      "Epoch 18/20  Iteration 3147/3560 Training loss: 1.8293 0.4515 sec/batch\n",
      "Epoch 18/20  Iteration 3148/3560 Training loss: 1.8289 0.3653 sec/batch\n",
      "Epoch 18/20  Iteration 3149/3560 Training loss: 1.8286 0.4252 sec/batch\n",
      "Epoch 18/20  Iteration 3150/3560 Training loss: 1.8288 0.4553 sec/batch\n",
      "Epoch 18/20  Iteration 3151/3560 Training loss: 1.8288 0.5521 sec/batch\n",
      "Epoch 18/20  Iteration 3152/3560 Training loss: 1.8284 0.3837 sec/batch\n",
      "Epoch 18/20  Iteration 3153/3560 Training loss: 1.8286 0.4244 sec/batch\n",
      "Epoch 18/20  Iteration 3154/3560 Training loss: 1.8287 0.4141 sec/batch\n",
      "Epoch 18/20  Iteration 3155/3560 Training loss: 1.8287 0.4029 sec/batch\n",
      "Epoch 18/20  Iteration 3156/3560 Training loss: 1.8286 0.3878 sec/batch\n",
      "Epoch 18/20  Iteration 3157/3560 Training loss: 1.8283 0.3732 sec/batch\n",
      "Epoch 18/20  Iteration 3158/3560 Training loss: 1.8281 0.5397 sec/batch\n",
      "Epoch 18/20  Iteration 3159/3560 Training loss: 1.8281 0.3771 sec/batch\n",
      "Epoch 18/20  Iteration 3160/3560 Training loss: 1.8281 0.4582 sec/batch\n",
      "Epoch 18/20  Iteration 3161/3560 Training loss: 1.8281 0.4642 sec/batch\n",
      "Epoch 18/20  Iteration 3162/3560 Training loss: 1.8282 0.5416 sec/batch\n",
      "Epoch 18/20  Iteration 3163/3560 Training loss: 1.8283 0.4711 sec/batch\n",
      "Epoch 18/20  Iteration 3164/3560 Training loss: 1.8283 0.4849 sec/batch\n",
      "Epoch 18/20  Iteration 3165/3560 Training loss: 1.8284 0.4961 sec/batch\n",
      "Epoch 18/20  Iteration 3166/3560 Training loss: 1.8283 0.5380 sec/batch\n",
      "Epoch 18/20  Iteration 3167/3560 Training loss: 1.8285 0.3919 sec/batch\n",
      "Epoch 18/20  Iteration 3168/3560 Training loss: 1.8284 0.4637 sec/batch\n",
      "Epoch 18/20  Iteration 3169/3560 Training loss: 1.8284 0.5701 sec/batch\n",
      "Epoch 18/20  Iteration 3170/3560 Training loss: 1.8285 0.4470 sec/batch\n",
      "Epoch 18/20  Iteration 3171/3560 Training loss: 1.8284 0.4979 sec/batch\n",
      "Epoch 18/20  Iteration 3172/3560 Training loss: 1.8285 0.5117 sec/batch\n",
      "Epoch 18/20  Iteration 3173/3560 Training loss: 1.8286 0.5655 sec/batch\n",
      "Epoch 18/20  Iteration 3174/3560 Training loss: 1.8287 0.5308 sec/batch\n",
      "Epoch 18/20  Iteration 3175/3560 Training loss: 1.8287 0.3831 sec/batch\n",
      "Epoch 18/20  Iteration 3176/3560 Training loss: 1.8285 0.4036 sec/batch\n",
      "Epoch 18/20  Iteration 3177/3560 Training loss: 1.8283 0.5033 sec/batch\n",
      "Epoch 18/20  Iteration 3178/3560 Training loss: 1.8284 0.4270 sec/batch\n",
      "Epoch 18/20  Iteration 3179/3560 Training loss: 1.8285 0.4753 sec/batch\n",
      "Epoch 18/20  Iteration 3180/3560 Training loss: 1.8286 0.4368 sec/batch\n",
      "Epoch 18/20  Iteration 3181/3560 Training loss: 1.8285 0.4867 sec/batch\n",
      "Epoch 18/20  Iteration 3182/3560 Training loss: 1.8285 0.4142 sec/batch\n",
      "Epoch 18/20  Iteration 3183/3560 Training loss: 1.8285 0.5168 sec/batch\n",
      "Epoch 18/20  Iteration 3184/3560 Training loss: 1.8285 0.4627 sec/batch\n",
      "Epoch 18/20  Iteration 3185/3560 Training loss: 1.8284 0.4261 sec/batch\n",
      "Epoch 18/20  Iteration 3186/3560 Training loss: 1.8285 0.4001 sec/batch\n",
      "Epoch 18/20  Iteration 3187/3560 Training loss: 1.8287 0.3935 sec/batch\n",
      "Epoch 18/20  Iteration 3188/3560 Training loss: 1.8287 0.4116 sec/batch\n",
      "Epoch 18/20  Iteration 3189/3560 Training loss: 1.8287 0.4153 sec/batch\n",
      "Epoch 18/20  Iteration 3190/3560 Training loss: 1.8288 0.5069 sec/batch\n",
      "Epoch 18/20  Iteration 3191/3560 Training loss: 1.8288 0.3861 sec/batch\n",
      "Epoch 18/20  Iteration 3192/3560 Training loss: 1.8287 0.4719 sec/batch\n",
      "Epoch 18/20  Iteration 3193/3560 Training loss: 1.8288 0.4292 sec/batch\n",
      "Epoch 18/20  Iteration 3194/3560 Training loss: 1.8292 0.3352 sec/batch\n",
      "Epoch 18/20  Iteration 3195/3560 Training loss: 1.8292 0.3724 sec/batch\n",
      "Epoch 18/20  Iteration 3196/3560 Training loss: 1.8291 0.4178 sec/batch\n",
      "Epoch 18/20  Iteration 3197/3560 Training loss: 1.8289 0.4177 sec/batch\n",
      "Epoch 18/20  Iteration 3198/3560 Training loss: 1.8288 0.4070 sec/batch\n",
      "Epoch 18/20  Iteration 3199/3560 Training loss: 1.8288 0.3988 sec/batch\n",
      "Epoch 18/20  Iteration 3200/3560 Training loss: 1.8289 0.4446 sec/batch\n",
      "Epoch 18/20  Iteration 3201/3560 Training loss: 1.8290 0.4037 sec/batch\n",
      "Epoch 18/20  Iteration 3202/3560 Training loss: 1.8290 0.4192 sec/batch\n",
      "Epoch 18/20  Iteration 3203/3560 Training loss: 1.8289 0.4448 sec/batch\n",
      "Epoch 18/20  Iteration 3204/3560 Training loss: 1.8290 0.5173 sec/batch\n",
      "Epoch 19/20  Iteration 3205/3560 Training loss: 1.8776 0.4547 sec/batch\n",
      "Epoch 19/20  Iteration 3206/3560 Training loss: 1.8500 0.4458 sec/batch\n",
      "Epoch 19/20  Iteration 3207/3560 Training loss: 1.8385 0.3712 sec/batch\n",
      "Epoch 19/20  Iteration 3208/3560 Training loss: 1.8334 0.4164 sec/batch\n",
      "Epoch 19/20  Iteration 3209/3560 Training loss: 1.8297 0.5086 sec/batch\n",
      "Epoch 19/20  Iteration 3210/3560 Training loss: 1.8220 0.5649 sec/batch\n",
      "Epoch 19/20  Iteration 3211/3560 Training loss: 1.8224 0.3932 sec/batch\n",
      "Epoch 19/20  Iteration 3212/3560 Training loss: 1.8213 0.4537 sec/batch\n",
      "Epoch 19/20  Iteration 3213/3560 Training loss: 1.8240 0.3951 sec/batch\n",
      "Epoch 19/20  Iteration 3214/3560 Training loss: 1.8240 0.4733 sec/batch\n",
      "Epoch 19/20  Iteration 3215/3560 Training loss: 1.8192 0.3432 sec/batch\n",
      "Epoch 19/20  Iteration 3216/3560 Training loss: 1.8178 0.3968 sec/batch\n",
      "Epoch 19/20  Iteration 3217/3560 Training loss: 1.8169 0.3977 sec/batch\n",
      "Epoch 19/20  Iteration 3218/3560 Training loss: 1.8200 0.3861 sec/batch\n",
      "Epoch 19/20  Iteration 3219/3560 Training loss: 1.8195 0.4344 sec/batch\n",
      "Epoch 19/20  Iteration 3220/3560 Training loss: 1.8187 0.4210 sec/batch\n",
      "Epoch 19/20  Iteration 3221/3560 Training loss: 1.8187 0.4981 sec/batch\n",
      "Epoch 19/20  Iteration 3222/3560 Training loss: 1.8213 0.4939 sec/batch\n",
      "Epoch 19/20  Iteration 3223/3560 Training loss: 1.8212 0.4375 sec/batch\n",
      "Epoch 19/20  Iteration 3224/3560 Training loss: 1.8221 0.4004 sec/batch\n",
      "Epoch 19/20  Iteration 3225/3560 Training loss: 1.8216 0.4024 sec/batch\n",
      "Epoch 19/20  Iteration 3226/3560 Training loss: 1.8222 0.4205 sec/batch\n",
      "Epoch 19/20  Iteration 3227/3560 Training loss: 1.8219 0.4681 sec/batch\n",
      "Epoch 19/20  Iteration 3228/3560 Training loss: 1.8217 0.3986 sec/batch\n",
      "Epoch 19/20  Iteration 3229/3560 Training loss: 1.8216 0.4067 sec/batch\n",
      "Epoch 19/20  Iteration 3230/3560 Training loss: 1.8202 0.4663 sec/batch\n",
      "Epoch 19/20  Iteration 3231/3560 Training loss: 1.8192 0.4357 sec/batch\n",
      "Epoch 19/20  Iteration 3232/3560 Training loss: 1.8197 0.4299 sec/batch\n",
      "Epoch 19/20  Iteration 3233/3560 Training loss: 1.8207 0.4127 sec/batch\n",
      "Epoch 19/20  Iteration 3234/3560 Training loss: 1.8213 0.4768 sec/batch\n",
      "Epoch 19/20  Iteration 3235/3560 Training loss: 1.8210 0.4981 sec/batch\n",
      "Epoch 19/20  Iteration 3236/3560 Training loss: 1.8203 0.3940 sec/batch\n",
      "Epoch 19/20  Iteration 3237/3560 Training loss: 1.8206 0.4693 sec/batch\n",
      "Epoch 19/20  Iteration 3238/3560 Training loss: 1.8212 0.5587 sec/batch\n",
      "Epoch 19/20  Iteration 3239/3560 Training loss: 1.8211 0.3965 sec/batch\n",
      "Epoch 19/20  Iteration 3240/3560 Training loss: 1.8208 0.4555 sec/batch\n",
      "Epoch 19/20  Iteration 3241/3560 Training loss: 1.8205 0.4717 sec/batch\n",
      "Epoch 19/20  Iteration 3242/3560 Training loss: 1.8196 0.4121 sec/batch\n",
      "Epoch 19/20  Iteration 3243/3560 Training loss: 1.8185 0.4235 sec/batch\n",
      "Epoch 19/20  Iteration 3244/3560 Training loss: 1.8179 0.3953 sec/batch\n",
      "Epoch 19/20  Iteration 3245/3560 Training loss: 1.8175 0.4136 sec/batch\n",
      "Epoch 19/20  Iteration 3246/3560 Training loss: 1.8179 0.4582 sec/batch\n",
      "Epoch 19/20  Iteration 3247/3560 Training loss: 1.8176 0.3930 sec/batch\n",
      "Epoch 19/20  Iteration 3248/3560 Training loss: 1.8170 0.4710 sec/batch\n",
      "Epoch 19/20  Iteration 3249/3560 Training loss: 1.8171 0.4294 sec/batch\n",
      "Epoch 19/20  Iteration 3250/3560 Training loss: 1.8159 0.4221 sec/batch\n",
      "Epoch 19/20  Iteration 3251/3560 Training loss: 1.8158 0.3784 sec/batch\n",
      "Epoch 19/20  Iteration 3252/3560 Training loss: 1.8156 0.4042 sec/batch\n",
      "Epoch 19/20  Iteration 3253/3560 Training loss: 1.8155 0.4118 sec/batch\n",
      "Epoch 19/20  Iteration 3254/3560 Training loss: 1.8163 0.4451 sec/batch\n",
      "Epoch 19/20  Iteration 3255/3560 Training loss: 1.8157 0.4348 sec/batch\n",
      "Epoch 19/20  Iteration 3256/3560 Training loss: 1.8167 0.5026 sec/batch\n",
      "Epoch 19/20  Iteration 3257/3560 Training loss: 1.8166 0.4757 sec/batch\n",
      "Epoch 19/20  Iteration 3258/3560 Training loss: 1.8167 0.4727 sec/batch\n",
      "Epoch 19/20  Iteration 3259/3560 Training loss: 1.8166 0.6024 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3260/3560 Training loss: 1.8167 0.4195 sec/batch\n",
      "Epoch 19/20  Iteration 3261/3560 Training loss: 1.8172 0.4555 sec/batch\n",
      "Epoch 19/20  Iteration 3262/3560 Training loss: 1.8169 0.4382 sec/batch\n",
      "Epoch 19/20  Iteration 3263/3560 Training loss: 1.8165 0.4970 sec/batch\n",
      "Epoch 19/20  Iteration 3264/3560 Training loss: 1.8171 0.4028 sec/batch\n",
      "Epoch 19/20  Iteration 3265/3560 Training loss: 1.8169 0.4254 sec/batch\n",
      "Epoch 19/20  Iteration 3266/3560 Training loss: 1.8178 0.4720 sec/batch\n",
      "Epoch 19/20  Iteration 3267/3560 Training loss: 1.8181 0.4459 sec/batch\n",
      "Epoch 19/20  Iteration 3268/3560 Training loss: 1.8184 0.4756 sec/batch\n",
      "Epoch 19/20  Iteration 3269/3560 Training loss: 1.8183 0.4029 sec/batch\n",
      "Epoch 19/20  Iteration 3270/3560 Training loss: 1.8186 0.4186 sec/batch\n",
      "Epoch 19/20  Iteration 3271/3560 Training loss: 1.8188 0.4428 sec/batch\n",
      "Epoch 19/20  Iteration 3272/3560 Training loss: 1.8185 0.5446 sec/batch\n",
      "Epoch 19/20  Iteration 3273/3560 Training loss: 1.8185 0.4617 sec/batch\n",
      "Epoch 19/20  Iteration 3274/3560 Training loss: 1.8185 0.3872 sec/batch\n",
      "Epoch 19/20  Iteration 3275/3560 Training loss: 1.8191 0.4235 sec/batch\n",
      "Epoch 19/20  Iteration 3276/3560 Training loss: 1.8192 0.3832 sec/batch\n",
      "Epoch 19/20  Iteration 3277/3560 Training loss: 1.8198 0.4024 sec/batch\n",
      "Epoch 19/20  Iteration 3278/3560 Training loss: 1.8196 0.4431 sec/batch\n",
      "Epoch 19/20  Iteration 3279/3560 Training loss: 1.8195 0.4002 sec/batch\n",
      "Epoch 19/20  Iteration 3280/3560 Training loss: 1.8197 0.4493 sec/batch\n",
      "Epoch 19/20  Iteration 3281/3560 Training loss: 1.8196 0.4594 sec/batch\n",
      "Epoch 19/20  Iteration 3282/3560 Training loss: 1.8198 0.4665 sec/batch\n",
      "Epoch 19/20  Iteration 3283/3560 Training loss: 1.8194 0.3789 sec/batch\n",
      "Epoch 19/20  Iteration 3284/3560 Training loss: 1.8193 0.4711 sec/batch\n",
      "Epoch 19/20  Iteration 3285/3560 Training loss: 1.8189 0.4269 sec/batch\n",
      "Epoch 19/20  Iteration 3286/3560 Training loss: 1.8190 0.3606 sec/batch\n",
      "Epoch 19/20  Iteration 3287/3560 Training loss: 1.8186 0.4750 sec/batch\n",
      "Epoch 19/20  Iteration 3288/3560 Training loss: 1.8185 0.7012 sec/batch\n",
      "Epoch 19/20  Iteration 3289/3560 Training loss: 1.8180 0.3424 sec/batch\n",
      "Epoch 19/20  Iteration 3290/3560 Training loss: 1.8178 0.5014 sec/batch\n",
      "Epoch 19/20  Iteration 3291/3560 Training loss: 1.8177 0.4758 sec/batch\n",
      "Epoch 19/20  Iteration 3292/3560 Training loss: 1.8173 0.3776 sec/batch\n",
      "Epoch 19/20  Iteration 3293/3560 Training loss: 1.8169 0.4300 sec/batch\n",
      "Epoch 19/20  Iteration 3294/3560 Training loss: 1.8170 0.4185 sec/batch\n",
      "Epoch 19/20  Iteration 3295/3560 Training loss: 1.8168 0.4412 sec/batch\n",
      "Epoch 19/20  Iteration 3296/3560 Training loss: 1.8166 0.4017 sec/batch\n",
      "Epoch 19/20  Iteration 3297/3560 Training loss: 1.8163 0.4315 sec/batch\n",
      "Epoch 19/20  Iteration 3298/3560 Training loss: 1.8160 0.4047 sec/batch\n",
      "Epoch 19/20  Iteration 3299/3560 Training loss: 1.8157 0.4380 sec/batch\n",
      "Epoch 19/20  Iteration 3300/3560 Training loss: 1.8157 0.4556 sec/batch\n",
      "Epoch 19/20  Iteration 3301/3560 Training loss: 1.8156 0.4357 sec/batch\n",
      "Epoch 19/20  Iteration 3302/3560 Training loss: 1.8152 0.3539 sec/batch\n",
      "Epoch 19/20  Iteration 3303/3560 Training loss: 1.8148 0.5979 sec/batch\n",
      "Epoch 19/20  Iteration 3304/3560 Training loss: 1.8144 0.4168 sec/batch\n",
      "Epoch 19/20  Iteration 3305/3560 Training loss: 1.8144 0.5032 sec/batch\n",
      "Epoch 19/20  Iteration 3306/3560 Training loss: 1.8144 0.4774 sec/batch\n",
      "Epoch 19/20  Iteration 3307/3560 Training loss: 1.8142 0.5004 sec/batch\n",
      "Epoch 19/20  Iteration 3308/3560 Training loss: 1.8141 0.4773 sec/batch\n",
      "Epoch 19/20  Iteration 3309/3560 Training loss: 1.8138 0.3652 sec/batch\n",
      "Epoch 19/20  Iteration 3310/3560 Training loss: 1.8137 0.4192 sec/batch\n",
      "Epoch 19/20  Iteration 3311/3560 Training loss: 1.8137 0.3941 sec/batch\n",
      "Epoch 19/20  Iteration 3312/3560 Training loss: 1.8137 0.4938 sec/batch\n",
      "Epoch 19/20  Iteration 3313/3560 Training loss: 1.8137 0.5369 sec/batch\n",
      "Epoch 19/20  Iteration 3314/3560 Training loss: 1.8137 0.4951 sec/batch\n",
      "Epoch 19/20  Iteration 3315/3560 Training loss: 1.8136 0.3782 sec/batch\n",
      "Epoch 19/20  Iteration 3316/3560 Training loss: 1.8135 0.4834 sec/batch\n",
      "Epoch 19/20  Iteration 3317/3560 Training loss: 1.8134 0.4790 sec/batch\n",
      "Epoch 19/20  Iteration 3318/3560 Training loss: 1.8133 0.5189 sec/batch\n",
      "Epoch 19/20  Iteration 3319/3560 Training loss: 1.8131 0.3726 sec/batch\n",
      "Epoch 19/20  Iteration 3320/3560 Training loss: 1.8128 0.3948 sec/batch\n",
      "Epoch 19/20  Iteration 3321/3560 Training loss: 1.8128 0.4008 sec/batch\n",
      "Epoch 19/20  Iteration 3322/3560 Training loss: 1.8128 0.4344 sec/batch\n",
      "Epoch 19/20  Iteration 3323/3560 Training loss: 1.8126 0.4289 sec/batch\n",
      "Epoch 19/20  Iteration 3324/3560 Training loss: 1.8126 0.4617 sec/batch\n",
      "Epoch 19/20  Iteration 3325/3560 Training loss: 1.8126 0.4634 sec/batch\n",
      "Epoch 19/20  Iteration 3326/3560 Training loss: 1.8122 0.4160 sec/batch\n",
      "Epoch 19/20  Iteration 3327/3560 Training loss: 1.8119 0.4473 sec/batch\n",
      "Epoch 19/20  Iteration 3328/3560 Training loss: 1.8121 0.3538 sec/batch\n",
      "Epoch 19/20  Iteration 3329/3560 Training loss: 1.8121 0.3930 sec/batch\n",
      "Epoch 19/20  Iteration 3330/3560 Training loss: 1.8117 0.3965 sec/batch\n",
      "Epoch 19/20  Iteration 3331/3560 Training loss: 1.8118 0.4198 sec/batch\n",
      "Epoch 19/20  Iteration 3332/3560 Training loss: 1.8119 0.3881 sec/batch\n",
      "Epoch 19/20  Iteration 3333/3560 Training loss: 1.8118 0.4138 sec/batch\n",
      "Epoch 19/20  Iteration 3334/3560 Training loss: 1.8118 0.5232 sec/batch\n",
      "Epoch 19/20  Iteration 3335/3560 Training loss: 1.8116 0.3961 sec/batch\n",
      "Epoch 19/20  Iteration 3336/3560 Training loss: 1.8114 0.4048 sec/batch\n",
      "Epoch 19/20  Iteration 3337/3560 Training loss: 1.8114 0.4158 sec/batch\n",
      "Epoch 19/20  Iteration 3338/3560 Training loss: 1.8114 0.4217 sec/batch\n",
      "Epoch 19/20  Iteration 3339/3560 Training loss: 1.8115 0.3809 sec/batch\n",
      "Epoch 19/20  Iteration 3340/3560 Training loss: 1.8115 0.3936 sec/batch\n",
      "Epoch 19/20  Iteration 3341/3560 Training loss: 1.8116 0.3850 sec/batch\n",
      "Epoch 19/20  Iteration 3342/3560 Training loss: 1.8117 0.4897 sec/batch\n",
      "Epoch 19/20  Iteration 3343/3560 Training loss: 1.8120 0.4236 sec/batch\n",
      "Epoch 19/20  Iteration 3344/3560 Training loss: 1.8119 0.4311 sec/batch\n",
      "Epoch 19/20  Iteration 3345/3560 Training loss: 1.8122 0.4068 sec/batch\n",
      "Epoch 19/20  Iteration 3346/3560 Training loss: 1.8121 0.4672 sec/batch\n",
      "Epoch 19/20  Iteration 3347/3560 Training loss: 1.8122 0.3843 sec/batch\n",
      "Epoch 19/20  Iteration 3348/3560 Training loss: 1.8122 0.3828 sec/batch\n",
      "Epoch 19/20  Iteration 3349/3560 Training loss: 1.8121 0.4039 sec/batch\n",
      "Epoch 19/20  Iteration 3350/3560 Training loss: 1.8121 0.3838 sec/batch\n",
      "Epoch 19/20  Iteration 3351/3560 Training loss: 1.8122 0.4199 sec/batch\n",
      "Epoch 19/20  Iteration 3352/3560 Training loss: 1.8124 0.4370 sec/batch\n",
      "Epoch 19/20  Iteration 3353/3560 Training loss: 1.8124 0.3802 sec/batch\n",
      "Epoch 19/20  Iteration 3354/3560 Training loss: 1.8123 0.3890 sec/batch\n",
      "Epoch 19/20  Iteration 3355/3560 Training loss: 1.8121 0.4936 sec/batch\n",
      "Epoch 19/20  Iteration 3356/3560 Training loss: 1.8122 0.3905 sec/batch\n",
      "Epoch 19/20  Iteration 3357/3560 Training loss: 1.8123 0.4806 sec/batch\n",
      "Epoch 19/20  Iteration 3358/3560 Training loss: 1.8123 0.3713 sec/batch\n",
      "Epoch 19/20  Iteration 3359/3560 Training loss: 1.8123 0.4252 sec/batch\n",
      "Epoch 19/20  Iteration 3360/3560 Training loss: 1.8123 0.4145 sec/batch\n",
      "Epoch 19/20  Iteration 3361/3560 Training loss: 1.8124 0.3966 sec/batch\n",
      "Epoch 19/20  Iteration 3362/3560 Training loss: 1.8123 0.3938 sec/batch\n",
      "Epoch 19/20  Iteration 3363/3560 Training loss: 1.8121 0.4229 sec/batch\n",
      "Epoch 19/20  Iteration 3364/3560 Training loss: 1.8123 0.4290 sec/batch\n",
      "Epoch 19/20  Iteration 3365/3560 Training loss: 1.8125 0.3946 sec/batch\n",
      "Epoch 19/20  Iteration 3366/3560 Training loss: 1.8125 0.4483 sec/batch\n",
      "Epoch 19/20  Iteration 3367/3560 Training loss: 1.8126 0.3898 sec/batch\n",
      "Epoch 19/20  Iteration 3368/3560 Training loss: 1.8126 0.3925 sec/batch\n",
      "Epoch 19/20  Iteration 3369/3560 Training loss: 1.8127 0.3968 sec/batch\n",
      "Epoch 19/20  Iteration 3370/3560 Training loss: 1.8127 0.4381 sec/batch\n",
      "Epoch 19/20  Iteration 3371/3560 Training loss: 1.8127 0.4450 sec/batch\n",
      "Epoch 19/20  Iteration 3372/3560 Training loss: 1.8131 0.4247 sec/batch\n",
      "Epoch 19/20  Iteration 3373/3560 Training loss: 1.8132 0.4643 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3374/3560 Training loss: 1.8132 0.4264 sec/batch\n",
      "Epoch 19/20  Iteration 3375/3560 Training loss: 1.8132 0.3971 sec/batch\n",
      "Epoch 19/20  Iteration 3376/3560 Training loss: 1.8131 0.4498 sec/batch\n",
      "Epoch 19/20  Iteration 3377/3560 Training loss: 1.8132 0.3861 sec/batch\n",
      "Epoch 19/20  Iteration 3378/3560 Training loss: 1.8133 0.3706 sec/batch\n",
      "Epoch 19/20  Iteration 3379/3560 Training loss: 1.8134 0.4415 sec/batch\n",
      "Epoch 19/20  Iteration 3380/3560 Training loss: 1.8133 0.4625 sec/batch\n",
      "Epoch 19/20  Iteration 3381/3560 Training loss: 1.8133 0.5214 sec/batch\n",
      "Epoch 19/20  Iteration 3382/3560 Training loss: 1.8133 0.5717 sec/batch\n",
      "Epoch 20/20  Iteration 3383/3560 Training loss: 1.8791 0.4771 sec/batch\n",
      "Epoch 20/20  Iteration 3384/3560 Training loss: 1.8430 0.5068 sec/batch\n",
      "Epoch 20/20  Iteration 3385/3560 Training loss: 1.8309 0.4454 sec/batch\n",
      "Epoch 20/20  Iteration 3386/3560 Training loss: 1.8261 0.4350 sec/batch\n",
      "Epoch 20/20  Iteration 3387/3560 Training loss: 1.8223 0.5522 sec/batch\n",
      "Epoch 20/20  Iteration 3388/3560 Training loss: 1.8152 0.4493 sec/batch\n",
      "Epoch 20/20  Iteration 3389/3560 Training loss: 1.8138 0.3455 sec/batch\n",
      "Epoch 20/20  Iteration 3390/3560 Training loss: 1.8106 0.4570 sec/batch\n",
      "Epoch 20/20  Iteration 3391/3560 Training loss: 1.8129 0.3634 sec/batch\n",
      "Epoch 20/20  Iteration 3392/3560 Training loss: 1.8127 0.4288 sec/batch\n",
      "Epoch 20/20  Iteration 3393/3560 Training loss: 1.8092 0.3977 sec/batch\n",
      "Epoch 20/20  Iteration 3394/3560 Training loss: 1.8073 0.5414 sec/batch\n",
      "Epoch 20/20  Iteration 3395/3560 Training loss: 1.8067 0.3834 sec/batch\n",
      "Epoch 20/20  Iteration 3396/3560 Training loss: 1.8085 0.5017 sec/batch\n",
      "Epoch 20/20  Iteration 3397/3560 Training loss: 1.8077 0.4551 sec/batch\n",
      "Epoch 20/20  Iteration 3398/3560 Training loss: 1.8056 0.3841 sec/batch\n",
      "Epoch 20/20  Iteration 3399/3560 Training loss: 1.8060 0.4435 sec/batch\n",
      "Epoch 20/20  Iteration 3400/3560 Training loss: 1.8080 0.4074 sec/batch\n",
      "Epoch 20/20  Iteration 3401/3560 Training loss: 1.8081 0.4664 sec/batch\n",
      "Epoch 20/20  Iteration 3402/3560 Training loss: 1.8087 0.4428 sec/batch\n",
      "Epoch 20/20  Iteration 3403/3560 Training loss: 1.8080 0.4740 sec/batch\n",
      "Epoch 20/20  Iteration 3404/3560 Training loss: 1.8085 0.4258 sec/batch\n",
      "Epoch 20/20  Iteration 3405/3560 Training loss: 1.8082 0.4315 sec/batch\n",
      "Epoch 20/20  Iteration 3406/3560 Training loss: 1.8076 0.4128 sec/batch\n",
      "Epoch 20/20  Iteration 3407/3560 Training loss: 1.8075 0.3941 sec/batch\n",
      "Epoch 20/20  Iteration 3408/3560 Training loss: 1.8064 0.4130 sec/batch\n",
      "Epoch 20/20  Iteration 3409/3560 Training loss: 1.8057 0.3934 sec/batch\n",
      "Epoch 20/20  Iteration 3410/3560 Training loss: 1.8064 0.3868 sec/batch\n",
      "Epoch 20/20  Iteration 3411/3560 Training loss: 1.8073 0.4875 sec/batch\n",
      "Epoch 20/20  Iteration 3412/3560 Training loss: 1.8075 0.4537 sec/batch\n",
      "Epoch 20/20  Iteration 3413/3560 Training loss: 1.8070 0.4175 sec/batch\n",
      "Epoch 20/20  Iteration 3414/3560 Training loss: 1.8062 0.4543 sec/batch\n",
      "Epoch 20/20  Iteration 3415/3560 Training loss: 1.8065 0.4207 sec/batch\n",
      "Epoch 20/20  Iteration 3416/3560 Training loss: 1.8073 0.4674 sec/batch\n",
      "Epoch 20/20  Iteration 3417/3560 Training loss: 1.8071 0.4389 sec/batch\n",
      "Epoch 20/20  Iteration 3418/3560 Training loss: 1.8070 0.5056 sec/batch\n",
      "Epoch 20/20  Iteration 3419/3560 Training loss: 1.8065 0.4531 sec/batch\n",
      "Epoch 20/20  Iteration 3420/3560 Training loss: 1.8057 0.4829 sec/batch\n",
      "Epoch 20/20  Iteration 3421/3560 Training loss: 1.8044 0.5260 sec/batch\n",
      "Epoch 20/20  Iteration 3422/3560 Training loss: 1.8038 0.4910 sec/batch\n",
      "Epoch 20/20  Iteration 3423/3560 Training loss: 1.8033 0.4409 sec/batch\n",
      "Epoch 20/20  Iteration 3424/3560 Training loss: 1.8038 0.4466 sec/batch\n",
      "Epoch 20/20  Iteration 3425/3560 Training loss: 1.8034 0.3943 sec/batch\n",
      "Epoch 20/20  Iteration 3426/3560 Training loss: 1.8027 0.4074 sec/batch\n",
      "Epoch 20/20  Iteration 3427/3560 Training loss: 1.8030 0.3585 sec/batch\n",
      "Epoch 20/20  Iteration 3428/3560 Training loss: 1.8021 0.4235 sec/batch\n",
      "Epoch 20/20  Iteration 3429/3560 Training loss: 1.8019 0.4414 sec/batch\n",
      "Epoch 20/20  Iteration 3430/3560 Training loss: 1.8017 0.3737 sec/batch\n",
      "Epoch 20/20  Iteration 3431/3560 Training loss: 1.8016 0.4287 sec/batch\n",
      "Epoch 20/20  Iteration 3432/3560 Training loss: 1.8024 0.4443 sec/batch\n",
      "Epoch 20/20  Iteration 3433/3560 Training loss: 1.8020 0.4077 sec/batch\n",
      "Epoch 20/20  Iteration 3434/3560 Training loss: 1.8029 0.4706 sec/batch\n",
      "Epoch 20/20  Iteration 3435/3560 Training loss: 1.8030 0.4568 sec/batch\n",
      "Epoch 20/20  Iteration 3436/3560 Training loss: 1.8028 0.4059 sec/batch\n",
      "Epoch 20/20  Iteration 3437/3560 Training loss: 1.8025 0.4236 sec/batch\n",
      "Epoch 20/20  Iteration 3438/3560 Training loss: 1.8026 0.4220 sec/batch\n",
      "Epoch 20/20  Iteration 3439/3560 Training loss: 1.8031 0.4061 sec/batch\n",
      "Epoch 20/20  Iteration 3440/3560 Training loss: 1.8027 0.4856 sec/batch\n",
      "Epoch 20/20  Iteration 3441/3560 Training loss: 1.8023 0.4085 sec/batch\n",
      "Epoch 20/20  Iteration 3442/3560 Training loss: 1.8028 0.4820 sec/batch\n",
      "Epoch 20/20  Iteration 3443/3560 Training loss: 1.8027 0.4367 sec/batch\n",
      "Epoch 20/20  Iteration 3444/3560 Training loss: 1.8036 0.4331 sec/batch\n",
      "Epoch 20/20  Iteration 3445/3560 Training loss: 1.8039 0.3973 sec/batch\n",
      "Epoch 20/20  Iteration 3446/3560 Training loss: 1.8040 0.4568 sec/batch\n",
      "Epoch 20/20  Iteration 3447/3560 Training loss: 1.8038 0.4140 sec/batch\n",
      "Epoch 20/20  Iteration 3448/3560 Training loss: 1.8040 0.3934 sec/batch\n",
      "Epoch 20/20  Iteration 3449/3560 Training loss: 1.8041 0.3824 sec/batch\n",
      "Epoch 20/20  Iteration 3450/3560 Training loss: 1.8038 0.4202 sec/batch\n",
      "Epoch 20/20  Iteration 3451/3560 Training loss: 1.8039 0.4327 sec/batch\n",
      "Epoch 20/20  Iteration 3452/3560 Training loss: 1.8037 0.4210 sec/batch\n",
      "Epoch 20/20  Iteration 3453/3560 Training loss: 1.8043 0.4625 sec/batch\n",
      "Epoch 20/20  Iteration 3454/3560 Training loss: 1.8045 0.5069 sec/batch\n",
      "Epoch 20/20  Iteration 3455/3560 Training loss: 1.8049 0.4302 sec/batch\n",
      "Epoch 20/20  Iteration 3456/3560 Training loss: 1.8048 0.4392 sec/batch\n",
      "Epoch 20/20  Iteration 3457/3560 Training loss: 1.8047 0.3631 sec/batch\n",
      "Epoch 20/20  Iteration 3458/3560 Training loss: 1.8049 0.4138 sec/batch\n",
      "Epoch 20/20  Iteration 3459/3560 Training loss: 1.8049 0.4281 sec/batch\n",
      "Epoch 20/20  Iteration 3460/3560 Training loss: 1.8052 0.4002 sec/batch\n",
      "Epoch 20/20  Iteration 3461/3560 Training loss: 1.8046 0.4353 sec/batch\n",
      "Epoch 20/20  Iteration 3462/3560 Training loss: 1.8046 0.4427 sec/batch\n",
      "Epoch 20/20  Iteration 3463/3560 Training loss: 1.8041 0.4710 sec/batch\n",
      "Epoch 20/20  Iteration 3464/3560 Training loss: 1.8041 0.3976 sec/batch\n",
      "Epoch 20/20  Iteration 3465/3560 Training loss: 1.8037 0.4601 sec/batch\n",
      "Epoch 20/20  Iteration 3466/3560 Training loss: 1.8036 0.4098 sec/batch\n",
      "Epoch 20/20  Iteration 3467/3560 Training loss: 1.8032 0.3994 sec/batch\n",
      "Epoch 20/20  Iteration 3468/3560 Training loss: 1.8029 0.4084 sec/batch\n",
      "Epoch 20/20  Iteration 3469/3560 Training loss: 1.8028 0.3783 sec/batch\n",
      "Epoch 20/20  Iteration 3470/3560 Training loss: 1.8025 0.3574 sec/batch\n",
      "Epoch 20/20  Iteration 3471/3560 Training loss: 1.8021 0.4515 sec/batch\n",
      "Epoch 20/20  Iteration 3472/3560 Training loss: 1.8023 0.4039 sec/batch\n",
      "Epoch 20/20  Iteration 3473/3560 Training loss: 1.8020 0.5369 sec/batch\n",
      "Epoch 20/20  Iteration 3474/3560 Training loss: 1.8018 0.5272 sec/batch\n",
      "Epoch 20/20  Iteration 3475/3560 Training loss: 1.8013 0.4333 sec/batch\n",
      "Epoch 20/20  Iteration 3476/3560 Training loss: 1.8011 0.4262 sec/batch\n",
      "Epoch 20/20  Iteration 3477/3560 Training loss: 1.8007 0.4024 sec/batch\n",
      "Epoch 20/20  Iteration 3478/3560 Training loss: 1.8008 0.4225 sec/batch\n",
      "Epoch 20/20  Iteration 3479/3560 Training loss: 1.8008 0.4174 sec/batch\n",
      "Epoch 20/20  Iteration 3480/3560 Training loss: 1.8004 0.4166 sec/batch\n",
      "Epoch 20/20  Iteration 3481/3560 Training loss: 1.8001 0.4269 sec/batch\n",
      "Epoch 20/20  Iteration 3482/3560 Training loss: 1.7998 0.4724 sec/batch\n",
      "Epoch 20/20  Iteration 3483/3560 Training loss: 1.7999 0.3941 sec/batch\n",
      "Epoch 20/20  Iteration 3484/3560 Training loss: 1.7999 0.3957 sec/batch\n",
      "Epoch 20/20  Iteration 3485/3560 Training loss: 1.7997 0.3897 sec/batch\n",
      "Epoch 20/20  Iteration 3486/3560 Training loss: 1.7995 0.4502 sec/batch\n",
      "Epoch 20/20  Iteration 3487/3560 Training loss: 1.7993 0.4004 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3488/3560 Training loss: 1.7992 0.4781 sec/batch\n",
      "Epoch 20/20  Iteration 3489/3560 Training loss: 1.7992 0.4132 sec/batch\n",
      "Epoch 20/20  Iteration 3490/3560 Training loss: 1.7992 0.4416 sec/batch\n",
      "Epoch 20/20  Iteration 3491/3560 Training loss: 1.7992 0.4615 sec/batch\n",
      "Epoch 20/20  Iteration 3492/3560 Training loss: 1.7993 0.4642 sec/batch\n",
      "Epoch 20/20  Iteration 3493/3560 Training loss: 1.7993 0.4010 sec/batch\n",
      "Epoch 20/20  Iteration 3494/3560 Training loss: 1.7992 0.4665 sec/batch\n",
      "Epoch 20/20  Iteration 3495/3560 Training loss: 1.7992 0.3661 sec/batch\n",
      "Epoch 20/20  Iteration 3496/3560 Training loss: 1.7991 0.4228 sec/batch\n",
      "Epoch 20/20  Iteration 3497/3560 Training loss: 1.7990 0.4043 sec/batch\n",
      "Epoch 20/20  Iteration 3498/3560 Training loss: 1.7987 0.5008 sec/batch\n",
      "Epoch 20/20  Iteration 3499/3560 Training loss: 1.7987 0.4198 sec/batch\n",
      "Epoch 20/20  Iteration 3500/3560 Training loss: 1.7986 0.4773 sec/batch\n",
      "Epoch 20/20  Iteration 3501/3560 Training loss: 1.7986 0.4096 sec/batch\n",
      "Epoch 20/20  Iteration 3502/3560 Training loss: 1.7985 0.4548 sec/batch\n",
      "Epoch 20/20  Iteration 3503/3560 Training loss: 1.7985 0.4473 sec/batch\n",
      "Epoch 20/20  Iteration 3504/3560 Training loss: 1.7981 0.3476 sec/batch\n",
      "Epoch 20/20  Iteration 3505/3560 Training loss: 1.7978 0.4299 sec/batch\n",
      "Epoch 20/20  Iteration 3506/3560 Training loss: 1.7979 0.3687 sec/batch\n",
      "Epoch 20/20  Iteration 3507/3560 Training loss: 1.7979 0.4173 sec/batch\n",
      "Epoch 20/20  Iteration 3508/3560 Training loss: 1.7975 0.4068 sec/batch\n",
      "Epoch 20/20  Iteration 3509/3560 Training loss: 1.7977 0.4749 sec/batch\n",
      "Epoch 20/20  Iteration 3510/3560 Training loss: 1.7978 0.4358 sec/batch\n",
      "Epoch 20/20  Iteration 3511/3560 Training loss: 1.7978 0.4374 sec/batch\n",
      "Epoch 20/20  Iteration 3512/3560 Training loss: 1.7978 0.3832 sec/batch\n",
      "Epoch 20/20  Iteration 3513/3560 Training loss: 1.7975 0.4382 sec/batch\n",
      "Epoch 20/20  Iteration 3514/3560 Training loss: 1.7973 0.3794 sec/batch\n",
      "Epoch 20/20  Iteration 3515/3560 Training loss: 1.7974 0.3751 sec/batch\n",
      "Epoch 20/20  Iteration 3516/3560 Training loss: 1.7973 0.4555 sec/batch\n",
      "Epoch 20/20  Iteration 3517/3560 Training loss: 1.7974 0.3942 sec/batch\n",
      "Epoch 20/20  Iteration 3518/3560 Training loss: 1.7975 0.3891 sec/batch\n",
      "Epoch 20/20  Iteration 3519/3560 Training loss: 1.7976 0.4509 sec/batch\n",
      "Epoch 20/20  Iteration 3520/3560 Training loss: 1.7976 0.4408 sec/batch\n",
      "Epoch 20/20  Iteration 3521/3560 Training loss: 1.7978 0.4177 sec/batch\n",
      "Epoch 20/20  Iteration 3522/3560 Training loss: 1.7976 0.4008 sec/batch\n",
      "Epoch 20/20  Iteration 3523/3560 Training loss: 1.7979 0.3862 sec/batch\n",
      "Epoch 20/20  Iteration 3524/3560 Training loss: 1.7979 0.4528 sec/batch\n",
      "Epoch 20/20  Iteration 3525/3560 Training loss: 1.7978 0.4121 sec/batch\n",
      "Epoch 20/20  Iteration 3526/3560 Training loss: 1.7979 0.4173 sec/batch\n",
      "Epoch 20/20  Iteration 3527/3560 Training loss: 1.7978 0.3836 sec/batch\n",
      "Epoch 20/20  Iteration 3528/3560 Training loss: 1.7979 0.3958 sec/batch\n",
      "Epoch 20/20  Iteration 3529/3560 Training loss: 1.7980 0.4632 sec/batch\n",
      "Epoch 20/20  Iteration 3530/3560 Training loss: 1.7981 0.3982 sec/batch\n",
      "Epoch 20/20  Iteration 3531/3560 Training loss: 1.7981 0.4314 sec/batch\n",
      "Epoch 20/20  Iteration 3532/3560 Training loss: 1.7980 0.4355 sec/batch\n",
      "Epoch 20/20  Iteration 3533/3560 Training loss: 1.7978 0.3987 sec/batch\n",
      "Epoch 20/20  Iteration 3534/3560 Training loss: 1.7979 0.4005 sec/batch\n",
      "Epoch 20/20  Iteration 3535/3560 Training loss: 1.7979 0.3955 sec/batch\n",
      "Epoch 20/20  Iteration 3536/3560 Training loss: 1.7981 0.4048 sec/batch\n",
      "Epoch 20/20  Iteration 3537/3560 Training loss: 1.7981 0.4768 sec/batch\n",
      "Epoch 20/20  Iteration 3538/3560 Training loss: 1.7980 0.4010 sec/batch\n",
      "Epoch 20/20  Iteration 3539/3560 Training loss: 1.7981 0.4240 sec/batch\n",
      "Epoch 20/20  Iteration 3540/3560 Training loss: 1.7980 0.3783 sec/batch\n",
      "Epoch 20/20  Iteration 3541/3560 Training loss: 1.7979 0.4511 sec/batch\n",
      "Epoch 20/20  Iteration 3542/3560 Training loss: 1.7981 0.3901 sec/batch\n",
      "Epoch 20/20  Iteration 3543/3560 Training loss: 1.7982 0.3926 sec/batch\n",
      "Epoch 20/20  Iteration 3544/3560 Training loss: 1.7982 0.3824 sec/batch\n",
      "Epoch 20/20  Iteration 3545/3560 Training loss: 1.7982 0.4435 sec/batch\n",
      "Epoch 20/20  Iteration 3546/3560 Training loss: 1.7983 0.3640 sec/batch\n",
      "Epoch 20/20  Iteration 3547/3560 Training loss: 1.7983 0.3939 sec/batch\n",
      "Epoch 20/20  Iteration 3548/3560 Training loss: 1.7983 0.4168 sec/batch\n",
      "Epoch 20/20  Iteration 3549/3560 Training loss: 1.7984 0.3889 sec/batch\n",
      "Epoch 20/20  Iteration 3550/3560 Training loss: 1.7988 0.4856 sec/batch\n",
      "Epoch 20/20  Iteration 3551/3560 Training loss: 1.7989 0.4155 sec/batch\n",
      "Epoch 20/20  Iteration 3552/3560 Training loss: 1.7988 0.4009 sec/batch\n",
      "Epoch 20/20  Iteration 3553/3560 Training loss: 1.7987 0.4340 sec/batch\n",
      "Epoch 20/20  Iteration 3554/3560 Training loss: 1.7986 0.3523 sec/batch\n",
      "Epoch 20/20  Iteration 3555/3560 Training loss: 1.7987 0.4193 sec/batch\n",
      "Epoch 20/20  Iteration 3556/3560 Training loss: 1.7987 0.4017 sec/batch\n",
      "Epoch 20/20  Iteration 3557/3560 Training loss: 1.7989 0.4263 sec/batch\n",
      "Epoch 20/20  Iteration 3558/3560 Training loss: 1.7988 0.4344 sec/batch\n",
      "Epoch 20/20  Iteration 3559/3560 Training loss: 1.7987 0.4646 sec/batch\n",
      "Epoch 20/20  Iteration 3560/3560 Training loss: 1.7988 0.3830 sec/batch\n",
      "Epoch 1/20  Iteration 1/3560 Training loss: 4.4190 0.5523 sec/batch\n",
      "Epoch 1/20  Iteration 2/3560 Training loss: 4.4039 0.6066 sec/batch\n",
      "Epoch 1/20  Iteration 3/3560 Training loss: 4.3786 0.6272 sec/batch\n",
      "Epoch 1/20  Iteration 4/3560 Training loss: 4.3190 0.6214 sec/batch\n",
      "Epoch 1/20  Iteration 5/3560 Training loss: 4.2207 0.6359 sec/batch\n",
      "Epoch 1/20  Iteration 6/3560 Training loss: 4.1251 0.6237 sec/batch\n",
      "Epoch 1/20  Iteration 7/3560 Training loss: 4.0411 0.7192 sec/batch\n",
      "Epoch 1/20  Iteration 8/3560 Training loss: 3.9712 0.6359 sec/batch\n",
      "Epoch 1/20  Iteration 9/3560 Training loss: 3.9080 0.6667 sec/batch\n",
      "Epoch 1/20  Iteration 10/3560 Training loss: 3.8537 0.5798 sec/batch\n",
      "Epoch 1/20  Iteration 11/3560 Training loss: 3.8071 0.6505 sec/batch\n",
      "Epoch 1/20  Iteration 12/3560 Training loss: 3.7682 0.5807 sec/batch\n",
      "Epoch 1/20  Iteration 13/3560 Training loss: 3.7337 0.6542 sec/batch\n",
      "Epoch 1/20  Iteration 14/3560 Training loss: 3.7042 0.6392 sec/batch\n",
      "Epoch 1/20  Iteration 15/3560 Training loss: 3.6773 0.7388 sec/batch\n",
      "Epoch 1/20  Iteration 16/3560 Training loss: 3.6528 0.5901 sec/batch\n",
      "Epoch 1/20  Iteration 17/3560 Training loss: 3.6302 0.6752 sec/batch\n",
      "Epoch 1/20  Iteration 18/3560 Training loss: 3.6110 0.6654 sec/batch\n",
      "Epoch 1/20  Iteration 19/3560 Training loss: 3.5920 0.6952 sec/batch\n",
      "Epoch 1/20  Iteration 20/3560 Training loss: 3.5730 0.6230 sec/batch\n",
      "Epoch 1/20  Iteration 21/3560 Training loss: 3.5566 0.6013 sec/batch\n",
      "Epoch 1/20  Iteration 22/3560 Training loss: 3.5416 0.6253 sec/batch\n",
      "Epoch 1/20  Iteration 23/3560 Training loss: 3.5277 0.5583 sec/batch\n",
      "Epoch 1/20  Iteration 24/3560 Training loss: 3.5149 0.6497 sec/batch\n",
      "Epoch 1/20  Iteration 25/3560 Training loss: 3.5024 0.6000 sec/batch\n",
      "Epoch 1/20  Iteration 26/3560 Training loss: 3.4915 0.6693 sec/batch\n",
      "Epoch 1/20  Iteration 27/3560 Training loss: 3.4810 0.6065 sec/batch\n",
      "Epoch 1/20  Iteration 28/3560 Training loss: 3.4704 0.7012 sec/batch\n",
      "Epoch 1/20  Iteration 29/3560 Training loss: 3.4609 0.6668 sec/batch\n",
      "Epoch 1/20  Iteration 30/3560 Training loss: 3.4520 0.6764 sec/batch\n",
      "Epoch 1/20  Iteration 31/3560 Training loss: 3.4444 0.6419 sec/batch\n",
      "Epoch 1/20  Iteration 32/3560 Training loss: 3.4364 0.6772 sec/batch\n",
      "Epoch 1/20  Iteration 33/3560 Training loss: 3.4285 0.6417 sec/batch\n",
      "Epoch 1/20  Iteration 34/3560 Training loss: 3.4218 0.6662 sec/batch\n",
      "Epoch 1/20  Iteration 35/3560 Training loss: 3.4147 0.6022 sec/batch\n",
      "Epoch 1/20  Iteration 36/3560 Training loss: 3.4085 0.6499 sec/batch\n",
      "Epoch 1/20  Iteration 37/3560 Training loss: 3.4016 0.5609 sec/batch\n",
      "Epoch 1/20  Iteration 38/3560 Training loss: 3.3952 0.6966 sec/batch\n",
      "Epoch 1/20  Iteration 39/3560 Training loss: 3.3891 0.6599 sec/batch\n",
      "Epoch 1/20  Iteration 40/3560 Training loss: 3.3833 0.6787 sec/batch\n",
      "Epoch 1/20  Iteration 41/3560 Training loss: 3.3777 0.6172 sec/batch\n",
      "Epoch 1/20  Iteration 42/3560 Training loss: 3.3724 0.6454 sec/batch\n",
      "Epoch 1/20  Iteration 43/3560 Training loss: 3.3674 0.6689 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 44/3560 Training loss: 3.3624 0.7416 sec/batch\n",
      "Epoch 1/20  Iteration 45/3560 Training loss: 3.3576 0.5823 sec/batch\n",
      "Epoch 1/20  Iteration 46/3560 Training loss: 3.3533 0.6600 sec/batch\n",
      "Epoch 1/20  Iteration 47/3560 Training loss: 3.3491 0.5892 sec/batch\n",
      "Epoch 1/20  Iteration 48/3560 Training loss: 3.3452 0.6192 sec/batch\n",
      "Epoch 1/20  Iteration 49/3560 Training loss: 3.3416 0.6201 sec/batch\n",
      "Epoch 1/20  Iteration 50/3560 Training loss: 3.3380 0.6924 sec/batch\n",
      "Epoch 1/20  Iteration 51/3560 Training loss: 3.3342 0.6407 sec/batch\n",
      "Epoch 1/20  Iteration 52/3560 Training loss: 3.3306 0.5888 sec/batch\n",
      "Epoch 1/20  Iteration 53/3560 Training loss: 3.3273 0.8104 sec/batch\n",
      "Epoch 1/20  Iteration 54/3560 Training loss: 3.3237 0.6613 sec/batch\n",
      "Epoch 1/20  Iteration 55/3560 Training loss: 3.3206 0.7480 sec/batch\n",
      "Epoch 1/20  Iteration 56/3560 Training loss: 3.3172 0.8200 sec/batch\n",
      "Epoch 1/20  Iteration 57/3560 Training loss: 3.3141 0.6724 sec/batch\n",
      "Epoch 1/20  Iteration 58/3560 Training loss: 3.3111 0.6431 sec/batch\n",
      "Epoch 1/20  Iteration 59/3560 Training loss: 3.3079 0.6015 sec/batch\n",
      "Epoch 1/20  Iteration 60/3560 Training loss: 3.3051 0.6986 sec/batch\n",
      "Epoch 1/20  Iteration 61/3560 Training loss: 3.3024 0.5444 sec/batch\n",
      "Epoch 1/20  Iteration 62/3560 Training loss: 3.3001 0.5826 sec/batch\n",
      "Epoch 1/20  Iteration 63/3560 Training loss: 3.2979 0.6988 sec/batch\n",
      "Epoch 1/20  Iteration 64/3560 Training loss: 3.2951 0.6513 sec/batch\n",
      "Epoch 1/20  Iteration 65/3560 Training loss: 3.2923 0.6502 sec/batch\n",
      "Epoch 1/20  Iteration 66/3560 Training loss: 3.2902 0.7118 sec/batch\n",
      "Epoch 1/20  Iteration 67/3560 Training loss: 3.2879 0.6777 sec/batch\n",
      "Epoch 1/20  Iteration 68/3560 Training loss: 3.2851 0.7308 sec/batch\n",
      "Epoch 1/20  Iteration 69/3560 Training loss: 3.2827 0.6269 sec/batch\n",
      "Epoch 1/20  Iteration 70/3560 Training loss: 3.2807 0.6783 sec/batch\n",
      "Epoch 1/20  Iteration 71/3560 Training loss: 3.2784 0.6309 sec/batch\n",
      "Epoch 1/20  Iteration 72/3560 Training loss: 3.2766 0.6281 sec/batch\n",
      "Epoch 1/20  Iteration 73/3560 Training loss: 3.2745 0.6085 sec/batch\n",
      "Epoch 1/20  Iteration 74/3560 Training loss: 3.2725 0.6528 sec/batch\n",
      "Epoch 1/20  Iteration 75/3560 Training loss: 3.2707 0.6208 sec/batch\n",
      "Epoch 1/20  Iteration 76/3560 Training loss: 3.2690 0.6704 sec/batch\n",
      "Epoch 1/20  Iteration 77/3560 Training loss: 3.2673 0.6779 sec/batch\n",
      "Epoch 1/20  Iteration 78/3560 Training loss: 3.2654 0.6833 sec/batch\n",
      "Epoch 1/20  Iteration 79/3560 Training loss: 3.2636 0.5811 sec/batch\n",
      "Epoch 1/20  Iteration 80/3560 Training loss: 3.2617 0.6378 sec/batch\n",
      "Epoch 1/20  Iteration 81/3560 Training loss: 3.2599 0.6660 sec/batch\n",
      "Epoch 1/20  Iteration 82/3560 Training loss: 3.2583 0.6850 sec/batch\n",
      "Epoch 1/20  Iteration 83/3560 Training loss: 3.2566 0.6576 sec/batch\n",
      "Epoch 1/20  Iteration 84/3560 Training loss: 3.2549 0.6158 sec/batch\n",
      "Epoch 1/20  Iteration 85/3560 Training loss: 3.2530 0.5582 sec/batch\n",
      "Epoch 1/20  Iteration 86/3560 Training loss: 3.2513 0.5988 sec/batch\n",
      "Epoch 1/20  Iteration 87/3560 Training loss: 3.2495 0.6884 sec/batch\n",
      "Epoch 1/20  Iteration 88/3560 Training loss: 3.2478 0.6381 sec/batch\n",
      "Epoch 1/20  Iteration 89/3560 Training loss: 3.2464 0.6516 sec/batch\n",
      "Epoch 1/20  Iteration 90/3560 Training loss: 3.2448 0.7022 sec/batch\n",
      "Epoch 1/20  Iteration 91/3560 Training loss: 3.2433 0.6406 sec/batch\n",
      "Epoch 1/20  Iteration 92/3560 Training loss: 3.2417 0.6197 sec/batch\n",
      "Epoch 1/20  Iteration 93/3560 Training loss: 3.2401 0.7452 sec/batch\n",
      "Epoch 1/20  Iteration 94/3560 Training loss: 3.2385 0.6403 sec/batch\n",
      "Epoch 1/20  Iteration 95/3560 Training loss: 3.2368 0.6436 sec/batch\n",
      "Epoch 1/20  Iteration 96/3560 Training loss: 3.2352 0.5906 sec/batch\n",
      "Epoch 1/20  Iteration 97/3560 Training loss: 3.2336 0.7217 sec/batch\n",
      "Epoch 1/20  Iteration 98/3560 Training loss: 3.2319 0.6024 sec/batch\n",
      "Epoch 1/20  Iteration 99/3560 Training loss: 3.2303 0.6053 sec/batch\n",
      "Epoch 1/20  Iteration 100/3560 Training loss: 3.2287 0.6030 sec/batch\n",
      "Epoch 1/20  Iteration 101/3560 Training loss: 3.2271 0.6506 sec/batch\n",
      "Epoch 1/20  Iteration 102/3560 Training loss: 3.2255 0.7195 sec/batch\n",
      "Epoch 1/20  Iteration 103/3560 Training loss: 3.2238 0.5843 sec/batch\n",
      "Epoch 1/20  Iteration 104/3560 Training loss: 3.2220 0.6836 sec/batch\n",
      "Epoch 1/20  Iteration 105/3560 Training loss: 3.2203 0.6221 sec/batch\n",
      "Epoch 1/20  Iteration 106/3560 Training loss: 3.2186 0.5763 sec/batch\n",
      "Epoch 1/20  Iteration 107/3560 Training loss: 3.2166 0.7260 sec/batch\n",
      "Epoch 1/20  Iteration 108/3560 Training loss: 3.2146 0.6969 sec/batch\n",
      "Epoch 1/20  Iteration 109/3560 Training loss: 3.2128 0.6721 sec/batch\n",
      "Epoch 1/20  Iteration 110/3560 Training loss: 3.2106 0.6229 sec/batch\n",
      "Epoch 1/20  Iteration 111/3560 Training loss: 3.2087 0.6454 sec/batch\n",
      "Epoch 1/20  Iteration 112/3560 Training loss: 3.2068 0.5857 sec/batch\n",
      "Epoch 1/20  Iteration 113/3560 Training loss: 3.2047 0.6390 sec/batch\n",
      "Epoch 1/20  Iteration 114/3560 Training loss: 3.2026 0.6697 sec/batch\n",
      "Epoch 1/20  Iteration 115/3560 Training loss: 3.2006 0.6777 sec/batch\n",
      "Epoch 1/20  Iteration 116/3560 Training loss: 3.1986 0.6891 sec/batch\n",
      "Epoch 1/20  Iteration 117/3560 Training loss: 3.1964 0.5874 sec/batch\n",
      "Epoch 1/20  Iteration 118/3560 Training loss: 3.1946 0.6611 sec/batch\n",
      "Epoch 1/20  Iteration 119/3560 Training loss: 3.1927 0.7148 sec/batch\n",
      "Epoch 1/20  Iteration 120/3560 Training loss: 3.1907 0.6562 sec/batch\n",
      "Epoch 1/20  Iteration 121/3560 Training loss: 3.1889 0.6284 sec/batch\n",
      "Epoch 1/20  Iteration 122/3560 Training loss: 3.1870 0.7280 sec/batch\n",
      "Epoch 1/20  Iteration 123/3560 Training loss: 3.1851 0.6149 sec/batch\n",
      "Epoch 1/20  Iteration 124/3560 Training loss: 3.1832 0.6247 sec/batch\n",
      "Epoch 1/20  Iteration 125/3560 Training loss: 3.1811 0.6623 sec/batch\n",
      "Epoch 1/20  Iteration 126/3560 Training loss: 3.1789 0.7221 sec/batch\n",
      "Epoch 1/20  Iteration 127/3560 Training loss: 3.1769 0.7074 sec/batch\n",
      "Epoch 1/20  Iteration 128/3560 Training loss: 3.1749 0.7504 sec/batch\n",
      "Epoch 1/20  Iteration 129/3560 Training loss: 3.1728 0.7530 sec/batch\n",
      "Epoch 1/20  Iteration 130/3560 Training loss: 3.1707 0.8785 sec/batch\n",
      "Epoch 1/20  Iteration 131/3560 Training loss: 3.1687 0.8786 sec/batch\n",
      "Epoch 1/20  Iteration 132/3560 Training loss: 3.1665 0.8553 sec/batch\n",
      "Epoch 1/20  Iteration 133/3560 Training loss: 3.1644 0.8332 sec/batch\n",
      "Epoch 1/20  Iteration 134/3560 Training loss: 3.1622 0.7758 sec/batch\n",
      "Epoch 1/20  Iteration 135/3560 Training loss: 3.1598 0.6250 sec/batch\n",
      "Epoch 1/20  Iteration 136/3560 Training loss: 3.1574 0.6045 sec/batch\n",
      "Epoch 1/20  Iteration 137/3560 Training loss: 3.1552 0.6303 sec/batch\n",
      "Epoch 1/20  Iteration 138/3560 Training loss: 3.1530 0.6333 sec/batch\n",
      "Epoch 1/20  Iteration 139/3560 Training loss: 3.1509 0.7756 sec/batch\n",
      "Epoch 1/20  Iteration 140/3560 Training loss: 3.1487 0.6448 sec/batch\n",
      "Epoch 1/20  Iteration 141/3560 Training loss: 3.1465 0.6663 sec/batch\n",
      "Epoch 1/20  Iteration 142/3560 Training loss: 3.1443 0.6313 sec/batch\n",
      "Epoch 1/20  Iteration 143/3560 Training loss: 3.1420 0.6376 sec/batch\n",
      "Epoch 1/20  Iteration 144/3560 Training loss: 3.1398 0.7544 sec/batch\n",
      "Epoch 1/20  Iteration 145/3560 Training loss: 3.1376 0.7281 sec/batch\n",
      "Epoch 1/20  Iteration 146/3560 Training loss: 3.1355 0.7654 sec/batch\n",
      "Epoch 1/20  Iteration 147/3560 Training loss: 3.1333 0.5995 sec/batch\n",
      "Epoch 1/20  Iteration 148/3560 Training loss: 3.1313 0.6799 sec/batch\n",
      "Epoch 1/20  Iteration 149/3560 Training loss: 3.1290 0.7599 sec/batch\n",
      "Epoch 1/20  Iteration 150/3560 Training loss: 3.1267 0.7199 sec/batch\n",
      "Epoch 1/20  Iteration 151/3560 Training loss: 3.1247 0.6442 sec/batch\n",
      "Epoch 1/20  Iteration 152/3560 Training loss: 3.1227 0.7176 sec/batch\n",
      "Epoch 1/20  Iteration 153/3560 Training loss: 3.1205 0.6370 sec/batch\n",
      "Epoch 1/20  Iteration 154/3560 Training loss: 3.1182 0.6867 sec/batch\n",
      "Epoch 1/20  Iteration 155/3560 Training loss: 3.1160 0.7225 sec/batch\n",
      "Epoch 1/20  Iteration 156/3560 Training loss: 3.1137 0.7094 sec/batch\n",
      "Epoch 1/20  Iteration 157/3560 Training loss: 3.1113 0.6570 sec/batch\n",
      "Epoch 1/20  Iteration 158/3560 Training loss: 3.1091 0.7686 sec/batch\n",
      "Epoch 1/20  Iteration 159/3560 Training loss: 3.1066 0.7192 sec/batch\n",
      "Epoch 1/20  Iteration 160/3560 Training loss: 3.1044 0.8273 sec/batch\n",
      "Epoch 1/20  Iteration 161/3560 Training loss: 3.1022 0.6800 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 162/3560 Training loss: 3.0997 0.6521 sec/batch\n",
      "Epoch 1/20  Iteration 163/3560 Training loss: 3.0973 0.6088 sec/batch\n",
      "Epoch 1/20  Iteration 164/3560 Training loss: 3.0950 0.7194 sec/batch\n",
      "Epoch 1/20  Iteration 165/3560 Training loss: 3.0928 0.8849 sec/batch\n",
      "Epoch 1/20  Iteration 166/3560 Training loss: 3.0905 0.7318 sec/batch\n",
      "Epoch 1/20  Iteration 167/3560 Training loss: 3.0883 0.7354 sec/batch\n",
      "Epoch 1/20  Iteration 168/3560 Training loss: 3.0860 0.7280 sec/batch\n",
      "Epoch 1/20  Iteration 169/3560 Training loss: 3.0838 0.6536 sec/batch\n",
      "Epoch 1/20  Iteration 170/3560 Training loss: 3.0815 0.6396 sec/batch\n",
      "Epoch 1/20  Iteration 171/3560 Training loss: 3.0793 0.6482 sec/batch\n",
      "Epoch 1/20  Iteration 172/3560 Training loss: 3.0773 0.6151 sec/batch\n",
      "Epoch 1/20  Iteration 173/3560 Training loss: 3.0753 0.6024 sec/batch\n",
      "Epoch 1/20  Iteration 174/3560 Training loss: 3.0734 0.6597 sec/batch\n",
      "Epoch 1/20  Iteration 175/3560 Training loss: 3.0714 0.6074 sec/batch\n",
      "Epoch 1/20  Iteration 176/3560 Training loss: 3.0692 0.6698 sec/batch\n",
      "Epoch 1/20  Iteration 177/3560 Training loss: 3.0670 0.7184 sec/batch\n",
      "Epoch 1/20  Iteration 178/3560 Training loss: 3.0646 0.7595 sec/batch\n",
      "Epoch 2/20  Iteration 179/3560 Training loss: 2.7055 0.7608 sec/batch\n",
      "Epoch 2/20  Iteration 180/3560 Training loss: 2.6731 0.7524 sec/batch\n",
      "Epoch 2/20  Iteration 181/3560 Training loss: 2.6663 0.7969 sec/batch\n",
      "Epoch 2/20  Iteration 182/3560 Training loss: 2.6636 0.6478 sec/batch\n",
      "Epoch 2/20  Iteration 183/3560 Training loss: 2.6602 0.8220 sec/batch\n",
      "Epoch 2/20  Iteration 184/3560 Training loss: 2.6576 0.6249 sec/batch\n",
      "Epoch 2/20  Iteration 185/3560 Training loss: 2.6581 0.7230 sec/batch\n",
      "Epoch 2/20  Iteration 186/3560 Training loss: 2.6583 0.6427 sec/batch\n",
      "Epoch 2/20  Iteration 187/3560 Training loss: 2.6581 0.7497 sec/batch\n",
      "Epoch 2/20  Iteration 188/3560 Training loss: 2.6567 0.5999 sec/batch\n",
      "Epoch 2/20  Iteration 189/3560 Training loss: 2.6553 0.6307 sec/batch\n",
      "Epoch 2/20  Iteration 190/3560 Training loss: 2.6543 0.6470 sec/batch\n",
      "Epoch 2/20  Iteration 191/3560 Training loss: 2.6530 0.6701 sec/batch\n",
      "Epoch 2/20  Iteration 192/3560 Training loss: 2.6532 0.6781 sec/batch\n",
      "Epoch 2/20  Iteration 193/3560 Training loss: 2.6525 0.5490 sec/batch\n",
      "Epoch 2/20  Iteration 194/3560 Training loss: 2.6521 0.7750 sec/batch\n",
      "Epoch 2/20  Iteration 195/3560 Training loss: 2.6511 0.5952 sec/batch\n",
      "Epoch 2/20  Iteration 196/3560 Training loss: 2.6523 0.7074 sec/batch\n",
      "Epoch 2/20  Iteration 197/3560 Training loss: 2.6515 0.5705 sec/batch\n",
      "Epoch 2/20  Iteration 198/3560 Training loss: 2.6486 0.6804 sec/batch\n",
      "Epoch 2/20  Iteration 199/3560 Training loss: 2.6473 0.5724 sec/batch\n",
      "Epoch 2/20  Iteration 200/3560 Training loss: 2.6470 0.6334 sec/batch\n",
      "Epoch 2/20  Iteration 201/3560 Training loss: 2.6460 0.6190 sec/batch\n",
      "Epoch 2/20  Iteration 202/3560 Training loss: 2.6442 0.6706 sec/batch\n",
      "Epoch 2/20  Iteration 203/3560 Training loss: 2.6423 0.6730 sec/batch\n",
      "Epoch 2/20  Iteration 204/3560 Training loss: 2.6415 0.6513 sec/batch\n",
      "Epoch 2/20  Iteration 205/3560 Training loss: 2.6403 0.6831 sec/batch\n",
      "Epoch 2/20  Iteration 206/3560 Training loss: 2.6385 0.6295 sec/batch\n",
      "Epoch 2/20  Iteration 207/3560 Training loss: 2.6375 0.6593 sec/batch\n",
      "Epoch 2/20  Iteration 208/3560 Training loss: 2.6364 0.7004 sec/batch\n",
      "Epoch 2/20  Iteration 209/3560 Training loss: 2.6357 0.6770 sec/batch\n",
      "Epoch 2/20  Iteration 210/3560 Training loss: 2.6344 0.6532 sec/batch\n",
      "Epoch 2/20  Iteration 211/3560 Training loss: 2.6324 0.6122 sec/batch\n",
      "Epoch 2/20  Iteration 212/3560 Training loss: 2.6314 0.6537 sec/batch\n",
      "Epoch 2/20  Iteration 213/3560 Training loss: 2.6296 0.5893 sec/batch\n",
      "Epoch 2/20  Iteration 214/3560 Training loss: 2.6286 0.5156 sec/batch\n",
      "Epoch 2/20  Iteration 215/3560 Training loss: 2.6272 0.7391 sec/batch\n",
      "Epoch 2/20  Iteration 216/3560 Training loss: 2.6252 0.6142 sec/batch\n",
      "Epoch 2/20  Iteration 217/3560 Training loss: 2.6237 0.6680 sec/batch\n",
      "Epoch 2/20  Iteration 218/3560 Training loss: 2.6220 0.6248 sec/batch\n",
      "Epoch 2/20  Iteration 219/3560 Training loss: 2.6203 0.6128 sec/batch\n",
      "Epoch 2/20  Iteration 220/3560 Training loss: 2.6187 0.6761 sec/batch\n",
      "Epoch 2/20  Iteration 221/3560 Training loss: 2.6172 0.6687 sec/batch\n",
      "Epoch 2/20  Iteration 222/3560 Training loss: 2.6159 0.6168 sec/batch\n",
      "Epoch 2/20  Iteration 223/3560 Training loss: 2.6143 0.5653 sec/batch\n",
      "Epoch 2/20  Iteration 224/3560 Training loss: 2.6125 0.6522 sec/batch\n",
      "Epoch 2/20  Iteration 225/3560 Training loss: 2.6115 0.6243 sec/batch\n",
      "Epoch 2/20  Iteration 226/3560 Training loss: 2.6105 0.6852 sec/batch\n",
      "Epoch 2/20  Iteration 227/3560 Training loss: 2.6095 0.6652 sec/batch\n",
      "Epoch 2/20  Iteration 228/3560 Training loss: 2.6086 0.6747 sec/batch\n",
      "Epoch 2/20  Iteration 229/3560 Training loss: 2.6073 0.7211 sec/batch\n",
      "Epoch 2/20  Iteration 230/3560 Training loss: 2.6063 0.6388 sec/batch\n",
      "Epoch 2/20  Iteration 231/3560 Training loss: 2.6049 0.6631 sec/batch\n",
      "Epoch 2/20  Iteration 232/3560 Training loss: 2.6037 0.6998 sec/batch\n",
      "Epoch 2/20  Iteration 233/3560 Training loss: 2.6024 0.6945 sec/batch\n",
      "Epoch 2/20  Iteration 234/3560 Training loss: 2.6013 0.6479 sec/batch\n",
      "Epoch 2/20  Iteration 235/3560 Training loss: 2.6001 0.5749 sec/batch\n",
      "Epoch 2/20  Iteration 236/3560 Training loss: 2.5988 0.6307 sec/batch\n",
      "Epoch 2/20  Iteration 237/3560 Training loss: 2.5976 0.6401 sec/batch\n",
      "Epoch 2/20  Iteration 238/3560 Training loss: 2.5966 0.5563 sec/batch\n",
      "Epoch 2/20  Iteration 239/3560 Training loss: 2.5955 0.6272 sec/batch\n",
      "Epoch 2/20  Iteration 240/3560 Training loss: 2.5946 0.7278 sec/batch\n",
      "Epoch 2/20  Iteration 241/3560 Training loss: 2.5938 0.6011 sec/batch\n",
      "Epoch 2/20  Iteration 242/3560 Training loss: 2.5925 0.6751 sec/batch\n",
      "Epoch 2/20  Iteration 243/3560 Training loss: 2.5911 0.5823 sec/batch\n",
      "Epoch 2/20  Iteration 244/3560 Training loss: 2.5904 0.6375 sec/batch\n",
      "Epoch 2/20  Iteration 245/3560 Training loss: 2.5895 0.7264 sec/batch\n",
      "Epoch 2/20  Iteration 246/3560 Training loss: 2.5879 0.6480 sec/batch\n",
      "Epoch 2/20  Iteration 247/3560 Training loss: 2.5866 0.6213 sec/batch\n",
      "Epoch 2/20  Iteration 248/3560 Training loss: 2.5858 0.5754 sec/batch\n",
      "Epoch 2/20  Iteration 249/3560 Training loss: 2.5847 0.6413 sec/batch\n",
      "Epoch 2/20  Iteration 250/3560 Training loss: 2.5840 0.5905 sec/batch\n",
      "Epoch 2/20  Iteration 251/3560 Training loss: 2.5830 0.5878 sec/batch\n",
      "Epoch 2/20  Iteration 252/3560 Training loss: 2.5820 0.6436 sec/batch\n",
      "Epoch 2/20  Iteration 253/3560 Training loss: 2.5811 0.6554 sec/batch\n",
      "Epoch 2/20  Iteration 254/3560 Training loss: 2.5806 0.6664 sec/batch\n",
      "Epoch 2/20  Iteration 255/3560 Training loss: 2.5794 0.7886 sec/batch\n",
      "Epoch 2/20  Iteration 256/3560 Training loss: 2.5787 0.5909 sec/batch\n",
      "Epoch 2/20  Iteration 257/3560 Training loss: 2.5777 0.6829 sec/batch\n",
      "Epoch 2/20  Iteration 258/3560 Training loss: 2.5766 0.6974 sec/batch\n",
      "Epoch 2/20  Iteration 259/3560 Training loss: 2.5754 0.6207 sec/batch\n",
      "Epoch 2/20  Iteration 260/3560 Training loss: 2.5746 0.6556 sec/batch\n",
      "Epoch 2/20  Iteration 261/3560 Training loss: 2.5736 0.6108 sec/batch\n",
      "Epoch 2/20  Iteration 262/3560 Training loss: 2.5725 0.6004 sec/batch\n",
      "Epoch 2/20  Iteration 263/3560 Training loss: 2.5712 0.6603 sec/batch\n",
      "Epoch 2/20  Iteration 264/3560 Training loss: 2.5701 0.6065 sec/batch\n",
      "Epoch 2/20  Iteration 265/3560 Training loss: 2.5692 0.6370 sec/batch\n",
      "Epoch 2/20  Iteration 266/3560 Training loss: 2.5681 0.7212 sec/batch\n",
      "Epoch 2/20  Iteration 267/3560 Training loss: 2.5672 0.6375 sec/batch\n",
      "Epoch 2/20  Iteration 268/3560 Training loss: 2.5663 0.6281 sec/batch\n",
      "Epoch 2/20  Iteration 269/3560 Training loss: 2.5654 0.6496 sec/batch\n",
      "Epoch 2/20  Iteration 270/3560 Training loss: 2.5647 0.6004 sec/batch\n",
      "Epoch 2/20  Iteration 271/3560 Training loss: 2.5638 0.7308 sec/batch\n",
      "Epoch 2/20  Iteration 272/3560 Training loss: 2.5627 0.6745 sec/batch\n",
      "Epoch 2/20  Iteration 273/3560 Training loss: 2.5616 0.5868 sec/batch\n",
      "Epoch 2/20  Iteration 274/3560 Training loss: 2.5607 0.7345 sec/batch\n",
      "Epoch 2/20  Iteration 275/3560 Training loss: 2.5599 0.5971 sec/batch\n",
      "Epoch 2/20  Iteration 276/3560 Training loss: 2.5591 0.6091 sec/batch\n",
      "Epoch 2/20  Iteration 277/3560 Training loss: 2.5582 0.5867 sec/batch\n",
      "Epoch 2/20  Iteration 278/3560 Training loss: 2.5574 0.6953 sec/batch\n",
      "Epoch 2/20  Iteration 279/3560 Training loss: 2.5567 0.6879 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 280/3560 Training loss: 2.5559 0.6031 sec/batch\n",
      "Epoch 2/20  Iteration 281/3560 Training loss: 2.5548 0.6190 sec/batch\n",
      "Epoch 2/20  Iteration 282/3560 Training loss: 2.5539 0.6472 sec/batch\n",
      "Epoch 2/20  Iteration 283/3560 Training loss: 2.5531 0.6619 sec/batch\n",
      "Epoch 2/20  Iteration 284/3560 Training loss: 2.5522 0.6833 sec/batch\n",
      "Epoch 2/20  Iteration 285/3560 Training loss: 2.5512 0.5926 sec/batch\n",
      "Epoch 2/20  Iteration 286/3560 Training loss: 2.5505 0.5941 sec/batch\n",
      "Epoch 2/20  Iteration 287/3560 Training loss: 2.5498 0.6243 sec/batch\n",
      "Epoch 2/20  Iteration 288/3560 Training loss: 2.5487 0.6642 sec/batch\n",
      "Epoch 2/20  Iteration 289/3560 Training loss: 2.5480 0.6349 sec/batch\n",
      "Epoch 2/20  Iteration 290/3560 Training loss: 2.5473 0.6440 sec/batch\n",
      "Epoch 2/20  Iteration 291/3560 Training loss: 2.5465 0.6565 sec/batch\n",
      "Epoch 2/20  Iteration 292/3560 Training loss: 2.5456 0.6843 sec/batch\n",
      "Epoch 2/20  Iteration 293/3560 Training loss: 2.5447 0.7028 sec/batch\n",
      "Epoch 2/20  Iteration 294/3560 Training loss: 2.5437 0.6854 sec/batch\n",
      "Epoch 2/20  Iteration 295/3560 Training loss: 2.5430 0.6554 sec/batch\n",
      "Epoch 2/20  Iteration 296/3560 Training loss: 2.5422 0.6691 sec/batch\n",
      "Epoch 2/20  Iteration 297/3560 Training loss: 2.5417 0.6175 sec/batch\n",
      "Epoch 2/20  Iteration 298/3560 Training loss: 2.5409 0.6445 sec/batch\n",
      "Epoch 2/20  Iteration 299/3560 Training loss: 2.5403 0.5847 sec/batch\n",
      "Epoch 2/20  Iteration 300/3560 Training loss: 2.5396 0.6435 sec/batch\n",
      "Epoch 2/20  Iteration 301/3560 Training loss: 2.5388 0.6053 sec/batch\n",
      "Epoch 2/20  Iteration 302/3560 Training loss: 2.5382 0.6826 sec/batch\n",
      "Epoch 2/20  Iteration 303/3560 Training loss: 2.5374 0.6262 sec/batch\n",
      "Epoch 2/20  Iteration 304/3560 Training loss: 2.5366 0.6895 sec/batch\n",
      "Epoch 2/20  Iteration 305/3560 Training loss: 2.5359 0.5878 sec/batch\n",
      "Epoch 2/20  Iteration 306/3560 Training loss: 2.5354 0.5172 sec/batch\n",
      "Epoch 2/20  Iteration 307/3560 Training loss: 2.5345 0.6747 sec/batch\n",
      "Epoch 2/20  Iteration 308/3560 Training loss: 2.5339 0.6304 sec/batch\n",
      "Epoch 2/20  Iteration 309/3560 Training loss: 2.5332 0.7147 sec/batch\n",
      "Epoch 2/20  Iteration 310/3560 Training loss: 2.5324 0.6539 sec/batch\n",
      "Epoch 2/20  Iteration 311/3560 Training loss: 2.5318 0.5973 sec/batch\n",
      "Epoch 2/20  Iteration 312/3560 Training loss: 2.5312 0.6075 sec/batch\n",
      "Epoch 2/20  Iteration 313/3560 Training loss: 2.5303 0.6800 sec/batch\n",
      "Epoch 2/20  Iteration 314/3560 Training loss: 2.5296 0.6312 sec/batch\n",
      "Epoch 2/20  Iteration 315/3560 Training loss: 2.5289 0.6994 sec/batch\n",
      "Epoch 2/20  Iteration 316/3560 Training loss: 2.5282 0.5938 sec/batch\n",
      "Epoch 2/20  Iteration 317/3560 Training loss: 2.5277 0.8045 sec/batch\n",
      "Epoch 2/20  Iteration 318/3560 Training loss: 2.5269 0.6328 sec/batch\n",
      "Epoch 2/20  Iteration 319/3560 Training loss: 2.5263 0.6575 sec/batch\n",
      "Epoch 2/20  Iteration 320/3560 Training loss: 2.5255 0.6229 sec/batch\n",
      "Epoch 2/20  Iteration 321/3560 Training loss: 2.5249 0.6672 sec/batch\n",
      "Epoch 2/20  Iteration 322/3560 Training loss: 2.5242 0.7127 sec/batch\n",
      "Epoch 2/20  Iteration 323/3560 Training loss: 2.5235 0.6931 sec/batch\n",
      "Epoch 2/20  Iteration 324/3560 Training loss: 2.5230 0.6107 sec/batch\n",
      "Epoch 2/20  Iteration 325/3560 Training loss: 2.5224 0.6084 sec/batch\n",
      "Epoch 2/20  Iteration 326/3560 Training loss: 2.5220 0.5929 sec/batch\n",
      "Epoch 2/20  Iteration 327/3560 Training loss: 2.5212 0.6489 sec/batch\n",
      "Epoch 2/20  Iteration 328/3560 Training loss: 2.5206 0.6257 sec/batch\n",
      "Epoch 2/20  Iteration 329/3560 Training loss: 2.5201 0.6334 sec/batch\n",
      "Epoch 2/20  Iteration 330/3560 Training loss: 2.5197 0.6579 sec/batch\n",
      "Epoch 2/20  Iteration 331/3560 Training loss: 2.5191 0.6253 sec/batch\n",
      "Epoch 2/20  Iteration 332/3560 Training loss: 2.5186 0.6276 sec/batch\n",
      "Epoch 2/20  Iteration 333/3560 Training loss: 2.5180 0.6358 sec/batch\n",
      "Epoch 2/20  Iteration 334/3560 Training loss: 2.5173 0.6359 sec/batch\n",
      "Epoch 2/20  Iteration 335/3560 Training loss: 2.5166 0.7063 sec/batch\n",
      "Epoch 2/20  Iteration 336/3560 Training loss: 2.5160 0.7449 sec/batch\n",
      "Epoch 2/20  Iteration 337/3560 Training loss: 2.5152 0.6296 sec/batch\n",
      "Epoch 2/20  Iteration 338/3560 Training loss: 2.5146 0.5877 sec/batch\n",
      "Epoch 2/20  Iteration 339/3560 Training loss: 2.5140 0.5909 sec/batch\n",
      "Epoch 2/20  Iteration 340/3560 Training loss: 2.5131 0.5887 sec/batch\n",
      "Epoch 2/20  Iteration 341/3560 Training loss: 2.5124 0.7173 sec/batch\n",
      "Epoch 2/20  Iteration 342/3560 Training loss: 2.5118 0.7290 sec/batch\n",
      "Epoch 2/20  Iteration 343/3560 Training loss: 2.5112 0.5796 sec/batch\n",
      "Epoch 2/20  Iteration 344/3560 Training loss: 2.5106 0.6878 sec/batch\n",
      "Epoch 2/20  Iteration 345/3560 Training loss: 2.5100 0.6095 sec/batch\n",
      "Epoch 2/20  Iteration 346/3560 Training loss: 2.5094 0.6180 sec/batch\n",
      "Epoch 2/20  Iteration 347/3560 Training loss: 2.5089 0.7340 sec/batch\n",
      "Epoch 2/20  Iteration 348/3560 Training loss: 2.5081 0.6334 sec/batch\n",
      "Epoch 2/20  Iteration 349/3560 Training loss: 2.5075 0.6481 sec/batch\n",
      "Epoch 2/20  Iteration 350/3560 Training loss: 2.5071 0.6871 sec/batch\n",
      "Epoch 2/20  Iteration 351/3560 Training loss: 2.5067 0.5560 sec/batch\n",
      "Epoch 2/20  Iteration 352/3560 Training loss: 2.5064 0.5791 sec/batch\n",
      "Epoch 2/20  Iteration 353/3560 Training loss: 2.5061 0.6983 sec/batch\n",
      "Epoch 2/20  Iteration 354/3560 Training loss: 2.5055 0.5728 sec/batch\n",
      "Epoch 2/20  Iteration 355/3560 Training loss: 2.5048 0.7060 sec/batch\n",
      "Epoch 2/20  Iteration 356/3560 Training loss: 2.5041 0.6477 sec/batch\n",
      "Epoch 3/20  Iteration 357/3560 Training loss: 2.4368 0.6638 sec/batch\n",
      "Epoch 3/20  Iteration 358/3560 Training loss: 2.3963 0.6188 sec/batch\n",
      "Epoch 3/20  Iteration 359/3560 Training loss: 2.3860 0.6606 sec/batch\n",
      "Epoch 3/20  Iteration 360/3560 Training loss: 2.3851 0.6651 sec/batch\n",
      "Epoch 3/20  Iteration 361/3560 Training loss: 2.3857 0.7332 sec/batch\n",
      "Epoch 3/20  Iteration 362/3560 Training loss: 2.3849 0.6654 sec/batch\n",
      "Epoch 3/20  Iteration 363/3560 Training loss: 2.3857 0.6195 sec/batch\n",
      "Epoch 3/20  Iteration 364/3560 Training loss: 2.3877 0.6215 sec/batch\n",
      "Epoch 3/20  Iteration 365/3560 Training loss: 2.3889 0.6487 sec/batch\n",
      "Epoch 3/20  Iteration 366/3560 Training loss: 2.3881 0.7401 sec/batch\n",
      "Epoch 3/20  Iteration 367/3560 Training loss: 2.3865 0.8550 sec/batch\n",
      "Epoch 3/20  Iteration 368/3560 Training loss: 2.3863 0.7749 sec/batch\n",
      "Epoch 3/20  Iteration 369/3560 Training loss: 2.3860 0.8175 sec/batch\n",
      "Epoch 3/20  Iteration 370/3560 Training loss: 2.3877 0.6772 sec/batch\n",
      "Epoch 3/20  Iteration 371/3560 Training loss: 2.3876 0.8255 sec/batch\n",
      "Epoch 3/20  Iteration 372/3560 Training loss: 2.3877 0.7110 sec/batch\n",
      "Epoch 3/20  Iteration 373/3560 Training loss: 2.3866 0.7196 sec/batch\n",
      "Epoch 3/20  Iteration 374/3560 Training loss: 2.3880 0.7287 sec/batch\n",
      "Epoch 3/20  Iteration 375/3560 Training loss: 2.3887 0.5422 sec/batch\n",
      "Epoch 3/20  Iteration 376/3560 Training loss: 2.3871 0.6943 sec/batch\n",
      "Epoch 3/20  Iteration 377/3560 Training loss: 2.3859 0.6144 sec/batch\n",
      "Epoch 3/20  Iteration 378/3560 Training loss: 2.3863 0.6837 sec/batch\n",
      "Epoch 3/20  Iteration 379/3560 Training loss: 2.3858 0.7095 sec/batch\n",
      "Epoch 3/20  Iteration 380/3560 Training loss: 2.3849 0.6477 sec/batch\n",
      "Epoch 3/20  Iteration 381/3560 Training loss: 2.3839 0.6526 sec/batch\n",
      "Epoch 3/20  Iteration 382/3560 Training loss: 2.3835 0.6049 sec/batch\n",
      "Epoch 3/20  Iteration 383/3560 Training loss: 2.3829 0.6762 sec/batch\n",
      "Epoch 3/20  Iteration 384/3560 Training loss: 2.3824 0.6978 sec/batch\n",
      "Epoch 3/20  Iteration 385/3560 Training loss: 2.3825 0.6177 sec/batch\n",
      "Epoch 3/20  Iteration 386/3560 Training loss: 2.3820 0.7555 sec/batch\n",
      "Epoch 3/20  Iteration 387/3560 Training loss: 2.3821 0.6401 sec/batch\n",
      "Epoch 3/20  Iteration 388/3560 Training loss: 2.3818 0.5824 sec/batch\n",
      "Epoch 3/20  Iteration 389/3560 Training loss: 2.3809 0.6964 sec/batch\n",
      "Epoch 3/20  Iteration 390/3560 Training loss: 2.3808 0.6130 sec/batch\n",
      "Epoch 3/20  Iteration 391/3560 Training loss: 2.3802 0.6070 sec/batch\n",
      "Epoch 3/20  Iteration 392/3560 Training loss: 2.3799 0.6466 sec/batch\n",
      "Epoch 3/20  Iteration 393/3560 Training loss: 2.3793 0.6216 sec/batch\n",
      "Epoch 3/20  Iteration 394/3560 Training loss: 2.3782 0.6358 sec/batch\n",
      "Epoch 3/20  Iteration 395/3560 Training loss: 2.3772 0.6658 sec/batch\n",
      "Epoch 3/20  Iteration 396/3560 Training loss: 2.3760 0.6424 sec/batch\n",
      "Epoch 3/20  Iteration 397/3560 Training loss: 2.3751 0.7850 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 398/3560 Training loss: 2.3742 0.6628 sec/batch\n",
      "Epoch 3/20  Iteration 399/3560 Training loss: 2.3733 0.6793 sec/batch\n",
      "Epoch 3/20  Iteration 400/3560 Training loss: 2.3725 0.5861 sec/batch\n",
      "Epoch 3/20  Iteration 401/3560 Training loss: 2.3719 0.6475 sec/batch\n",
      "Epoch 3/20  Iteration 402/3560 Training loss: 2.3706 0.6801 sec/batch\n",
      "Epoch 3/20  Iteration 403/3560 Training loss: 2.3706 0.6374 sec/batch\n",
      "Epoch 3/20  Iteration 404/3560 Training loss: 2.3699 0.6016 sec/batch\n",
      "Epoch 3/20  Iteration 405/3560 Training loss: 2.3693 0.6475 sec/batch\n",
      "Epoch 3/20  Iteration 406/3560 Training loss: 2.3692 0.6464 sec/batch\n",
      "Epoch 3/20  Iteration 407/3560 Training loss: 2.3684 0.6521 sec/batch\n",
      "Epoch 3/20  Iteration 408/3560 Training loss: 2.3680 0.5802 sec/batch\n",
      "Epoch 3/20  Iteration 409/3560 Training loss: 2.3674 0.7089 sec/batch\n",
      "Epoch 3/20  Iteration 410/3560 Training loss: 2.3669 0.6320 sec/batch\n",
      "Epoch 3/20  Iteration 411/3560 Training loss: 2.3664 0.6835 sec/batch\n",
      "Epoch 3/20  Iteration 412/3560 Training loss: 2.3658 0.6516 sec/batch\n",
      "Epoch 3/20  Iteration 413/3560 Training loss: 2.3654 0.6624 sec/batch\n",
      "Epoch 3/20  Iteration 414/3560 Training loss: 2.3649 0.5830 sec/batch\n",
      "Epoch 3/20  Iteration 415/3560 Training loss: 2.3643 0.6143 sec/batch\n",
      "Epoch 3/20  Iteration 416/3560 Training loss: 2.3640 0.6732 sec/batch\n",
      "Epoch 3/20  Iteration 417/3560 Training loss: 2.3635 0.6580 sec/batch\n",
      "Epoch 3/20  Iteration 418/3560 Training loss: 2.3631 0.6315 sec/batch\n",
      "Epoch 3/20  Iteration 419/3560 Training loss: 2.3629 0.6152 sec/batch\n",
      "Epoch 3/20  Iteration 420/3560 Training loss: 2.3623 0.6500 sec/batch\n",
      "Epoch 3/20  Iteration 421/3560 Training loss: 2.3615 0.6933 sec/batch\n",
      "Epoch 3/20  Iteration 422/3560 Training loss: 2.3614 0.6517 sec/batch\n",
      "Epoch 3/20  Iteration 423/3560 Training loss: 2.3611 0.6550 sec/batch\n",
      "Epoch 3/20  Iteration 424/3560 Training loss: 2.3603 0.6671 sec/batch\n",
      "Epoch 3/20  Iteration 425/3560 Training loss: 2.3595 0.6649 sec/batch\n",
      "Epoch 3/20  Iteration 426/3560 Training loss: 2.3593 0.6909 sec/batch\n",
      "Epoch 3/20  Iteration 427/3560 Training loss: 2.3591 0.6087 sec/batch\n",
      "Epoch 3/20  Iteration 428/3560 Training loss: 2.3589 0.6142 sec/batch\n",
      "Epoch 3/20  Iteration 429/3560 Training loss: 2.3586 0.6541 sec/batch\n",
      "Epoch 3/20  Iteration 430/3560 Training loss: 2.3581 0.6492 sec/batch\n",
      "Epoch 3/20  Iteration 431/3560 Training loss: 2.3577 0.5699 sec/batch\n",
      "Epoch 3/20  Iteration 432/3560 Training loss: 2.3577 0.7661 sec/batch\n",
      "Epoch 3/20  Iteration 433/3560 Training loss: 2.3571 0.6523 sec/batch\n",
      "Epoch 3/20  Iteration 434/3560 Training loss: 2.3570 0.6374 sec/batch\n",
      "Epoch 3/20  Iteration 435/3560 Training loss: 2.3564 0.6922 sec/batch\n",
      "Epoch 3/20  Iteration 436/3560 Training loss: 2.3560 0.6003 sec/batch\n",
      "Epoch 3/20  Iteration 437/3560 Training loss: 2.3555 0.6726 sec/batch\n",
      "Epoch 3/20  Iteration 438/3560 Training loss: 2.3552 0.6357 sec/batch\n",
      "Epoch 3/20  Iteration 439/3560 Training loss: 2.3548 0.6094 sec/batch\n",
      "Epoch 3/20  Iteration 440/3560 Training loss: 2.3542 0.6124 sec/batch\n",
      "Epoch 3/20  Iteration 441/3560 Training loss: 2.3533 0.7450 sec/batch\n",
      "Epoch 3/20  Iteration 442/3560 Training loss: 2.3528 0.6277 sec/batch\n",
      "Epoch 3/20  Iteration 443/3560 Training loss: 2.3523 0.5792 sec/batch\n",
      "Epoch 3/20  Iteration 444/3560 Training loss: 2.3519 0.7244 sec/batch\n",
      "Epoch 3/20  Iteration 445/3560 Training loss: 2.3513 0.6471 sec/batch\n",
      "Epoch 3/20  Iteration 446/3560 Training loss: 2.3510 0.6923 sec/batch\n",
      "Epoch 3/20  Iteration 447/3560 Training loss: 2.3506 0.6571 sec/batch\n",
      "Epoch 3/20  Iteration 448/3560 Training loss: 2.3503 0.7599 sec/batch\n",
      "Epoch 3/20  Iteration 449/3560 Training loss: 2.3498 0.6992 sec/batch\n",
      "Epoch 3/20  Iteration 450/3560 Training loss: 2.3492 0.8239 sec/batch\n",
      "Epoch 3/20  Iteration 451/3560 Training loss: 2.3486 0.6944 sec/batch\n",
      "Epoch 3/20  Iteration 452/3560 Training loss: 2.3481 0.6605 sec/batch\n",
      "Epoch 3/20  Iteration 453/3560 Training loss: 2.3477 0.6522 sec/batch\n",
      "Epoch 3/20  Iteration 454/3560 Training loss: 2.3473 0.6090 sec/batch\n",
      "Epoch 3/20  Iteration 455/3560 Training loss: 2.3469 0.6006 sec/batch\n",
      "Epoch 3/20  Iteration 456/3560 Training loss: 2.3464 0.6714 sec/batch\n",
      "Epoch 3/20  Iteration 457/3560 Training loss: 2.3462 0.6620 sec/batch\n",
      "Epoch 3/20  Iteration 458/3560 Training loss: 2.3458 0.6755 sec/batch\n",
      "Epoch 3/20  Iteration 459/3560 Training loss: 2.3452 0.6518 sec/batch\n",
      "Epoch 3/20  Iteration 460/3560 Training loss: 2.3448 0.6588 sec/batch\n",
      "Epoch 3/20  Iteration 461/3560 Training loss: 2.3441 0.6084 sec/batch\n",
      "Epoch 3/20  Iteration 462/3560 Training loss: 2.3438 0.6138 sec/batch\n",
      "Epoch 3/20  Iteration 463/3560 Training loss: 2.3433 0.6739 sec/batch\n",
      "Epoch 3/20  Iteration 464/3560 Training loss: 2.3429 0.5884 sec/batch\n",
      "Epoch 3/20  Iteration 465/3560 Training loss: 2.3427 0.5798 sec/batch\n",
      "Epoch 3/20  Iteration 466/3560 Training loss: 2.3421 0.6287 sec/batch\n",
      "Epoch 3/20  Iteration 467/3560 Training loss: 2.3419 0.6225 sec/batch\n",
      "Epoch 3/20  Iteration 468/3560 Training loss: 2.3417 0.6361 sec/batch\n",
      "Epoch 3/20  Iteration 469/3560 Training loss: 2.3413 0.6709 sec/batch\n",
      "Epoch 3/20  Iteration 470/3560 Training loss: 2.3408 0.6284 sec/batch\n",
      "Epoch 3/20  Iteration 471/3560 Training loss: 2.3404 0.6612 sec/batch\n",
      "Epoch 3/20  Iteration 472/3560 Training loss: 2.3398 0.6434 sec/batch\n",
      "Epoch 3/20  Iteration 473/3560 Training loss: 2.3394 0.6422 sec/batch\n",
      "Epoch 3/20  Iteration 474/3560 Training loss: 2.3390 0.7440 sec/batch\n",
      "Epoch 3/20  Iteration 475/3560 Training loss: 2.3389 0.6867 sec/batch\n",
      "Epoch 3/20  Iteration 476/3560 Training loss: 2.3386 0.6205 sec/batch\n",
      "Epoch 3/20  Iteration 477/3560 Training loss: 2.3384 0.6102 sec/batch\n",
      "Epoch 3/20  Iteration 478/3560 Training loss: 2.3380 0.6126 sec/batch\n",
      "Epoch 3/20  Iteration 479/3560 Training loss: 2.3377 0.6242 sec/batch\n",
      "Epoch 3/20  Iteration 480/3560 Training loss: 2.3375 0.5793 sec/batch\n",
      "Epoch 3/20  Iteration 481/3560 Training loss: 2.3372 0.5765 sec/batch\n",
      "Epoch 3/20  Iteration 482/3560 Training loss: 2.3366 0.6875 sec/batch\n",
      "Epoch 3/20  Iteration 483/3560 Training loss: 2.3363 0.6659 sec/batch\n",
      "Epoch 3/20  Iteration 484/3560 Training loss: 2.3362 0.7022 sec/batch\n",
      "Epoch 3/20  Iteration 485/3560 Training loss: 2.3359 0.5701 sec/batch\n",
      "Epoch 3/20  Iteration 486/3560 Training loss: 2.3356 0.7298 sec/batch\n",
      "Epoch 3/20  Iteration 487/3560 Training loss: 2.3353 0.6872 sec/batch\n",
      "Epoch 3/20  Iteration 488/3560 Training loss: 2.3348 0.6028 sec/batch\n",
      "Epoch 3/20  Iteration 489/3560 Training loss: 2.3346 0.6663 sec/batch\n",
      "Epoch 3/20  Iteration 490/3560 Training loss: 2.3344 0.6355 sec/batch\n",
      "Epoch 3/20  Iteration 491/3560 Training loss: 2.3339 0.5783 sec/batch\n",
      "Epoch 3/20  Iteration 492/3560 Training loss: 2.3336 0.6476 sec/batch\n",
      "Epoch 3/20  Iteration 493/3560 Training loss: 2.3332 0.5984 sec/batch\n",
      "Epoch 3/20  Iteration 494/3560 Training loss: 2.3329 0.5817 sec/batch\n",
      "Epoch 3/20  Iteration 495/3560 Training loss: 2.3328 0.6817 sec/batch\n",
      "Epoch 3/20  Iteration 496/3560 Training loss: 2.3323 0.6602 sec/batch\n",
      "Epoch 3/20  Iteration 497/3560 Training loss: 2.3321 0.6563 sec/batch\n",
      "Epoch 3/20  Iteration 498/3560 Training loss: 2.3317 0.7236 sec/batch\n",
      "Epoch 3/20  Iteration 499/3560 Training loss: 2.3314 0.6724 sec/batch\n",
      "Epoch 3/20  Iteration 500/3560 Training loss: 2.3310 0.6330 sec/batch\n",
      "Epoch 3/20  Iteration 501/3560 Training loss: 2.3306 0.6441 sec/batch\n",
      "Epoch 3/20  Iteration 502/3560 Training loss: 2.3304 0.6805 sec/batch\n",
      "Epoch 3/20  Iteration 503/3560 Training loss: 2.3302 0.7036 sec/batch\n",
      "Epoch 3/20  Iteration 504/3560 Training loss: 2.3300 0.5642 sec/batch\n",
      "Epoch 3/20  Iteration 505/3560 Training loss: 2.3296 0.5610 sec/batch\n",
      "Epoch 3/20  Iteration 506/3560 Training loss: 2.3292 0.6427 sec/batch\n",
      "Epoch 3/20  Iteration 507/3560 Training loss: 2.3291 0.6004 sec/batch\n",
      "Epoch 3/20  Iteration 508/3560 Training loss: 2.3290 0.6941 sec/batch\n",
      "Epoch 3/20  Iteration 509/3560 Training loss: 2.3287 0.6454 sec/batch\n",
      "Epoch 3/20  Iteration 510/3560 Training loss: 2.3286 0.6905 sec/batch\n",
      "Epoch 3/20  Iteration 511/3560 Training loss: 2.3282 0.6581 sec/batch\n",
      "Epoch 3/20  Iteration 512/3560 Training loss: 2.3279 0.7080 sec/batch\n",
      "Epoch 3/20  Iteration 513/3560 Training loss: 2.3276 0.6789 sec/batch\n",
      "Epoch 3/20  Iteration 514/3560 Training loss: 2.3273 0.5953 sec/batch\n",
      "Epoch 3/20  Iteration 515/3560 Training loss: 2.3268 0.6100 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 516/3560 Training loss: 2.3266 0.6877 sec/batch\n",
      "Epoch 3/20  Iteration 517/3560 Training loss: 2.3263 0.5925 sec/batch\n",
      "Epoch 3/20  Iteration 518/3560 Training loss: 2.3259 0.6379 sec/batch\n",
      "Epoch 3/20  Iteration 519/3560 Training loss: 2.3256 0.5812 sec/batch\n",
      "Epoch 3/20  Iteration 520/3560 Training loss: 2.3253 0.6330 sec/batch\n",
      "Epoch 3/20  Iteration 521/3560 Training loss: 2.3250 0.6123 sec/batch\n",
      "Epoch 3/20  Iteration 522/3560 Training loss: 2.3247 0.7084 sec/batch\n",
      "Epoch 3/20  Iteration 523/3560 Training loss: 2.3245 0.6292 sec/batch\n",
      "Epoch 3/20  Iteration 524/3560 Training loss: 2.3243 0.6711 sec/batch\n",
      "Epoch 3/20  Iteration 525/3560 Training loss: 2.3241 0.7004 sec/batch\n",
      "Epoch 3/20  Iteration 526/3560 Training loss: 2.3237 0.6502 sec/batch\n",
      "Epoch 3/20  Iteration 527/3560 Training loss: 2.3234 0.5764 sec/batch\n",
      "Epoch 3/20  Iteration 528/3560 Training loss: 2.3233 0.5876 sec/batch\n",
      "Epoch 3/20  Iteration 529/3560 Training loss: 2.3233 0.6203 sec/batch\n",
      "Epoch 3/20  Iteration 530/3560 Training loss: 2.3234 0.6417 sec/batch\n",
      "Epoch 3/20  Iteration 531/3560 Training loss: 2.3234 0.6205 sec/batch\n",
      "Epoch 3/20  Iteration 532/3560 Training loss: 2.3231 0.5997 sec/batch\n",
      "Epoch 3/20  Iteration 533/3560 Training loss: 2.3227 0.6160 sec/batch\n",
      "Epoch 3/20  Iteration 534/3560 Training loss: 2.3223 0.6977 sec/batch\n",
      "Epoch 4/20  Iteration 535/3560 Training loss: 2.3170 0.5765 sec/batch\n",
      "Epoch 4/20  Iteration 536/3560 Training loss: 2.2810 0.7922 sec/batch\n",
      "Epoch 4/20  Iteration 537/3560 Training loss: 2.2664 0.7093 sec/batch\n",
      "Epoch 4/20  Iteration 538/3560 Training loss: 2.2626 0.6398 sec/batch\n",
      "Epoch 4/20  Iteration 539/3560 Training loss: 2.2617 0.5957 sec/batch\n",
      "Epoch 4/20  Iteration 540/3560 Training loss: 2.2582 0.6496 sec/batch\n",
      "Epoch 4/20  Iteration 541/3560 Training loss: 2.2588 0.5343 sec/batch\n",
      "Epoch 4/20  Iteration 542/3560 Training loss: 2.2611 0.6807 sec/batch\n",
      "Epoch 4/20  Iteration 543/3560 Training loss: 2.2626 0.6102 sec/batch\n",
      "Epoch 4/20  Iteration 544/3560 Training loss: 2.2620 0.6157 sec/batch\n",
      "Epoch 4/20  Iteration 545/3560 Training loss: 2.2606 0.6146 sec/batch\n",
      "Epoch 4/20  Iteration 546/3560 Training loss: 2.2606 0.6681 sec/batch\n",
      "Epoch 4/20  Iteration 547/3560 Training loss: 2.2616 0.6401 sec/batch\n",
      "Epoch 4/20  Iteration 548/3560 Training loss: 2.2636 0.7057 sec/batch\n",
      "Epoch 4/20  Iteration 549/3560 Training loss: 2.2637 0.6627 sec/batch\n",
      "Epoch 4/20  Iteration 550/3560 Training loss: 2.2636 0.7426 sec/batch\n",
      "Epoch 4/20  Iteration 551/3560 Training loss: 2.2634 0.7230 sec/batch\n",
      "Epoch 4/20  Iteration 552/3560 Training loss: 2.2653 0.6981 sec/batch\n",
      "Epoch 4/20  Iteration 553/3560 Training loss: 2.2649 0.5989 sec/batch\n",
      "Epoch 4/20  Iteration 554/3560 Training loss: 2.2637 0.6570 sec/batch\n",
      "Epoch 4/20  Iteration 555/3560 Training loss: 2.2623 0.5945 sec/batch\n",
      "Epoch 4/20  Iteration 556/3560 Training loss: 2.2634 0.6216 sec/batch\n",
      "Epoch 4/20  Iteration 557/3560 Training loss: 2.2626 0.6159 sec/batch\n",
      "Epoch 4/20  Iteration 558/3560 Training loss: 2.2620 0.6877 sec/batch\n",
      "Epoch 4/20  Iteration 559/3560 Training loss: 2.2612 0.6845 sec/batch\n",
      "Epoch 4/20  Iteration 560/3560 Training loss: 2.2608 0.6325 sec/batch\n",
      "Epoch 4/20  Iteration 561/3560 Training loss: 2.2599 0.6536 sec/batch\n",
      "Epoch 4/20  Iteration 562/3560 Training loss: 2.2596 0.6817 sec/batch\n",
      "Epoch 4/20  Iteration 563/3560 Training loss: 2.2598 0.6971 sec/batch\n",
      "Epoch 4/20  Iteration 564/3560 Training loss: 2.2598 0.6229 sec/batch\n",
      "Epoch 4/20  Iteration 565/3560 Training loss: 2.2603 0.6179 sec/batch\n",
      "Epoch 4/20  Iteration 566/3560 Training loss: 2.2597 0.6992 sec/batch\n",
      "Epoch 4/20  Iteration 567/3560 Training loss: 2.2589 0.5622 sec/batch\n",
      "Epoch 4/20  Iteration 568/3560 Training loss: 2.2591 0.6722 sec/batch\n",
      "Epoch 4/20  Iteration 569/3560 Training loss: 2.2585 0.6545 sec/batch\n",
      "Epoch 4/20  Iteration 570/3560 Training loss: 2.2583 0.6206 sec/batch\n",
      "Epoch 4/20  Iteration 571/3560 Training loss: 2.2579 0.6526 sec/batch\n",
      "Epoch 4/20  Iteration 572/3560 Training loss: 2.2569 0.6676 sec/batch\n",
      "Epoch 4/20  Iteration 573/3560 Training loss: 2.2560 0.6040 sec/batch\n",
      "Epoch 4/20  Iteration 574/3560 Training loss: 2.2550 0.7136 sec/batch\n",
      "Epoch 4/20  Iteration 575/3560 Training loss: 2.2542 0.6870 sec/batch\n",
      "Epoch 4/20  Iteration 576/3560 Training loss: 2.2534 0.6560 sec/batch\n",
      "Epoch 4/20  Iteration 577/3560 Training loss: 2.2526 0.6700 sec/batch\n",
      "Epoch 4/20  Iteration 578/3560 Training loss: 2.2521 0.7016 sec/batch\n",
      "Epoch 4/20  Iteration 579/3560 Training loss: 2.2517 0.6059 sec/batch\n",
      "Epoch 4/20  Iteration 580/3560 Training loss: 2.2505 0.6128 sec/batch\n",
      "Epoch 4/20  Iteration 581/3560 Training loss: 2.2503 0.6255 sec/batch\n",
      "Epoch 4/20  Iteration 582/3560 Training loss: 2.2497 0.6082 sec/batch\n",
      "Epoch 4/20  Iteration 583/3560 Training loss: 2.2493 0.6512 sec/batch\n",
      "Epoch 4/20  Iteration 584/3560 Training loss: 2.2494 0.6443 sec/batch\n",
      "Epoch 4/20  Iteration 585/3560 Training loss: 2.2485 0.7443 sec/batch\n",
      "Epoch 4/20  Iteration 586/3560 Training loss: 2.2485 0.7820 sec/batch\n",
      "Epoch 4/20  Iteration 587/3560 Training loss: 2.2481 0.7729 sec/batch\n",
      "Epoch 4/20  Iteration 588/3560 Training loss: 2.2479 0.6894 sec/batch\n",
      "Epoch 4/20  Iteration 589/3560 Training loss: 2.2473 0.7339 sec/batch\n",
      "Epoch 4/20  Iteration 590/3560 Training loss: 2.2471 0.6481 sec/batch\n",
      "Epoch 4/20  Iteration 591/3560 Training loss: 2.2470 0.6441 sec/batch\n",
      "Epoch 4/20  Iteration 592/3560 Training loss: 2.2466 0.6039 sec/batch\n",
      "Epoch 4/20  Iteration 593/3560 Training loss: 2.2461 0.6399 sec/batch\n",
      "Epoch 4/20  Iteration 594/3560 Training loss: 2.2460 0.5907 sec/batch\n",
      "Epoch 4/20  Iteration 595/3560 Training loss: 2.2457 0.6750 sec/batch\n",
      "Epoch 4/20  Iteration 596/3560 Training loss: 2.2456 0.5953 sec/batch\n",
      "Epoch 4/20  Iteration 597/3560 Training loss: 2.2458 0.6178 sec/batch\n",
      "Epoch 4/20  Iteration 598/3560 Training loss: 2.2453 0.7097 sec/batch\n",
      "Epoch 4/20  Iteration 599/3560 Training loss: 2.2448 0.6628 sec/batch\n",
      "Epoch 4/20  Iteration 600/3560 Training loss: 2.2448 0.8033 sec/batch\n",
      "Epoch 4/20  Iteration 601/3560 Training loss: 2.2447 0.7507 sec/batch\n",
      "Epoch 4/20  Iteration 602/3560 Training loss: 2.2441 0.6820 sec/batch\n",
      "Epoch 4/20  Iteration 603/3560 Training loss: 2.2435 0.6322 sec/batch\n",
      "Epoch 4/20  Iteration 604/3560 Training loss: 2.2434 0.6325 sec/batch\n",
      "Epoch 4/20  Iteration 605/3560 Training loss: 2.2433 0.7089 sec/batch\n",
      "Epoch 4/20  Iteration 606/3560 Training loss: 2.2433 0.6388 sec/batch\n",
      "Epoch 4/20  Iteration 607/3560 Training loss: 2.2434 0.6328 sec/batch\n",
      "Epoch 4/20  Iteration 608/3560 Training loss: 2.2429 0.6089 sec/batch\n",
      "Epoch 4/20  Iteration 609/3560 Training loss: 2.2427 0.7054 sec/batch\n",
      "Epoch 4/20  Iteration 610/3560 Training loss: 2.2429 0.6727 sec/batch\n",
      "Epoch 4/20  Iteration 611/3560 Training loss: 2.2425 0.8546 sec/batch\n",
      "Epoch 4/20  Iteration 612/3560 Training loss: 2.2425 0.8890 sec/batch\n",
      "Epoch 4/20  Iteration 613/3560 Training loss: 2.2420 0.9743 sec/batch\n",
      "Epoch 4/20  Iteration 614/3560 Training loss: 2.2416 0.7710 sec/batch\n",
      "Epoch 4/20  Iteration 615/3560 Training loss: 2.2411 0.6991 sec/batch\n",
      "Epoch 4/20  Iteration 616/3560 Training loss: 2.2411 0.7189 sec/batch\n",
      "Epoch 4/20  Iteration 617/3560 Training loss: 2.2406 0.6461 sec/batch\n",
      "Epoch 4/20  Iteration 618/3560 Training loss: 2.2401 0.5426 sec/batch\n",
      "Epoch 4/20  Iteration 619/3560 Training loss: 2.2394 0.6990 sec/batch\n",
      "Epoch 4/20  Iteration 620/3560 Training loss: 2.2390 0.5397 sec/batch\n",
      "Epoch 4/20  Iteration 621/3560 Training loss: 2.2386 0.6763 sec/batch\n",
      "Epoch 4/20  Iteration 622/3560 Training loss: 2.2382 0.6657 sec/batch\n",
      "Epoch 4/20  Iteration 623/3560 Training loss: 2.2377 0.6404 sec/batch\n",
      "Epoch 4/20  Iteration 624/3560 Training loss: 2.2377 0.6696 sec/batch\n",
      "Epoch 4/20  Iteration 625/3560 Training loss: 2.2374 0.7235 sec/batch\n",
      "Epoch 4/20  Iteration 626/3560 Training loss: 2.2372 0.6963 sec/batch\n",
      "Epoch 4/20  Iteration 627/3560 Training loss: 2.2368 0.7191 sec/batch\n",
      "Epoch 4/20  Iteration 628/3560 Training loss: 2.2364 0.6730 sec/batch\n",
      "Epoch 4/20  Iteration 629/3560 Training loss: 2.2359 0.5248 sec/batch\n",
      "Epoch 4/20  Iteration 630/3560 Training loss: 2.2355 0.6470 sec/batch\n",
      "Epoch 4/20  Iteration 631/3560 Training loss: 2.2352 0.6500 sec/batch\n",
      "Epoch 4/20  Iteration 632/3560 Training loss: 2.2349 0.6029 sec/batch\n",
      "Epoch 4/20  Iteration 633/3560 Training loss: 2.2346 0.5011 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 634/3560 Training loss: 2.2342 0.6582 sec/batch\n",
      "Epoch 4/20  Iteration 635/3560 Training loss: 2.2341 0.7245 sec/batch\n",
      "Epoch 4/20  Iteration 636/3560 Training loss: 2.2338 0.6322 sec/batch\n",
      "Epoch 4/20  Iteration 637/3560 Training loss: 2.2334 0.6527 sec/batch\n",
      "Epoch 4/20  Iteration 638/3560 Training loss: 2.2331 0.6958 sec/batch\n",
      "Epoch 4/20  Iteration 639/3560 Training loss: 2.2327 0.7450 sec/batch\n",
      "Epoch 4/20  Iteration 640/3560 Training loss: 2.2324 0.6723 sec/batch\n",
      "Epoch 4/20  Iteration 641/3560 Training loss: 2.2321 0.7327 sec/batch\n",
      "Epoch 4/20  Iteration 642/3560 Training loss: 2.2319 0.7578 sec/batch\n",
      "Epoch 4/20  Iteration 643/3560 Training loss: 2.2319 0.5680 sec/batch\n",
      "Epoch 4/20  Iteration 644/3560 Training loss: 2.2314 0.7176 sec/batch\n",
      "Epoch 4/20  Iteration 645/3560 Training loss: 2.2312 0.6590 sec/batch\n",
      "Epoch 4/20  Iteration 646/3560 Training loss: 2.2310 0.6401 sec/batch\n",
      "Epoch 4/20  Iteration 647/3560 Training loss: 2.2307 0.7176 sec/batch\n",
      "Epoch 4/20  Iteration 648/3560 Training loss: 2.2304 0.6931 sec/batch\n",
      "Epoch 4/20  Iteration 649/3560 Training loss: 2.2302 0.6618 sec/batch\n",
      "Epoch 4/20  Iteration 650/3560 Training loss: 2.2295 0.7981 sec/batch\n",
      "Epoch 4/20  Iteration 651/3560 Training loss: 2.2293 0.7532 sec/batch\n",
      "Epoch 4/20  Iteration 652/3560 Training loss: 2.2292 0.8176 sec/batch\n",
      "Epoch 4/20  Iteration 653/3560 Training loss: 2.2290 0.6910 sec/batch\n",
      "Epoch 4/20  Iteration 654/3560 Training loss: 2.2288 0.6589 sec/batch\n",
      "Epoch 4/20  Iteration 655/3560 Training loss: 2.2287 0.7636 sec/batch\n",
      "Epoch 4/20  Iteration 656/3560 Training loss: 2.2284 0.7272 sec/batch\n",
      "Epoch 4/20  Iteration 657/3560 Training loss: 2.2281 0.6488 sec/batch\n",
      "Epoch 4/20  Iteration 658/3560 Training loss: 2.2280 0.6165 sec/batch\n",
      "Epoch 4/20  Iteration 659/3560 Training loss: 2.2277 0.6728 sec/batch\n",
      "Epoch 4/20  Iteration 660/3560 Training loss: 2.2273 0.6181 sec/batch\n",
      "Epoch 4/20  Iteration 661/3560 Training loss: 2.2271 0.7042 sec/batch\n",
      "Epoch 4/20  Iteration 662/3560 Training loss: 2.2270 0.7082 sec/batch\n",
      "Epoch 4/20  Iteration 663/3560 Training loss: 2.2268 0.6572 sec/batch\n",
      "Epoch 4/20  Iteration 664/3560 Training loss: 2.2266 0.6382 sec/batch\n",
      "Epoch 4/20  Iteration 665/3560 Training loss: 2.2264 0.7424 sec/batch\n",
      "Epoch 4/20  Iteration 666/3560 Training loss: 2.2260 0.5435 sec/batch\n",
      "Epoch 4/20  Iteration 667/3560 Training loss: 2.2259 0.7194 sec/batch\n",
      "Epoch 4/20  Iteration 668/3560 Training loss: 2.2258 0.6808 sec/batch\n",
      "Epoch 4/20  Iteration 669/3560 Training loss: 2.2255 0.6296 sec/batch\n",
      "Epoch 4/20  Iteration 670/3560 Training loss: 2.2254 0.5734 sec/batch\n",
      "Epoch 4/20  Iteration 671/3560 Training loss: 2.2251 0.7266 sec/batch\n",
      "Epoch 4/20  Iteration 672/3560 Training loss: 2.2249 0.6437 sec/batch\n",
      "Epoch 4/20  Iteration 673/3560 Training loss: 2.2249 0.6315 sec/batch\n",
      "Epoch 4/20  Iteration 674/3560 Training loss: 2.2247 0.7375 sec/batch\n",
      "Epoch 4/20  Iteration 675/3560 Training loss: 2.2246 0.7347 sec/batch\n",
      "Epoch 4/20  Iteration 676/3560 Training loss: 2.2244 0.6829 sec/batch\n",
      "Epoch 4/20  Iteration 677/3560 Training loss: 2.2242 0.6869 sec/batch\n",
      "Epoch 4/20  Iteration 678/3560 Training loss: 2.2238 0.6740 sec/batch\n",
      "Epoch 4/20  Iteration 679/3560 Training loss: 2.2235 0.5889 sec/batch\n",
      "Epoch 4/20  Iteration 680/3560 Training loss: 2.2235 0.6133 sec/batch\n",
      "Epoch 4/20  Iteration 681/3560 Training loss: 2.2232 0.6407 sec/batch\n",
      "Epoch 4/20  Iteration 682/3560 Training loss: 2.2231 0.5967 sec/batch\n",
      "Epoch 4/20  Iteration 683/3560 Training loss: 2.2229 0.5804 sec/batch\n",
      "Epoch 4/20  Iteration 684/3560 Training loss: 2.2226 0.6151 sec/batch\n",
      "Epoch 4/20  Iteration 685/3560 Training loss: 2.2224 0.7051 sec/batch\n",
      "Epoch 4/20  Iteration 686/3560 Training loss: 2.2225 0.6717 sec/batch\n",
      "Epoch 4/20  Iteration 687/3560 Training loss: 2.2223 0.7517 sec/batch\n",
      "Epoch 4/20  Iteration 688/3560 Training loss: 2.2222 0.6796 sec/batch\n",
      "Epoch 4/20  Iteration 689/3560 Training loss: 2.2220 0.7043 sec/batch\n",
      "Epoch 4/20  Iteration 690/3560 Training loss: 2.2218 0.6509 sec/batch\n",
      "Epoch 4/20  Iteration 691/3560 Training loss: 2.2216 0.6571 sec/batch\n",
      "Epoch 4/20  Iteration 692/3560 Training loss: 2.2214 0.6324 sec/batch\n",
      "Epoch 4/20  Iteration 693/3560 Training loss: 2.2211 0.5915 sec/batch\n",
      "Epoch 4/20  Iteration 694/3560 Training loss: 2.2210 0.6200 sec/batch\n",
      "Epoch 4/20  Iteration 695/3560 Training loss: 2.2209 0.5936 sec/batch\n",
      "Epoch 4/20  Iteration 696/3560 Training loss: 2.2206 0.6567 sec/batch\n",
      "Epoch 4/20  Iteration 697/3560 Training loss: 2.2204 0.6048 sec/batch\n",
      "Epoch 4/20  Iteration 698/3560 Training loss: 2.2202 0.6572 sec/batch\n",
      "Epoch 4/20  Iteration 699/3560 Training loss: 2.2201 0.6845 sec/batch\n",
      "Epoch 4/20  Iteration 700/3560 Training loss: 2.2198 0.7104 sec/batch\n",
      "Epoch 4/20  Iteration 701/3560 Training loss: 2.2197 0.7749 sec/batch\n",
      "Epoch 4/20  Iteration 702/3560 Training loss: 2.2196 0.7600 sec/batch\n",
      "Epoch 4/20  Iteration 703/3560 Training loss: 2.2194 0.5862 sec/batch\n",
      "Epoch 4/20  Iteration 704/3560 Training loss: 2.2192 0.6280 sec/batch\n",
      "Epoch 4/20  Iteration 705/3560 Training loss: 2.2190 0.6217 sec/batch\n",
      "Epoch 4/20  Iteration 706/3560 Training loss: 2.2189 0.6123 sec/batch\n",
      "Epoch 4/20  Iteration 707/3560 Training loss: 2.2188 0.6436 sec/batch\n",
      "Epoch 4/20  Iteration 708/3560 Training loss: 2.2188 0.6574 sec/batch\n",
      "Epoch 4/20  Iteration 709/3560 Training loss: 2.2188 0.6075 sec/batch\n",
      "Epoch 4/20  Iteration 710/3560 Training loss: 2.2187 0.6550 sec/batch\n",
      "Epoch 4/20  Iteration 711/3560 Training loss: 2.2183 0.6508 sec/batch\n",
      "Epoch 4/20  Iteration 712/3560 Training loss: 2.2181 0.6110 sec/batch\n",
      "Epoch 5/20  Iteration 713/3560 Training loss: 2.2418 0.6595 sec/batch\n",
      "Epoch 5/20  Iteration 714/3560 Training loss: 2.1996 0.6792 sec/batch\n",
      "Epoch 5/20  Iteration 715/3560 Training loss: 2.1855 0.7261 sec/batch\n",
      "Epoch 5/20  Iteration 716/3560 Training loss: 2.1816 0.6633 sec/batch\n",
      "Epoch 5/20  Iteration 717/3560 Training loss: 2.1806 0.6401 sec/batch\n",
      "Epoch 5/20  Iteration 718/3560 Training loss: 2.1783 0.6177 sec/batch\n",
      "Epoch 5/20  Iteration 719/3560 Training loss: 2.1799 0.6199 sec/batch\n",
      "Epoch 5/20  Iteration 720/3560 Training loss: 2.1785 0.6514 sec/batch\n",
      "Epoch 5/20  Iteration 721/3560 Training loss: 2.1805 0.6806 sec/batch\n",
      "Epoch 5/20  Iteration 722/3560 Training loss: 2.1795 0.5915 sec/batch\n",
      "Epoch 5/20  Iteration 723/3560 Training loss: 2.1793 0.6683 sec/batch\n",
      "Epoch 5/20  Iteration 724/3560 Training loss: 2.1781 0.7196 sec/batch\n",
      "Epoch 5/20  Iteration 725/3560 Training loss: 2.1786 0.6299 sec/batch\n",
      "Epoch 5/20  Iteration 726/3560 Training loss: 2.1809 0.7471 sec/batch\n",
      "Epoch 5/20  Iteration 727/3560 Training loss: 2.1806 0.6139 sec/batch\n",
      "Epoch 5/20  Iteration 728/3560 Training loss: 2.1802 0.6992 sec/batch\n",
      "Epoch 5/20  Iteration 729/3560 Training loss: 2.1800 0.6729 sec/batch\n",
      "Epoch 5/20  Iteration 730/3560 Training loss: 2.1820 0.6102 sec/batch\n",
      "Epoch 5/20  Iteration 731/3560 Training loss: 2.1821 0.6284 sec/batch\n",
      "Epoch 5/20  Iteration 732/3560 Training loss: 2.1818 0.6387 sec/batch\n",
      "Epoch 5/20  Iteration 733/3560 Training loss: 2.1808 0.5966 sec/batch\n",
      "Epoch 5/20  Iteration 734/3560 Training loss: 2.1823 0.6645 sec/batch\n",
      "Epoch 5/20  Iteration 735/3560 Training loss: 2.1818 0.6172 sec/batch\n",
      "Epoch 5/20  Iteration 736/3560 Training loss: 2.1805 0.7022 sec/batch\n",
      "Epoch 5/20  Iteration 737/3560 Training loss: 2.1800 0.6427 sec/batch\n",
      "Epoch 5/20  Iteration 738/3560 Training loss: 2.1798 0.6881 sec/batch\n",
      "Epoch 5/20  Iteration 739/3560 Training loss: 2.1788 0.6864 sec/batch\n",
      "Epoch 5/20  Iteration 740/3560 Training loss: 2.1786 0.6570 sec/batch\n",
      "Epoch 5/20  Iteration 741/3560 Training loss: 2.1790 0.7178 sec/batch\n",
      "Epoch 5/20  Iteration 742/3560 Training loss: 2.1793 0.6094 sec/batch\n",
      "Epoch 5/20  Iteration 743/3560 Training loss: 2.1796 0.6677 sec/batch\n",
      "Epoch 5/20  Iteration 744/3560 Training loss: 2.1792 0.6280 sec/batch\n",
      "Epoch 5/20  Iteration 745/3560 Training loss: 2.1787 0.6120 sec/batch\n",
      "Epoch 5/20  Iteration 746/3560 Training loss: 2.1793 0.6921 sec/batch\n",
      "Epoch 5/20  Iteration 747/3560 Training loss: 2.1789 0.6002 sec/batch\n",
      "Epoch 5/20  Iteration 748/3560 Training loss: 2.1786 0.6668 sec/batch\n",
      "Epoch 5/20  Iteration 749/3560 Training loss: 2.1784 0.6892 sec/batch\n",
      "Epoch 5/20  Iteration 750/3560 Training loss: 2.1773 0.6140 sec/batch\n",
      "Epoch 5/20  Iteration 751/3560 Training loss: 2.1764 0.7010 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 752/3560 Training loss: 2.1755 0.6997 sec/batch\n",
      "Epoch 5/20  Iteration 753/3560 Training loss: 2.1748 0.6270 sec/batch\n",
      "Epoch 5/20  Iteration 754/3560 Training loss: 2.1743 0.6966 sec/batch\n",
      "Epoch 5/20  Iteration 755/3560 Training loss: 2.1737 0.5991 sec/batch\n",
      "Epoch 5/20  Iteration 756/3560 Training loss: 2.1729 0.5932 sec/batch\n",
      "Epoch 5/20  Iteration 757/3560 Training loss: 2.1725 0.6270 sec/batch\n",
      "Epoch 5/20  Iteration 758/3560 Training loss: 2.1712 0.6337 sec/batch\n",
      "Epoch 5/20  Iteration 759/3560 Training loss: 2.1711 0.6394 sec/batch\n",
      "Epoch 5/20  Iteration 760/3560 Training loss: 2.1708 0.7013 sec/batch\n",
      "Epoch 5/20  Iteration 761/3560 Training loss: 2.1706 0.6321 sec/batch\n",
      "Epoch 5/20  Iteration 762/3560 Training loss: 2.1709 0.6981 sec/batch\n",
      "Epoch 5/20  Iteration 763/3560 Training loss: 2.1702 0.6505 sec/batch\n",
      "Epoch 5/20  Iteration 764/3560 Training loss: 2.1705 0.6717 sec/batch\n",
      "Epoch 5/20  Iteration 765/3560 Training loss: 2.1705 0.6855 sec/batch\n",
      "Epoch 5/20  Iteration 766/3560 Training loss: 2.1699 0.6433 sec/batch\n",
      "Epoch 5/20  Iteration 767/3560 Training loss: 2.1693 0.6259 sec/batch\n",
      "Epoch 5/20  Iteration 768/3560 Training loss: 2.1693 0.6383 sec/batch\n",
      "Epoch 5/20  Iteration 769/3560 Training loss: 2.1690 0.6124 sec/batch\n",
      "Epoch 5/20  Iteration 770/3560 Training loss: 2.1685 0.5964 sec/batch\n",
      "Epoch 5/20  Iteration 771/3560 Training loss: 2.1681 0.6090 sec/batch\n",
      "Epoch 5/20  Iteration 772/3560 Training loss: 2.1680 0.7000 sec/batch\n",
      "Epoch 5/20  Iteration 773/3560 Training loss: 2.1678 0.6096 sec/batch\n",
      "Epoch 5/20  Iteration 774/3560 Training loss: 2.1680 0.6520 sec/batch\n",
      "Epoch 5/20  Iteration 775/3560 Training loss: 2.1681 0.6876 sec/batch\n",
      "Epoch 5/20  Iteration 776/3560 Training loss: 2.1677 0.6682 sec/batch\n",
      "Epoch 5/20  Iteration 777/3560 Training loss: 2.1672 0.7291 sec/batch\n",
      "Epoch 5/20  Iteration 778/3560 Training loss: 2.1674 0.6729 sec/batch\n",
      "Epoch 5/20  Iteration 779/3560 Training loss: 2.1673 0.6466 sec/batch\n",
      "Epoch 5/20  Iteration 780/3560 Training loss: 2.1664 0.6511 sec/batch\n",
      "Epoch 5/20  Iteration 781/3560 Training loss: 2.1659 0.6403 sec/batch\n",
      "Epoch 5/20  Iteration 782/3560 Training loss: 2.1656 0.6007 sec/batch\n",
      "Epoch 5/20  Iteration 783/3560 Training loss: 2.1656 0.6050 sec/batch\n",
      "Epoch 5/20  Iteration 784/3560 Training loss: 2.1656 0.5940 sec/batch\n",
      "Epoch 5/20  Iteration 785/3560 Training loss: 2.1656 0.7327 sec/batch\n",
      "Epoch 5/20  Iteration 786/3560 Training loss: 2.1652 0.6782 sec/batch\n",
      "Epoch 5/20  Iteration 787/3560 Training loss: 2.1649 0.6268 sec/batch\n",
      "Epoch 5/20  Iteration 788/3560 Training loss: 2.1651 0.7856 sec/batch\n",
      "Epoch 5/20  Iteration 789/3560 Training loss: 2.1647 0.6553 sec/batch\n",
      "Epoch 5/20  Iteration 790/3560 Training loss: 2.1647 0.6646 sec/batch\n",
      "Epoch 5/20  Iteration 791/3560 Training loss: 2.1642 0.6406 sec/batch\n",
      "Epoch 5/20  Iteration 792/3560 Training loss: 2.1640 0.6330 sec/batch\n",
      "Epoch 5/20  Iteration 793/3560 Training loss: 2.1634 0.8681 sec/batch\n",
      "Epoch 5/20  Iteration 794/3560 Training loss: 2.1633 0.5379 sec/batch\n",
      "Epoch 5/20  Iteration 795/3560 Training loss: 2.1628 0.6040 sec/batch\n",
      "Epoch 5/20  Iteration 796/3560 Training loss: 2.1626 0.6135 sec/batch\n",
      "Epoch 5/20  Iteration 797/3560 Training loss: 2.1618 0.6769 sec/batch\n",
      "Epoch 5/20  Iteration 798/3560 Training loss: 2.1613 0.6772 sec/batch\n",
      "Epoch 5/20  Iteration 799/3560 Training loss: 2.1611 0.6856 sec/batch\n",
      "Epoch 5/20  Iteration 800/3560 Training loss: 2.1606 0.6345 sec/batch\n",
      "Epoch 5/20  Iteration 801/3560 Training loss: 2.1603 0.6903 sec/batch\n",
      "Epoch 5/20  Iteration 802/3560 Training loss: 2.1601 0.7061 sec/batch\n",
      "Epoch 5/20  Iteration 803/3560 Training loss: 2.1598 0.6725 sec/batch\n",
      "Epoch 5/20  Iteration 804/3560 Training loss: 2.1596 0.6295 sec/batch\n",
      "Epoch 5/20  Iteration 805/3560 Training loss: 2.1592 0.6340 sec/batch\n",
      "Epoch 5/20  Iteration 806/3560 Training loss: 2.1587 0.6457 sec/batch\n",
      "Epoch 5/20  Iteration 807/3560 Training loss: 2.1582 0.5742 sec/batch\n",
      "Epoch 5/20  Iteration 808/3560 Training loss: 2.1580 0.6855 sec/batch\n",
      "Epoch 5/20  Iteration 809/3560 Training loss: 2.1579 0.6104 sec/batch\n",
      "Epoch 5/20  Iteration 810/3560 Training loss: 2.1576 0.6805 sec/batch\n",
      "Epoch 5/20  Iteration 811/3560 Training loss: 2.1573 0.5908 sec/batch\n",
      "Epoch 5/20  Iteration 812/3560 Training loss: 2.1568 0.7129 sec/batch\n",
      "Epoch 5/20  Iteration 813/3560 Training loss: 2.1567 0.6273 sec/batch\n",
      "Epoch 5/20  Iteration 814/3560 Training loss: 2.1566 0.6521 sec/batch\n",
      "Epoch 5/20  Iteration 815/3560 Training loss: 2.1561 0.6589 sec/batch\n",
      "Epoch 5/20  Iteration 816/3560 Training loss: 2.1558 0.5897 sec/batch\n",
      "Epoch 5/20  Iteration 817/3560 Training loss: 2.1555 0.6730 sec/batch\n",
      "Epoch 5/20  Iteration 818/3560 Training loss: 2.1553 0.5780 sec/batch\n",
      "Epoch 5/20  Iteration 819/3560 Training loss: 2.1551 0.6009 sec/batch\n",
      "Epoch 5/20  Iteration 820/3560 Training loss: 2.1549 0.6772 sec/batch\n",
      "Epoch 5/20  Iteration 821/3560 Training loss: 2.1548 0.6384 sec/batch\n",
      "Epoch 5/20  Iteration 822/3560 Training loss: 2.1544 0.5957 sec/batch\n",
      "Epoch 5/20  Iteration 823/3560 Training loss: 2.1543 0.6849 sec/batch\n",
      "Epoch 5/20  Iteration 824/3560 Training loss: 2.1542 0.6816 sec/batch\n",
      "Epoch 5/20  Iteration 825/3560 Training loss: 2.1539 0.6203 sec/batch\n",
      "Epoch 5/20  Iteration 826/3560 Training loss: 2.1537 0.7245 sec/batch\n",
      "Epoch 5/20  Iteration 827/3560 Training loss: 2.1533 0.6661 sec/batch\n",
      "Epoch 5/20  Iteration 828/3560 Training loss: 2.1529 0.6038 sec/batch\n",
      "Epoch 5/20  Iteration 829/3560 Training loss: 2.1527 0.6541 sec/batch\n",
      "Epoch 5/20  Iteration 830/3560 Training loss: 2.1525 0.6020 sec/batch\n",
      "Epoch 5/20  Iteration 831/3560 Training loss: 2.1525 0.6419 sec/batch\n",
      "Epoch 5/20  Iteration 832/3560 Training loss: 2.1523 0.5732 sec/batch\n",
      "Epoch 5/20  Iteration 833/3560 Training loss: 2.1522 0.5691 sec/batch\n",
      "Epoch 5/20  Iteration 834/3560 Training loss: 2.1519 0.6402 sec/batch\n",
      "Epoch 5/20  Iteration 835/3560 Training loss: 2.1516 0.6070 sec/batch\n",
      "Epoch 5/20  Iteration 836/3560 Training loss: 2.1516 0.6321 sec/batch\n",
      "Epoch 5/20  Iteration 837/3560 Training loss: 2.1515 0.6425 sec/batch\n",
      "Epoch 5/20  Iteration 838/3560 Training loss: 2.1511 0.6775 sec/batch\n",
      "Epoch 5/20  Iteration 839/3560 Training loss: 2.1509 0.6669 sec/batch\n",
      "Epoch 5/20  Iteration 840/3560 Training loss: 2.1508 0.6836 sec/batch\n",
      "Epoch 5/20  Iteration 841/3560 Training loss: 2.1507 0.7092 sec/batch\n",
      "Epoch 5/20  Iteration 842/3560 Training loss: 2.1505 0.6168 sec/batch\n",
      "Epoch 5/20  Iteration 843/3560 Training loss: 2.1503 0.6179 sec/batch\n",
      "Epoch 5/20  Iteration 844/3560 Training loss: 2.1500 0.5997 sec/batch\n",
      "Epoch 5/20  Iteration 845/3560 Training loss: 2.1499 0.6529 sec/batch\n",
      "Epoch 5/20  Iteration 846/3560 Training loss: 2.1498 0.7140 sec/batch\n",
      "Epoch 5/20  Iteration 847/3560 Training loss: 2.1496 0.6272 sec/batch\n",
      "Epoch 5/20  Iteration 848/3560 Training loss: 2.1495 0.6150 sec/batch\n",
      "Epoch 5/20  Iteration 849/3560 Training loss: 2.1493 0.7252 sec/batch\n",
      "Epoch 5/20  Iteration 850/3560 Training loss: 2.1492 0.5926 sec/batch\n",
      "Epoch 5/20  Iteration 851/3560 Training loss: 2.1493 0.7869 sec/batch\n",
      "Epoch 5/20  Iteration 852/3560 Training loss: 2.1490 0.6306 sec/batch\n",
      "Epoch 5/20  Iteration 853/3560 Training loss: 2.1490 0.7207 sec/batch\n",
      "Epoch 5/20  Iteration 854/3560 Training loss: 2.1487 0.8429 sec/batch\n",
      "Epoch 5/20  Iteration 855/3560 Training loss: 2.1485 0.7853 sec/batch\n",
      "Epoch 5/20  Iteration 856/3560 Training loss: 2.1483 0.8310 sec/batch\n",
      "Epoch 5/20  Iteration 857/3560 Training loss: 2.1481 0.8269 sec/batch\n",
      "Epoch 5/20  Iteration 858/3560 Training loss: 2.1482 0.6882 sec/batch\n",
      "Epoch 5/20  Iteration 859/3560 Training loss: 2.1480 0.7061 sec/batch\n",
      "Epoch 5/20  Iteration 860/3560 Training loss: 2.1480 0.7903 sec/batch\n",
      "Epoch 5/20  Iteration 861/3560 Training loss: 2.1478 0.6966 sec/batch\n",
      "Epoch 5/20  Iteration 862/3560 Training loss: 2.1475 0.6527 sec/batch\n",
      "Epoch 5/20  Iteration 863/3560 Training loss: 2.1475 0.6932 sec/batch\n",
      "Epoch 5/20  Iteration 864/3560 Training loss: 2.1476 0.7520 sec/batch\n",
      "Epoch 5/20  Iteration 865/3560 Training loss: 2.1474 0.7252 sec/batch\n",
      "Epoch 5/20  Iteration 866/3560 Training loss: 2.1474 0.6277 sec/batch\n",
      "Epoch 5/20  Iteration 867/3560 Training loss: 2.1472 0.6343 sec/batch\n",
      "Epoch 5/20  Iteration 868/3560 Training loss: 2.1470 0.6823 sec/batch\n",
      "Epoch 5/20  Iteration 869/3560 Training loss: 2.1468 0.6572 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 870/3560 Training loss: 2.1466 0.6149 sec/batch\n",
      "Epoch 5/20  Iteration 871/3560 Training loss: 2.1463 0.6355 sec/batch\n",
      "Epoch 5/20  Iteration 872/3560 Training loss: 2.1463 0.6254 sec/batch\n",
      "Epoch 5/20  Iteration 873/3560 Training loss: 2.1462 0.6454 sec/batch\n",
      "Epoch 5/20  Iteration 874/3560 Training loss: 2.1459 0.6530 sec/batch\n",
      "Epoch 5/20  Iteration 875/3560 Training loss: 2.1458 0.6384 sec/batch\n",
      "Epoch 5/20  Iteration 876/3560 Training loss: 2.1456 0.6905 sec/batch\n",
      "Epoch 5/20  Iteration 877/3560 Training loss: 2.1456 0.6565 sec/batch\n",
      "Epoch 5/20  Iteration 878/3560 Training loss: 2.1454 0.6801 sec/batch\n",
      "Epoch 5/20  Iteration 879/3560 Training loss: 2.1452 0.6484 sec/batch\n",
      "Epoch 5/20  Iteration 880/3560 Training loss: 2.1452 0.6599 sec/batch\n",
      "Epoch 5/20  Iteration 881/3560 Training loss: 2.1451 0.6343 sec/batch\n",
      "Epoch 5/20  Iteration 882/3560 Training loss: 2.1448 0.5899 sec/batch\n",
      "Epoch 5/20  Iteration 883/3560 Training loss: 2.1446 0.6049 sec/batch\n",
      "Epoch 5/20  Iteration 884/3560 Training loss: 2.1444 0.6583 sec/batch\n",
      "Epoch 5/20  Iteration 885/3560 Training loss: 2.1445 0.6788 sec/batch\n",
      "Epoch 5/20  Iteration 886/3560 Training loss: 2.1445 0.5903 sec/batch\n",
      "Epoch 5/20  Iteration 887/3560 Training loss: 2.1445 0.6202 sec/batch\n",
      "Epoch 5/20  Iteration 888/3560 Training loss: 2.1444 0.7269 sec/batch\n",
      "Epoch 5/20  Iteration 889/3560 Training loss: 2.1441 0.6609 sec/batch\n",
      "Epoch 5/20  Iteration 890/3560 Training loss: 2.1439 0.6564 sec/batch\n",
      "Epoch 6/20  Iteration 891/3560 Training loss: 2.1699 0.6601 sec/batch\n",
      "Epoch 6/20  Iteration 892/3560 Training loss: 2.1335 0.6900 sec/batch\n",
      "Epoch 6/20  Iteration 893/3560 Training loss: 2.1195 0.5885 sec/batch\n",
      "Epoch 6/20  Iteration 894/3560 Training loss: 2.1162 0.5554 sec/batch\n",
      "Epoch 6/20  Iteration 895/3560 Training loss: 2.1152 0.6434 sec/batch\n",
      "Epoch 6/20  Iteration 896/3560 Training loss: 2.1113 0.6536 sec/batch\n",
      "Epoch 6/20  Iteration 897/3560 Training loss: 2.1116 0.5730 sec/batch\n",
      "Epoch 6/20  Iteration 898/3560 Training loss: 2.1120 0.6299 sec/batch\n",
      "Epoch 6/20  Iteration 899/3560 Training loss: 2.1142 0.6493 sec/batch\n",
      "Epoch 6/20  Iteration 900/3560 Training loss: 2.1127 0.6985 sec/batch\n",
      "Epoch 6/20  Iteration 901/3560 Training loss: 2.1111 0.6272 sec/batch\n",
      "Epoch 6/20  Iteration 902/3560 Training loss: 2.1100 0.7435 sec/batch\n",
      "Epoch 6/20  Iteration 903/3560 Training loss: 2.1105 0.6335 sec/batch\n",
      "Epoch 6/20  Iteration 904/3560 Training loss: 2.1133 0.6572 sec/batch\n",
      "Epoch 6/20  Iteration 905/3560 Training loss: 2.1125 0.6272 sec/batch\n",
      "Epoch 6/20  Iteration 906/3560 Training loss: 2.1121 0.6329 sec/batch\n",
      "Epoch 6/20  Iteration 907/3560 Training loss: 2.1117 0.6364 sec/batch\n",
      "Epoch 6/20  Iteration 908/3560 Training loss: 2.1140 0.6628 sec/batch\n",
      "Epoch 6/20  Iteration 909/3560 Training loss: 2.1131 0.6204 sec/batch\n",
      "Epoch 6/20  Iteration 910/3560 Training loss: 2.1124 0.6261 sec/batch\n",
      "Epoch 6/20  Iteration 911/3560 Training loss: 2.1116 0.5605 sec/batch\n",
      "Epoch 6/20  Iteration 912/3560 Training loss: 2.1127 0.7419 sec/batch\n",
      "Epoch 6/20  Iteration 913/3560 Training loss: 2.1121 0.6452 sec/batch\n",
      "Epoch 6/20  Iteration 914/3560 Training loss: 2.1110 0.6972 sec/batch\n",
      "Epoch 6/20  Iteration 915/3560 Training loss: 2.1102 0.6674 sec/batch\n",
      "Epoch 6/20  Iteration 916/3560 Training loss: 2.1097 0.6768 sec/batch\n",
      "Epoch 6/20  Iteration 917/3560 Training loss: 2.1092 0.6430 sec/batch\n",
      "Epoch 6/20  Iteration 918/3560 Training loss: 2.1089 0.6825 sec/batch\n",
      "Epoch 6/20  Iteration 919/3560 Training loss: 2.1096 0.5536 sec/batch\n",
      "Epoch 6/20  Iteration 920/3560 Training loss: 2.1097 0.7037 sec/batch\n",
      "Epoch 6/20  Iteration 921/3560 Training loss: 2.1103 0.5701 sec/batch\n",
      "Epoch 6/20  Iteration 922/3560 Training loss: 2.1097 0.6360 sec/batch\n",
      "Epoch 6/20  Iteration 923/3560 Training loss: 2.1091 0.6240 sec/batch\n",
      "Epoch 6/20  Iteration 924/3560 Training loss: 2.1097 0.6835 sec/batch\n",
      "Epoch 6/20  Iteration 925/3560 Training loss: 2.1094 0.6336 sec/batch\n",
      "Epoch 6/20  Iteration 926/3560 Training loss: 2.1094 0.6683 sec/batch\n",
      "Epoch 6/20  Iteration 927/3560 Training loss: 2.1090 0.6870 sec/batch\n",
      "Epoch 6/20  Iteration 928/3560 Training loss: 2.1081 0.6317 sec/batch\n",
      "Epoch 6/20  Iteration 929/3560 Training loss: 2.1074 0.6452 sec/batch\n",
      "Epoch 6/20  Iteration 930/3560 Training loss: 2.1065 0.6420 sec/batch\n",
      "Epoch 6/20  Iteration 931/3560 Training loss: 2.1057 0.6012 sec/batch\n",
      "Epoch 6/20  Iteration 932/3560 Training loss: 2.1052 0.6315 sec/batch\n",
      "Epoch 6/20  Iteration 933/3560 Training loss: 2.1046 0.6543 sec/batch\n",
      "Epoch 6/20  Iteration 934/3560 Training loss: 2.1040 0.6346 sec/batch\n",
      "Epoch 6/20  Iteration 935/3560 Training loss: 2.1039 0.5762 sec/batch\n",
      "Epoch 6/20  Iteration 936/3560 Training loss: 2.1027 0.6396 sec/batch\n",
      "Epoch 6/20  Iteration 937/3560 Training loss: 2.1028 0.6525 sec/batch\n",
      "Epoch 6/20  Iteration 938/3560 Training loss: 2.1024 0.6818 sec/batch\n",
      "Epoch 6/20  Iteration 939/3560 Training loss: 2.1021 0.6202 sec/batch\n",
      "Epoch 6/20  Iteration 940/3560 Training loss: 2.1025 0.7008 sec/batch\n",
      "Epoch 6/20  Iteration 941/3560 Training loss: 2.1018 0.6605 sec/batch\n",
      "Epoch 6/20  Iteration 942/3560 Training loss: 2.1022 0.6495 sec/batch\n",
      "Epoch 6/20  Iteration 943/3560 Training loss: 2.1018 0.6311 sec/batch\n",
      "Epoch 6/20  Iteration 944/3560 Training loss: 2.1015 0.6213 sec/batch\n",
      "Epoch 6/20  Iteration 945/3560 Training loss: 2.1010 0.6319 sec/batch\n",
      "Epoch 6/20  Iteration 946/3560 Training loss: 2.1007 0.5951 sec/batch\n",
      "Epoch 6/20  Iteration 947/3560 Training loss: 2.1005 0.6478 sec/batch\n",
      "Epoch 6/20  Iteration 948/3560 Training loss: 2.1001 0.6561 sec/batch\n",
      "Epoch 6/20  Iteration 949/3560 Training loss: 2.0996 0.6197 sec/batch\n",
      "Epoch 6/20  Iteration 950/3560 Training loss: 2.0994 0.6450 sec/batch\n",
      "Epoch 6/20  Iteration 951/3560 Training loss: 2.0990 0.6978 sec/batch\n",
      "Epoch 6/20  Iteration 952/3560 Training loss: 2.0992 0.7000 sec/batch\n",
      "Epoch 6/20  Iteration 953/3560 Training loss: 2.0995 0.7074 sec/batch\n",
      "Epoch 6/20  Iteration 954/3560 Training loss: 2.0991 0.6209 sec/batch\n",
      "Epoch 6/20  Iteration 955/3560 Training loss: 2.0986 0.5661 sec/batch\n",
      "Epoch 6/20  Iteration 956/3560 Training loss: 2.0987 0.6535 sec/batch\n",
      "Epoch 6/20  Iteration 957/3560 Training loss: 2.0987 0.5864 sec/batch\n",
      "Epoch 6/20  Iteration 958/3560 Training loss: 2.0981 0.6612 sec/batch\n",
      "Epoch 6/20  Iteration 959/3560 Training loss: 2.0980 0.5963 sec/batch\n",
      "Epoch 6/20  Iteration 960/3560 Training loss: 2.0978 0.6711 sec/batch\n",
      "Epoch 6/20  Iteration 961/3560 Training loss: 2.0978 0.6109 sec/batch\n",
      "Epoch 6/20  Iteration 962/3560 Training loss: 2.0978 0.7344 sec/batch\n",
      "Epoch 6/20  Iteration 963/3560 Training loss: 2.0980 0.6399 sec/batch\n",
      "Epoch 6/20  Iteration 964/3560 Training loss: 2.0975 0.6827 sec/batch\n",
      "Epoch 6/20  Iteration 965/3560 Training loss: 2.0972 0.6774 sec/batch\n",
      "Epoch 6/20  Iteration 966/3560 Training loss: 2.0975 0.7141 sec/batch\n",
      "Epoch 6/20  Iteration 967/3560 Training loss: 2.0971 0.6175 sec/batch\n",
      "Epoch 6/20  Iteration 968/3560 Training loss: 2.0971 0.6943 sec/batch\n",
      "Epoch 6/20  Iteration 969/3560 Training loss: 2.0968 0.5722 sec/batch\n",
      "Epoch 6/20  Iteration 970/3560 Training loss: 2.0966 0.5997 sec/batch\n",
      "Epoch 6/20  Iteration 971/3560 Training loss: 2.0960 0.6038 sec/batch\n",
      "Epoch 6/20  Iteration 972/3560 Training loss: 2.0960 0.5659 sec/batch\n",
      "Epoch 6/20  Iteration 973/3560 Training loss: 2.0956 0.6456 sec/batch\n",
      "Epoch 6/20  Iteration 974/3560 Training loss: 2.0952 0.6357 sec/batch\n",
      "Epoch 6/20  Iteration 975/3560 Training loss: 2.0946 0.7001 sec/batch\n",
      "Epoch 6/20  Iteration 976/3560 Training loss: 2.0942 0.6589 sec/batch\n",
      "Epoch 6/20  Iteration 977/3560 Training loss: 2.0940 0.6713 sec/batch\n",
      "Epoch 6/20  Iteration 978/3560 Training loss: 2.0937 0.7044 sec/batch\n",
      "Epoch 6/20  Iteration 979/3560 Training loss: 2.0933 0.7211 sec/batch\n",
      "Epoch 6/20  Iteration 980/3560 Training loss: 2.0932 0.5347 sec/batch\n",
      "Epoch 6/20  Iteration 981/3560 Training loss: 2.0930 0.6811 sec/batch\n",
      "Epoch 6/20  Iteration 982/3560 Training loss: 2.0930 0.5566 sec/batch\n",
      "Epoch 6/20  Iteration 983/3560 Training loss: 2.0925 0.5931 sec/batch\n",
      "Epoch 6/20  Iteration 984/3560 Training loss: 2.0921 0.6263 sec/batch\n",
      "Epoch 6/20  Iteration 985/3560 Training loss: 2.0916 0.5862 sec/batch\n",
      "Epoch 6/20  Iteration 986/3560 Training loss: 2.0914 0.6313 sec/batch\n",
      "Epoch 6/20  Iteration 987/3560 Training loss: 2.0911 0.6643 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 988/3560 Training loss: 2.0908 0.6776 sec/batch\n",
      "Epoch 6/20  Iteration 989/3560 Training loss: 2.0904 0.6687 sec/batch\n",
      "Epoch 6/20  Iteration 990/3560 Training loss: 2.0900 0.6177 sec/batch\n",
      "Epoch 6/20  Iteration 991/3560 Training loss: 2.0900 0.6682 sec/batch\n",
      "Epoch 6/20  Iteration 992/3560 Training loss: 2.0897 0.7019 sec/batch\n",
      "Epoch 6/20  Iteration 993/3560 Training loss: 2.0894 0.5878 sec/batch\n",
      "Epoch 6/20  Iteration 994/3560 Training loss: 2.0891 0.6983 sec/batch\n",
      "Epoch 6/20  Iteration 995/3560 Training loss: 2.0888 0.6256 sec/batch\n",
      "Epoch 6/20  Iteration 996/3560 Training loss: 2.0886 0.6814 sec/batch\n",
      "Epoch 6/20  Iteration 997/3560 Training loss: 2.0884 0.5995 sec/batch\n",
      "Epoch 6/20  Iteration 998/3560 Training loss: 2.0882 0.6034 sec/batch\n",
      "Epoch 6/20  Iteration 999/3560 Training loss: 2.0882 0.6420 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3560 Training loss: 2.0880 0.6334 sec/batch\n",
      "Epoch 6/20  Iteration 1001/3560 Training loss: 2.0879 0.6951 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3560 Training loss: 2.0877 0.7173 sec/batch\n",
      "Epoch 6/20  Iteration 1003/3560 Training loss: 2.0876 0.6796 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3560 Training loss: 2.0873 0.6517 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3560 Training loss: 2.0872 0.6939 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3560 Training loss: 2.0866 0.5959 sec/batch\n",
      "Epoch 6/20  Iteration 1007/3560 Training loss: 2.0864 0.5839 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3560 Training loss: 2.0863 0.6568 sec/batch\n",
      "Epoch 6/20  Iteration 1009/3560 Training loss: 2.0863 0.6546 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3560 Training loss: 2.0861 0.6467 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3560 Training loss: 2.0861 0.5493 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3560 Training loss: 2.0859 0.7438 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3560 Training loss: 2.0857 0.6356 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3560 Training loss: 2.0857 0.6191 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3560 Training loss: 2.0856 0.6689 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3560 Training loss: 2.0852 0.6735 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3560 Training loss: 2.0851 0.7250 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3560 Training loss: 2.0851 0.6281 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3560 Training loss: 2.0850 0.5935 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3560 Training loss: 2.0849 0.6385 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3560 Training loss: 2.0847 0.6482 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3560 Training loss: 2.0843 0.6606 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3560 Training loss: 2.0842 0.5608 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3560 Training loss: 2.0842 0.6201 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3560 Training loss: 2.0840 0.7266 sec/batch\n",
      "Epoch 6/20  Iteration 1026/3560 Training loss: 2.0838 0.6977 sec/batch\n",
      "Epoch 6/20  Iteration 1027/3560 Training loss: 2.0836 0.6675 sec/batch\n",
      "Epoch 6/20  Iteration 1028/3560 Training loss: 2.0835 0.5967 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3560 Training loss: 2.0837 0.6905 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3560 Training loss: 2.0835 0.6538 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3560 Training loss: 2.0836 0.5963 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3560 Training loss: 2.0834 0.6219 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3560 Training loss: 2.0833 0.6287 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3560 Training loss: 2.0831 0.5366 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3560 Training loss: 2.0830 0.6754 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3560 Training loss: 2.0829 0.6064 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3560 Training loss: 2.0828 0.6271 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3560 Training loss: 2.0829 0.6759 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3560 Training loss: 2.0827 0.6297 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3560 Training loss: 2.0824 0.7369 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3560 Training loss: 2.0823 0.6600 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3560 Training loss: 2.0824 0.6844 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3560 Training loss: 2.0823 0.6022 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3560 Training loss: 2.0822 0.5815 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3560 Training loss: 2.0821 0.6207 sec/batch\n",
      "Epoch 6/20  Iteration 1046/3560 Training loss: 2.0820 0.6616 sec/batch\n",
      "Epoch 6/20  Iteration 1047/3560 Training loss: 2.0819 0.5930 sec/batch\n",
      "Epoch 6/20  Iteration 1048/3560 Training loss: 2.0817 0.5935 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3560 Training loss: 2.0814 0.6349 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3560 Training loss: 2.0815 0.6550 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3560 Training loss: 2.0815 0.7055 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3560 Training loss: 2.0813 0.6591 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3560 Training loss: 2.0812 0.7218 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3560 Training loss: 2.0811 0.6497 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3560 Training loss: 2.0811 0.6544 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3560 Training loss: 2.0808 0.6242 sec/batch\n",
      "Epoch 6/20  Iteration 1057/3560 Training loss: 2.0807 0.6461 sec/batch\n",
      "Epoch 6/20  Iteration 1058/3560 Training loss: 2.0809 0.7055 sec/batch\n",
      "Epoch 6/20  Iteration 1059/3560 Training loss: 2.0807 0.6101 sec/batch\n",
      "Epoch 6/20  Iteration 1060/3560 Training loss: 2.0805 0.6383 sec/batch\n",
      "Epoch 6/20  Iteration 1061/3560 Training loss: 2.0803 0.5750 sec/batch\n",
      "Epoch 6/20  Iteration 1062/3560 Training loss: 2.0801 0.7107 sec/batch\n",
      "Epoch 6/20  Iteration 1063/3560 Training loss: 2.0800 0.6782 sec/batch\n",
      "Epoch 6/20  Iteration 1064/3560 Training loss: 2.0800 0.6492 sec/batch\n",
      "Epoch 6/20  Iteration 1065/3560 Training loss: 2.0799 0.6168 sec/batch\n",
      "Epoch 6/20  Iteration 1066/3560 Training loss: 2.0798 0.6427 sec/batch\n",
      "Epoch 6/20  Iteration 1067/3560 Training loss: 2.0796 0.6506 sec/batch\n",
      "Epoch 6/20  Iteration 1068/3560 Training loss: 2.0794 0.6751 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3560 Training loss: 2.1066 0.6879 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3560 Training loss: 2.0777 0.5665 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3560 Training loss: 2.0656 0.6860 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3560 Training loss: 2.0603 0.5968 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3560 Training loss: 2.0579 0.6254 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3560 Training loss: 2.0531 0.6684 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3560 Training loss: 2.0541 0.6379 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3560 Training loss: 2.0541 0.6081 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3560 Training loss: 2.0556 0.6896 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3560 Training loss: 2.0556 0.6281 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3560 Training loss: 2.0537 0.6030 sec/batch\n",
      "Epoch 7/20  Iteration 1080/3560 Training loss: 2.0527 0.6850 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3560 Training loss: 2.0531 0.6491 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3560 Training loss: 2.0556 0.6174 sec/batch\n",
      "Epoch 7/20  Iteration 1083/3560 Training loss: 2.0553 0.5968 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3560 Training loss: 2.0543 0.6433 sec/batch\n",
      "Epoch 7/20  Iteration 1085/3560 Training loss: 2.0541 0.6190 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3560 Training loss: 2.0563 0.5778 sec/batch\n",
      "Epoch 7/20  Iteration 1087/3560 Training loss: 2.0557 0.6675 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3560 Training loss: 2.0555 0.6390 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3560 Training loss: 2.0545 0.7262 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3560 Training loss: 2.0554 0.6390 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3560 Training loss: 2.0545 0.7256 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3560 Training loss: 2.0534 0.6124 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3560 Training loss: 2.0528 0.7255 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3560 Training loss: 2.0520 0.6265 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3560 Training loss: 2.0514 0.6330 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3560 Training loss: 2.0512 0.6477 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3560 Training loss: 2.0519 0.6845 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3560 Training loss: 2.0519 0.5334 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3560 Training loss: 2.0516 0.8075 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3560 Training loss: 2.0508 0.7571 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3560 Training loss: 2.0505 0.8413 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3560 Training loss: 2.0513 0.9445 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3560 Training loss: 2.0507 0.6472 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1104/3560 Training loss: 2.0506 0.8660 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3560 Training loss: 2.0500 0.9743 sec/batch\n",
      "Epoch 7/20  Iteration 1106/3560 Training loss: 2.0489 0.8656 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3560 Training loss: 2.0477 0.5876 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3560 Training loss: 2.0470 0.6107 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3560 Training loss: 2.0466 0.6925 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3560 Training loss: 2.0462 0.5892 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3560 Training loss: 2.0455 0.6901 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3560 Training loss: 2.0446 0.6332 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3560 Training loss: 2.0444 0.5758 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3560 Training loss: 2.0431 0.7300 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3560 Training loss: 2.0432 0.6818 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3560 Training loss: 2.0426 0.7626 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3560 Training loss: 2.0425 0.6639 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3560 Training loss: 2.0432 0.7058 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3560 Training loss: 2.0426 0.6173 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3560 Training loss: 2.0429 0.6693 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3560 Training loss: 2.0425 0.5808 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3560 Training loss: 2.0423 0.6487 sec/batch\n",
      "Epoch 7/20  Iteration 1123/3560 Training loss: 2.0415 0.5459 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3560 Training loss: 2.0417 0.6747 sec/batch\n",
      "Epoch 7/20  Iteration 1125/3560 Training loss: 2.0419 0.5712 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3560 Training loss: 2.0414 0.6226 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3560 Training loss: 2.0410 0.6287 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3560 Training loss: 2.0410 0.6277 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3560 Training loss: 2.0408 0.7303 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3560 Training loss: 2.0414 0.7723 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3560 Training loss: 2.0417 0.7223 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3560 Training loss: 2.0417 0.6702 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3560 Training loss: 2.0415 0.7048 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3560 Training loss: 2.0418 0.5544 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3560 Training loss: 2.0418 0.5661 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3560 Training loss: 2.0413 0.6704 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3560 Training loss: 2.0410 0.6123 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3560 Training loss: 2.0409 0.6871 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3560 Training loss: 2.0411 0.6225 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3560 Training loss: 2.0412 0.6883 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3560 Training loss: 2.0414 0.7263 sec/batch\n",
      "Epoch 7/20  Iteration 1142/3560 Training loss: 2.0412 0.7146 sec/batch\n",
      "Epoch 7/20  Iteration 1143/3560 Training loss: 2.0410 0.7940 sec/batch\n",
      "Epoch 7/20  Iteration 1144/3560 Training loss: 2.0412 0.8896 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3560 Training loss: 2.0410 0.6662 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3560 Training loss: 2.0411 0.7009 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3560 Training loss: 2.0405 0.7334 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3560 Training loss: 2.0403 0.6349 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3560 Training loss: 2.0398 0.6365 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3560 Training loss: 2.0398 0.6112 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3560 Training loss: 2.0392 0.7049 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3560 Training loss: 2.0389 0.7199 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3560 Training loss: 2.0382 0.6259 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3560 Training loss: 2.0378 0.6785 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3560 Training loss: 2.0376 0.6834 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3560 Training loss: 2.0373 0.6288 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3560 Training loss: 2.0369 0.6535 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3560 Training loss: 2.0368 0.6110 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3560 Training loss: 2.0364 0.5886 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3560 Training loss: 2.0363 0.7346 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3560 Training loss: 2.0359 0.6171 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3560 Training loss: 2.0357 0.6122 sec/batch\n",
      "Epoch 7/20  Iteration 1163/3560 Training loss: 2.0352 0.6728 sec/batch\n",
      "Epoch 7/20  Iteration 1164/3560 Training loss: 2.0350 0.6299 sec/batch\n",
      "Epoch 7/20  Iteration 1165/3560 Training loss: 2.0347 0.6739 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3560 Training loss: 2.0343 0.6286 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3560 Training loss: 2.0340 0.7100 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3560 Training loss: 2.0336 0.8242 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3560 Training loss: 2.0336 0.7044 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3560 Training loss: 2.0335 0.6441 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3560 Training loss: 2.0333 0.6597 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3560 Training loss: 2.0330 0.6377 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3560 Training loss: 2.0327 0.6368 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3560 Training loss: 2.0325 0.5933 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3560 Training loss: 2.0322 0.6348 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3560 Training loss: 2.0321 0.7049 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3560 Training loss: 2.0321 0.7493 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3560 Training loss: 2.0320 0.7326 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3560 Training loss: 2.0319 0.6708 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3560 Training loss: 2.0317 0.7896 sec/batch\n",
      "Epoch 7/20  Iteration 1181/3560 Training loss: 2.0315 0.8531 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3560 Training loss: 2.0314 0.7376 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3560 Training loss: 2.0311 0.5914 sec/batch\n",
      "Epoch 7/20  Iteration 1184/3560 Training loss: 2.0307 0.6219 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3560 Training loss: 2.0305 0.6842 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3560 Training loss: 2.0303 0.6175 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3560 Training loss: 2.0303 0.6089 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3560 Training loss: 2.0301 0.6462 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3560 Training loss: 2.0302 0.6355 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3560 Training loss: 2.0300 0.6879 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3560 Training loss: 2.0298 0.6578 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3560 Training loss: 2.0298 0.7650 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3560 Training loss: 2.0298 0.7690 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3560 Training loss: 2.0293 0.6640 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3560 Training loss: 2.0294 0.6378 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3560 Training loss: 2.0293 0.6720 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3560 Training loss: 2.0293 0.5860 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3560 Training loss: 2.0292 0.6643 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3560 Training loss: 2.0290 0.6229 sec/batch\n",
      "Epoch 7/20  Iteration 1200/3560 Training loss: 2.0288 0.5847 sec/batch\n",
      "Epoch 7/20  Iteration 1201/3560 Training loss: 2.0287 0.6450 sec/batch\n",
      "Epoch 7/20  Iteration 1202/3560 Training loss: 2.0287 0.6915 sec/batch\n",
      "Epoch 7/20  Iteration 1203/3560 Training loss: 2.0285 0.5786 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3560 Training loss: 2.0285 0.7150 sec/batch\n",
      "Epoch 7/20  Iteration 1205/3560 Training loss: 2.0283 0.6587 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3560 Training loss: 2.0282 0.8253 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3560 Training loss: 2.0283 0.6653 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3560 Training loss: 2.0281 0.7256 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3560 Training loss: 2.0282 0.6132 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3560 Training loss: 2.0281 0.6293 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3560 Training loss: 2.0279 0.6323 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3560 Training loss: 2.0278 0.6388 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3560 Training loss: 2.0276 0.6556 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3560 Training loss: 2.0277 0.6181 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3560 Training loss: 2.0276 0.7221 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3560 Training loss: 2.0275 0.6780 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3560 Training loss: 2.0274 0.7120 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3560 Training loss: 2.0272 0.6887 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3560 Training loss: 2.0270 0.7566 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1220/3560 Training loss: 2.0272 0.6632 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3560 Training loss: 2.0271 0.6277 sec/batch\n",
      "Epoch 7/20  Iteration 1222/3560 Training loss: 2.0270 0.6581 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3560 Training loss: 2.0270 0.6312 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3560 Training loss: 2.0268 0.6381 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3560 Training loss: 2.0268 0.6044 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3560 Training loss: 2.0267 0.6487 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3560 Training loss: 2.0265 0.6146 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3560 Training loss: 2.0265 0.6137 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3560 Training loss: 2.0265 0.7414 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3560 Training loss: 2.0264 0.7415 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3560 Training loss: 2.0263 0.6217 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3560 Training loss: 2.0262 0.7197 sec/batch\n",
      "Epoch 7/20  Iteration 1233/3560 Training loss: 2.0261 0.6374 sec/batch\n",
      "Epoch 7/20  Iteration 1234/3560 Training loss: 2.0260 0.6477 sec/batch\n",
      "Epoch 7/20  Iteration 1235/3560 Training loss: 2.0259 0.5980 sec/batch\n",
      "Epoch 7/20  Iteration 1236/3560 Training loss: 2.0260 0.5800 sec/batch\n",
      "Epoch 7/20  Iteration 1237/3560 Training loss: 2.0260 0.7086 sec/batch\n",
      "Epoch 7/20  Iteration 1238/3560 Training loss: 2.0258 0.6901 sec/batch\n",
      "Epoch 7/20  Iteration 1239/3560 Training loss: 2.0257 0.6086 sec/batch\n",
      "Epoch 7/20  Iteration 1240/3560 Training loss: 2.0255 0.7417 sec/batch\n",
      "Epoch 7/20  Iteration 1241/3560 Training loss: 2.0254 0.6419 sec/batch\n",
      "Epoch 7/20  Iteration 1242/3560 Training loss: 2.0253 0.7479 sec/batch\n",
      "Epoch 7/20  Iteration 1243/3560 Training loss: 2.0253 0.5996 sec/batch\n",
      "Epoch 7/20  Iteration 1244/3560 Training loss: 2.0253 0.7790 sec/batch\n",
      "Epoch 7/20  Iteration 1245/3560 Training loss: 2.0250 0.6878 sec/batch\n",
      "Epoch 7/20  Iteration 1246/3560 Training loss: 2.0250 0.5582 sec/batch\n",
      "Epoch 8/20  Iteration 1247/3560 Training loss: 2.0580 0.6161 sec/batch\n",
      "Epoch 8/20  Iteration 1248/3560 Training loss: 2.0173 0.6206 sec/batch\n",
      "Epoch 8/20  Iteration 1249/3560 Training loss: 2.0076 0.5935 sec/batch\n",
      "Epoch 8/20  Iteration 1250/3560 Training loss: 2.0027 0.6455 sec/batch\n",
      "Epoch 8/20  Iteration 1251/3560 Training loss: 2.0035 0.6237 sec/batch\n",
      "Epoch 8/20  Iteration 1252/3560 Training loss: 1.9968 0.6287 sec/batch\n",
      "Epoch 8/20  Iteration 1253/3560 Training loss: 1.9986 0.6954 sec/batch\n",
      "Epoch 8/20  Iteration 1254/3560 Training loss: 1.9999 0.7362 sec/batch\n",
      "Epoch 8/20  Iteration 1255/3560 Training loss: 2.0027 0.6788 sec/batch\n",
      "Epoch 8/20  Iteration 1256/3560 Training loss: 2.0036 0.6625 sec/batch\n",
      "Epoch 8/20  Iteration 1257/3560 Training loss: 2.0013 0.6140 sec/batch\n",
      "Epoch 8/20  Iteration 1258/3560 Training loss: 2.0003 0.6646 sec/batch\n",
      "Epoch 8/20  Iteration 1259/3560 Training loss: 2.0004 0.5510 sec/batch\n",
      "Epoch 8/20  Iteration 1260/3560 Training loss: 2.0031 0.6543 sec/batch\n",
      "Epoch 8/20  Iteration 1261/3560 Training loss: 2.0025 0.6354 sec/batch\n",
      "Epoch 8/20  Iteration 1262/3560 Training loss: 2.0016 0.6018 sec/batch\n",
      "Epoch 8/20  Iteration 1263/3560 Training loss: 2.0011 0.5814 sec/batch\n",
      "Epoch 8/20  Iteration 1264/3560 Training loss: 2.0036 0.6516 sec/batch\n",
      "Epoch 8/20  Iteration 1265/3560 Training loss: 2.0035 0.5713 sec/batch\n",
      "Epoch 8/20  Iteration 1266/3560 Training loss: 2.0039 0.7626 sec/batch\n",
      "Epoch 8/20  Iteration 1267/3560 Training loss: 2.0029 0.6533 sec/batch\n",
      "Epoch 8/20  Iteration 1268/3560 Training loss: 2.0041 0.6834 sec/batch\n",
      "Epoch 8/20  Iteration 1269/3560 Training loss: 2.0029 0.7167 sec/batch\n",
      "Epoch 8/20  Iteration 1270/3560 Training loss: 2.0021 0.6892 sec/batch\n",
      "Epoch 8/20  Iteration 1271/3560 Training loss: 2.0017 0.6079 sec/batch\n",
      "Epoch 8/20  Iteration 1272/3560 Training loss: 2.0011 0.6213 sec/batch\n",
      "Epoch 8/20  Iteration 1273/3560 Training loss: 2.0005 0.6228 sec/batch\n",
      "Epoch 8/20  Iteration 1274/3560 Training loss: 2.0004 0.6679 sec/batch\n",
      "Epoch 8/20  Iteration 1275/3560 Training loss: 2.0008 0.5894 sec/batch\n",
      "Epoch 8/20  Iteration 1276/3560 Training loss: 2.0012 0.6158 sec/batch\n",
      "Epoch 8/20  Iteration 1277/3560 Training loss: 2.0012 0.5968 sec/batch\n",
      "Epoch 8/20  Iteration 1278/3560 Training loss: 2.0007 0.6187 sec/batch\n",
      "Epoch 8/20  Iteration 1279/3560 Training loss: 2.0009 0.7117 sec/batch\n",
      "Epoch 8/20  Iteration 1280/3560 Training loss: 2.0015 0.5636 sec/batch\n",
      "Epoch 8/20  Iteration 1281/3560 Training loss: 2.0009 0.7194 sec/batch\n",
      "Epoch 8/20  Iteration 1282/3560 Training loss: 2.0008 0.6514 sec/batch\n",
      "Epoch 8/20  Iteration 1283/3560 Training loss: 2.0004 0.6536 sec/batch\n",
      "Epoch 8/20  Iteration 1284/3560 Training loss: 1.9995 0.6914 sec/batch\n",
      "Epoch 8/20  Iteration 1285/3560 Training loss: 1.9987 0.6031 sec/batch\n",
      "Epoch 8/20  Iteration 1286/3560 Training loss: 1.9977 0.6566 sec/batch\n",
      "Epoch 8/20  Iteration 1287/3560 Training loss: 1.9973 0.6214 sec/batch\n",
      "Epoch 8/20  Iteration 1288/3560 Training loss: 1.9972 0.6214 sec/batch\n",
      "Epoch 8/20  Iteration 1289/3560 Training loss: 1.9965 0.5973 sec/batch\n",
      "Epoch 8/20  Iteration 1290/3560 Training loss: 1.9958 0.6053 sec/batch\n",
      "Epoch 8/20  Iteration 1291/3560 Training loss: 1.9958 0.6503 sec/batch\n",
      "Epoch 8/20  Iteration 1292/3560 Training loss: 1.9945 0.7764 sec/batch\n",
      "Epoch 8/20  Iteration 1293/3560 Training loss: 1.9946 0.6419 sec/batch\n",
      "Epoch 8/20  Iteration 1294/3560 Training loss: 1.9942 0.7028 sec/batch\n",
      "Epoch 8/20  Iteration 1295/3560 Training loss: 1.9939 0.6306 sec/batch\n",
      "Epoch 8/20  Iteration 1296/3560 Training loss: 1.9946 0.6931 sec/batch\n",
      "Epoch 8/20  Iteration 1297/3560 Training loss: 1.9940 0.6101 sec/batch\n",
      "Epoch 8/20  Iteration 1298/3560 Training loss: 1.9944 0.6373 sec/batch\n",
      "Epoch 8/20  Iteration 1299/3560 Training loss: 1.9942 0.6233 sec/batch\n",
      "Epoch 8/20  Iteration 1300/3560 Training loss: 1.9939 0.6557 sec/batch\n",
      "Epoch 8/20  Iteration 1301/3560 Training loss: 1.9934 0.6185 sec/batch\n",
      "Epoch 8/20  Iteration 1302/3560 Training loss: 1.9934 0.6235 sec/batch\n",
      "Epoch 8/20  Iteration 1303/3560 Training loss: 1.9936 0.6330 sec/batch\n",
      "Epoch 8/20  Iteration 1304/3560 Training loss: 1.9932 0.6369 sec/batch\n",
      "Epoch 8/20  Iteration 1305/3560 Training loss: 1.9926 0.6907 sec/batch\n",
      "Epoch 8/20  Iteration 1306/3560 Training loss: 1.9927 0.6979 sec/batch\n",
      "Epoch 8/20  Iteration 1307/3560 Training loss: 1.9924 0.7038 sec/batch\n",
      "Epoch 8/20  Iteration 1308/3560 Training loss: 1.9929 0.6454 sec/batch\n",
      "Epoch 8/20  Iteration 1309/3560 Training loss: 1.9933 0.6669 sec/batch\n",
      "Epoch 8/20  Iteration 1310/3560 Training loss: 1.9933 0.5979 sec/batch\n",
      "Epoch 8/20  Iteration 1311/3560 Training loss: 1.9931 0.6165 sec/batch\n",
      "Epoch 8/20  Iteration 1312/3560 Training loss: 1.9933 0.5872 sec/batch\n",
      "Epoch 8/20  Iteration 1313/3560 Training loss: 1.9933 0.6719 sec/batch\n",
      "Epoch 8/20  Iteration 1314/3560 Training loss: 1.9927 0.6296 sec/batch\n",
      "Epoch 8/20  Iteration 1315/3560 Training loss: 1.9924 0.6146 sec/batch\n",
      "Epoch 8/20  Iteration 1316/3560 Training loss: 1.9923 0.7159 sec/batch\n",
      "Epoch 8/20  Iteration 1317/3560 Training loss: 1.9925 0.6363 sec/batch\n",
      "Epoch 8/20  Iteration 1318/3560 Training loss: 1.9925 0.6816 sec/batch\n",
      "Epoch 8/20  Iteration 1319/3560 Training loss: 1.9927 0.7495 sec/batch\n",
      "Epoch 8/20  Iteration 1320/3560 Training loss: 1.9924 0.6657 sec/batch\n",
      "Epoch 8/20  Iteration 1321/3560 Training loss: 1.9922 0.6807 sec/batch\n",
      "Epoch 8/20  Iteration 1322/3560 Training loss: 1.9924 0.6451 sec/batch\n",
      "Epoch 8/20  Iteration 1323/3560 Training loss: 1.9923 0.6038 sec/batch\n",
      "Epoch 8/20  Iteration 1324/3560 Training loss: 1.9923 0.6731 sec/batch\n",
      "Epoch 8/20  Iteration 1325/3560 Training loss: 1.9920 0.5862 sec/batch\n",
      "Epoch 8/20  Iteration 1326/3560 Training loss: 1.9917 0.6559 sec/batch\n",
      "Epoch 8/20  Iteration 1327/3560 Training loss: 1.9911 0.5238 sec/batch\n",
      "Epoch 8/20  Iteration 1328/3560 Training loss: 1.9913 0.6409 sec/batch\n",
      "Epoch 8/20  Iteration 1329/3560 Training loss: 1.9909 0.6469 sec/batch\n",
      "Epoch 8/20  Iteration 1330/3560 Training loss: 1.9906 0.6729 sec/batch\n",
      "Epoch 8/20  Iteration 1331/3560 Training loss: 1.9900 0.7617 sec/batch\n",
      "Epoch 8/20  Iteration 1332/3560 Training loss: 1.9897 0.6247 sec/batch\n",
      "Epoch 8/20  Iteration 1333/3560 Training loss: 1.9895 0.7058 sec/batch\n",
      "Epoch 8/20  Iteration 1334/3560 Training loss: 1.9891 0.6618 sec/batch\n",
      "Epoch 8/20  Iteration 1335/3560 Training loss: 1.9886 0.5813 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1336/3560 Training loss: 1.9885 0.6430 sec/batch\n",
      "Epoch 8/20  Iteration 1337/3560 Training loss: 1.9882 0.6944 sec/batch\n",
      "Epoch 8/20  Iteration 1338/3560 Training loss: 1.9881 0.6390 sec/batch\n",
      "Epoch 8/20  Iteration 1339/3560 Training loss: 1.9876 0.7108 sec/batch\n",
      "Epoch 8/20  Iteration 1340/3560 Training loss: 1.9873 0.8297 sec/batch\n",
      "Epoch 8/20  Iteration 1341/3560 Training loss: 1.9868 0.7702 sec/batch\n",
      "Epoch 8/20  Iteration 1342/3560 Training loss: 1.9867 0.8767 sec/batch\n",
      "Epoch 8/20  Iteration 1343/3560 Training loss: 1.9866 0.8703 sec/batch\n",
      "Epoch 8/20  Iteration 1344/3560 Training loss: 1.9862 0.7452 sec/batch\n",
      "Epoch 8/20  Iteration 1345/3560 Training loss: 1.9858 0.8412 sec/batch\n",
      "Epoch 8/20  Iteration 1346/3560 Training loss: 1.9853 0.6189 sec/batch\n",
      "Epoch 8/20  Iteration 1347/3560 Training loss: 1.9852 0.7732 sec/batch\n",
      "Epoch 8/20  Iteration 1348/3560 Training loss: 1.9851 0.5981 sec/batch\n",
      "Epoch 8/20  Iteration 1349/3560 Training loss: 1.9847 0.7068 sec/batch\n",
      "Epoch 8/20  Iteration 1350/3560 Training loss: 1.9844 0.5502 sec/batch\n",
      "Epoch 8/20  Iteration 1351/3560 Training loss: 1.9841 0.6393 sec/batch\n",
      "Epoch 8/20  Iteration 1352/3560 Training loss: 1.9839 0.6460 sec/batch\n",
      "Epoch 8/20  Iteration 1353/3560 Training loss: 1.9838 0.6504 sec/batch\n",
      "Epoch 8/20  Iteration 1354/3560 Training loss: 1.9836 0.7113 sec/batch\n",
      "Epoch 8/20  Iteration 1355/3560 Training loss: 1.9836 0.6383 sec/batch\n",
      "Epoch 8/20  Iteration 1356/3560 Training loss: 1.9835 0.6807 sec/batch\n",
      "Epoch 8/20  Iteration 1357/3560 Training loss: 1.9834 0.6742 sec/batch\n",
      "Epoch 8/20  Iteration 1358/3560 Training loss: 1.9832 0.6335 sec/batch\n",
      "Epoch 8/20  Iteration 1359/3560 Training loss: 1.9831 0.6458 sec/batch\n",
      "Epoch 8/20  Iteration 1360/3560 Training loss: 1.9830 0.6472 sec/batch\n",
      "Epoch 8/20  Iteration 1361/3560 Training loss: 1.9827 0.6755 sec/batch\n",
      "Epoch 8/20  Iteration 1362/3560 Training loss: 1.9823 0.5966 sec/batch\n",
      "Epoch 8/20  Iteration 1363/3560 Training loss: 1.9822 0.6866 sec/batch\n",
      "Epoch 8/20  Iteration 1364/3560 Training loss: 1.9821 0.5800 sec/batch\n",
      "Epoch 8/20  Iteration 1365/3560 Training loss: 1.9820 0.6982 sec/batch\n",
      "Epoch 8/20  Iteration 1366/3560 Training loss: 1.9819 0.6515 sec/batch\n",
      "Epoch 8/20  Iteration 1367/3560 Training loss: 1.9818 0.6559 sec/batch\n",
      "Epoch 8/20  Iteration 1368/3560 Training loss: 1.9816 0.6705 sec/batch\n",
      "Epoch 8/20  Iteration 1369/3560 Training loss: 1.9815 0.6685 sec/batch\n",
      "Epoch 8/20  Iteration 1370/3560 Training loss: 1.9816 0.6338 sec/batch\n",
      "Epoch 8/20  Iteration 1371/3560 Training loss: 1.9815 0.6506 sec/batch\n",
      "Epoch 8/20  Iteration 1372/3560 Training loss: 1.9812 0.5856 sec/batch\n",
      "Epoch 8/20  Iteration 1373/3560 Training loss: 1.9812 0.6399 sec/batch\n",
      "Epoch 8/20  Iteration 1374/3560 Training loss: 1.9812 0.6255 sec/batch\n",
      "Epoch 8/20  Iteration 1375/3560 Training loss: 1.9812 0.6597 sec/batch\n",
      "Epoch 8/20  Iteration 1376/3560 Training loss: 1.9811 0.6101 sec/batch\n",
      "Epoch 8/20  Iteration 1377/3560 Training loss: 1.9809 0.6398 sec/batch\n",
      "Epoch 8/20  Iteration 1378/3560 Training loss: 1.9805 0.6363 sec/batch\n",
      "Epoch 8/20  Iteration 1379/3560 Training loss: 1.9805 0.6593 sec/batch\n",
      "Epoch 8/20  Iteration 1380/3560 Training loss: 1.9805 0.7193 sec/batch\n",
      "Epoch 8/20  Iteration 1381/3560 Training loss: 1.9804 0.6518 sec/batch\n",
      "Epoch 8/20  Iteration 1382/3560 Training loss: 1.9803 0.6346 sec/batch\n",
      "Epoch 8/20  Iteration 1383/3560 Training loss: 1.9803 0.6389 sec/batch\n",
      "Epoch 8/20  Iteration 1384/3560 Training loss: 1.9803 0.6563 sec/batch\n",
      "Epoch 8/20  Iteration 1385/3560 Training loss: 1.9804 0.6118 sec/batch\n",
      "Epoch 8/20  Iteration 1386/3560 Training loss: 1.9802 0.6634 sec/batch\n",
      "Epoch 8/20  Iteration 1387/3560 Training loss: 1.9803 0.6016 sec/batch\n",
      "Epoch 8/20  Iteration 1388/3560 Training loss: 1.9801 0.5529 sec/batch\n",
      "Epoch 8/20  Iteration 1389/3560 Training loss: 1.9800 0.6589 sec/batch\n",
      "Epoch 8/20  Iteration 1390/3560 Training loss: 1.9800 0.6842 sec/batch\n",
      "Epoch 8/20  Iteration 1391/3560 Training loss: 1.9798 0.7051 sec/batch\n",
      "Epoch 8/20  Iteration 1392/3560 Training loss: 1.9798 0.6301 sec/batch\n",
      "Epoch 8/20  Iteration 1393/3560 Training loss: 1.9799 0.6968 sec/batch\n",
      "Epoch 8/20  Iteration 1394/3560 Training loss: 1.9799 0.7031 sec/batch\n",
      "Epoch 8/20  Iteration 1395/3560 Training loss: 1.9798 0.6500 sec/batch\n",
      "Epoch 8/20  Iteration 1396/3560 Training loss: 1.9796 0.5708 sec/batch\n",
      "Epoch 8/20  Iteration 1397/3560 Training loss: 1.9793 0.6678 sec/batch\n",
      "Epoch 8/20  Iteration 1398/3560 Training loss: 1.9795 0.6591 sec/batch\n",
      "Epoch 8/20  Iteration 1399/3560 Training loss: 1.9795 0.6312 sec/batch\n",
      "Epoch 8/20  Iteration 1400/3560 Training loss: 1.9794 0.6672 sec/batch\n",
      "Epoch 8/20  Iteration 1401/3560 Training loss: 1.9793 0.6267 sec/batch\n",
      "Epoch 8/20  Iteration 1402/3560 Training loss: 1.9792 0.6668 sec/batch\n",
      "Epoch 8/20  Iteration 1403/3560 Training loss: 1.9791 0.6354 sec/batch\n",
      "Epoch 8/20  Iteration 1404/3560 Training loss: 1.9790 0.6855 sec/batch\n",
      "Epoch 8/20  Iteration 1405/3560 Training loss: 1.9787 0.7555 sec/batch\n",
      "Epoch 8/20  Iteration 1406/3560 Training loss: 1.9788 0.6634 sec/batch\n",
      "Epoch 8/20  Iteration 1407/3560 Training loss: 1.9789 0.6435 sec/batch\n",
      "Epoch 8/20  Iteration 1408/3560 Training loss: 1.9787 0.5935 sec/batch\n",
      "Epoch 8/20  Iteration 1409/3560 Training loss: 1.9787 0.6383 sec/batch\n",
      "Epoch 8/20  Iteration 1410/3560 Training loss: 1.9786 0.6796 sec/batch\n",
      "Epoch 8/20  Iteration 1411/3560 Training loss: 1.9786 0.7409 sec/batch\n",
      "Epoch 8/20  Iteration 1412/3560 Training loss: 1.9784 0.7216 sec/batch\n",
      "Epoch 8/20  Iteration 1413/3560 Training loss: 1.9784 0.5880 sec/batch\n",
      "Epoch 8/20  Iteration 1414/3560 Training loss: 1.9786 0.6382 sec/batch\n",
      "Epoch 8/20  Iteration 1415/3560 Training loss: 1.9785 0.6441 sec/batch\n",
      "Epoch 8/20  Iteration 1416/3560 Training loss: 1.9784 0.6218 sec/batch\n",
      "Epoch 8/20  Iteration 1417/3560 Training loss: 1.9782 0.6814 sec/batch\n",
      "Epoch 8/20  Iteration 1418/3560 Training loss: 1.9780 0.6965 sec/batch\n",
      "Epoch 8/20  Iteration 1419/3560 Training loss: 1.9780 0.6611 sec/batch\n",
      "Epoch 8/20  Iteration 1420/3560 Training loss: 1.9780 0.6298 sec/batch\n",
      "Epoch 8/20  Iteration 1421/3560 Training loss: 1.9780 0.6337 sec/batch\n",
      "Epoch 8/20  Iteration 1422/3560 Training loss: 1.9778 0.7535 sec/batch\n",
      "Epoch 8/20  Iteration 1423/3560 Training loss: 1.9776 0.8001 sec/batch\n",
      "Epoch 8/20  Iteration 1424/3560 Training loss: 1.9775 0.6318 sec/batch\n",
      "Epoch 9/20  Iteration 1425/3560 Training loss: 2.0133 0.6169 sec/batch\n",
      "Epoch 9/20  Iteration 1426/3560 Training loss: 1.9799 0.6724 sec/batch\n",
      "Epoch 9/20  Iteration 1427/3560 Training loss: 1.9674 0.6539 sec/batch\n",
      "Epoch 9/20  Iteration 1428/3560 Training loss: 1.9627 0.5523 sec/batch\n",
      "Epoch 9/20  Iteration 1429/3560 Training loss: 1.9624 0.6843 sec/batch\n",
      "Epoch 9/20  Iteration 1430/3560 Training loss: 1.9563 0.6476 sec/batch\n",
      "Epoch 9/20  Iteration 1431/3560 Training loss: 1.9590 0.6561 sec/batch\n",
      "Epoch 9/20  Iteration 1432/3560 Training loss: 1.9594 0.6263 sec/batch\n",
      "Epoch 9/20  Iteration 1433/3560 Training loss: 1.9627 0.6597 sec/batch\n",
      "Epoch 9/20  Iteration 1434/3560 Training loss: 1.9620 0.6119 sec/batch\n",
      "Epoch 9/20  Iteration 1435/3560 Training loss: 1.9600 0.6322 sec/batch\n",
      "Epoch 9/20  Iteration 1436/3560 Training loss: 1.9587 0.6138 sec/batch\n",
      "Epoch 9/20  Iteration 1437/3560 Training loss: 1.9590 0.7726 sec/batch\n",
      "Epoch 9/20  Iteration 1438/3560 Training loss: 1.9614 0.6180 sec/batch\n",
      "Epoch 9/20  Iteration 1439/3560 Training loss: 1.9608 0.5911 sec/batch\n",
      "Epoch 9/20  Iteration 1440/3560 Training loss: 1.9593 0.6331 sec/batch\n",
      "Epoch 9/20  Iteration 1441/3560 Training loss: 1.9586 0.6006 sec/batch\n",
      "Epoch 9/20  Iteration 1442/3560 Training loss: 1.9601 0.6748 sec/batch\n",
      "Epoch 9/20  Iteration 1443/3560 Training loss: 1.9597 0.6312 sec/batch\n",
      "Epoch 9/20  Iteration 1444/3560 Training loss: 1.9596 0.6809 sec/batch\n",
      "Epoch 9/20  Iteration 1445/3560 Training loss: 1.9586 0.6135 sec/batch\n",
      "Epoch 9/20  Iteration 1446/3560 Training loss: 1.9602 0.5900 sec/batch\n",
      "Epoch 9/20  Iteration 1447/3560 Training loss: 1.9596 0.7135 sec/batch\n",
      "Epoch 9/20  Iteration 1448/3560 Training loss: 1.9589 0.6136 sec/batch\n",
      "Epoch 9/20  Iteration 1449/3560 Training loss: 1.9586 0.6816 sec/batch\n",
      "Epoch 9/20  Iteration 1450/3560 Training loss: 1.9574 0.6585 sec/batch\n",
      "Epoch 9/20  Iteration 1451/3560 Training loss: 1.9570 0.6098 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1452/3560 Training loss: 1.9573 0.5613 sec/batch\n",
      "Epoch 9/20  Iteration 1453/3560 Training loss: 1.9578 0.6286 sec/batch\n",
      "Epoch 9/20  Iteration 1454/3560 Training loss: 1.9579 0.6496 sec/batch\n",
      "Epoch 9/20  Iteration 1455/3560 Training loss: 1.9578 0.6419 sec/batch\n",
      "Epoch 9/20  Iteration 1456/3560 Training loss: 1.9570 0.6809 sec/batch\n",
      "Epoch 9/20  Iteration 1457/3560 Training loss: 1.9572 0.6542 sec/batch\n",
      "Epoch 9/20  Iteration 1458/3560 Training loss: 1.9577 0.6612 sec/batch\n",
      "Epoch 9/20  Iteration 1459/3560 Training loss: 1.9571 0.6247 sec/batch\n",
      "Epoch 9/20  Iteration 1460/3560 Training loss: 1.9569 0.6394 sec/batch\n",
      "Epoch 9/20  Iteration 1461/3560 Training loss: 1.9563 0.6592 sec/batch\n",
      "Epoch 9/20  Iteration 1462/3560 Training loss: 1.9553 0.6808 sec/batch\n",
      "Epoch 9/20  Iteration 1463/3560 Training loss: 1.9540 0.6435 sec/batch\n",
      "Epoch 9/20  Iteration 1464/3560 Training loss: 1.9536 0.6995 sec/batch\n",
      "Epoch 9/20  Iteration 1465/3560 Training loss: 1.9531 0.5472 sec/batch\n",
      "Epoch 9/20  Iteration 1466/3560 Training loss: 1.9527 0.6326 sec/batch\n",
      "Epoch 9/20  Iteration 1467/3560 Training loss: 1.9519 0.6663 sec/batch\n",
      "Epoch 9/20  Iteration 1468/3560 Training loss: 1.9511 0.6993 sec/batch\n",
      "Epoch 9/20  Iteration 1469/3560 Training loss: 1.9513 0.7226 sec/batch\n",
      "Epoch 9/20  Iteration 1470/3560 Training loss: 1.9501 0.6302 sec/batch\n",
      "Epoch 9/20  Iteration 1471/3560 Training loss: 1.9502 0.5995 sec/batch\n",
      "Epoch 9/20  Iteration 1472/3560 Training loss: 1.9499 0.6916 sec/batch\n",
      "Epoch 9/20  Iteration 1473/3560 Training loss: 1.9496 0.6244 sec/batch\n",
      "Epoch 9/20  Iteration 1474/3560 Training loss: 1.9505 0.7082 sec/batch\n",
      "Epoch 9/20  Iteration 1475/3560 Training loss: 1.9497 0.6250 sec/batch\n",
      "Epoch 9/20  Iteration 1476/3560 Training loss: 1.9504 0.6465 sec/batch\n",
      "Epoch 9/20  Iteration 1477/3560 Training loss: 1.9501 0.6328 sec/batch\n",
      "Epoch 9/20  Iteration 1478/3560 Training loss: 1.9499 0.6009 sec/batch\n",
      "Epoch 9/20  Iteration 1479/3560 Training loss: 1.9494 0.6141 sec/batch\n",
      "Epoch 9/20  Iteration 1480/3560 Training loss: 1.9496 0.7001 sec/batch\n",
      "Epoch 9/20  Iteration 1481/3560 Training loss: 1.9498 0.6546 sec/batch\n",
      "Epoch 9/20  Iteration 1482/3560 Training loss: 1.9498 0.6752 sec/batch\n",
      "Epoch 9/20  Iteration 1483/3560 Training loss: 1.9495 0.6185 sec/batch\n",
      "Epoch 9/20  Iteration 1484/3560 Training loss: 1.9497 0.6084 sec/batch\n",
      "Epoch 9/20  Iteration 1485/3560 Training loss: 1.9496 0.6831 sec/batch\n",
      "Epoch 9/20  Iteration 1486/3560 Training loss: 1.9500 0.6946 sec/batch\n",
      "Epoch 9/20  Iteration 1487/3560 Training loss: 1.9501 0.6047 sec/batch\n",
      "Epoch 9/20  Iteration 1488/3560 Training loss: 1.9502 0.6366 sec/batch\n",
      "Epoch 9/20  Iteration 1489/3560 Training loss: 1.9500 0.6400 sec/batch\n",
      "Epoch 9/20  Iteration 1490/3560 Training loss: 1.9503 0.6075 sec/batch\n",
      "Epoch 9/20  Iteration 1491/3560 Training loss: 1.9503 0.6582 sec/batch\n",
      "Epoch 9/20  Iteration 1492/3560 Training loss: 1.9499 0.6039 sec/batch\n",
      "Epoch 9/20  Iteration 1493/3560 Training loss: 1.9496 0.6557 sec/batch\n",
      "Epoch 9/20  Iteration 1494/3560 Training loss: 1.9494 0.6755 sec/batch\n",
      "Epoch 9/20  Iteration 1495/3560 Training loss: 1.9497 0.6553 sec/batch\n",
      "Epoch 9/20  Iteration 1496/3560 Training loss: 1.9500 0.6269 sec/batch\n",
      "Epoch 9/20  Iteration 1497/3560 Training loss: 1.9503 0.6644 sec/batch\n",
      "Epoch 9/20  Iteration 1498/3560 Training loss: 1.9499 0.6272 sec/batch\n",
      "Epoch 9/20  Iteration 1499/3560 Training loss: 1.9499 0.6977 sec/batch\n",
      "Epoch 9/20  Iteration 1500/3560 Training loss: 1.9500 0.7288 sec/batch\n",
      "Epoch 9/20  Iteration 1501/3560 Training loss: 1.9499 0.5811 sec/batch\n",
      "Epoch 9/20  Iteration 1502/3560 Training loss: 1.9501 0.6773 sec/batch\n",
      "Epoch 9/20  Iteration 1503/3560 Training loss: 1.9496 0.5740 sec/batch\n",
      "Epoch 9/20  Iteration 1504/3560 Training loss: 1.9493 0.6449 sec/batch\n",
      "Epoch 9/20  Iteration 1505/3560 Training loss: 1.9487 0.7258 sec/batch\n",
      "Epoch 9/20  Iteration 1506/3560 Training loss: 1.9487 0.5470 sec/batch\n",
      "Epoch 9/20  Iteration 1507/3560 Training loss: 1.9483 0.6379 sec/batch\n",
      "Epoch 9/20  Iteration 1508/3560 Training loss: 1.9482 0.6145 sec/batch\n",
      "Epoch 9/20  Iteration 1509/3560 Training loss: 1.9477 0.6431 sec/batch\n",
      "Epoch 9/20  Iteration 1510/3560 Training loss: 1.9473 0.6675 sec/batch\n",
      "Epoch 9/20  Iteration 1511/3560 Training loss: 1.9473 0.5886 sec/batch\n",
      "Epoch 9/20  Iteration 1512/3560 Training loss: 1.9469 0.6636 sec/batch\n",
      "Epoch 9/20  Iteration 1513/3560 Training loss: 1.9465 0.6477 sec/batch\n",
      "Epoch 9/20  Iteration 1514/3560 Training loss: 1.9466 0.6268 sec/batch\n",
      "Epoch 9/20  Iteration 1515/3560 Training loss: 1.9464 0.6706 sec/batch\n",
      "Epoch 9/20  Iteration 1516/3560 Training loss: 1.9462 0.6545 sec/batch\n",
      "Epoch 9/20  Iteration 1517/3560 Training loss: 1.9456 0.6514 sec/batch\n",
      "Epoch 9/20  Iteration 1518/3560 Training loss: 1.9453 0.5647 sec/batch\n",
      "Epoch 9/20  Iteration 1519/3560 Training loss: 1.9450 0.6449 sec/batch\n",
      "Epoch 9/20  Iteration 1520/3560 Training loss: 1.9450 0.7276 sec/batch\n",
      "Epoch 9/20  Iteration 1521/3560 Training loss: 1.9448 0.5930 sec/batch\n",
      "Epoch 9/20  Iteration 1522/3560 Training loss: 1.9443 0.6126 sec/batch\n",
      "Epoch 9/20  Iteration 1523/3560 Training loss: 1.9439 0.7424 sec/batch\n",
      "Epoch 9/20  Iteration 1524/3560 Training loss: 1.9434 0.6716 sec/batch\n",
      "Epoch 9/20  Iteration 1525/3560 Training loss: 1.9434 0.5711 sec/batch\n",
      "Epoch 9/20  Iteration 1526/3560 Training loss: 1.9433 0.5806 sec/batch\n",
      "Epoch 9/20  Iteration 1527/3560 Training loss: 1.9430 0.6017 sec/batch\n",
      "Epoch 9/20  Iteration 1528/3560 Training loss: 1.9427 0.6715 sec/batch\n",
      "Epoch 9/20  Iteration 1529/3560 Training loss: 1.9424 0.6515 sec/batch\n",
      "Epoch 9/20  Iteration 1530/3560 Training loss: 1.9422 0.6388 sec/batch\n",
      "Epoch 9/20  Iteration 1531/3560 Training loss: 1.9422 0.6926 sec/batch\n",
      "Epoch 9/20  Iteration 1532/3560 Training loss: 1.9420 0.6564 sec/batch\n",
      "Epoch 9/20  Iteration 1533/3560 Training loss: 1.9419 0.6423 sec/batch\n",
      "Epoch 9/20  Iteration 1534/3560 Training loss: 1.9419 0.5922 sec/batch\n",
      "Epoch 9/20  Iteration 1535/3560 Training loss: 1.9418 0.7124 sec/batch\n",
      "Epoch 9/20  Iteration 1536/3560 Training loss: 1.9416 0.6892 sec/batch\n",
      "Epoch 9/20  Iteration 1537/3560 Training loss: 1.9414 0.6973 sec/batch\n",
      "Epoch 9/20  Iteration 1538/3560 Training loss: 1.9413 0.6034 sec/batch\n",
      "Epoch 9/20  Iteration 1539/3560 Training loss: 1.9411 0.6377 sec/batch\n",
      "Epoch 9/20  Iteration 1540/3560 Training loss: 1.9406 0.6188 sec/batch\n",
      "Epoch 9/20  Iteration 1541/3560 Training loss: 1.9405 0.6142 sec/batch\n",
      "Epoch 9/20  Iteration 1542/3560 Training loss: 1.9404 0.6364 sec/batch\n",
      "Epoch 9/20  Iteration 1543/3560 Training loss: 1.9404 0.6128 sec/batch\n",
      "Epoch 9/20  Iteration 1544/3560 Training loss: 1.9403 0.6868 sec/batch\n",
      "Epoch 9/20  Iteration 1545/3560 Training loss: 1.9402 0.8106 sec/batch\n",
      "Epoch 9/20  Iteration 1546/3560 Training loss: 1.9400 0.5800 sec/batch\n",
      "Epoch 9/20  Iteration 1547/3560 Training loss: 1.9398 0.7466 sec/batch\n",
      "Epoch 9/20  Iteration 1548/3560 Training loss: 1.9399 0.6686 sec/batch\n",
      "Epoch 9/20  Iteration 1549/3560 Training loss: 1.9398 0.7040 sec/batch\n",
      "Epoch 9/20  Iteration 1550/3560 Training loss: 1.9394 0.6278 sec/batch\n",
      "Epoch 9/20  Iteration 1551/3560 Training loss: 1.9394 0.6436 sec/batch\n",
      "Epoch 9/20  Iteration 1552/3560 Training loss: 1.9394 0.6663 sec/batch\n",
      "Epoch 9/20  Iteration 1553/3560 Training loss: 1.9393 0.6095 sec/batch\n",
      "Epoch 9/20  Iteration 1554/3560 Training loss: 1.9393 0.5718 sec/batch\n",
      "Epoch 9/20  Iteration 1555/3560 Training loss: 1.9391 0.6071 sec/batch\n",
      "Epoch 9/20  Iteration 1556/3560 Training loss: 1.9389 0.6404 sec/batch\n",
      "Epoch 9/20  Iteration 1557/3560 Training loss: 1.9388 0.6655 sec/batch\n",
      "Epoch 9/20  Iteration 1558/3560 Training loss: 1.9387 0.6946 sec/batch\n",
      "Epoch 9/20  Iteration 1559/3560 Training loss: 1.9387 0.6688 sec/batch\n",
      "Epoch 9/20  Iteration 1560/3560 Training loss: 1.9386 0.6551 sec/batch\n",
      "Epoch 9/20  Iteration 1561/3560 Training loss: 1.9387 0.6611 sec/batch\n",
      "Epoch 9/20  Iteration 1562/3560 Training loss: 1.9386 0.7998 sec/batch\n",
      "Epoch 9/20  Iteration 1563/3560 Training loss: 1.9387 0.4879 sec/batch\n",
      "Epoch 9/20  Iteration 1564/3560 Training loss: 1.9385 0.6629 sec/batch\n",
      "Epoch 9/20  Iteration 1565/3560 Training loss: 1.9387 0.5847 sec/batch\n",
      "Epoch 9/20  Iteration 1566/3560 Training loss: 1.9386 0.6114 sec/batch\n",
      "Epoch 9/20  Iteration 1567/3560 Training loss: 1.9386 0.6157 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1568/3560 Training loss: 1.9385 0.6869 sec/batch\n",
      "Epoch 9/20  Iteration 1569/3560 Training loss: 1.9384 0.6144 sec/batch\n",
      "Epoch 9/20  Iteration 1570/3560 Training loss: 1.9384 0.6760 sec/batch\n",
      "Epoch 9/20  Iteration 1571/3560 Training loss: 1.9384 0.6809 sec/batch\n",
      "Epoch 9/20  Iteration 1572/3560 Training loss: 1.9385 0.6734 sec/batch\n",
      "Epoch 9/20  Iteration 1573/3560 Training loss: 1.9385 0.7404 sec/batch\n",
      "Epoch 9/20  Iteration 1574/3560 Training loss: 1.9382 0.6105 sec/batch\n",
      "Epoch 9/20  Iteration 1575/3560 Training loss: 1.9381 0.7167 sec/batch\n",
      "Epoch 9/20  Iteration 1576/3560 Training loss: 1.9382 0.6465 sec/batch\n",
      "Epoch 9/20  Iteration 1577/3560 Training loss: 1.9383 0.6040 sec/batch\n",
      "Epoch 9/20  Iteration 1578/3560 Training loss: 1.9382 0.6585 sec/batch\n",
      "Epoch 9/20  Iteration 1579/3560 Training loss: 1.9381 0.6255 sec/batch\n",
      "Epoch 9/20  Iteration 1580/3560 Training loss: 1.9381 0.5923 sec/batch\n",
      "Epoch 9/20  Iteration 1581/3560 Training loss: 1.9381 0.7026 sec/batch\n",
      "Epoch 9/20  Iteration 1582/3560 Training loss: 1.9380 0.6233 sec/batch\n",
      "Epoch 9/20  Iteration 1583/3560 Training loss: 1.9378 0.7497 sec/batch\n",
      "Epoch 9/20  Iteration 1584/3560 Training loss: 1.9378 0.8511 sec/batch\n",
      "Epoch 9/20  Iteration 1585/3560 Training loss: 1.9379 0.8960 sec/batch\n",
      "Epoch 9/20  Iteration 1586/3560 Training loss: 1.9378 0.9425 sec/batch\n",
      "Epoch 9/20  Iteration 1587/3560 Training loss: 1.9378 0.7918 sec/batch\n",
      "Epoch 9/20  Iteration 1588/3560 Training loss: 1.9379 0.7230 sec/batch\n",
      "Epoch 9/20  Iteration 1589/3560 Training loss: 1.9378 0.6461 sec/batch\n",
      "Epoch 9/20  Iteration 1590/3560 Training loss: 1.9377 0.7044 sec/batch\n",
      "Epoch 9/20  Iteration 1591/3560 Training loss: 1.9377 0.6857 sec/batch\n",
      "Epoch 9/20  Iteration 1592/3560 Training loss: 1.9379 0.6361 sec/batch\n",
      "Epoch 9/20  Iteration 1593/3560 Training loss: 1.9377 0.6589 sec/batch\n",
      "Epoch 9/20  Iteration 1594/3560 Training loss: 1.9376 0.6148 sec/batch\n",
      "Epoch 9/20  Iteration 1595/3560 Training loss: 1.9375 0.6853 sec/batch\n",
      "Epoch 9/20  Iteration 1596/3560 Training loss: 1.9373 0.6198 sec/batch\n",
      "Epoch 9/20  Iteration 1597/3560 Training loss: 1.9372 0.7061 sec/batch\n",
      "Epoch 9/20  Iteration 1598/3560 Training loss: 1.9372 0.6436 sec/batch\n",
      "Epoch 9/20  Iteration 1599/3560 Training loss: 1.9373 0.6679 sec/batch\n",
      "Epoch 9/20  Iteration 1600/3560 Training loss: 1.9372 0.6224 sec/batch\n",
      "Epoch 9/20  Iteration 1601/3560 Training loss: 1.9369 0.5805 sec/batch\n",
      "Epoch 9/20  Iteration 1602/3560 Training loss: 1.9369 0.6635 sec/batch\n",
      "Epoch 10/20  Iteration 1603/3560 Training loss: 1.9782 0.7028 sec/batch\n",
      "Epoch 10/20  Iteration 1604/3560 Training loss: 1.9452 0.6671 sec/batch\n",
      "Epoch 10/20  Iteration 1605/3560 Training loss: 1.9336 0.5701 sec/batch\n",
      "Epoch 10/20  Iteration 1606/3560 Training loss: 1.9275 0.5931 sec/batch\n",
      "Epoch 10/20  Iteration 1607/3560 Training loss: 1.9255 0.6744 sec/batch\n",
      "Epoch 10/20  Iteration 1608/3560 Training loss: 1.9187 0.6378 sec/batch\n",
      "Epoch 10/20  Iteration 1609/3560 Training loss: 1.9220 0.7315 sec/batch\n",
      "Epoch 10/20  Iteration 1610/3560 Training loss: 1.9203 0.6572 sec/batch\n",
      "Epoch 10/20  Iteration 1611/3560 Training loss: 1.9232 0.6646 sec/batch\n",
      "Epoch 10/20  Iteration 1612/3560 Training loss: 1.9228 0.6664 sec/batch\n",
      "Epoch 10/20  Iteration 1613/3560 Training loss: 1.9201 0.6093 sec/batch\n",
      "Epoch 10/20  Iteration 1614/3560 Training loss: 1.9188 0.6321 sec/batch\n",
      "Epoch 10/20  Iteration 1615/3560 Training loss: 1.9179 0.5933 sec/batch\n",
      "Epoch 10/20  Iteration 1616/3560 Training loss: 1.9200 0.6051 sec/batch\n",
      "Epoch 10/20  Iteration 1617/3560 Training loss: 1.9197 0.6237 sec/batch\n",
      "Epoch 10/20  Iteration 1618/3560 Training loss: 1.9181 0.6321 sec/batch\n",
      "Epoch 10/20  Iteration 1619/3560 Training loss: 1.9176 0.6062 sec/batch\n",
      "Epoch 10/20  Iteration 1620/3560 Training loss: 1.9196 0.6620 sec/batch\n",
      "Epoch 10/20  Iteration 1621/3560 Training loss: 1.9191 0.5866 sec/batch\n",
      "Epoch 10/20  Iteration 1622/3560 Training loss: 1.9193 0.7187 sec/batch\n",
      "Epoch 10/20  Iteration 1623/3560 Training loss: 1.9184 0.6840 sec/batch\n",
      "Epoch 10/20  Iteration 1624/3560 Training loss: 1.9196 0.6758 sec/batch\n",
      "Epoch 10/20  Iteration 1625/3560 Training loss: 1.9186 0.6960 sec/batch\n",
      "Epoch 10/20  Iteration 1626/3560 Training loss: 1.9179 0.6486 sec/batch\n",
      "Epoch 10/20  Iteration 1627/3560 Training loss: 1.9174 0.6124 sec/batch\n",
      "Epoch 10/20  Iteration 1628/3560 Training loss: 1.9172 0.5443 sec/batch\n",
      "Epoch 10/20  Iteration 1629/3560 Training loss: 1.9169 0.6484 sec/batch\n",
      "Epoch 10/20  Iteration 1630/3560 Training loss: 1.9171 0.6447 sec/batch\n",
      "Epoch 10/20  Iteration 1631/3560 Training loss: 1.9180 0.5601 sec/batch\n",
      "Epoch 10/20  Iteration 1632/3560 Training loss: 1.9184 0.6167 sec/batch\n",
      "Epoch 10/20  Iteration 1633/3560 Training loss: 1.9186 0.6941 sec/batch\n",
      "Epoch 10/20  Iteration 1634/3560 Training loss: 1.9181 0.6792 sec/batch\n",
      "Epoch 10/20  Iteration 1635/3560 Training loss: 1.9181 0.6811 sec/batch\n",
      "Epoch 10/20  Iteration 1636/3560 Training loss: 1.9185 0.6812 sec/batch\n",
      "Epoch 10/20  Iteration 1637/3560 Training loss: 1.9178 0.6882 sec/batch\n",
      "Epoch 10/20  Iteration 1638/3560 Training loss: 1.9176 0.6685 sec/batch\n",
      "Epoch 10/20  Iteration 1639/3560 Training loss: 1.9174 0.6608 sec/batch\n",
      "Epoch 10/20  Iteration 1640/3560 Training loss: 1.9162 0.6354 sec/batch\n",
      "Epoch 10/20  Iteration 1641/3560 Training loss: 1.9152 0.6005 sec/batch\n",
      "Epoch 10/20  Iteration 1642/3560 Training loss: 1.9147 0.6638 sec/batch\n",
      "Epoch 10/20  Iteration 1643/3560 Training loss: 1.9139 0.6078 sec/batch\n",
      "Epoch 10/20  Iteration 1644/3560 Training loss: 1.9140 0.6166 sec/batch\n",
      "Epoch 10/20  Iteration 1645/3560 Training loss: 1.9136 0.6810 sec/batch\n",
      "Epoch 10/20  Iteration 1646/3560 Training loss: 1.9128 0.6625 sec/batch\n",
      "Epoch 10/20  Iteration 1647/3560 Training loss: 1.9127 0.6280 sec/batch\n",
      "Epoch 10/20  Iteration 1648/3560 Training loss: 1.9115 0.6817 sec/batch\n",
      "Epoch 10/20  Iteration 1649/3560 Training loss: 1.9115 0.5983 sec/batch\n",
      "Epoch 10/20  Iteration 1650/3560 Training loss: 1.9110 0.7378 sec/batch\n",
      "Epoch 10/20  Iteration 1651/3560 Training loss: 1.9109 0.6635 sec/batch\n",
      "Epoch 10/20  Iteration 1652/3560 Training loss: 1.9117 0.7000 sec/batch\n",
      "Epoch 10/20  Iteration 1653/3560 Training loss: 1.9113 0.6166 sec/batch\n",
      "Epoch 10/20  Iteration 1654/3560 Training loss: 1.9119 0.6519 sec/batch\n",
      "Epoch 10/20  Iteration 1655/3560 Training loss: 1.9117 0.6099 sec/batch\n",
      "Epoch 10/20  Iteration 1656/3560 Training loss: 1.9117 0.6070 sec/batch\n",
      "Epoch 10/20  Iteration 1657/3560 Training loss: 1.9113 0.5909 sec/batch\n",
      "Epoch 10/20  Iteration 1658/3560 Training loss: 1.9114 0.6705 sec/batch\n",
      "Epoch 10/20  Iteration 1659/3560 Training loss: 1.9114 0.6954 sec/batch\n",
      "Epoch 10/20  Iteration 1660/3560 Training loss: 1.9111 0.6612 sec/batch\n",
      "Epoch 10/20  Iteration 1661/3560 Training loss: 1.9109 0.6112 sec/batch\n",
      "Epoch 10/20  Iteration 1662/3560 Training loss: 1.9113 0.6649 sec/batch\n",
      "Epoch 10/20  Iteration 1663/3560 Training loss: 1.9112 0.6762 sec/batch\n",
      "Epoch 10/20  Iteration 1664/3560 Training loss: 1.9119 0.6365 sec/batch\n",
      "Epoch 10/20  Iteration 1665/3560 Training loss: 1.9121 0.6102 sec/batch\n",
      "Epoch 10/20  Iteration 1666/3560 Training loss: 1.9123 0.5884 sec/batch\n",
      "Epoch 10/20  Iteration 1667/3560 Training loss: 1.9122 0.6205 sec/batch\n",
      "Epoch 10/20  Iteration 1668/3560 Training loss: 1.9124 0.7020 sec/batch\n",
      "Epoch 10/20  Iteration 1669/3560 Training loss: 1.9126 0.6003 sec/batch\n",
      "Epoch 10/20  Iteration 1670/3560 Training loss: 1.9121 0.6967 sec/batch\n",
      "Epoch 10/20  Iteration 1671/3560 Training loss: 1.9119 0.6245 sec/batch\n",
      "Epoch 10/20  Iteration 1672/3560 Training loss: 1.9117 0.6645 sec/batch\n",
      "Epoch 10/20  Iteration 1673/3560 Training loss: 1.9120 0.7309 sec/batch\n",
      "Epoch 10/20  Iteration 1674/3560 Training loss: 1.9122 0.7083 sec/batch\n",
      "Epoch 10/20  Iteration 1675/3560 Training loss: 1.9125 0.6535 sec/batch\n",
      "Epoch 10/20  Iteration 1676/3560 Training loss: 1.9122 0.6805 sec/batch\n",
      "Epoch 10/20  Iteration 1677/3560 Training loss: 1.9121 0.6682 sec/batch\n",
      "Epoch 10/20  Iteration 1678/3560 Training loss: 1.9122 0.6248 sec/batch\n",
      "Epoch 10/20  Iteration 1679/3560 Training loss: 1.9121 0.7818 sec/batch\n",
      "Epoch 10/20  Iteration 1680/3560 Training loss: 1.9122 0.6803 sec/batch\n",
      "Epoch 10/20  Iteration 1681/3560 Training loss: 1.9118 0.6398 sec/batch\n",
      "Epoch 10/20  Iteration 1682/3560 Training loss: 1.9115 0.6859 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1683/3560 Training loss: 1.9111 0.6598 sec/batch\n",
      "Epoch 10/20  Iteration 1684/3560 Training loss: 1.9111 0.6678 sec/batch\n",
      "Epoch 10/20  Iteration 1685/3560 Training loss: 1.9107 0.7580 sec/batch\n",
      "Epoch 10/20  Iteration 1686/3560 Training loss: 1.9106 0.6129 sec/batch\n",
      "Epoch 10/20  Iteration 1687/3560 Training loss: 1.9101 0.8148 sec/batch\n",
      "Epoch 10/20  Iteration 1688/3560 Training loss: 1.9096 0.6851 sec/batch\n",
      "Epoch 10/20  Iteration 1689/3560 Training loss: 1.9095 0.6449 sec/batch\n",
      "Epoch 10/20  Iteration 1690/3560 Training loss: 1.9092 0.7939 sec/batch\n",
      "Epoch 10/20  Iteration 1691/3560 Training loss: 1.9087 0.7132 sec/batch\n",
      "Epoch 10/20  Iteration 1692/3560 Training loss: 1.9088 0.6040 sec/batch\n",
      "Epoch 10/20  Iteration 1693/3560 Training loss: 1.9086 0.6109 sec/batch\n",
      "Epoch 10/20  Iteration 1694/3560 Training loss: 1.9085 0.7112 sec/batch\n",
      "Epoch 10/20  Iteration 1695/3560 Training loss: 1.9081 0.7358 sec/batch\n",
      "Epoch 10/20  Iteration 1696/3560 Training loss: 1.9077 0.6668 sec/batch\n",
      "Epoch 10/20  Iteration 1697/3560 Training loss: 1.9073 0.7780 sec/batch\n",
      "Epoch 10/20  Iteration 1698/3560 Training loss: 1.9072 0.7119 sec/batch\n",
      "Epoch 10/20  Iteration 1699/3560 Training loss: 1.9071 0.7156 sec/batch\n",
      "Epoch 10/20  Iteration 1700/3560 Training loss: 1.9068 0.6974 sec/batch\n",
      "Epoch 10/20  Iteration 1701/3560 Training loss: 1.9064 0.7105 sec/batch\n",
      "Epoch 10/20  Iteration 1702/3560 Training loss: 1.9060 0.6370 sec/batch\n",
      "Epoch 10/20  Iteration 1703/3560 Training loss: 1.9061 0.6317 sec/batch\n",
      "Epoch 10/20  Iteration 1704/3560 Training loss: 1.9060 0.5784 sec/batch\n",
      "Epoch 10/20  Iteration 1705/3560 Training loss: 1.9057 0.5848 sec/batch\n",
      "Epoch 10/20  Iteration 1706/3560 Training loss: 1.9054 0.7101 sec/batch\n",
      "Epoch 10/20  Iteration 1707/3560 Training loss: 1.9051 0.6608 sec/batch\n",
      "Epoch 10/20  Iteration 1708/3560 Training loss: 1.9050 0.6872 sec/batch\n",
      "Epoch 10/20  Iteration 1709/3560 Training loss: 1.9049 0.6424 sec/batch\n",
      "Epoch 10/20  Iteration 1710/3560 Training loss: 1.9048 0.7213 sec/batch\n",
      "Epoch 10/20  Iteration 1711/3560 Training loss: 1.9048 0.6586 sec/batch\n",
      "Epoch 10/20  Iteration 1712/3560 Training loss: 1.9047 0.6944 sec/batch\n",
      "Epoch 10/20  Iteration 1713/3560 Training loss: 1.9048 0.7309 sec/batch\n",
      "Epoch 10/20  Iteration 1714/3560 Training loss: 1.9046 0.7716 sec/batch\n",
      "Epoch 10/20  Iteration 1715/3560 Training loss: 1.9046 0.6541 sec/batch\n",
      "Epoch 10/20  Iteration 1716/3560 Training loss: 1.9045 0.6568 sec/batch\n",
      "Epoch 10/20  Iteration 1717/3560 Training loss: 1.9043 0.7078 sec/batch\n",
      "Epoch 10/20  Iteration 1718/3560 Training loss: 1.9040 0.6112 sec/batch\n",
      "Epoch 10/20  Iteration 1719/3560 Training loss: 1.9040 0.6139 sec/batch\n",
      "Epoch 10/20  Iteration 1720/3560 Training loss: 1.9039 0.6810 sec/batch\n",
      "Epoch 10/20  Iteration 1721/3560 Training loss: 1.9040 0.6695 sec/batch\n",
      "Epoch 10/20  Iteration 1722/3560 Training loss: 1.9038 0.5789 sec/batch\n",
      "Epoch 10/20  Iteration 1723/3560 Training loss: 1.9038 0.7113 sec/batch\n",
      "Epoch 10/20  Iteration 1724/3560 Training loss: 1.9036 0.6900 sec/batch\n",
      "Epoch 10/20  Iteration 1725/3560 Training loss: 1.9033 0.7484 sec/batch\n",
      "Epoch 10/20  Iteration 1726/3560 Training loss: 1.9034 0.8183 sec/batch\n",
      "Epoch 10/20  Iteration 1727/3560 Training loss: 1.9033 0.7123 sec/batch\n",
      "Epoch 10/20  Iteration 1728/3560 Training loss: 1.9028 0.6421 sec/batch\n",
      "Epoch 10/20  Iteration 1729/3560 Training loss: 1.9029 0.6946 sec/batch\n",
      "Epoch 10/20  Iteration 1730/3560 Training loss: 1.9029 0.6083 sec/batch\n",
      "Epoch 10/20  Iteration 1731/3560 Training loss: 1.9028 0.6448 sec/batch\n",
      "Epoch 10/20  Iteration 1732/3560 Training loss: 1.9028 0.7735 sec/batch\n",
      "Epoch 10/20  Iteration 1733/3560 Training loss: 1.9026 0.6238 sec/batch\n",
      "Epoch 10/20  Iteration 1734/3560 Training loss: 1.9022 0.6322 sec/batch\n",
      "Epoch 10/20  Iteration 1735/3560 Training loss: 1.9023 0.7127 sec/batch\n",
      "Epoch 10/20  Iteration 1736/3560 Training loss: 1.9023 0.7028 sec/batch\n",
      "Epoch 10/20  Iteration 1737/3560 Training loss: 1.9022 0.7288 sec/batch\n",
      "Epoch 10/20  Iteration 1738/3560 Training loss: 1.9022 0.6130 sec/batch\n",
      "Epoch 10/20  Iteration 1739/3560 Training loss: 1.9022 0.6389 sec/batch\n",
      "Epoch 10/20  Iteration 1740/3560 Training loss: 1.9022 0.6382 sec/batch\n",
      "Epoch 10/20  Iteration 1741/3560 Training loss: 1.9024 0.6393 sec/batch\n",
      "Epoch 10/20  Iteration 1742/3560 Training loss: 1.9022 0.5907 sec/batch\n",
      "Epoch 10/20  Iteration 1743/3560 Training loss: 1.9023 0.6897 sec/batch\n",
      "Epoch 10/20  Iteration 1744/3560 Training loss: 1.9022 0.6573 sec/batch\n",
      "Epoch 10/20  Iteration 1745/3560 Training loss: 1.9022 0.7362 sec/batch\n",
      "Epoch 10/20  Iteration 1746/3560 Training loss: 1.9022 0.6146 sec/batch\n",
      "Epoch 10/20  Iteration 1747/3560 Training loss: 1.9020 0.7574 sec/batch\n",
      "Epoch 10/20  Iteration 1748/3560 Training loss: 1.9020 0.6625 sec/batch\n",
      "Epoch 10/20  Iteration 1749/3560 Training loss: 1.9019 0.7394 sec/batch\n",
      "Epoch 10/20  Iteration 1750/3560 Training loss: 1.9020 0.6172 sec/batch\n",
      "Epoch 10/20  Iteration 1751/3560 Training loss: 1.9020 0.6939 sec/batch\n",
      "Epoch 10/20  Iteration 1752/3560 Training loss: 1.9019 0.7649 sec/batch\n",
      "Epoch 10/20  Iteration 1753/3560 Training loss: 1.9016 0.5459 sec/batch\n",
      "Epoch 10/20  Iteration 1754/3560 Training loss: 1.9018 0.6800 sec/batch\n",
      "Epoch 10/20  Iteration 1755/3560 Training loss: 1.9017 0.6500 sec/batch\n",
      "Epoch 10/20  Iteration 1756/3560 Training loss: 1.9017 0.6138 sec/batch\n",
      "Epoch 10/20  Iteration 1757/3560 Training loss: 1.9016 0.6867 sec/batch\n",
      "Epoch 10/20  Iteration 1758/3560 Training loss: 1.9016 0.6358 sec/batch\n",
      "Epoch 10/20  Iteration 1759/3560 Training loss: 1.9016 0.7215 sec/batch\n",
      "Epoch 10/20  Iteration 1760/3560 Training loss: 1.9014 0.6440 sec/batch\n",
      "Epoch 10/20  Iteration 1761/3560 Training loss: 1.9012 0.6898 sec/batch\n",
      "Epoch 10/20  Iteration 1762/3560 Training loss: 1.9012 0.6956 sec/batch\n",
      "Epoch 10/20  Iteration 1763/3560 Training loss: 1.9014 0.7335 sec/batch\n",
      "Epoch 10/20  Iteration 1764/3560 Training loss: 1.9013 0.6141 sec/batch\n",
      "Epoch 10/20  Iteration 1765/3560 Training loss: 1.9012 0.6333 sec/batch\n",
      "Epoch 10/20  Iteration 1766/3560 Training loss: 1.9012 0.6771 sec/batch\n",
      "Epoch 10/20  Iteration 1767/3560 Training loss: 1.9012 0.5983 sec/batch\n",
      "Epoch 10/20  Iteration 1768/3560 Training loss: 1.9011 0.6477 sec/batch\n",
      "Epoch 10/20  Iteration 1769/3560 Training loss: 1.9011 0.6493 sec/batch\n",
      "Epoch 10/20  Iteration 1770/3560 Training loss: 1.9014 0.6344 sec/batch\n",
      "Epoch 10/20  Iteration 1771/3560 Training loss: 1.9013 0.6119 sec/batch\n",
      "Epoch 10/20  Iteration 1772/3560 Training loss: 1.9012 0.6133 sec/batch\n",
      "Epoch 10/20  Iteration 1773/3560 Training loss: 1.9011 0.6886 sec/batch\n",
      "Epoch 10/20  Iteration 1774/3560 Training loss: 1.9009 0.6586 sec/batch\n",
      "Epoch 10/20  Iteration 1775/3560 Training loss: 1.9009 0.7202 sec/batch\n",
      "Epoch 10/20  Iteration 1776/3560 Training loss: 1.9009 0.6597 sec/batch\n",
      "Epoch 10/20  Iteration 1777/3560 Training loss: 1.9010 0.6520 sec/batch\n",
      "Epoch 10/20  Iteration 1778/3560 Training loss: 1.9009 0.6530 sec/batch\n",
      "Epoch 10/20  Iteration 1779/3560 Training loss: 1.9008 0.6640 sec/batch\n",
      "Epoch 10/20  Iteration 1780/3560 Training loss: 1.9008 0.6072 sec/batch\n",
      "Epoch 11/20  Iteration 1781/3560 Training loss: 1.9515 0.7579 sec/batch\n",
      "Epoch 11/20  Iteration 1782/3560 Training loss: 1.9115 0.6598 sec/batch\n",
      "Epoch 11/20  Iteration 1783/3560 Training loss: 1.9002 0.5655 sec/batch\n",
      "Epoch 11/20  Iteration 1784/3560 Training loss: 1.8965 0.7123 sec/batch\n",
      "Epoch 11/20  Iteration 1785/3560 Training loss: 1.8938 0.6472 sec/batch\n",
      "Epoch 11/20  Iteration 1786/3560 Training loss: 1.8852 0.7379 sec/batch\n",
      "Epoch 11/20  Iteration 1787/3560 Training loss: 1.8856 0.9369 sec/batch\n",
      "Epoch 11/20  Iteration 1788/3560 Training loss: 1.8861 0.7990 sec/batch\n",
      "Epoch 11/20  Iteration 1789/3560 Training loss: 1.8890 0.8417 sec/batch\n",
      "Epoch 11/20  Iteration 1790/3560 Training loss: 1.8890 0.6944 sec/batch\n",
      "Epoch 11/20  Iteration 1791/3560 Training loss: 1.8868 0.7601 sec/batch\n",
      "Epoch 11/20  Iteration 1792/3560 Training loss: 1.8856 0.6901 sec/batch\n",
      "Epoch 11/20  Iteration 1793/3560 Training loss: 1.8858 0.6846 sec/batch\n",
      "Epoch 11/20  Iteration 1794/3560 Training loss: 1.8878 0.6368 sec/batch\n",
      "Epoch 11/20  Iteration 1795/3560 Training loss: 1.8874 0.6801 sec/batch\n",
      "Epoch 11/20  Iteration 1796/3560 Training loss: 1.8858 0.6436 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1797/3560 Training loss: 1.8862 0.6054 sec/batch\n",
      "Epoch 11/20  Iteration 1798/3560 Training loss: 1.8881 0.6922 sec/batch\n",
      "Epoch 11/20  Iteration 1799/3560 Training loss: 1.8872 0.6808 sec/batch\n",
      "Epoch 11/20  Iteration 1800/3560 Training loss: 1.8876 0.9169 sec/batch\n",
      "Epoch 11/20  Iteration 1801/3560 Training loss: 1.8862 0.7769 sec/batch\n",
      "Epoch 11/20  Iteration 1802/3560 Training loss: 1.8875 0.7055 sec/batch\n",
      "Epoch 11/20  Iteration 1803/3560 Training loss: 1.8867 0.6012 sec/batch\n",
      "Epoch 11/20  Iteration 1804/3560 Training loss: 1.8865 0.5644 sec/batch\n",
      "Epoch 11/20  Iteration 1805/3560 Training loss: 1.8858 0.6783 sec/batch\n",
      "Epoch 11/20  Iteration 1806/3560 Training loss: 1.8850 0.6342 sec/batch\n",
      "Epoch 11/20  Iteration 1807/3560 Training loss: 1.8846 0.6084 sec/batch\n",
      "Epoch 11/20  Iteration 1808/3560 Training loss: 1.8852 0.6133 sec/batch\n",
      "Epoch 11/20  Iteration 1809/3560 Training loss: 1.8863 0.6533 sec/batch\n",
      "Epoch 11/20  Iteration 1810/3560 Training loss: 1.8867 0.5664 sec/batch\n",
      "Epoch 11/20  Iteration 1811/3560 Training loss: 1.8866 0.7189 sec/batch\n",
      "Epoch 11/20  Iteration 1812/3560 Training loss: 1.8856 0.6507 sec/batch\n",
      "Epoch 11/20  Iteration 1813/3560 Training loss: 1.8859 0.7324 sec/batch\n",
      "Epoch 11/20  Iteration 1814/3560 Training loss: 1.8864 0.7001 sec/batch\n",
      "Epoch 11/20  Iteration 1815/3560 Training loss: 1.8857 0.6490 sec/batch\n",
      "Epoch 11/20  Iteration 1816/3560 Training loss: 1.8858 0.6253 sec/batch\n",
      "Epoch 11/20  Iteration 1817/3560 Training loss: 1.8852 0.7045 sec/batch\n",
      "Epoch 11/20  Iteration 1818/3560 Training loss: 1.8841 0.6796 sec/batch\n",
      "Epoch 11/20  Iteration 1819/3560 Training loss: 1.8829 0.6476 sec/batch\n",
      "Epoch 11/20  Iteration 1820/3560 Training loss: 1.8823 0.5816 sec/batch\n",
      "Epoch 11/20  Iteration 1821/3560 Training loss: 1.8819 0.6818 sec/batch\n",
      "Epoch 11/20  Iteration 1822/3560 Training loss: 1.8816 0.7272 sec/batch\n",
      "Epoch 11/20  Iteration 1823/3560 Training loss: 1.8809 0.8010 sec/batch\n",
      "Epoch 11/20  Iteration 1824/3560 Training loss: 1.8801 0.9693 sec/batch\n",
      "Epoch 11/20  Iteration 1825/3560 Training loss: 1.8801 0.9921 sec/batch\n",
      "Epoch 11/20  Iteration 1826/3560 Training loss: 1.8792 0.7052 sec/batch\n",
      "Epoch 11/20  Iteration 1827/3560 Training loss: 1.8791 0.8173 sec/batch\n",
      "Epoch 11/20  Iteration 1828/3560 Training loss: 1.8786 0.7397 sec/batch\n",
      "Epoch 11/20  Iteration 1829/3560 Training loss: 1.8786 0.6422 sec/batch\n",
      "Epoch 11/20  Iteration 1830/3560 Training loss: 1.8793 0.6267 sec/batch\n",
      "Epoch 11/20  Iteration 1831/3560 Training loss: 1.8789 0.6476 sec/batch\n",
      "Epoch 11/20  Iteration 1832/3560 Training loss: 1.8799 0.6314 sec/batch\n",
      "Epoch 11/20  Iteration 1833/3560 Training loss: 1.8798 0.6526 sec/batch\n",
      "Epoch 11/20  Iteration 1834/3560 Training loss: 1.8796 0.6117 sec/batch\n",
      "Epoch 11/20  Iteration 1835/3560 Training loss: 1.8794 0.6176 sec/batch\n",
      "Epoch 11/20  Iteration 1836/3560 Training loss: 1.8795 0.6275 sec/batch\n",
      "Epoch 11/20  Iteration 1837/3560 Training loss: 1.8796 0.7873 sec/batch\n",
      "Epoch 11/20  Iteration 1838/3560 Training loss: 1.8794 0.6969 sec/batch\n",
      "Epoch 11/20  Iteration 1839/3560 Training loss: 1.8789 0.6890 sec/batch\n",
      "Epoch 11/20  Iteration 1840/3560 Training loss: 1.8792 0.6590 sec/batch\n",
      "Epoch 11/20  Iteration 1841/3560 Training loss: 1.8791 0.6495 sec/batch\n",
      "Epoch 11/20  Iteration 1842/3560 Training loss: 1.8799 0.5578 sec/batch\n",
      "Epoch 11/20  Iteration 1843/3560 Training loss: 1.8803 0.6808 sec/batch\n",
      "Epoch 11/20  Iteration 1844/3560 Training loss: 1.8803 0.5553 sec/batch\n",
      "Epoch 11/20  Iteration 1845/3560 Training loss: 1.8800 0.6869 sec/batch\n",
      "Epoch 11/20  Iteration 1846/3560 Training loss: 1.8802 0.6334 sec/batch\n",
      "Epoch 11/20  Iteration 1847/3560 Training loss: 1.8804 0.5893 sec/batch\n",
      "Epoch 11/20  Iteration 1848/3560 Training loss: 1.8800 0.5712 sec/batch\n",
      "Epoch 11/20  Iteration 1849/3560 Training loss: 1.8801 0.6754 sec/batch\n",
      "Epoch 11/20  Iteration 1850/3560 Training loss: 1.8800 0.7137 sec/batch\n",
      "Epoch 11/20  Iteration 1851/3560 Training loss: 1.8805 0.7716 sec/batch\n",
      "Epoch 11/20  Iteration 1852/3560 Training loss: 1.8806 0.6072 sec/batch\n",
      "Epoch 11/20  Iteration 1853/3560 Training loss: 1.8809 0.5820 sec/batch\n",
      "Epoch 11/20  Iteration 1854/3560 Training loss: 1.8806 0.6063 sec/batch\n",
      "Epoch 11/20  Iteration 1855/3560 Training loss: 1.8804 0.6518 sec/batch\n",
      "Epoch 11/20  Iteration 1856/3560 Training loss: 1.8805 0.6699 sec/batch\n",
      "Epoch 11/20  Iteration 1857/3560 Training loss: 1.8805 0.7126 sec/batch\n",
      "Epoch 11/20  Iteration 1858/3560 Training loss: 1.8805 0.6639 sec/batch\n",
      "Epoch 11/20  Iteration 1859/3560 Training loss: 1.8800 0.6002 sec/batch\n",
      "Epoch 11/20  Iteration 1860/3560 Training loss: 1.8797 0.6042 sec/batch\n",
      "Epoch 11/20  Iteration 1861/3560 Training loss: 1.8792 0.6018 sec/batch\n",
      "Epoch 11/20  Iteration 1862/3560 Training loss: 1.8793 0.7673 sec/batch\n",
      "Epoch 11/20  Iteration 1863/3560 Training loss: 1.8789 0.7224 sec/batch\n",
      "Epoch 11/20  Iteration 1864/3560 Training loss: 1.8787 0.6916 sec/batch\n",
      "Epoch 11/20  Iteration 1865/3560 Training loss: 1.8783 0.6605 sec/batch\n",
      "Epoch 11/20  Iteration 1866/3560 Training loss: 1.8779 0.6610 sec/batch\n",
      "Epoch 11/20  Iteration 1867/3560 Training loss: 1.8779 0.5978 sec/batch\n",
      "Epoch 11/20  Iteration 1868/3560 Training loss: 1.8774 0.6013 sec/batch\n",
      "Epoch 11/20  Iteration 1869/3560 Training loss: 1.8769 0.6516 sec/batch\n",
      "Epoch 11/20  Iteration 1870/3560 Training loss: 1.8769 0.6396 sec/batch\n",
      "Epoch 11/20  Iteration 1871/3560 Training loss: 1.8766 0.6399 sec/batch\n",
      "Epoch 11/20  Iteration 1872/3560 Training loss: 1.8765 0.5835 sec/batch\n",
      "Epoch 11/20  Iteration 1873/3560 Training loss: 1.8762 0.6465 sec/batch\n",
      "Epoch 11/20  Iteration 1874/3560 Training loss: 1.8758 0.6716 sec/batch\n",
      "Epoch 11/20  Iteration 1875/3560 Training loss: 1.8755 0.6994 sec/batch\n",
      "Epoch 11/20  Iteration 1876/3560 Training loss: 1.8754 0.6651 sec/batch\n",
      "Epoch 11/20  Iteration 1877/3560 Training loss: 1.8753 0.6835 sec/batch\n",
      "Epoch 11/20  Iteration 1878/3560 Training loss: 1.8750 0.6464 sec/batch\n",
      "Epoch 11/20  Iteration 1879/3560 Training loss: 1.8745 0.6054 sec/batch\n",
      "Epoch 11/20  Iteration 1880/3560 Training loss: 1.8741 0.6506 sec/batch\n",
      "Epoch 11/20  Iteration 1881/3560 Training loss: 1.8740 0.6230 sec/batch\n",
      "Epoch 11/20  Iteration 1882/3560 Training loss: 1.8739 0.6220 sec/batch\n",
      "Epoch 11/20  Iteration 1883/3560 Training loss: 1.8737 0.6623 sec/batch\n",
      "Epoch 11/20  Iteration 1884/3560 Training loss: 1.8734 0.6131 sec/batch\n",
      "Epoch 11/20  Iteration 1885/3560 Training loss: 1.8732 0.5837 sec/batch\n",
      "Epoch 11/20  Iteration 1886/3560 Training loss: 1.8731 0.5863 sec/batch\n",
      "Epoch 11/20  Iteration 1887/3560 Training loss: 1.8730 0.6324 sec/batch\n",
      "Epoch 11/20  Iteration 1888/3560 Training loss: 1.8730 0.7407 sec/batch\n",
      "Epoch 11/20  Iteration 1889/3560 Training loss: 1.8729 0.6474 sec/batch\n",
      "Epoch 11/20  Iteration 1890/3560 Training loss: 1.8729 0.7097 sec/batch\n",
      "Epoch 11/20  Iteration 1891/3560 Training loss: 1.8728 0.6228 sec/batch\n",
      "Epoch 11/20  Iteration 1892/3560 Training loss: 1.8726 0.6131 sec/batch\n",
      "Epoch 11/20  Iteration 1893/3560 Training loss: 1.8725 0.6233 sec/batch\n",
      "Epoch 11/20  Iteration 1894/3560 Training loss: 1.8723 0.6032 sec/batch\n",
      "Epoch 11/20  Iteration 1895/3560 Training loss: 1.8721 0.5987 sec/batch\n",
      "Epoch 11/20  Iteration 1896/3560 Training loss: 1.8717 0.6248 sec/batch\n",
      "Epoch 11/20  Iteration 1897/3560 Training loss: 1.8716 0.6644 sec/batch\n",
      "Epoch 11/20  Iteration 1898/3560 Training loss: 1.8714 0.5564 sec/batch\n",
      "Epoch 11/20  Iteration 1899/3560 Training loss: 1.8714 0.6252 sec/batch\n",
      "Epoch 11/20  Iteration 1900/3560 Training loss: 1.8712 0.6735 sec/batch\n",
      "Epoch 11/20  Iteration 1901/3560 Training loss: 1.8711 0.7045 sec/batch\n",
      "Epoch 11/20  Iteration 1902/3560 Training loss: 1.8708 0.7273 sec/batch\n",
      "Epoch 11/20  Iteration 1903/3560 Training loss: 1.8706 0.7492 sec/batch\n",
      "Epoch 11/20  Iteration 1904/3560 Training loss: 1.8708 0.6735 sec/batch\n",
      "Epoch 11/20  Iteration 1905/3560 Training loss: 1.8708 0.6659 sec/batch\n",
      "Epoch 11/20  Iteration 1906/3560 Training loss: 1.8704 0.5651 sec/batch\n",
      "Epoch 11/20  Iteration 1907/3560 Training loss: 1.8705 0.6501 sec/batch\n",
      "Epoch 11/20  Iteration 1908/3560 Training loss: 1.8706 0.6972 sec/batch\n",
      "Epoch 11/20  Iteration 1909/3560 Training loss: 1.8705 0.7042 sec/batch\n",
      "Epoch 11/20  Iteration 1910/3560 Training loss: 1.8705 0.6372 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1911/3560 Training loss: 1.8702 0.6180 sec/batch\n",
      "Epoch 11/20  Iteration 1912/3560 Training loss: 1.8699 0.7092 sec/batch\n",
      "Epoch 11/20  Iteration 1913/3560 Training loss: 1.8700 0.6843 sec/batch\n",
      "Epoch 11/20  Iteration 1914/3560 Training loss: 1.8700 0.7334 sec/batch\n",
      "Epoch 11/20  Iteration 1915/3560 Training loss: 1.8700 0.7570 sec/batch\n",
      "Epoch 11/20  Iteration 1916/3560 Training loss: 1.8700 0.7337 sec/batch\n",
      "Epoch 11/20  Iteration 1917/3560 Training loss: 1.8700 0.6149 sec/batch\n",
      "Epoch 11/20  Iteration 1918/3560 Training loss: 1.8701 0.6249 sec/batch\n",
      "Epoch 11/20  Iteration 1919/3560 Training loss: 1.8703 0.5907 sec/batch\n",
      "Epoch 11/20  Iteration 1920/3560 Training loss: 1.8702 0.6448 sec/batch\n",
      "Epoch 11/20  Iteration 1921/3560 Training loss: 1.8703 0.6325 sec/batch\n",
      "Epoch 11/20  Iteration 1922/3560 Training loss: 1.8702 0.6283 sec/batch\n",
      "Epoch 11/20  Iteration 1923/3560 Training loss: 1.8701 0.6395 sec/batch\n",
      "Epoch 11/20  Iteration 1924/3560 Training loss: 1.8701 0.5788 sec/batch\n",
      "Epoch 11/20  Iteration 1925/3560 Training loss: 1.8700 0.7459 sec/batch\n",
      "Epoch 11/20  Iteration 1926/3560 Training loss: 1.8700 0.6889 sec/batch\n",
      "Epoch 11/20  Iteration 1927/3560 Training loss: 1.8700 0.7085 sec/batch\n",
      "Epoch 11/20  Iteration 1928/3560 Training loss: 1.8703 0.7024 sec/batch\n",
      "Epoch 11/20  Iteration 1929/3560 Training loss: 1.8701 0.6198 sec/batch\n",
      "Epoch 11/20  Iteration 1930/3560 Training loss: 1.8700 0.5828 sec/batch\n",
      "Epoch 11/20  Iteration 1931/3560 Training loss: 1.8697 0.6803 sec/batch\n",
      "Epoch 11/20  Iteration 1932/3560 Training loss: 1.8699 0.5773 sec/batch\n",
      "Epoch 11/20  Iteration 1933/3560 Training loss: 1.8699 0.6318 sec/batch\n",
      "Epoch 11/20  Iteration 1934/3560 Training loss: 1.8699 0.6428 sec/batch\n",
      "Epoch 11/20  Iteration 1935/3560 Training loss: 1.8698 0.5926 sec/batch\n",
      "Epoch 11/20  Iteration 1936/3560 Training loss: 1.8697 0.6515 sec/batch\n",
      "Epoch 11/20  Iteration 1937/3560 Training loss: 1.8698 0.6568 sec/batch\n",
      "Epoch 11/20  Iteration 1938/3560 Training loss: 1.8697 0.6759 sec/batch\n",
      "Epoch 11/20  Iteration 1939/3560 Training loss: 1.8695 0.7529 sec/batch\n",
      "Epoch 11/20  Iteration 1940/3560 Training loss: 1.8696 0.7247 sec/batch\n",
      "Epoch 11/20  Iteration 1941/3560 Training loss: 1.8697 0.6708 sec/batch\n",
      "Epoch 11/20  Iteration 1942/3560 Training loss: 1.8697 0.6365 sec/batch\n",
      "Epoch 11/20  Iteration 1943/3560 Training loss: 1.8697 0.5992 sec/batch\n",
      "Epoch 11/20  Iteration 1944/3560 Training loss: 1.8697 0.6550 sec/batch\n",
      "Epoch 11/20  Iteration 1945/3560 Training loss: 1.8695 0.6071 sec/batch\n",
      "Epoch 11/20  Iteration 1946/3560 Training loss: 1.8695 0.6126 sec/batch\n",
      "Epoch 11/20  Iteration 1947/3560 Training loss: 1.8694 0.6158 sec/batch\n",
      "Epoch 11/20  Iteration 1948/3560 Training loss: 1.8698 0.6601 sec/batch\n",
      "Epoch 11/20  Iteration 1949/3560 Training loss: 1.8697 0.6655 sec/batch\n",
      "Epoch 11/20  Iteration 1950/3560 Training loss: 1.8697 0.6412 sec/batch\n",
      "Epoch 11/20  Iteration 1951/3560 Training loss: 1.8695 0.7271 sec/batch\n",
      "Epoch 11/20  Iteration 1952/3560 Training loss: 1.8694 0.6948 sec/batch\n",
      "Epoch 11/20  Iteration 1953/3560 Training loss: 1.8695 0.6519 sec/batch\n",
      "Epoch 11/20  Iteration 1954/3560 Training loss: 1.8695 0.6219 sec/batch\n",
      "Epoch 11/20  Iteration 1955/3560 Training loss: 1.8696 0.6346 sec/batch\n",
      "Epoch 11/20  Iteration 1956/3560 Training loss: 1.8695 0.6316 sec/batch\n",
      "Epoch 11/20  Iteration 1957/3560 Training loss: 1.8693 0.6240 sec/batch\n",
      "Epoch 11/20  Iteration 1958/3560 Training loss: 1.8694 0.5963 sec/batch\n",
      "Epoch 12/20  Iteration 1959/3560 Training loss: 1.9238 0.6387 sec/batch\n",
      "Epoch 12/20  Iteration 1960/3560 Training loss: 1.8919 0.5913 sec/batch\n",
      "Epoch 12/20  Iteration 1961/3560 Training loss: 1.8777 0.6353 sec/batch\n",
      "Epoch 12/20  Iteration 1962/3560 Training loss: 1.8698 0.6086 sec/batch\n",
      "Epoch 12/20  Iteration 1963/3560 Training loss: 1.8673 0.6701 sec/batch\n",
      "Epoch 12/20  Iteration 1964/3560 Training loss: 1.8590 0.7614 sec/batch\n",
      "Epoch 12/20  Iteration 1965/3560 Training loss: 1.8609 0.7208 sec/batch\n",
      "Epoch 12/20  Iteration 1966/3560 Training loss: 1.8598 0.7051 sec/batch\n",
      "Epoch 12/20  Iteration 1967/3560 Training loss: 1.8634 0.5909 sec/batch\n",
      "Epoch 12/20  Iteration 1968/3560 Training loss: 1.8630 0.5396 sec/batch\n",
      "Epoch 12/20  Iteration 1969/3560 Training loss: 1.8603 0.6932 sec/batch\n",
      "Epoch 12/20  Iteration 1970/3560 Training loss: 1.8589 0.5361 sec/batch\n",
      "Epoch 12/20  Iteration 1971/3560 Training loss: 1.8583 0.8192 sec/batch\n",
      "Epoch 12/20  Iteration 1972/3560 Training loss: 1.8602 0.5870 sec/batch\n",
      "Epoch 12/20  Iteration 1973/3560 Training loss: 1.8591 0.5891 sec/batch\n",
      "Epoch 12/20  Iteration 1974/3560 Training loss: 1.8577 0.6373 sec/batch\n",
      "Epoch 12/20  Iteration 1975/3560 Training loss: 1.8577 0.6678 sec/batch\n",
      "Epoch 12/20  Iteration 1976/3560 Training loss: 1.8602 0.7014 sec/batch\n",
      "Epoch 12/20  Iteration 1977/3560 Training loss: 1.8593 0.6702 sec/batch\n",
      "Epoch 12/20  Iteration 1978/3560 Training loss: 1.8603 0.6801 sec/batch\n",
      "Epoch 12/20  Iteration 1979/3560 Training loss: 1.8596 0.5613 sec/batch\n",
      "Epoch 12/20  Iteration 1980/3560 Training loss: 1.8604 0.7079 sec/batch\n",
      "Epoch 12/20  Iteration 1981/3560 Training loss: 1.8597 0.6743 sec/batch\n",
      "Epoch 12/20  Iteration 1982/3560 Training loss: 1.8597 0.6493 sec/batch\n",
      "Epoch 12/20  Iteration 1983/3560 Training loss: 1.8593 0.6162 sec/batch\n",
      "Epoch 12/20  Iteration 1984/3560 Training loss: 1.8585 0.5719 sec/batch\n",
      "Epoch 12/20  Iteration 1985/3560 Training loss: 1.8580 0.6764 sec/batch\n",
      "Epoch 12/20  Iteration 1986/3560 Training loss: 1.8582 0.7408 sec/batch\n",
      "Epoch 12/20  Iteration 1987/3560 Training loss: 1.8590 0.6719 sec/batch\n",
      "Epoch 12/20  Iteration 1988/3560 Training loss: 1.8594 0.6407 sec/batch\n",
      "Epoch 12/20  Iteration 1989/3560 Training loss: 1.8596 0.7246 sec/batch\n",
      "Epoch 12/20  Iteration 1990/3560 Training loss: 1.8589 0.7615 sec/batch\n",
      "Epoch 12/20  Iteration 1991/3560 Training loss: 1.8590 0.7270 sec/batch\n",
      "Epoch 12/20  Iteration 1992/3560 Training loss: 1.8595 0.6737 sec/batch\n",
      "Epoch 12/20  Iteration 1993/3560 Training loss: 1.8592 0.7086 sec/batch\n",
      "Epoch 12/20  Iteration 1994/3560 Training loss: 1.8588 0.6513 sec/batch\n",
      "Epoch 12/20  Iteration 1995/3560 Training loss: 1.8582 0.7239 sec/batch\n",
      "Epoch 12/20  Iteration 1996/3560 Training loss: 1.8571 0.7916 sec/batch\n",
      "Epoch 12/20  Iteration 1997/3560 Training loss: 1.8560 0.7932 sec/batch\n",
      "Epoch 12/20  Iteration 1998/3560 Training loss: 1.8553 0.6166 sec/batch\n",
      "Epoch 12/20  Iteration 1999/3560 Training loss: 1.8548 0.8796 sec/batch\n",
      "Epoch 12/20  Iteration 2000/3560 Training loss: 1.8549 0.7330 sec/batch\n",
      "Epoch 12/20  Iteration 2001/3560 Training loss: 1.8542 0.7797 sec/batch\n",
      "Epoch 12/20  Iteration 2002/3560 Training loss: 1.8536 0.8352 sec/batch\n",
      "Epoch 12/20  Iteration 2003/3560 Training loss: 1.8538 0.8723 sec/batch\n",
      "Epoch 12/20  Iteration 2004/3560 Training loss: 1.8527 0.7585 sec/batch\n",
      "Epoch 12/20  Iteration 2005/3560 Training loss: 1.8525 0.7834 sec/batch\n",
      "Epoch 12/20  Iteration 2006/3560 Training loss: 1.8522 0.7269 sec/batch\n",
      "Epoch 12/20  Iteration 2007/3560 Training loss: 1.8520 0.6441 sec/batch\n",
      "Epoch 12/20  Iteration 2008/3560 Training loss: 1.8527 0.6531 sec/batch\n",
      "Epoch 12/20  Iteration 2009/3560 Training loss: 1.8521 0.6436 sec/batch\n",
      "Epoch 12/20  Iteration 2010/3560 Training loss: 1.8528 0.6328 sec/batch\n",
      "Epoch 12/20  Iteration 2011/3560 Training loss: 1.8526 0.6818 sec/batch\n",
      "Epoch 12/20  Iteration 2012/3560 Training loss: 1.8527 0.6898 sec/batch\n",
      "Epoch 12/20  Iteration 2013/3560 Training loss: 1.8526 0.6844 sec/batch\n",
      "Epoch 12/20  Iteration 2014/3560 Training loss: 1.8527 0.8232 sec/batch\n",
      "Epoch 12/20  Iteration 2015/3560 Training loss: 1.8530 0.7982 sec/batch\n",
      "Epoch 12/20  Iteration 2016/3560 Training loss: 1.8527 0.8169 sec/batch\n",
      "Epoch 12/20  Iteration 2017/3560 Training loss: 1.8524 0.6640 sec/batch\n",
      "Epoch 12/20  Iteration 2018/3560 Training loss: 1.8527 0.6413 sec/batch\n",
      "Epoch 12/20  Iteration 2019/3560 Training loss: 1.8525 0.6121 sec/batch\n",
      "Epoch 12/20  Iteration 2020/3560 Training loss: 1.8534 0.6625 sec/batch\n",
      "Epoch 12/20  Iteration 2021/3560 Training loss: 1.8536 0.7314 sec/batch\n",
      "Epoch 12/20  Iteration 2022/3560 Training loss: 1.8537 0.7120 sec/batch\n",
      "Epoch 12/20  Iteration 2023/3560 Training loss: 1.8536 0.6291 sec/batch\n",
      "Epoch 12/20  Iteration 2024/3560 Training loss: 1.8539 0.5612 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 2025/3560 Training loss: 1.8540 0.7279 sec/batch\n",
      "Epoch 12/20  Iteration 2026/3560 Training loss: 1.8535 0.6511 sec/batch\n",
      "Epoch 12/20  Iteration 2027/3560 Training loss: 1.8534 0.9678 sec/batch\n",
      "Epoch 12/20  Iteration 2028/3560 Training loss: 1.8533 0.8107 sec/batch\n",
      "Epoch 12/20  Iteration 2029/3560 Training loss: 1.8536 0.7051 sec/batch\n",
      "Epoch 12/20  Iteration 2030/3560 Training loss: 1.8538 0.7917 sec/batch\n",
      "Epoch 12/20  Iteration 2031/3560 Training loss: 1.8542 0.6701 sec/batch\n",
      "Epoch 12/20  Iteration 2032/3560 Training loss: 1.8539 0.6249 sec/batch\n",
      "Epoch 12/20  Iteration 2033/3560 Training loss: 1.8539 0.6261 sec/batch\n",
      "Epoch 12/20  Iteration 2034/3560 Training loss: 1.8540 0.6184 sec/batch\n",
      "Epoch 12/20  Iteration 2035/3560 Training loss: 1.8539 0.6585 sec/batch\n",
      "Epoch 12/20  Iteration 2036/3560 Training loss: 1.8539 0.6313 sec/batch\n",
      "Epoch 12/20  Iteration 2037/3560 Training loss: 1.8533 0.7481 sec/batch\n",
      "Epoch 12/20  Iteration 2038/3560 Training loss: 1.8532 0.6525 sec/batch\n",
      "Epoch 12/20  Iteration 2039/3560 Training loss: 1.8528 0.6873 sec/batch\n",
      "Epoch 12/20  Iteration 2040/3560 Training loss: 1.8528 0.8571 sec/batch\n",
      "Epoch 12/20  Iteration 2041/3560 Training loss: 1.8523 0.6942 sec/batch\n",
      "Epoch 12/20  Iteration 2042/3560 Training loss: 1.8523 0.5998 sec/batch\n",
      "Epoch 12/20  Iteration 2043/3560 Training loss: 1.8518 0.6351 sec/batch\n",
      "Epoch 12/20  Iteration 2044/3560 Training loss: 1.8514 0.6583 sec/batch\n",
      "Epoch 12/20  Iteration 2045/3560 Training loss: 1.8514 0.6735 sec/batch\n",
      "Epoch 12/20  Iteration 2046/3560 Training loss: 1.8511 0.6279 sec/batch\n",
      "Epoch 12/20  Iteration 2047/3560 Training loss: 1.8506 0.6488 sec/batch\n",
      "Epoch 12/20  Iteration 2048/3560 Training loss: 1.8507 0.6357 sec/batch\n",
      "Epoch 12/20  Iteration 2049/3560 Training loss: 1.8504 0.6908 sec/batch\n",
      "Epoch 12/20  Iteration 2050/3560 Training loss: 1.8503 0.6860 sec/batch\n",
      "Epoch 12/20  Iteration 2051/3560 Training loss: 1.8499 0.8120 sec/batch\n",
      "Epoch 12/20  Iteration 2052/3560 Training loss: 1.8494 0.6533 sec/batch\n",
      "Epoch 12/20  Iteration 2053/3560 Training loss: 1.8491 0.7353 sec/batch\n",
      "Epoch 12/20  Iteration 2054/3560 Training loss: 1.8491 0.6855 sec/batch\n",
      "Epoch 12/20  Iteration 2055/3560 Training loss: 1.8488 0.6733 sec/batch\n",
      "Epoch 12/20  Iteration 2056/3560 Training loss: 1.8484 0.6609 sec/batch\n",
      "Epoch 12/20  Iteration 2057/3560 Training loss: 1.8481 0.7002 sec/batch\n",
      "Epoch 12/20  Iteration 2058/3560 Training loss: 1.8475 0.6706 sec/batch\n",
      "Epoch 12/20  Iteration 2059/3560 Training loss: 1.8475 0.6374 sec/batch\n",
      "Epoch 12/20  Iteration 2060/3560 Training loss: 1.8473 0.7241 sec/batch\n",
      "Epoch 12/20  Iteration 2061/3560 Training loss: 1.8471 0.7662 sec/batch\n",
      "Epoch 12/20  Iteration 2062/3560 Training loss: 1.8468 0.7103 sec/batch\n",
      "Epoch 12/20  Iteration 2063/3560 Training loss: 1.8466 0.9662 sec/batch\n",
      "Epoch 12/20  Iteration 2064/3560 Training loss: 1.8466 0.9714 sec/batch\n",
      "Epoch 12/20  Iteration 2065/3560 Training loss: 1.8464 0.8652 sec/batch\n",
      "Epoch 12/20  Iteration 2066/3560 Training loss: 1.8464 0.9819 sec/batch\n",
      "Epoch 12/20  Iteration 2067/3560 Training loss: 1.8464 0.7698 sec/batch\n",
      "Epoch 12/20  Iteration 2068/3560 Training loss: 1.8464 0.8266 sec/batch\n",
      "Epoch 12/20  Iteration 2069/3560 Training loss: 1.8463 0.6267 sec/batch\n",
      "Epoch 12/20  Iteration 2070/3560 Training loss: 1.8461 0.5929 sec/batch\n",
      "Epoch 12/20  Iteration 2071/3560 Training loss: 1.8460 0.6655 sec/batch\n",
      "Epoch 12/20  Iteration 2072/3560 Training loss: 1.8459 0.6200 sec/batch\n",
      "Epoch 12/20  Iteration 2073/3560 Training loss: 1.8457 0.6281 sec/batch\n",
      "Epoch 12/20  Iteration 2074/3560 Training loss: 1.8453 0.5857 sec/batch\n",
      "Epoch 12/20  Iteration 2075/3560 Training loss: 1.8453 0.6114 sec/batch\n",
      "Epoch 12/20  Iteration 2076/3560 Training loss: 1.8453 0.6037 sec/batch\n",
      "Epoch 12/20  Iteration 2077/3560 Training loss: 1.8453 0.7116 sec/batch\n",
      "Epoch 12/20  Iteration 2078/3560 Training loss: 1.8453 0.6728 sec/batch\n",
      "Epoch 12/20  Iteration 2079/3560 Training loss: 1.8452 0.7176 sec/batch\n",
      "Epoch 12/20  Iteration 2080/3560 Training loss: 1.8449 0.6527 sec/batch\n",
      "Epoch 12/20  Iteration 2081/3560 Training loss: 1.8446 0.6217 sec/batch\n",
      "Epoch 12/20  Iteration 2082/3560 Training loss: 1.8448 0.6539 sec/batch\n",
      "Epoch 12/20  Iteration 2083/3560 Training loss: 1.8447 0.6823 sec/batch\n",
      "Epoch 12/20  Iteration 2084/3560 Training loss: 1.8443 0.6171 sec/batch\n",
      "Epoch 12/20  Iteration 2085/3560 Training loss: 1.8444 0.7863 sec/batch\n",
      "Epoch 12/20  Iteration 2086/3560 Training loss: 1.8445 0.6474 sec/batch\n",
      "Epoch 12/20  Iteration 2087/3560 Training loss: 1.8444 0.5905 sec/batch\n",
      "Epoch 12/20  Iteration 2088/3560 Training loss: 1.8443 0.6896 sec/batch\n",
      "Epoch 12/20  Iteration 2089/3560 Training loss: 1.8440 0.6534 sec/batch\n",
      "Epoch 12/20  Iteration 2090/3560 Training loss: 1.8439 0.8661 sec/batch\n",
      "Epoch 12/20  Iteration 2091/3560 Training loss: 1.8439 0.6788 sec/batch\n",
      "Epoch 12/20  Iteration 2092/3560 Training loss: 1.8438 0.6757 sec/batch\n",
      "Epoch 12/20  Iteration 2093/3560 Training loss: 1.8438 0.6630 sec/batch\n",
      "Epoch 12/20  Iteration 2094/3560 Training loss: 1.8439 0.6486 sec/batch\n",
      "Epoch 12/20  Iteration 2095/3560 Training loss: 1.8439 0.5610 sec/batch\n",
      "Epoch 12/20  Iteration 2096/3560 Training loss: 1.8440 0.6459 sec/batch\n",
      "Epoch 12/20  Iteration 2097/3560 Training loss: 1.8441 0.6203 sec/batch\n",
      "Epoch 12/20  Iteration 2098/3560 Training loss: 1.8440 0.6443 sec/batch\n",
      "Epoch 12/20  Iteration 2099/3560 Training loss: 1.8443 0.6665 sec/batch\n",
      "Epoch 12/20  Iteration 2100/3560 Training loss: 1.8442 0.5911 sec/batch\n",
      "Epoch 12/20  Iteration 2101/3560 Training loss: 1.8442 0.6812 sec/batch\n",
      "Epoch 12/20  Iteration 2102/3560 Training loss: 1.8442 0.5899 sec/batch\n",
      "Epoch 12/20  Iteration 2103/3560 Training loss: 1.8441 0.7446 sec/batch\n",
      "Epoch 12/20  Iteration 2104/3560 Training loss: 1.8442 0.7313 sec/batch\n",
      "Epoch 12/20  Iteration 2105/3560 Training loss: 1.8442 0.5830 sec/batch\n",
      "Epoch 12/20  Iteration 2106/3560 Training loss: 1.8443 0.6648 sec/batch\n",
      "Epoch 12/20  Iteration 2107/3560 Training loss: 1.8443 0.6186 sec/batch\n",
      "Epoch 12/20  Iteration 2108/3560 Training loss: 1.8441 0.6691 sec/batch\n",
      "Epoch 12/20  Iteration 2109/3560 Training loss: 1.8439 0.6377 sec/batch\n",
      "Epoch 12/20  Iteration 2110/3560 Training loss: 1.8439 0.6267 sec/batch\n",
      "Epoch 12/20  Iteration 2111/3560 Training loss: 1.8440 0.6137 sec/batch\n",
      "Epoch 12/20  Iteration 2112/3560 Training loss: 1.8439 0.6720 sec/batch\n",
      "Epoch 12/20  Iteration 2113/3560 Training loss: 1.8439 0.5965 sec/batch\n",
      "Epoch 12/20  Iteration 2114/3560 Training loss: 1.8438 0.6682 sec/batch\n",
      "Epoch 12/20  Iteration 2115/3560 Training loss: 1.8438 0.7019 sec/batch\n",
      "Epoch 12/20  Iteration 2116/3560 Training loss: 1.8437 0.6918 sec/batch\n",
      "Epoch 12/20  Iteration 2117/3560 Training loss: 1.8435 0.6678 sec/batch\n",
      "Epoch 12/20  Iteration 2118/3560 Training loss: 1.8436 0.6069 sec/batch\n",
      "Epoch 12/20  Iteration 2119/3560 Training loss: 1.8438 0.6847 sec/batch\n",
      "Epoch 12/20  Iteration 2120/3560 Training loss: 1.8436 0.6417 sec/batch\n",
      "Epoch 12/20  Iteration 2121/3560 Training loss: 1.8437 0.6350 sec/batch\n",
      "Epoch 12/20  Iteration 2122/3560 Training loss: 1.8437 0.6071 sec/batch\n",
      "Epoch 12/20  Iteration 2123/3560 Training loss: 1.8437 0.6228 sec/batch\n",
      "Epoch 12/20  Iteration 2124/3560 Training loss: 1.8437 0.5915 sec/batch\n",
      "Epoch 12/20  Iteration 2125/3560 Training loss: 1.8437 0.6332 sec/batch\n",
      "Epoch 12/20  Iteration 2126/3560 Training loss: 1.8440 0.6781 sec/batch\n",
      "Epoch 12/20  Iteration 2127/3560 Training loss: 1.8439 0.6622 sec/batch\n",
      "Epoch 12/20  Iteration 2128/3560 Training loss: 1.8439 0.8083 sec/batch\n",
      "Epoch 12/20  Iteration 2129/3560 Training loss: 1.8437 0.6492 sec/batch\n",
      "Epoch 12/20  Iteration 2130/3560 Training loss: 1.8435 0.7333 sec/batch\n",
      "Epoch 12/20  Iteration 2131/3560 Training loss: 1.8436 0.5903 sec/batch\n",
      "Epoch 12/20  Iteration 2132/3560 Training loss: 1.8436 0.6952 sec/batch\n",
      "Epoch 12/20  Iteration 2133/3560 Training loss: 1.8437 0.6813 sec/batch\n",
      "Epoch 12/20  Iteration 2134/3560 Training loss: 1.8436 0.6107 sec/batch\n",
      "Epoch 12/20  Iteration 2135/3560 Training loss: 1.8434 0.6439 sec/batch\n",
      "Epoch 12/20  Iteration 2136/3560 Training loss: 1.8434 0.6367 sec/batch\n",
      "Epoch 13/20  Iteration 2137/3560 Training loss: 1.8939 0.5746 sec/batch\n",
      "Epoch 13/20  Iteration 2138/3560 Training loss: 1.8605 0.6140 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2139/3560 Training loss: 1.8517 0.6713 sec/batch\n",
      "Epoch 13/20  Iteration 2140/3560 Training loss: 1.8456 0.6418 sec/batch\n",
      "Epoch 13/20  Iteration 2141/3560 Training loss: 1.8407 0.7484 sec/batch\n",
      "Epoch 13/20  Iteration 2142/3560 Training loss: 1.8314 0.6162 sec/batch\n",
      "Epoch 13/20  Iteration 2143/3560 Training loss: 1.8327 0.7202 sec/batch\n",
      "Epoch 13/20  Iteration 2144/3560 Training loss: 1.8338 0.6291 sec/batch\n",
      "Epoch 13/20  Iteration 2145/3560 Training loss: 1.8376 0.7576 sec/batch\n",
      "Epoch 13/20  Iteration 2146/3560 Training loss: 1.8369 0.5325 sec/batch\n",
      "Epoch 13/20  Iteration 2147/3560 Training loss: 1.8344 0.7048 sec/batch\n",
      "Epoch 13/20  Iteration 2148/3560 Training loss: 1.8333 0.6458 sec/batch\n",
      "Epoch 13/20  Iteration 2149/3560 Training loss: 1.8327 0.6104 sec/batch\n",
      "Epoch 13/20  Iteration 2150/3560 Training loss: 1.8355 0.6202 sec/batch\n",
      "Epoch 13/20  Iteration 2151/3560 Training loss: 1.8346 0.7128 sec/batch\n",
      "Epoch 13/20  Iteration 2152/3560 Training loss: 1.8332 0.6518 sec/batch\n",
      "Epoch 13/20  Iteration 2153/3560 Training loss: 1.8331 0.6585 sec/batch\n",
      "Epoch 13/20  Iteration 2154/3560 Training loss: 1.8355 0.7529 sec/batch\n",
      "Epoch 13/20  Iteration 2155/3560 Training loss: 1.8351 0.6370 sec/batch\n",
      "Epoch 13/20  Iteration 2156/3560 Training loss: 1.8359 0.6471 sec/batch\n",
      "Epoch 13/20  Iteration 2157/3560 Training loss: 1.8351 0.5754 sec/batch\n",
      "Epoch 13/20  Iteration 2158/3560 Training loss: 1.8357 0.6360 sec/batch\n",
      "Epoch 13/20  Iteration 2159/3560 Training loss: 1.8351 0.6882 sec/batch\n",
      "Epoch 13/20  Iteration 2160/3560 Training loss: 1.8345 0.5623 sec/batch\n",
      "Epoch 13/20  Iteration 2161/3560 Training loss: 1.8340 0.6429 sec/batch\n",
      "Epoch 13/20  Iteration 2162/3560 Training loss: 1.8334 0.6423 sec/batch\n",
      "Epoch 13/20  Iteration 2163/3560 Training loss: 1.8326 0.6448 sec/batch\n",
      "Epoch 13/20  Iteration 2164/3560 Training loss: 1.8329 0.6726 sec/batch\n",
      "Epoch 13/20  Iteration 2165/3560 Training loss: 1.8338 0.6973 sec/batch\n",
      "Epoch 13/20  Iteration 2166/3560 Training loss: 1.8342 0.6686 sec/batch\n",
      "Epoch 13/20  Iteration 2167/3560 Training loss: 1.8339 0.6506 sec/batch\n",
      "Epoch 13/20  Iteration 2168/3560 Training loss: 1.8333 0.6679 sec/batch\n",
      "Epoch 13/20  Iteration 2169/3560 Training loss: 1.8334 0.6822 sec/batch\n",
      "Epoch 13/20  Iteration 2170/3560 Training loss: 1.8341 0.6447 sec/batch\n",
      "Epoch 13/20  Iteration 2171/3560 Training loss: 1.8337 0.5641 sec/batch\n",
      "Epoch 13/20  Iteration 2172/3560 Training loss: 1.8335 0.6757 sec/batch\n",
      "Epoch 13/20  Iteration 2173/3560 Training loss: 1.8328 0.6109 sec/batch\n",
      "Epoch 13/20  Iteration 2174/3560 Training loss: 1.8321 0.7381 sec/batch\n",
      "Epoch 13/20  Iteration 2175/3560 Training loss: 1.8310 0.6336 sec/batch\n",
      "Epoch 13/20  Iteration 2176/3560 Training loss: 1.8302 0.7339 sec/batch\n",
      "Epoch 13/20  Iteration 2177/3560 Training loss: 1.8296 0.7035 sec/batch\n",
      "Epoch 13/20  Iteration 2178/3560 Training loss: 1.8299 0.6458 sec/batch\n",
      "Epoch 13/20  Iteration 2179/3560 Training loss: 1.8295 0.8035 sec/batch\n",
      "Epoch 13/20  Iteration 2180/3560 Training loss: 1.8289 0.6576 sec/batch\n",
      "Epoch 13/20  Iteration 2181/3560 Training loss: 1.8291 0.6976 sec/batch\n",
      "Epoch 13/20  Iteration 2182/3560 Training loss: 1.8282 0.6976 sec/batch\n",
      "Epoch 13/20  Iteration 2183/3560 Training loss: 1.8280 0.7184 sec/batch\n",
      "Epoch 13/20  Iteration 2184/3560 Training loss: 1.8275 0.6460 sec/batch\n",
      "Epoch 13/20  Iteration 2185/3560 Training loss: 1.8271 0.6811 sec/batch\n",
      "Epoch 13/20  Iteration 2186/3560 Training loss: 1.8279 0.6734 sec/batch\n",
      "Epoch 13/20  Iteration 2187/3560 Training loss: 1.8274 0.7147 sec/batch\n",
      "Epoch 13/20  Iteration 2188/3560 Training loss: 1.8281 0.6980 sec/batch\n",
      "Epoch 13/20  Iteration 2189/3560 Training loss: 1.8280 0.7572 sec/batch\n",
      "Epoch 13/20  Iteration 2190/3560 Training loss: 1.8280 0.6482 sec/batch\n",
      "Epoch 13/20  Iteration 2191/3560 Training loss: 1.8275 0.8318 sec/batch\n",
      "Epoch 13/20  Iteration 2192/3560 Training loss: 1.8277 0.6667 sec/batch\n",
      "Epoch 13/20  Iteration 2193/3560 Training loss: 1.8281 0.7172 sec/batch\n",
      "Epoch 13/20  Iteration 2194/3560 Training loss: 1.8278 0.6449 sec/batch\n",
      "Epoch 13/20  Iteration 2195/3560 Training loss: 1.8274 0.7116 sec/batch\n",
      "Epoch 13/20  Iteration 2196/3560 Training loss: 1.8279 0.6329 sec/batch\n",
      "Epoch 13/20  Iteration 2197/3560 Training loss: 1.8278 0.7185 sec/batch\n",
      "Epoch 13/20  Iteration 2198/3560 Training loss: 1.8287 0.6609 sec/batch\n",
      "Epoch 13/20  Iteration 2199/3560 Training loss: 1.8290 0.5702 sec/batch\n",
      "Epoch 13/20  Iteration 2200/3560 Training loss: 1.8291 0.7176 sec/batch\n",
      "Epoch 13/20  Iteration 2201/3560 Training loss: 1.8290 0.6641 sec/batch\n",
      "Epoch 13/20  Iteration 2202/3560 Training loss: 1.8291 0.6167 sec/batch\n",
      "Epoch 13/20  Iteration 2203/3560 Training loss: 1.8292 0.7092 sec/batch\n",
      "Epoch 13/20  Iteration 2204/3560 Training loss: 1.8288 0.7377 sec/batch\n",
      "Epoch 13/20  Iteration 2205/3560 Training loss: 1.8286 0.6071 sec/batch\n",
      "Epoch 13/20  Iteration 2206/3560 Training loss: 1.8286 0.7446 sec/batch\n",
      "Epoch 13/20  Iteration 2207/3560 Training loss: 1.8290 0.6134 sec/batch\n",
      "Epoch 13/20  Iteration 2208/3560 Training loss: 1.8291 0.7661 sec/batch\n",
      "Epoch 13/20  Iteration 2209/3560 Training loss: 1.8295 0.6603 sec/batch\n",
      "Epoch 13/20  Iteration 2210/3560 Training loss: 1.8293 0.6450 sec/batch\n",
      "Epoch 13/20  Iteration 2211/3560 Training loss: 1.8292 0.6243 sec/batch\n",
      "Epoch 13/20  Iteration 2212/3560 Training loss: 1.8293 0.6040 sec/batch\n",
      "Epoch 13/20  Iteration 2213/3560 Training loss: 1.8293 0.5563 sec/batch\n",
      "Epoch 13/20  Iteration 2214/3560 Training loss: 1.8295 0.7321 sec/batch\n",
      "Epoch 13/20  Iteration 2215/3560 Training loss: 1.8291 0.6465 sec/batch\n",
      "Epoch 13/20  Iteration 2216/3560 Training loss: 1.8288 0.7733 sec/batch\n",
      "Epoch 13/20  Iteration 2217/3560 Training loss: 1.8282 0.7093 sec/batch\n",
      "Epoch 13/20  Iteration 2218/3560 Training loss: 1.8283 0.6984 sec/batch\n",
      "Epoch 13/20  Iteration 2219/3560 Training loss: 1.8279 0.7216 sec/batch\n",
      "Epoch 13/20  Iteration 2220/3560 Training loss: 1.8278 0.5969 sec/batch\n",
      "Epoch 13/20  Iteration 2221/3560 Training loss: 1.8273 0.7106 sec/batch\n",
      "Epoch 13/20  Iteration 2222/3560 Training loss: 1.8268 0.6398 sec/batch\n",
      "Epoch 13/20  Iteration 2223/3560 Training loss: 1.8268 0.6253 sec/batch\n",
      "Epoch 13/20  Iteration 2224/3560 Training loss: 1.8264 0.6594 sec/batch\n",
      "Epoch 13/20  Iteration 2225/3560 Training loss: 1.8260 0.6224 sec/batch\n",
      "Epoch 13/20  Iteration 2226/3560 Training loss: 1.8260 0.6126 sec/batch\n",
      "Epoch 13/20  Iteration 2227/3560 Training loss: 1.8257 0.6864 sec/batch\n",
      "Epoch 13/20  Iteration 2228/3560 Training loss: 1.8256 0.6566 sec/batch\n",
      "Epoch 13/20  Iteration 2229/3560 Training loss: 1.8251 0.7999 sec/batch\n",
      "Epoch 13/20  Iteration 2230/3560 Training loss: 1.8248 0.6673 sec/batch\n",
      "Epoch 13/20  Iteration 2231/3560 Training loss: 1.8244 0.6543 sec/batch\n",
      "Epoch 13/20  Iteration 2232/3560 Training loss: 1.8243 0.7528 sec/batch\n",
      "Epoch 13/20  Iteration 2233/3560 Training loss: 1.8241 0.6760 sec/batch\n",
      "Epoch 13/20  Iteration 2234/3560 Training loss: 1.8238 0.6657 sec/batch\n",
      "Epoch 13/20  Iteration 2235/3560 Training loss: 1.8234 0.5638 sec/batch\n",
      "Epoch 13/20  Iteration 2236/3560 Training loss: 1.8231 0.6967 sec/batch\n",
      "Epoch 13/20  Iteration 2237/3560 Training loss: 1.8231 0.5707 sec/batch\n",
      "Epoch 13/20  Iteration 2238/3560 Training loss: 1.8230 0.7492 sec/batch\n",
      "Epoch 13/20  Iteration 2239/3560 Training loss: 1.8227 0.6898 sec/batch\n",
      "Epoch 13/20  Iteration 2240/3560 Training loss: 1.8225 0.8318 sec/batch\n",
      "Epoch 13/20  Iteration 2241/3560 Training loss: 1.8224 0.7468 sec/batch\n",
      "Epoch 13/20  Iteration 2242/3560 Training loss: 1.8223 0.6912 sec/batch\n",
      "Epoch 13/20  Iteration 2243/3560 Training loss: 1.8222 0.6837 sec/batch\n",
      "Epoch 13/20  Iteration 2244/3560 Training loss: 1.8222 0.6679 sec/batch\n",
      "Epoch 13/20  Iteration 2245/3560 Training loss: 1.8222 0.6587 sec/batch\n",
      "Epoch 13/20  Iteration 2246/3560 Training loss: 1.8221 0.6040 sec/batch\n",
      "Epoch 13/20  Iteration 2247/3560 Training loss: 1.8220 0.7469 sec/batch\n",
      "Epoch 13/20  Iteration 2248/3560 Training loss: 1.8217 0.5648 sec/batch\n",
      "Epoch 13/20  Iteration 2249/3560 Training loss: 1.8216 0.6323 sec/batch\n",
      "Epoch 13/20  Iteration 2250/3560 Training loss: 1.8214 0.6806 sec/batch\n",
      "Epoch 13/20  Iteration 2251/3560 Training loss: 1.8213 0.6548 sec/batch\n",
      "Epoch 13/20  Iteration 2252/3560 Training loss: 1.8209 0.7155 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2253/3560 Training loss: 1.8209 0.6621 sec/batch\n",
      "Epoch 13/20  Iteration 2254/3560 Training loss: 1.8209 0.7415 sec/batch\n",
      "Epoch 13/20  Iteration 2255/3560 Training loss: 1.8208 0.8260 sec/batch\n",
      "Epoch 13/20  Iteration 2256/3560 Training loss: 1.8208 0.6427 sec/batch\n",
      "Epoch 13/20  Iteration 2257/3560 Training loss: 1.8208 0.7151 sec/batch\n",
      "Epoch 13/20  Iteration 2258/3560 Training loss: 1.8204 0.6420 sec/batch\n",
      "Epoch 13/20  Iteration 2259/3560 Training loss: 1.8202 0.6493 sec/batch\n",
      "Epoch 13/20  Iteration 2260/3560 Training loss: 1.8204 0.6212 sec/batch\n",
      "Epoch 13/20  Iteration 2261/3560 Training loss: 1.8203 0.6751 sec/batch\n",
      "Epoch 13/20  Iteration 2262/3560 Training loss: 1.8199 0.6479 sec/batch\n",
      "Epoch 13/20  Iteration 2263/3560 Training loss: 1.8199 0.6865 sec/batch\n",
      "Epoch 13/20  Iteration 2264/3560 Training loss: 1.8200 0.7155 sec/batch\n",
      "Epoch 13/20  Iteration 2265/3560 Training loss: 1.8199 0.6857 sec/batch\n",
      "Epoch 13/20  Iteration 2266/3560 Training loss: 1.8198 0.7554 sec/batch\n",
      "Epoch 13/20  Iteration 2267/3560 Training loss: 1.8195 0.7773 sec/batch\n",
      "Epoch 13/20  Iteration 2268/3560 Training loss: 1.8193 0.7478 sec/batch\n",
      "Epoch 13/20  Iteration 2269/3560 Training loss: 1.8193 0.7595 sec/batch\n",
      "Epoch 13/20  Iteration 2270/3560 Training loss: 1.8194 0.6970 sec/batch\n",
      "Epoch 13/20  Iteration 2271/3560 Training loss: 1.8193 0.5693 sec/batch\n",
      "Epoch 13/20  Iteration 2272/3560 Training loss: 1.8193 0.6033 sec/batch\n",
      "Epoch 13/20  Iteration 2273/3560 Training loss: 1.8195 0.6673 sec/batch\n",
      "Epoch 13/20  Iteration 2274/3560 Training loss: 1.8195 0.6531 sec/batch\n",
      "Epoch 13/20  Iteration 2275/3560 Training loss: 1.8196 0.6464 sec/batch\n",
      "Epoch 13/20  Iteration 2276/3560 Training loss: 1.8194 0.5365 sec/batch\n",
      "Epoch 13/20  Iteration 2277/3560 Training loss: 1.8197 0.6218 sec/batch\n",
      "Epoch 13/20  Iteration 2278/3560 Training loss: 1.8195 0.6575 sec/batch\n",
      "Epoch 13/20  Iteration 2279/3560 Training loss: 1.8195 0.7330 sec/batch\n",
      "Epoch 13/20  Iteration 2280/3560 Training loss: 1.8196 0.8185 sec/batch\n",
      "Epoch 13/20  Iteration 2281/3560 Training loss: 1.8195 0.6450 sec/batch\n",
      "Epoch 13/20  Iteration 2282/3560 Training loss: 1.8196 0.7538 sec/batch\n",
      "Epoch 13/20  Iteration 2283/3560 Training loss: 1.8195 0.7017 sec/batch\n",
      "Epoch 13/20  Iteration 2284/3560 Training loss: 1.8196 0.6222 sec/batch\n",
      "Epoch 13/20  Iteration 2285/3560 Training loss: 1.8196 0.6980 sec/batch\n",
      "Epoch 13/20  Iteration 2286/3560 Training loss: 1.8195 0.6928 sec/batch\n",
      "Epoch 13/20  Iteration 2287/3560 Training loss: 1.8192 0.6744 sec/batch\n",
      "Epoch 13/20  Iteration 2288/3560 Training loss: 1.8193 0.6743 sec/batch\n",
      "Epoch 13/20  Iteration 2289/3560 Training loss: 1.8193 0.6044 sec/batch\n",
      "Epoch 13/20  Iteration 2290/3560 Training loss: 1.8193 0.6384 sec/batch\n",
      "Epoch 13/20  Iteration 2291/3560 Training loss: 1.8193 0.7084 sec/batch\n",
      "Epoch 13/20  Iteration 2292/3560 Training loss: 1.8193 0.7504 sec/batch\n",
      "Epoch 13/20  Iteration 2293/3560 Training loss: 1.8193 0.7004 sec/batch\n",
      "Epoch 13/20  Iteration 2294/3560 Training loss: 1.8192 0.8083 sec/batch\n",
      "Epoch 13/20  Iteration 2295/3560 Training loss: 1.8190 0.7159 sec/batch\n",
      "Epoch 13/20  Iteration 2296/3560 Training loss: 1.8191 0.6860 sec/batch\n",
      "Epoch 13/20  Iteration 2297/3560 Training loss: 1.8193 0.6959 sec/batch\n",
      "Epoch 13/20  Iteration 2298/3560 Training loss: 1.8192 0.7024 sec/batch\n",
      "Epoch 13/20  Iteration 2299/3560 Training loss: 1.8193 0.8602 sec/batch\n",
      "Epoch 13/20  Iteration 2300/3560 Training loss: 1.8192 0.9006 sec/batch\n",
      "Epoch 13/20  Iteration 2301/3560 Training loss: 1.8193 0.7518 sec/batch\n",
      "Epoch 13/20  Iteration 2302/3560 Training loss: 1.8192 0.9301 sec/batch\n",
      "Epoch 13/20  Iteration 2303/3560 Training loss: 1.8192 0.7043 sec/batch\n",
      "Epoch 13/20  Iteration 2304/3560 Training loss: 1.8195 0.9044 sec/batch\n",
      "Epoch 13/20  Iteration 2305/3560 Training loss: 1.8195 0.6811 sec/batch\n",
      "Epoch 13/20  Iteration 2306/3560 Training loss: 1.8195 0.8466 sec/batch\n",
      "Epoch 13/20  Iteration 2307/3560 Training loss: 1.8194 0.8219 sec/batch\n",
      "Epoch 13/20  Iteration 2308/3560 Training loss: 1.8192 0.6625 sec/batch\n",
      "Epoch 13/20  Iteration 2309/3560 Training loss: 1.8193 0.7054 sec/batch\n",
      "Epoch 13/20  Iteration 2310/3560 Training loss: 1.8193 0.6458 sec/batch\n",
      "Epoch 13/20  Iteration 2311/3560 Training loss: 1.8194 0.7412 sec/batch\n",
      "Epoch 13/20  Iteration 2312/3560 Training loss: 1.8193 0.6374 sec/batch\n",
      "Epoch 13/20  Iteration 2313/3560 Training loss: 1.8192 0.6530 sec/batch\n",
      "Epoch 13/20  Iteration 2314/3560 Training loss: 1.8192 0.7690 sec/batch\n",
      "Epoch 14/20  Iteration 2315/3560 Training loss: 1.8665 0.7067 sec/batch\n",
      "Epoch 14/20  Iteration 2316/3560 Training loss: 1.8378 0.6233 sec/batch\n",
      "Epoch 14/20  Iteration 2317/3560 Training loss: 1.8258 0.7040 sec/batch\n",
      "Epoch 14/20  Iteration 2318/3560 Training loss: 1.8224 0.7359 sec/batch\n",
      "Epoch 14/20  Iteration 2319/3560 Training loss: 1.8193 0.6453 sec/batch\n",
      "Epoch 14/20  Iteration 2320/3560 Training loss: 1.8104 0.5967 sec/batch\n",
      "Epoch 14/20  Iteration 2321/3560 Training loss: 1.8120 0.7084 sec/batch\n",
      "Epoch 14/20  Iteration 2322/3560 Training loss: 1.8115 0.6197 sec/batch\n",
      "Epoch 14/20  Iteration 2323/3560 Training loss: 1.8134 0.6098 sec/batch\n",
      "Epoch 14/20  Iteration 2324/3560 Training loss: 1.8145 0.6077 sec/batch\n",
      "Epoch 14/20  Iteration 2325/3560 Training loss: 1.8118 0.6134 sec/batch\n",
      "Epoch 14/20  Iteration 2326/3560 Training loss: 1.8103 0.6348 sec/batch\n",
      "Epoch 14/20  Iteration 2327/3560 Training loss: 1.8106 0.5988 sec/batch\n",
      "Epoch 14/20  Iteration 2328/3560 Training loss: 1.8140 0.5793 sec/batch\n",
      "Epoch 14/20  Iteration 2329/3560 Training loss: 1.8122 0.7009 sec/batch\n",
      "Epoch 14/20  Iteration 2330/3560 Training loss: 1.8108 0.7062 sec/batch\n",
      "Epoch 14/20  Iteration 2331/3560 Training loss: 1.8112 0.6452 sec/batch\n",
      "Epoch 14/20  Iteration 2332/3560 Training loss: 1.8134 0.6769 sec/batch\n",
      "Epoch 14/20  Iteration 2333/3560 Training loss: 1.8130 0.7155 sec/batch\n",
      "Epoch 14/20  Iteration 2334/3560 Training loss: 1.8135 0.6682 sec/batch\n",
      "Epoch 14/20  Iteration 2335/3560 Training loss: 1.8127 0.6414 sec/batch\n",
      "Epoch 14/20  Iteration 2336/3560 Training loss: 1.8138 0.6399 sec/batch\n",
      "Epoch 14/20  Iteration 2337/3560 Training loss: 1.8128 0.6095 sec/batch\n",
      "Epoch 14/20  Iteration 2338/3560 Training loss: 1.8124 0.6156 sec/batch\n",
      "Epoch 14/20  Iteration 2339/3560 Training loss: 1.8118 0.6321 sec/batch\n",
      "Epoch 14/20  Iteration 2340/3560 Training loss: 1.8111 0.6864 sec/batch\n",
      "Epoch 14/20  Iteration 2341/3560 Training loss: 1.8100 0.6697 sec/batch\n",
      "Epoch 14/20  Iteration 2342/3560 Training loss: 1.8104 0.6740 sec/batch\n",
      "Epoch 14/20  Iteration 2343/3560 Training loss: 1.8110 0.7424 sec/batch\n",
      "Epoch 14/20  Iteration 2344/3560 Training loss: 1.8111 0.6232 sec/batch\n",
      "Epoch 14/20  Iteration 2345/3560 Training loss: 1.8109 0.6657 sec/batch\n",
      "Epoch 14/20  Iteration 2346/3560 Training loss: 1.8103 0.5730 sec/batch\n",
      "Epoch 14/20  Iteration 2347/3560 Training loss: 1.8108 0.7178 sec/batch\n",
      "Epoch 14/20  Iteration 2348/3560 Training loss: 1.8114 0.5972 sec/batch\n",
      "Epoch 14/20  Iteration 2349/3560 Training loss: 1.8110 0.6028 sec/batch\n",
      "Epoch 14/20  Iteration 2350/3560 Training loss: 1.8109 0.6368 sec/batch\n",
      "Epoch 14/20  Iteration 2351/3560 Training loss: 1.8104 0.6114 sec/batch\n",
      "Epoch 14/20  Iteration 2352/3560 Training loss: 1.8092 0.6369 sec/batch\n",
      "Epoch 14/20  Iteration 2353/3560 Training loss: 1.8083 0.6921 sec/batch\n",
      "Epoch 14/20  Iteration 2354/3560 Training loss: 1.8076 0.6504 sec/batch\n",
      "Epoch 14/20  Iteration 2355/3560 Training loss: 1.8071 0.7071 sec/batch\n",
      "Epoch 14/20  Iteration 2356/3560 Training loss: 1.8072 0.7771 sec/batch\n",
      "Epoch 14/20  Iteration 2357/3560 Training loss: 1.8068 0.6402 sec/batch\n",
      "Epoch 14/20  Iteration 2358/3560 Training loss: 1.8060 0.6683 sec/batch\n",
      "Epoch 14/20  Iteration 2359/3560 Training loss: 1.8062 0.6450 sec/batch\n",
      "Epoch 14/20  Iteration 2360/3560 Training loss: 1.8054 0.6548 sec/batch\n",
      "Epoch 14/20  Iteration 2361/3560 Training loss: 1.8052 0.6548 sec/batch\n",
      "Epoch 14/20  Iteration 2362/3560 Training loss: 1.8050 0.6277 sec/batch\n",
      "Epoch 14/20  Iteration 2363/3560 Training loss: 1.8047 0.6180 sec/batch\n",
      "Epoch 14/20  Iteration 2364/3560 Training loss: 1.8054 0.6010 sec/batch\n",
      "Epoch 14/20  Iteration 2365/3560 Training loss: 1.8049 0.6172 sec/batch\n",
      "Epoch 14/20  Iteration 2366/3560 Training loss: 1.8057 0.6188 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2367/3560 Training loss: 1.8057 0.6143 sec/batch\n",
      "Epoch 14/20  Iteration 2368/3560 Training loss: 1.8059 0.6683 sec/batch\n",
      "Epoch 14/20  Iteration 2369/3560 Training loss: 1.8053 0.7007 sec/batch\n",
      "Epoch 14/20  Iteration 2370/3560 Training loss: 1.8055 0.7328 sec/batch\n",
      "Epoch 14/20  Iteration 2371/3560 Training loss: 1.8058 0.6832 sec/batch\n",
      "Epoch 14/20  Iteration 2372/3560 Training loss: 1.8056 0.6226 sec/batch\n",
      "Epoch 14/20  Iteration 2373/3560 Training loss: 1.8053 0.6601 sec/batch\n",
      "Epoch 14/20  Iteration 2374/3560 Training loss: 1.8057 0.5831 sec/batch\n",
      "Epoch 14/20  Iteration 2375/3560 Training loss: 1.8057 0.6647 sec/batch\n",
      "Epoch 14/20  Iteration 2376/3560 Training loss: 1.8065 0.5589 sec/batch\n",
      "Epoch 14/20  Iteration 2377/3560 Training loss: 1.8069 0.6288 sec/batch\n",
      "Epoch 14/20  Iteration 2378/3560 Training loss: 1.8070 0.6067 sec/batch\n",
      "Epoch 14/20  Iteration 2379/3560 Training loss: 1.8069 0.6831 sec/batch\n",
      "Epoch 14/20  Iteration 2380/3560 Training loss: 1.8071 0.6341 sec/batch\n",
      "Epoch 14/20  Iteration 2381/3560 Training loss: 1.8072 0.6348 sec/batch\n",
      "Epoch 14/20  Iteration 2382/3560 Training loss: 1.8067 0.7020 sec/batch\n",
      "Epoch 14/20  Iteration 2383/3560 Training loss: 1.8064 0.7139 sec/batch\n",
      "Epoch 14/20  Iteration 2384/3560 Training loss: 1.8064 0.5884 sec/batch\n",
      "Epoch 14/20  Iteration 2385/3560 Training loss: 1.8067 0.6571 sec/batch\n",
      "Epoch 14/20  Iteration 2386/3560 Training loss: 1.8068 0.6514 sec/batch\n",
      "Epoch 14/20  Iteration 2387/3560 Training loss: 1.8072 0.6451 sec/batch\n",
      "Epoch 14/20  Iteration 2388/3560 Training loss: 1.8071 0.6076 sec/batch\n",
      "Epoch 14/20  Iteration 2389/3560 Training loss: 1.8071 0.6213 sec/batch\n",
      "Epoch 14/20  Iteration 2390/3560 Training loss: 1.8071 0.5738 sec/batch\n",
      "Epoch 14/20  Iteration 2391/3560 Training loss: 1.8072 0.6033 sec/batch\n",
      "Epoch 14/20  Iteration 2392/3560 Training loss: 1.8073 0.6434 sec/batch\n",
      "Epoch 14/20  Iteration 2393/3560 Training loss: 1.8068 0.7033 sec/batch\n",
      "Epoch 14/20  Iteration 2394/3560 Training loss: 1.8066 0.6627 sec/batch\n",
      "Epoch 14/20  Iteration 2395/3560 Training loss: 1.8062 0.6430 sec/batch\n",
      "Epoch 14/20  Iteration 2396/3560 Training loss: 1.8063 0.6474 sec/batch\n",
      "Epoch 14/20  Iteration 2397/3560 Training loss: 1.8060 0.6750 sec/batch\n",
      "Epoch 14/20  Iteration 2398/3560 Training loss: 1.8059 0.6492 sec/batch\n",
      "Epoch 14/20  Iteration 2399/3560 Training loss: 1.8055 0.6425 sec/batch\n",
      "Epoch 14/20  Iteration 2400/3560 Training loss: 1.8051 0.5831 sec/batch\n",
      "Epoch 14/20  Iteration 2401/3560 Training loss: 1.8050 0.5485 sec/batch\n",
      "Epoch 14/20  Iteration 2402/3560 Training loss: 1.8047 0.6839 sec/batch\n",
      "Epoch 14/20  Iteration 2403/3560 Training loss: 1.8043 0.5726 sec/batch\n",
      "Epoch 14/20  Iteration 2404/3560 Training loss: 1.8043 0.6956 sec/batch\n",
      "Epoch 14/20  Iteration 2405/3560 Training loss: 1.8040 0.6726 sec/batch\n",
      "Epoch 14/20  Iteration 2406/3560 Training loss: 1.8038 0.6414 sec/batch\n",
      "Epoch 14/20  Iteration 2407/3560 Training loss: 1.8034 0.5913 sec/batch\n",
      "Epoch 14/20  Iteration 2408/3560 Training loss: 1.8031 0.6478 sec/batch\n",
      "Epoch 14/20  Iteration 2409/3560 Training loss: 1.8027 0.7132 sec/batch\n",
      "Epoch 14/20  Iteration 2410/3560 Training loss: 1.8028 0.6286 sec/batch\n",
      "Epoch 14/20  Iteration 2411/3560 Training loss: 1.8027 0.6460 sec/batch\n",
      "Epoch 14/20  Iteration 2412/3560 Training loss: 1.8024 0.6254 sec/batch\n",
      "Epoch 14/20  Iteration 2413/3560 Training loss: 1.8022 0.6005 sec/batch\n",
      "Epoch 14/20  Iteration 2414/3560 Training loss: 1.8018 0.6868 sec/batch\n",
      "Epoch 14/20  Iteration 2415/3560 Training loss: 1.8019 0.5742 sec/batch\n",
      "Epoch 14/20  Iteration 2416/3560 Training loss: 1.8018 0.6047 sec/batch\n",
      "Epoch 14/20  Iteration 2417/3560 Training loss: 1.8015 0.6439 sec/batch\n",
      "Epoch 14/20  Iteration 2418/3560 Training loss: 1.8013 0.6832 sec/batch\n",
      "Epoch 14/20  Iteration 2419/3560 Training loss: 1.8011 0.7013 sec/batch\n",
      "Epoch 14/20  Iteration 2420/3560 Training loss: 1.8010 0.6556 sec/batch\n",
      "Epoch 14/20  Iteration 2421/3560 Training loss: 1.8010 0.6670 sec/batch\n",
      "Epoch 14/20  Iteration 2422/3560 Training loss: 1.8009 0.7265 sec/batch\n",
      "Epoch 14/20  Iteration 2423/3560 Training loss: 1.8010 0.6372 sec/batch\n",
      "Epoch 14/20  Iteration 2424/3560 Training loss: 1.8009 0.6587 sec/batch\n",
      "Epoch 14/20  Iteration 2425/3560 Training loss: 1.8008 0.6413 sec/batch\n",
      "Epoch 14/20  Iteration 2426/3560 Training loss: 1.8006 0.5870 sec/batch\n",
      "Epoch 14/20  Iteration 2427/3560 Training loss: 1.8004 0.6222 sec/batch\n",
      "Epoch 14/20  Iteration 2428/3560 Training loss: 1.8002 0.6439 sec/batch\n",
      "Epoch 14/20  Iteration 2429/3560 Training loss: 1.8000 0.5851 sec/batch\n",
      "Epoch 14/20  Iteration 2430/3560 Training loss: 1.7996 0.6546 sec/batch\n",
      "Epoch 14/20  Iteration 2431/3560 Training loss: 1.7996 0.6461 sec/batch\n",
      "Epoch 14/20  Iteration 2432/3560 Training loss: 1.7996 0.7458 sec/batch\n",
      "Epoch 14/20  Iteration 2433/3560 Training loss: 1.7996 0.6086 sec/batch\n",
      "Epoch 14/20  Iteration 2434/3560 Training loss: 1.7995 0.7477 sec/batch\n",
      "Epoch 14/20  Iteration 2435/3560 Training loss: 1.7995 0.5236 sec/batch\n",
      "Epoch 14/20  Iteration 2436/3560 Training loss: 1.7993 0.7246 sec/batch\n",
      "Epoch 14/20  Iteration 2437/3560 Training loss: 1.7991 0.5902 sec/batch\n",
      "Epoch 14/20  Iteration 2438/3560 Training loss: 1.7992 0.6388 sec/batch\n",
      "Epoch 14/20  Iteration 2439/3560 Training loss: 1.7992 0.6560 sec/batch\n",
      "Epoch 14/20  Iteration 2440/3560 Training loss: 1.7989 0.5465 sec/batch\n",
      "Epoch 14/20  Iteration 2441/3560 Training loss: 1.7989 0.6102 sec/batch\n",
      "Epoch 14/20  Iteration 2442/3560 Training loss: 1.7990 0.6547 sec/batch\n",
      "Epoch 14/20  Iteration 2443/3560 Training loss: 1.7989 0.7134 sec/batch\n",
      "Epoch 14/20  Iteration 2444/3560 Training loss: 1.7988 0.6677 sec/batch\n",
      "Epoch 14/20  Iteration 2445/3560 Training loss: 1.7987 0.6884 sec/batch\n",
      "Epoch 14/20  Iteration 2446/3560 Training loss: 1.7984 0.6765 sec/batch\n",
      "Epoch 14/20  Iteration 2447/3560 Training loss: 1.7984 0.6923 sec/batch\n",
      "Epoch 14/20  Iteration 2448/3560 Training loss: 1.7984 0.6577 sec/batch\n",
      "Epoch 14/20  Iteration 2449/3560 Training loss: 1.7983 0.6035 sec/batch\n",
      "Epoch 14/20  Iteration 2450/3560 Training loss: 1.7984 0.6944 sec/batch\n",
      "Epoch 14/20  Iteration 2451/3560 Training loss: 1.7985 0.6549 sec/batch\n",
      "Epoch 14/20  Iteration 2452/3560 Training loss: 1.7985 0.6846 sec/batch\n",
      "Epoch 14/20  Iteration 2453/3560 Training loss: 1.7987 0.6498 sec/batch\n",
      "Epoch 14/20  Iteration 2454/3560 Training loss: 1.7985 0.6504 sec/batch\n",
      "Epoch 14/20  Iteration 2455/3560 Training loss: 1.7988 0.6404 sec/batch\n",
      "Epoch 14/20  Iteration 2456/3560 Training loss: 1.7987 0.6257 sec/batch\n",
      "Epoch 14/20  Iteration 2457/3560 Training loss: 1.7987 0.7402 sec/batch\n",
      "Epoch 14/20  Iteration 2458/3560 Training loss: 1.7987 0.6683 sec/batch\n",
      "Epoch 14/20  Iteration 2459/3560 Training loss: 1.7986 0.6504 sec/batch\n",
      "Epoch 14/20  Iteration 2460/3560 Training loss: 1.7986 0.7022 sec/batch\n",
      "Epoch 14/20  Iteration 2461/3560 Training loss: 1.7987 0.6187 sec/batch\n",
      "Epoch 14/20  Iteration 2462/3560 Training loss: 1.7989 0.6461 sec/batch\n",
      "Epoch 14/20  Iteration 2463/3560 Training loss: 1.7989 0.6120 sec/batch\n",
      "Epoch 14/20  Iteration 2464/3560 Training loss: 1.7988 0.6612 sec/batch\n",
      "Epoch 14/20  Iteration 2465/3560 Training loss: 1.7985 0.6691 sec/batch\n",
      "Epoch 14/20  Iteration 2466/3560 Training loss: 1.7986 0.5966 sec/batch\n",
      "Epoch 14/20  Iteration 2467/3560 Training loss: 1.7987 0.6583 sec/batch\n",
      "Epoch 14/20  Iteration 2468/3560 Training loss: 1.7987 0.6908 sec/batch\n",
      "Epoch 14/20  Iteration 2469/3560 Training loss: 1.7986 0.6551 sec/batch\n",
      "Epoch 14/20  Iteration 2470/3560 Training loss: 1.7986 0.6844 sec/batch\n",
      "Epoch 14/20  Iteration 2471/3560 Training loss: 1.7988 0.5984 sec/batch\n",
      "Epoch 14/20  Iteration 2472/3560 Training loss: 1.7987 0.6538 sec/batch\n",
      "Epoch 14/20  Iteration 2473/3560 Training loss: 1.7986 0.6492 sec/batch\n",
      "Epoch 14/20  Iteration 2474/3560 Training loss: 1.7987 0.6323 sec/batch\n",
      "Epoch 14/20  Iteration 2475/3560 Training loss: 1.7989 0.6417 sec/batch\n",
      "Epoch 14/20  Iteration 2476/3560 Training loss: 1.7988 0.6271 sec/batch\n",
      "Epoch 14/20  Iteration 2477/3560 Training loss: 1.7988 0.6073 sec/batch\n",
      "Epoch 14/20  Iteration 2478/3560 Training loss: 1.7988 0.6522 sec/batch\n",
      "Epoch 14/20  Iteration 2479/3560 Training loss: 1.7988 0.6235 sec/batch\n",
      "Epoch 14/20  Iteration 2480/3560 Training loss: 1.7987 0.8983 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2481/3560 Training loss: 1.7987 0.6261 sec/batch\n",
      "Epoch 14/20  Iteration 2482/3560 Training loss: 1.7990 0.6899 sec/batch\n",
      "Epoch 14/20  Iteration 2483/3560 Training loss: 1.7989 0.6779 sec/batch\n",
      "Epoch 14/20  Iteration 2484/3560 Training loss: 1.7988 0.6626 sec/batch\n",
      "Epoch 14/20  Iteration 2485/3560 Training loss: 1.7987 0.6869 sec/batch\n",
      "Epoch 14/20  Iteration 2486/3560 Training loss: 1.7986 0.6350 sec/batch\n",
      "Epoch 14/20  Iteration 2487/3560 Training loss: 1.7986 0.6407 sec/batch\n",
      "Epoch 14/20  Iteration 2488/3560 Training loss: 1.7986 0.6812 sec/batch\n",
      "Epoch 14/20  Iteration 2489/3560 Training loss: 1.7987 0.6614 sec/batch\n",
      "Epoch 14/20  Iteration 2490/3560 Training loss: 1.7986 0.5211 sec/batch\n",
      "Epoch 14/20  Iteration 2491/3560 Training loss: 1.7985 0.6354 sec/batch\n",
      "Epoch 14/20  Iteration 2492/3560 Training loss: 1.7985 0.5854 sec/batch\n",
      "Epoch 15/20  Iteration 2493/3560 Training loss: 1.8438 0.6903 sec/batch\n",
      "Epoch 15/20  Iteration 2494/3560 Training loss: 1.8126 0.6780 sec/batch\n",
      "Epoch 15/20  Iteration 2495/3560 Training loss: 1.8045 0.6931 sec/batch\n",
      "Epoch 15/20  Iteration 2496/3560 Training loss: 1.8032 0.6271 sec/batch\n",
      "Epoch 15/20  Iteration 2497/3560 Training loss: 1.7995 0.6858 sec/batch\n",
      "Epoch 15/20  Iteration 2498/3560 Training loss: 1.7911 0.7139 sec/batch\n",
      "Epoch 15/20  Iteration 2499/3560 Training loss: 1.7922 0.5873 sec/batch\n",
      "Epoch 15/20  Iteration 2500/3560 Training loss: 1.7925 0.6180 sec/batch\n",
      "Epoch 15/20  Iteration 2501/3560 Training loss: 1.7950 0.6387 sec/batch\n",
      "Epoch 15/20  Iteration 2502/3560 Training loss: 1.7944 0.6177 sec/batch\n",
      "Epoch 15/20  Iteration 2503/3560 Training loss: 1.7919 0.6103 sec/batch\n",
      "Epoch 15/20  Iteration 2504/3560 Training loss: 1.7905 0.6073 sec/batch\n",
      "Epoch 15/20  Iteration 2505/3560 Training loss: 1.7899 0.6528 sec/batch\n",
      "Epoch 15/20  Iteration 2506/3560 Training loss: 1.7926 0.6165 sec/batch\n",
      "Epoch 15/20  Iteration 2507/3560 Training loss: 1.7925 0.8616 sec/batch\n",
      "Epoch 15/20  Iteration 2508/3560 Training loss: 1.7910 0.6565 sec/batch\n",
      "Epoch 15/20  Iteration 2509/3560 Training loss: 1.7906 0.6861 sec/batch\n",
      "Epoch 15/20  Iteration 2510/3560 Training loss: 1.7927 0.7028 sec/batch\n",
      "Epoch 15/20  Iteration 2511/3560 Training loss: 1.7925 0.6163 sec/batch\n",
      "Epoch 15/20  Iteration 2512/3560 Training loss: 1.7934 0.6549 sec/batch\n",
      "Epoch 15/20  Iteration 2513/3560 Training loss: 1.7927 0.5474 sec/batch\n",
      "Epoch 15/20  Iteration 2514/3560 Training loss: 1.7935 0.6788 sec/batch\n",
      "Epoch 15/20  Iteration 2515/3560 Training loss: 1.7928 0.6994 sec/batch\n",
      "Epoch 15/20  Iteration 2516/3560 Training loss: 1.7924 0.6782 sec/batch\n",
      "Epoch 15/20  Iteration 2517/3560 Training loss: 1.7917 0.5605 sec/batch\n",
      "Epoch 15/20  Iteration 2518/3560 Training loss: 1.7910 0.5979 sec/batch\n",
      "Epoch 15/20  Iteration 2519/3560 Training loss: 1.7900 0.7305 sec/batch\n",
      "Epoch 15/20  Iteration 2520/3560 Training loss: 1.7903 0.6741 sec/batch\n",
      "Epoch 15/20  Iteration 2521/3560 Training loss: 1.7906 0.6899 sec/batch\n",
      "Epoch 15/20  Iteration 2522/3560 Training loss: 1.7913 0.7548 sec/batch\n",
      "Epoch 15/20  Iteration 2523/3560 Training loss: 1.7914 0.7213 sec/batch\n",
      "Epoch 15/20  Iteration 2524/3560 Training loss: 1.7907 0.6485 sec/batch\n",
      "Epoch 15/20  Iteration 2525/3560 Training loss: 1.7908 0.7824 sec/batch\n",
      "Epoch 15/20  Iteration 2526/3560 Training loss: 1.7915 0.6733 sec/batch\n",
      "Epoch 15/20  Iteration 2527/3560 Training loss: 1.7912 0.6366 sec/batch\n",
      "Epoch 15/20  Iteration 2528/3560 Training loss: 1.7911 0.7187 sec/batch\n",
      "Epoch 15/20  Iteration 2529/3560 Training loss: 1.7905 0.6425 sec/batch\n",
      "Epoch 15/20  Iteration 2530/3560 Training loss: 1.7896 0.6967 sec/batch\n",
      "Epoch 15/20  Iteration 2531/3560 Training loss: 1.7883 0.6626 sec/batch\n",
      "Epoch 15/20  Iteration 2532/3560 Training loss: 1.7873 0.7266 sec/batch\n",
      "Epoch 15/20  Iteration 2533/3560 Training loss: 1.7870 0.8834 sec/batch\n",
      "Epoch 15/20  Iteration 2534/3560 Training loss: 1.7872 0.6786 sec/batch\n",
      "Epoch 15/20  Iteration 2535/3560 Training loss: 1.7871 0.6738 sec/batch\n",
      "Epoch 15/20  Iteration 2536/3560 Training loss: 1.7865 0.7588 sec/batch\n",
      "Epoch 15/20  Iteration 2537/3560 Training loss: 1.7867 0.7954 sec/batch\n",
      "Epoch 15/20  Iteration 2538/3560 Training loss: 1.7856 0.5982 sec/batch\n",
      "Epoch 15/20  Iteration 2539/3560 Training loss: 1.7854 0.7280 sec/batch\n",
      "Epoch 15/20  Iteration 2540/3560 Training loss: 1.7852 0.7592 sec/batch\n",
      "Epoch 15/20  Iteration 2541/3560 Training loss: 1.7849 0.9485 sec/batch\n",
      "Epoch 15/20  Iteration 2542/3560 Training loss: 1.7856 0.8084 sec/batch\n",
      "Epoch 15/20  Iteration 2543/3560 Training loss: 1.7850 0.8802 sec/batch\n",
      "Epoch 15/20  Iteration 2544/3560 Training loss: 1.7858 0.7269 sec/batch\n",
      "Epoch 15/20  Iteration 2545/3560 Training loss: 1.7856 0.8910 sec/batch\n",
      "Epoch 15/20  Iteration 2546/3560 Training loss: 1.7855 0.7295 sec/batch\n",
      "Epoch 15/20  Iteration 2547/3560 Training loss: 1.7849 0.7224 sec/batch\n",
      "Epoch 15/20  Iteration 2548/3560 Training loss: 1.7851 0.6932 sec/batch\n",
      "Epoch 15/20  Iteration 2549/3560 Training loss: 1.7855 0.6642 sec/batch\n",
      "Epoch 15/20  Iteration 2550/3560 Training loss: 1.7852 0.5799 sec/batch\n",
      "Epoch 15/20  Iteration 2551/3560 Training loss: 1.7848 0.7633 sec/batch\n",
      "Epoch 15/20  Iteration 2552/3560 Training loss: 1.7853 0.7430 sec/batch\n",
      "Epoch 15/20  Iteration 2553/3560 Training loss: 1.7852 0.7124 sec/batch\n",
      "Epoch 15/20  Iteration 2554/3560 Training loss: 1.7861 0.6374 sec/batch\n",
      "Epoch 15/20  Iteration 2555/3560 Training loss: 1.7864 0.6650 sec/batch\n",
      "Epoch 15/20  Iteration 2556/3560 Training loss: 1.7865 0.6352 sec/batch\n",
      "Epoch 15/20  Iteration 2557/3560 Training loss: 1.7864 0.6353 sec/batch\n",
      "Epoch 15/20  Iteration 2558/3560 Training loss: 1.7867 0.6649 sec/batch\n",
      "Epoch 15/20  Iteration 2559/3560 Training loss: 1.7868 0.6467 sec/batch\n",
      "Epoch 15/20  Iteration 2560/3560 Training loss: 1.7863 0.7204 sec/batch\n",
      "Epoch 15/20  Iteration 2561/3560 Training loss: 1.7862 0.7165 sec/batch\n",
      "Epoch 15/20  Iteration 2562/3560 Training loss: 1.7862 0.6373 sec/batch\n",
      "Epoch 15/20  Iteration 2563/3560 Training loss: 1.7866 0.6731 sec/batch\n",
      "Epoch 15/20  Iteration 2564/3560 Training loss: 1.7868 0.5999 sec/batch\n",
      "Epoch 15/20  Iteration 2565/3560 Training loss: 1.7872 0.5851 sec/batch\n",
      "Epoch 15/20  Iteration 2566/3560 Training loss: 1.7869 0.6468 sec/batch\n",
      "Epoch 15/20  Iteration 2567/3560 Training loss: 1.7870 0.5634 sec/batch\n",
      "Epoch 15/20  Iteration 2568/3560 Training loss: 1.7871 0.6866 sec/batch\n",
      "Epoch 15/20  Iteration 2569/3560 Training loss: 1.7871 0.5989 sec/batch\n",
      "Epoch 15/20  Iteration 2570/3560 Training loss: 1.7871 0.7008 sec/batch\n",
      "Epoch 15/20  Iteration 2571/3560 Training loss: 1.7866 0.6688 sec/batch\n",
      "Epoch 15/20  Iteration 2572/3560 Training loss: 1.7863 0.6783 sec/batch\n",
      "Epoch 15/20  Iteration 2573/3560 Training loss: 1.7857 0.6595 sec/batch\n",
      "Epoch 15/20  Iteration 2574/3560 Training loss: 1.7859 0.6478 sec/batch\n",
      "Epoch 15/20  Iteration 2575/3560 Training loss: 1.7855 0.6206 sec/batch\n",
      "Epoch 15/20  Iteration 2576/3560 Training loss: 1.7853 0.5628 sec/batch\n",
      "Epoch 15/20  Iteration 2577/3560 Training loss: 1.7849 0.6558 sec/batch\n",
      "Epoch 15/20  Iteration 2578/3560 Training loss: 1.7846 0.6073 sec/batch\n",
      "Epoch 15/20  Iteration 2579/3560 Training loss: 1.7845 0.6295 sec/batch\n",
      "Epoch 15/20  Iteration 2580/3560 Training loss: 1.7843 0.6309 sec/batch\n",
      "Epoch 15/20  Iteration 2581/3560 Training loss: 1.7838 0.6132 sec/batch\n",
      "Epoch 15/20  Iteration 2582/3560 Training loss: 1.7840 0.6918 sec/batch\n",
      "Epoch 15/20  Iteration 2583/3560 Training loss: 1.7836 0.6638 sec/batch\n",
      "Epoch 15/20  Iteration 2584/3560 Training loss: 1.7835 0.6966 sec/batch\n",
      "Epoch 15/20  Iteration 2585/3560 Training loss: 1.7832 0.6396 sec/batch\n",
      "Epoch 15/20  Iteration 2586/3560 Training loss: 1.7828 0.6774 sec/batch\n",
      "Epoch 15/20  Iteration 2587/3560 Training loss: 1.7824 0.6535 sec/batch\n",
      "Epoch 15/20  Iteration 2588/3560 Training loss: 1.7824 0.6217 sec/batch\n",
      "Epoch 15/20  Iteration 2589/3560 Training loss: 1.7824 0.6119 sec/batch\n",
      "Epoch 15/20  Iteration 2590/3560 Training loss: 1.7820 0.6374 sec/batch\n",
      "Epoch 15/20  Iteration 2591/3560 Training loss: 1.7818 0.5948 sec/batch\n",
      "Epoch 15/20  Iteration 2592/3560 Training loss: 1.7814 0.6175 sec/batch\n",
      "Epoch 15/20  Iteration 2593/3560 Training loss: 1.7814 0.6288 sec/batch\n",
      "Epoch 15/20  Iteration 2594/3560 Training loss: 1.7812 0.5980 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2595/3560 Training loss: 1.7809 0.7311 sec/batch\n",
      "Epoch 15/20  Iteration 2596/3560 Training loss: 1.7807 0.6359 sec/batch\n",
      "Epoch 15/20  Iteration 2597/3560 Training loss: 1.7804 0.6551 sec/batch\n",
      "Epoch 15/20  Iteration 2598/3560 Training loss: 1.7804 0.6409 sec/batch\n",
      "Epoch 15/20  Iteration 2599/3560 Training loss: 1.7803 0.9062 sec/batch\n",
      "Epoch 15/20  Iteration 2600/3560 Training loss: 1.7802 0.7157 sec/batch\n",
      "Epoch 15/20  Iteration 2601/3560 Training loss: 1.7802 0.6046 sec/batch\n",
      "Epoch 15/20  Iteration 2602/3560 Training loss: 1.7802 0.7104 sec/batch\n",
      "Epoch 15/20  Iteration 2603/3560 Training loss: 1.7802 0.6218 sec/batch\n",
      "Epoch 15/20  Iteration 2604/3560 Training loss: 1.7800 0.5635 sec/batch\n",
      "Epoch 15/20  Iteration 2605/3560 Training loss: 1.7799 0.6752 sec/batch\n",
      "Epoch 15/20  Iteration 2606/3560 Training loss: 1.7799 0.5614 sec/batch\n",
      "Epoch 15/20  Iteration 2607/3560 Training loss: 1.7796 0.6443 sec/batch\n",
      "Epoch 15/20  Iteration 2608/3560 Training loss: 1.7792 0.6795 sec/batch\n",
      "Epoch 15/20  Iteration 2609/3560 Training loss: 1.7792 0.6869 sec/batch\n",
      "Epoch 15/20  Iteration 2610/3560 Training loss: 1.7791 0.6658 sec/batch\n",
      "Epoch 15/20  Iteration 2611/3560 Training loss: 1.7791 0.7543 sec/batch\n",
      "Epoch 15/20  Iteration 2612/3560 Training loss: 1.7790 0.6111 sec/batch\n",
      "Epoch 15/20  Iteration 2613/3560 Training loss: 1.7790 0.6761 sec/batch\n",
      "Epoch 15/20  Iteration 2614/3560 Training loss: 1.7787 0.6097 sec/batch\n",
      "Epoch 15/20  Iteration 2615/3560 Training loss: 1.7784 0.6244 sec/batch\n",
      "Epoch 15/20  Iteration 2616/3560 Training loss: 1.7785 0.5764 sec/batch\n",
      "Epoch 15/20  Iteration 2617/3560 Training loss: 1.7785 0.6923 sec/batch\n",
      "Epoch 15/20  Iteration 2618/3560 Training loss: 1.7781 0.6472 sec/batch\n",
      "Epoch 15/20  Iteration 2619/3560 Training loss: 1.7782 0.6039 sec/batch\n",
      "Epoch 15/20  Iteration 2620/3560 Training loss: 1.7782 0.6665 sec/batch\n",
      "Epoch 15/20  Iteration 2621/3560 Training loss: 1.7781 0.6318 sec/batch\n",
      "Epoch 15/20  Iteration 2622/3560 Training loss: 1.7780 0.7006 sec/batch\n",
      "Epoch 15/20  Iteration 2623/3560 Training loss: 1.7777 0.6417 sec/batch\n",
      "Epoch 15/20  Iteration 2624/3560 Training loss: 1.7774 0.7121 sec/batch\n",
      "Epoch 15/20  Iteration 2625/3560 Training loss: 1.7775 0.6560 sec/batch\n",
      "Epoch 15/20  Iteration 2626/3560 Training loss: 1.7774 0.6353 sec/batch\n",
      "Epoch 15/20  Iteration 2627/3560 Training loss: 1.7773 0.6102 sec/batch\n",
      "Epoch 15/20  Iteration 2628/3560 Training loss: 1.7773 0.6147 sec/batch\n",
      "Epoch 15/20  Iteration 2629/3560 Training loss: 1.7775 0.6497 sec/batch\n",
      "Epoch 15/20  Iteration 2630/3560 Training loss: 1.7775 0.6128 sec/batch\n",
      "Epoch 15/20  Iteration 2631/3560 Training loss: 1.7776 0.6355 sec/batch\n",
      "Epoch 15/20  Iteration 2632/3560 Training loss: 1.7774 0.6734 sec/batch\n",
      "Epoch 15/20  Iteration 2633/3560 Training loss: 1.7777 0.6880 sec/batch\n",
      "Epoch 15/20  Iteration 2634/3560 Training loss: 1.7776 0.6735 sec/batch\n",
      "Epoch 15/20  Iteration 2635/3560 Training loss: 1.7776 0.6655 sec/batch\n",
      "Epoch 15/20  Iteration 2636/3560 Training loss: 1.7776 0.7615 sec/batch\n",
      "Epoch 15/20  Iteration 2637/3560 Training loss: 1.7776 0.7589 sec/batch\n",
      "Epoch 15/20  Iteration 2638/3560 Training loss: 1.7776 0.6368 sec/batch\n",
      "Epoch 15/20  Iteration 2639/3560 Training loss: 1.7777 0.5923 sec/batch\n",
      "Epoch 15/20  Iteration 2640/3560 Training loss: 1.7779 0.7113 sec/batch\n",
      "Epoch 15/20  Iteration 2641/3560 Training loss: 1.7779 0.5727 sec/batch\n",
      "Epoch 15/20  Iteration 2642/3560 Training loss: 1.7777 0.7518 sec/batch\n",
      "Epoch 15/20  Iteration 2643/3560 Training loss: 1.7774 0.6283 sec/batch\n",
      "Epoch 15/20  Iteration 2644/3560 Training loss: 1.7775 0.6547 sec/batch\n",
      "Epoch 15/20  Iteration 2645/3560 Training loss: 1.7776 0.6319 sec/batch\n",
      "Epoch 15/20  Iteration 2646/3560 Training loss: 1.7776 0.7301 sec/batch\n",
      "Epoch 15/20  Iteration 2647/3560 Training loss: 1.7776 0.6943 sec/batch\n",
      "Epoch 15/20  Iteration 2648/3560 Training loss: 1.7776 0.7849 sec/batch\n",
      "Epoch 15/20  Iteration 2649/3560 Training loss: 1.7777 0.6552 sec/batch\n",
      "Epoch 15/20  Iteration 2650/3560 Training loss: 1.7777 0.6116 sec/batch\n",
      "Epoch 15/20  Iteration 2651/3560 Training loss: 1.7775 0.6173 sec/batch\n",
      "Epoch 15/20  Iteration 2652/3560 Training loss: 1.7776 0.6256 sec/batch\n",
      "Epoch 15/20  Iteration 2653/3560 Training loss: 1.7778 0.5831 sec/batch\n",
      "Epoch 15/20  Iteration 2654/3560 Training loss: 1.7777 0.7025 sec/batch\n",
      "Epoch 15/20  Iteration 2655/3560 Training loss: 1.7778 0.5689 sec/batch\n",
      "Epoch 15/20  Iteration 2656/3560 Training loss: 1.7778 0.6806 sec/batch\n",
      "Epoch 15/20  Iteration 2657/3560 Training loss: 1.7778 0.5633 sec/batch\n",
      "Epoch 15/20  Iteration 2658/3560 Training loss: 1.7778 0.7009 sec/batch\n",
      "Epoch 15/20  Iteration 2659/3560 Training loss: 1.7779 0.7115 sec/batch\n",
      "Epoch 15/20  Iteration 2660/3560 Training loss: 1.7783 0.6600 sec/batch\n",
      "Epoch 15/20  Iteration 2661/3560 Training loss: 1.7783 0.6635 sec/batch\n",
      "Epoch 15/20  Iteration 2662/3560 Training loss: 1.7782 0.7056 sec/batch\n",
      "Epoch 15/20  Iteration 2663/3560 Training loss: 1.7781 0.6376 sec/batch\n",
      "Epoch 15/20  Iteration 2664/3560 Training loss: 1.7780 0.6080 sec/batch\n",
      "Epoch 15/20  Iteration 2665/3560 Training loss: 1.7781 0.6574 sec/batch\n",
      "Epoch 15/20  Iteration 2666/3560 Training loss: 1.7782 0.5895 sec/batch\n",
      "Epoch 15/20  Iteration 2667/3560 Training loss: 1.7782 0.5741 sec/batch\n",
      "Epoch 15/20  Iteration 2668/3560 Training loss: 1.7782 0.6410 sec/batch\n",
      "Epoch 15/20  Iteration 2669/3560 Training loss: 1.7781 0.6500 sec/batch\n",
      "Epoch 15/20  Iteration 2670/3560 Training loss: 1.7781 0.6681 sec/batch\n",
      "Epoch 16/20  Iteration 2671/3560 Training loss: 1.8229 0.6993 sec/batch\n",
      "Epoch 16/20  Iteration 2672/3560 Training loss: 1.7918 0.6146 sec/batch\n",
      "Epoch 16/20  Iteration 2673/3560 Training loss: 1.7875 0.6928 sec/batch\n",
      "Epoch 16/20  Iteration 2674/3560 Training loss: 1.7839 0.6876 sec/batch\n",
      "Epoch 16/20  Iteration 2675/3560 Training loss: 1.7803 0.6127 sec/batch\n",
      "Epoch 16/20  Iteration 2676/3560 Training loss: 1.7719 0.6063 sec/batch\n",
      "Epoch 16/20  Iteration 2677/3560 Training loss: 1.7717 0.6573 sec/batch\n",
      "Epoch 16/20  Iteration 2678/3560 Training loss: 1.7725 0.6659 sec/batch\n",
      "Epoch 16/20  Iteration 2679/3560 Training loss: 1.7755 0.6500 sec/batch\n",
      "Epoch 16/20  Iteration 2680/3560 Training loss: 1.7738 0.6145 sec/batch\n",
      "Epoch 16/20  Iteration 2681/3560 Training loss: 1.7718 0.5720 sec/batch\n",
      "Epoch 16/20  Iteration 2682/3560 Training loss: 1.7703 0.6956 sec/batch\n",
      "Epoch 16/20  Iteration 2683/3560 Training loss: 1.7693 0.6557 sec/batch\n",
      "Epoch 16/20  Iteration 2684/3560 Training loss: 1.7729 0.6297 sec/batch\n",
      "Epoch 16/20  Iteration 2685/3560 Training loss: 1.7722 0.6845 sec/batch\n",
      "Epoch 16/20  Iteration 2686/3560 Training loss: 1.7704 0.6680 sec/batch\n",
      "Epoch 16/20  Iteration 2687/3560 Training loss: 1.7705 0.6729 sec/batch\n",
      "Epoch 16/20  Iteration 2688/3560 Training loss: 1.7729 0.6469 sec/batch\n",
      "Epoch 16/20  Iteration 2689/3560 Training loss: 1.7727 0.5832 sec/batch\n",
      "Epoch 16/20  Iteration 2690/3560 Training loss: 1.7733 0.6334 sec/batch\n",
      "Epoch 16/20  Iteration 2691/3560 Training loss: 1.7725 0.5947 sec/batch\n",
      "Epoch 16/20  Iteration 2692/3560 Training loss: 1.7730 0.6314 sec/batch\n",
      "Epoch 16/20  Iteration 2693/3560 Training loss: 1.7723 0.6342 sec/batch\n",
      "Epoch 16/20  Iteration 2694/3560 Training loss: 1.7721 0.5899 sec/batch\n",
      "Epoch 16/20  Iteration 2695/3560 Training loss: 1.7718 0.6725 sec/batch\n",
      "Epoch 16/20  Iteration 2696/3560 Training loss: 1.7708 0.7104 sec/batch\n",
      "Epoch 16/20  Iteration 2697/3560 Training loss: 1.7699 0.6750 sec/batch\n",
      "Epoch 16/20  Iteration 2698/3560 Training loss: 1.7703 0.6768 sec/batch\n",
      "Epoch 16/20  Iteration 2699/3560 Training loss: 1.7713 0.6544 sec/batch\n",
      "Epoch 16/20  Iteration 2700/3560 Training loss: 1.7720 0.6462 sec/batch\n",
      "Epoch 16/20  Iteration 2701/3560 Training loss: 1.7717 0.5885 sec/batch\n",
      "Epoch 16/20  Iteration 2702/3560 Training loss: 1.7712 0.6567 sec/batch\n",
      "Epoch 16/20  Iteration 2703/3560 Training loss: 1.7716 0.6481 sec/batch\n",
      "Epoch 16/20  Iteration 2704/3560 Training loss: 1.7722 0.6131 sec/batch\n",
      "Epoch 16/20  Iteration 2705/3560 Training loss: 1.7717 0.6528 sec/batch\n",
      "Epoch 16/20  Iteration 2706/3560 Training loss: 1.7717 0.5792 sec/batch\n",
      "Epoch 16/20  Iteration 2707/3560 Training loss: 1.7708 0.6389 sec/batch\n",
      "Epoch 16/20  Iteration 2708/3560 Training loss: 1.7699 0.6935 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2709/3560 Training loss: 1.7690 0.6614 sec/batch\n",
      "Epoch 16/20  Iteration 2710/3560 Training loss: 1.7684 0.7903 sec/batch\n",
      "Epoch 16/20  Iteration 2711/3560 Training loss: 1.7680 0.6061 sec/batch\n",
      "Epoch 16/20  Iteration 2712/3560 Training loss: 1.7683 0.7481 sec/batch\n",
      "Epoch 16/20  Iteration 2713/3560 Training loss: 1.7677 0.5883 sec/batch\n",
      "Epoch 16/20  Iteration 2714/3560 Training loss: 1.7669 0.6262 sec/batch\n",
      "Epoch 16/20  Iteration 2715/3560 Training loss: 1.7672 0.6767 sec/batch\n",
      "Epoch 16/20  Iteration 2716/3560 Training loss: 1.7662 0.6107 sec/batch\n",
      "Epoch 16/20  Iteration 2717/3560 Training loss: 1.7661 0.6289 sec/batch\n",
      "Epoch 16/20  Iteration 2718/3560 Training loss: 1.7659 0.6680 sec/batch\n",
      "Epoch 16/20  Iteration 2719/3560 Training loss: 1.7655 0.6022 sec/batch\n",
      "Epoch 16/20  Iteration 2720/3560 Training loss: 1.7662 0.6109 sec/batch\n",
      "Epoch 16/20  Iteration 2721/3560 Training loss: 1.7657 0.6687 sec/batch\n",
      "Epoch 16/20  Iteration 2722/3560 Training loss: 1.7666 0.6528 sec/batch\n",
      "Epoch 16/20  Iteration 2723/3560 Training loss: 1.7663 0.6789 sec/batch\n",
      "Epoch 16/20  Iteration 2724/3560 Training loss: 1.7663 0.7625 sec/batch\n",
      "Epoch 16/20  Iteration 2725/3560 Training loss: 1.7658 0.6137 sec/batch\n",
      "Epoch 16/20  Iteration 2726/3560 Training loss: 1.7659 0.6275 sec/batch\n",
      "Epoch 16/20  Iteration 2727/3560 Training loss: 1.7661 0.6366 sec/batch\n",
      "Epoch 16/20  Iteration 2728/3560 Training loss: 1.7661 0.6131 sec/batch\n",
      "Epoch 16/20  Iteration 2729/3560 Training loss: 1.7655 0.5387 sec/batch\n",
      "Epoch 16/20  Iteration 2730/3560 Training loss: 1.7660 0.6253 sec/batch\n",
      "Epoch 16/20  Iteration 2731/3560 Training loss: 1.7660 0.6216 sec/batch\n",
      "Epoch 16/20  Iteration 2732/3560 Training loss: 1.7668 0.6069 sec/batch\n",
      "Epoch 16/20  Iteration 2733/3560 Training loss: 1.7669 0.7210 sec/batch\n",
      "Epoch 16/20  Iteration 2734/3560 Training loss: 1.7671 0.6899 sec/batch\n",
      "Epoch 16/20  Iteration 2735/3560 Training loss: 1.7669 0.6415 sec/batch\n",
      "Epoch 16/20  Iteration 2736/3560 Training loss: 1.7673 0.6854 sec/batch\n",
      "Epoch 16/20  Iteration 2737/3560 Training loss: 1.7674 0.6945 sec/batch\n",
      "Epoch 16/20  Iteration 2738/3560 Training loss: 1.7670 0.5316 sec/batch\n",
      "Epoch 16/20  Iteration 2739/3560 Training loss: 1.7667 0.6576 sec/batch\n",
      "Epoch 16/20  Iteration 2740/3560 Training loss: 1.7666 0.6161 sec/batch\n",
      "Epoch 16/20  Iteration 2741/3560 Training loss: 1.7672 0.6236 sec/batch\n",
      "Epoch 16/20  Iteration 2742/3560 Training loss: 1.7672 0.6397 sec/batch\n",
      "Epoch 16/20  Iteration 2743/3560 Training loss: 1.7676 0.6196 sec/batch\n",
      "Epoch 16/20  Iteration 2744/3560 Training loss: 1.7672 0.5522 sec/batch\n",
      "Epoch 16/20  Iteration 2745/3560 Training loss: 1.7672 0.6546 sec/batch\n",
      "Epoch 16/20  Iteration 2746/3560 Training loss: 1.7672 0.6870 sec/batch\n",
      "Epoch 16/20  Iteration 2747/3560 Training loss: 1.7672 0.6201 sec/batch\n",
      "Epoch 16/20  Iteration 2748/3560 Training loss: 1.7674 0.6257 sec/batch\n",
      "Epoch 16/20  Iteration 2749/3560 Training loss: 1.7668 0.6471 sec/batch\n",
      "Epoch 16/20  Iteration 2750/3560 Training loss: 1.7667 0.6246 sec/batch\n",
      "Epoch 16/20  Iteration 2751/3560 Training loss: 1.7662 0.6989 sec/batch\n",
      "Epoch 16/20  Iteration 2752/3560 Training loss: 1.7663 0.6677 sec/batch\n",
      "Epoch 16/20  Iteration 2753/3560 Training loss: 1.7659 0.5658 sec/batch\n",
      "Epoch 16/20  Iteration 2754/3560 Training loss: 1.7660 0.6575 sec/batch\n",
      "Epoch 16/20  Iteration 2755/3560 Training loss: 1.7655 0.6979 sec/batch\n",
      "Epoch 16/20  Iteration 2756/3560 Training loss: 1.7652 0.6057 sec/batch\n",
      "Epoch 16/20  Iteration 2757/3560 Training loss: 1.7651 0.5599 sec/batch\n",
      "Epoch 16/20  Iteration 2758/3560 Training loss: 1.7649 0.7428 sec/batch\n",
      "Epoch 16/20  Iteration 2759/3560 Training loss: 1.7644 0.6462 sec/batch\n",
      "Epoch 16/20  Iteration 2760/3560 Training loss: 1.7646 0.6078 sec/batch\n",
      "Epoch 16/20  Iteration 2761/3560 Training loss: 1.7642 0.7512 sec/batch\n",
      "Epoch 16/20  Iteration 2762/3560 Training loss: 1.7641 0.6221 sec/batch\n",
      "Epoch 16/20  Iteration 2763/3560 Training loss: 1.7638 0.6379 sec/batch\n",
      "Epoch 16/20  Iteration 2764/3560 Training loss: 1.7637 0.6070 sec/batch\n",
      "Epoch 16/20  Iteration 2765/3560 Training loss: 1.7634 0.5809 sec/batch\n",
      "Epoch 16/20  Iteration 2766/3560 Training loss: 1.7634 0.6441 sec/batch\n",
      "Epoch 16/20  Iteration 2767/3560 Training loss: 1.7632 0.6710 sec/batch\n",
      "Epoch 16/20  Iteration 2768/3560 Training loss: 1.7628 0.5827 sec/batch\n",
      "Epoch 16/20  Iteration 2769/3560 Training loss: 1.7624 0.6011 sec/batch\n",
      "Epoch 16/20  Iteration 2770/3560 Training loss: 1.7620 0.6351 sec/batch\n",
      "Epoch 16/20  Iteration 2771/3560 Training loss: 1.7619 0.6805 sec/batch\n",
      "Epoch 16/20  Iteration 2772/3560 Training loss: 1.7618 0.6616 sec/batch\n",
      "Epoch 16/20  Iteration 2773/3560 Training loss: 1.7616 0.7322 sec/batch\n",
      "Epoch 16/20  Iteration 2774/3560 Training loss: 1.7614 0.6627 sec/batch\n",
      "Epoch 16/20  Iteration 2775/3560 Training loss: 1.7611 0.6408 sec/batch\n",
      "Epoch 16/20  Iteration 2776/3560 Training loss: 1.7610 0.6104 sec/batch\n",
      "Epoch 16/20  Iteration 2777/3560 Training loss: 1.7610 0.7229 sec/batch\n",
      "Epoch 16/20  Iteration 2778/3560 Training loss: 1.7610 0.6644 sec/batch\n",
      "Epoch 16/20  Iteration 2779/3560 Training loss: 1.7609 0.6182 sec/batch\n",
      "Epoch 16/20  Iteration 2780/3560 Training loss: 1.7610 0.6389 sec/batch\n",
      "Epoch 16/20  Iteration 2781/3560 Training loss: 1.7609 0.6078 sec/batch\n",
      "Epoch 16/20  Iteration 2782/3560 Training loss: 1.7609 0.6102 sec/batch\n",
      "Epoch 16/20  Iteration 2783/3560 Training loss: 1.7607 0.6710 sec/batch\n",
      "Epoch 16/20  Iteration 2784/3560 Training loss: 1.7606 0.7163 sec/batch\n",
      "Epoch 16/20  Iteration 2785/3560 Training loss: 1.7605 0.8180 sec/batch\n",
      "Epoch 16/20  Iteration 2786/3560 Training loss: 1.7601 0.8916 sec/batch\n",
      "Epoch 16/20  Iteration 2787/3560 Training loss: 1.7601 0.8351 sec/batch\n",
      "Epoch 16/20  Iteration 2788/3560 Training loss: 1.7601 0.8477 sec/batch\n",
      "Epoch 16/20  Iteration 2789/3560 Training loss: 1.7600 0.7018 sec/batch\n",
      "Epoch 16/20  Iteration 2790/3560 Training loss: 1.7599 0.7221 sec/batch\n",
      "Epoch 16/20  Iteration 2791/3560 Training loss: 1.7600 0.7882 sec/batch\n",
      "Epoch 16/20  Iteration 2792/3560 Training loss: 1.7596 0.6206 sec/batch\n",
      "Epoch 16/20  Iteration 2793/3560 Training loss: 1.7594 0.6826 sec/batch\n",
      "Epoch 16/20  Iteration 2794/3560 Training loss: 1.7595 0.6084 sec/batch\n",
      "Epoch 16/20  Iteration 2795/3560 Training loss: 1.7595 0.6299 sec/batch\n",
      "Epoch 16/20  Iteration 2796/3560 Training loss: 1.7591 0.7033 sec/batch\n",
      "Epoch 16/20  Iteration 2797/3560 Training loss: 1.7592 0.7012 sec/batch\n",
      "Epoch 16/20  Iteration 2798/3560 Training loss: 1.7592 0.5960 sec/batch\n",
      "Epoch 16/20  Iteration 2799/3560 Training loss: 1.7592 0.6831 sec/batch\n",
      "Epoch 16/20  Iteration 2800/3560 Training loss: 1.7590 0.7023 sec/batch\n",
      "Epoch 16/20  Iteration 2801/3560 Training loss: 1.7588 0.7768 sec/batch\n",
      "Epoch 16/20  Iteration 2802/3560 Training loss: 1.7586 0.7044 sec/batch\n",
      "Epoch 16/20  Iteration 2803/3560 Training loss: 1.7586 0.6408 sec/batch\n",
      "Epoch 16/20  Iteration 2804/3560 Training loss: 1.7586 0.6936 sec/batch\n",
      "Epoch 16/20  Iteration 2805/3560 Training loss: 1.7586 0.5899 sec/batch\n",
      "Epoch 16/20  Iteration 2806/3560 Training loss: 1.7586 0.6105 sec/batch\n",
      "Epoch 16/20  Iteration 2807/3560 Training loss: 1.7588 0.6283 sec/batch\n",
      "Epoch 16/20  Iteration 2808/3560 Training loss: 1.7587 0.7060 sec/batch\n",
      "Epoch 16/20  Iteration 2809/3560 Training loss: 1.7589 0.6868 sec/batch\n",
      "Epoch 16/20  Iteration 2810/3560 Training loss: 1.7588 0.6477 sec/batch\n",
      "Epoch 16/20  Iteration 2811/3560 Training loss: 1.7591 0.6757 sec/batch\n",
      "Epoch 16/20  Iteration 2812/3560 Training loss: 1.7590 0.6952 sec/batch\n",
      "Epoch 16/20  Iteration 2813/3560 Training loss: 1.7590 0.6874 sec/batch\n",
      "Epoch 16/20  Iteration 2814/3560 Training loss: 1.7591 0.7255 sec/batch\n",
      "Epoch 16/20  Iteration 2815/3560 Training loss: 1.7591 0.6412 sec/batch\n",
      "Epoch 16/20  Iteration 2816/3560 Training loss: 1.7592 0.7064 sec/batch\n",
      "Epoch 16/20  Iteration 2817/3560 Training loss: 1.7592 0.5683 sec/batch\n",
      "Epoch 16/20  Iteration 2818/3560 Training loss: 1.7594 0.6393 sec/batch\n",
      "Epoch 16/20  Iteration 2819/3560 Training loss: 1.7594 0.6466 sec/batch\n",
      "Epoch 16/20  Iteration 2820/3560 Training loss: 1.7593 0.6752 sec/batch\n",
      "Epoch 16/20  Iteration 2821/3560 Training loss: 1.7590 0.7048 sec/batch\n",
      "Epoch 16/20  Iteration 2822/3560 Training loss: 1.7591 0.6177 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2823/3560 Training loss: 1.7591 0.5942 sec/batch\n",
      "Epoch 16/20  Iteration 2824/3560 Training loss: 1.7592 0.6979 sec/batch\n",
      "Epoch 16/20  Iteration 2825/3560 Training loss: 1.7592 0.6751 sec/batch\n",
      "Epoch 16/20  Iteration 2826/3560 Training loss: 1.7591 0.7631 sec/batch\n",
      "Epoch 16/20  Iteration 2827/3560 Training loss: 1.7592 0.6709 sec/batch\n",
      "Epoch 16/20  Iteration 2828/3560 Training loss: 1.7593 0.6520 sec/batch\n",
      "Epoch 16/20  Iteration 2829/3560 Training loss: 1.7591 0.6562 sec/batch\n",
      "Epoch 16/20  Iteration 2830/3560 Training loss: 1.7592 0.6568 sec/batch\n",
      "Epoch 16/20  Iteration 2831/3560 Training loss: 1.7594 0.6976 sec/batch\n",
      "Epoch 16/20  Iteration 2832/3560 Training loss: 1.7594 0.7273 sec/batch\n",
      "Epoch 16/20  Iteration 2833/3560 Training loss: 1.7594 0.6906 sec/batch\n",
      "Epoch 16/20  Iteration 2834/3560 Training loss: 1.7594 0.6512 sec/batch\n",
      "Epoch 16/20  Iteration 2835/3560 Training loss: 1.7594 0.7156 sec/batch\n",
      "Epoch 16/20  Iteration 2836/3560 Training loss: 1.7594 0.5697 sec/batch\n",
      "Epoch 16/20  Iteration 2837/3560 Training loss: 1.7595 0.6940 sec/batch\n",
      "Epoch 16/20  Iteration 2838/3560 Training loss: 1.7598 0.6671 sec/batch\n",
      "Epoch 16/20  Iteration 2839/3560 Training loss: 1.7598 0.7572 sec/batch\n",
      "Epoch 16/20  Iteration 2840/3560 Training loss: 1.7598 0.6192 sec/batch\n",
      "Epoch 16/20  Iteration 2841/3560 Training loss: 1.7596 0.7030 sec/batch\n",
      "Epoch 16/20  Iteration 2842/3560 Training loss: 1.7595 0.6232 sec/batch\n",
      "Epoch 16/20  Iteration 2843/3560 Training loss: 1.7596 0.5607 sec/batch\n",
      "Epoch 16/20  Iteration 2844/3560 Training loss: 1.7596 0.5913 sec/batch\n",
      "Epoch 16/20  Iteration 2845/3560 Training loss: 1.7597 0.6489 sec/batch\n",
      "Epoch 16/20  Iteration 2846/3560 Training loss: 1.7596 0.6371 sec/batch\n",
      "Epoch 16/20  Iteration 2847/3560 Training loss: 1.7595 0.6156 sec/batch\n",
      "Epoch 16/20  Iteration 2848/3560 Training loss: 1.7596 0.7316 sec/batch\n",
      "Epoch 17/20  Iteration 2849/3560 Training loss: 1.8253 0.6317 sec/batch\n",
      "Epoch 17/20  Iteration 2850/3560 Training loss: 1.7900 0.6805 sec/batch\n",
      "Epoch 17/20  Iteration 2851/3560 Training loss: 1.7742 0.7503 sec/batch\n",
      "Epoch 17/20  Iteration 2852/3560 Training loss: 1.7719 0.7123 sec/batch\n",
      "Epoch 17/20  Iteration 2853/3560 Training loss: 1.7666 0.6324 sec/batch\n",
      "Epoch 17/20  Iteration 2854/3560 Training loss: 1.7584 0.5974 sec/batch\n",
      "Epoch 17/20  Iteration 2855/3560 Training loss: 1.7591 0.6515 sec/batch\n",
      "Epoch 17/20  Iteration 2856/3560 Training loss: 1.7570 0.6689 sec/batch\n",
      "Epoch 17/20  Iteration 2857/3560 Training loss: 1.7590 0.5732 sec/batch\n",
      "Epoch 17/20  Iteration 2858/3560 Training loss: 1.7584 0.6075 sec/batch\n",
      "Epoch 17/20  Iteration 2859/3560 Training loss: 1.7551 0.7934 sec/batch\n",
      "Epoch 17/20  Iteration 2860/3560 Training loss: 1.7528 0.5891 sec/batch\n",
      "Epoch 17/20  Iteration 2861/3560 Training loss: 1.7524 0.6280 sec/batch\n",
      "Epoch 17/20  Iteration 2862/3560 Training loss: 1.7554 0.6774 sec/batch\n",
      "Epoch 17/20  Iteration 2863/3560 Training loss: 1.7550 0.6820 sec/batch\n",
      "Epoch 17/20  Iteration 2864/3560 Training loss: 1.7529 0.8454 sec/batch\n",
      "Epoch 17/20  Iteration 2865/3560 Training loss: 1.7530 0.6664 sec/batch\n",
      "Epoch 17/20  Iteration 2866/3560 Training loss: 1.7555 0.5940 sec/batch\n",
      "Epoch 17/20  Iteration 2867/3560 Training loss: 1.7553 0.5437 sec/batch\n",
      "Epoch 17/20  Iteration 2868/3560 Training loss: 1.7560 0.6487 sec/batch\n",
      "Epoch 17/20  Iteration 2869/3560 Training loss: 1.7558 0.6667 sec/batch\n",
      "Epoch 17/20  Iteration 2870/3560 Training loss: 1.7568 0.6546 sec/batch\n",
      "Epoch 17/20  Iteration 2871/3560 Training loss: 1.7559 0.5815 sec/batch\n",
      "Epoch 17/20  Iteration 2872/3560 Training loss: 1.7554 0.6898 sec/batch\n",
      "Epoch 17/20  Iteration 2873/3560 Training loss: 1.7551 0.7732 sec/batch\n",
      "Epoch 17/20  Iteration 2874/3560 Training loss: 1.7539 0.7436 sec/batch\n",
      "Epoch 17/20  Iteration 2875/3560 Training loss: 1.7527 0.6908 sec/batch\n",
      "Epoch 17/20  Iteration 2876/3560 Training loss: 1.7533 0.7306 sec/batch\n",
      "Epoch 17/20  Iteration 2877/3560 Training loss: 1.7537 0.7775 sec/batch\n",
      "Epoch 17/20  Iteration 2878/3560 Training loss: 1.7540 0.6858 sec/batch\n",
      "Epoch 17/20  Iteration 2879/3560 Training loss: 1.7542 0.6445 sec/batch\n",
      "Epoch 17/20  Iteration 2880/3560 Training loss: 1.7536 0.6285 sec/batch\n",
      "Epoch 17/20  Iteration 2881/3560 Training loss: 1.7539 0.6312 sec/batch\n",
      "Epoch 17/20  Iteration 2882/3560 Training loss: 1.7544 0.5620 sec/batch\n",
      "Epoch 17/20  Iteration 2883/3560 Training loss: 1.7542 0.5853 sec/batch\n",
      "Epoch 17/20  Iteration 2884/3560 Training loss: 1.7541 0.7116 sec/batch\n",
      "Epoch 17/20  Iteration 2885/3560 Training loss: 1.7536 0.6294 sec/batch\n",
      "Epoch 17/20  Iteration 2886/3560 Training loss: 1.7527 0.7108 sec/batch\n",
      "Epoch 17/20  Iteration 2887/3560 Training loss: 1.7513 0.5863 sec/batch\n",
      "Epoch 17/20  Iteration 2888/3560 Training loss: 1.7508 0.7452 sec/batch\n",
      "Epoch 17/20  Iteration 2889/3560 Training loss: 1.7505 0.6641 sec/batch\n",
      "Epoch 17/20  Iteration 2890/3560 Training loss: 1.7510 0.7566 sec/batch\n",
      "Epoch 17/20  Iteration 2891/3560 Training loss: 1.7503 0.6892 sec/batch\n",
      "Epoch 17/20  Iteration 2892/3560 Training loss: 1.7494 0.5682 sec/batch\n",
      "Epoch 17/20  Iteration 2893/3560 Training loss: 1.7497 0.6450 sec/batch\n",
      "Epoch 17/20  Iteration 2894/3560 Training loss: 1.7486 0.5978 sec/batch\n",
      "Epoch 17/20  Iteration 2895/3560 Training loss: 1.7484 0.6515 sec/batch\n",
      "Epoch 17/20  Iteration 2896/3560 Training loss: 1.7479 0.6371 sec/batch\n",
      "Epoch 17/20  Iteration 2897/3560 Training loss: 1.7477 0.7122 sec/batch\n",
      "Epoch 17/20  Iteration 2898/3560 Training loss: 1.7485 0.7133 sec/batch\n",
      "Epoch 17/20  Iteration 2899/3560 Training loss: 1.7480 0.5981 sec/batch\n",
      "Epoch 17/20  Iteration 2900/3560 Training loss: 1.7488 0.7119 sec/batch\n",
      "Epoch 17/20  Iteration 2901/3560 Training loss: 1.7488 0.6654 sec/batch\n",
      "Epoch 17/20  Iteration 2902/3560 Training loss: 1.7488 0.8142 sec/batch\n",
      "Epoch 17/20  Iteration 2903/3560 Training loss: 1.7484 0.7443 sec/batch\n",
      "Epoch 17/20  Iteration 2904/3560 Training loss: 1.7487 0.5799 sec/batch\n",
      "Epoch 17/20  Iteration 2905/3560 Training loss: 1.7491 0.6072 sec/batch\n",
      "Epoch 17/20  Iteration 2906/3560 Training loss: 1.7490 0.6457 sec/batch\n",
      "Epoch 17/20  Iteration 2907/3560 Training loss: 1.7485 0.6259 sec/batch\n",
      "Epoch 17/20  Iteration 2908/3560 Training loss: 1.7488 0.5974 sec/batch\n",
      "Epoch 17/20  Iteration 2909/3560 Training loss: 1.7488 0.7134 sec/batch\n",
      "Epoch 17/20  Iteration 2910/3560 Training loss: 1.7497 0.6568 sec/batch\n",
      "Epoch 17/20  Iteration 2911/3560 Training loss: 1.7501 0.6887 sec/batch\n",
      "Epoch 17/20  Iteration 2912/3560 Training loss: 1.7502 0.6696 sec/batch\n",
      "Epoch 17/20  Iteration 2913/3560 Training loss: 1.7502 0.7546 sec/batch\n",
      "Epoch 17/20  Iteration 2914/3560 Training loss: 1.7504 0.6117 sec/batch\n",
      "Epoch 17/20  Iteration 2915/3560 Training loss: 1.7506 0.7507 sec/batch\n",
      "Epoch 17/20  Iteration 2916/3560 Training loss: 1.7502 0.7417 sec/batch\n",
      "Epoch 17/20  Iteration 2917/3560 Training loss: 1.7500 0.5630 sec/batch\n",
      "Epoch 17/20  Iteration 2918/3560 Training loss: 1.7498 0.5997 sec/batch\n",
      "Epoch 17/20  Iteration 2919/3560 Training loss: 1.7502 0.6531 sec/batch\n",
      "Epoch 17/20  Iteration 2920/3560 Training loss: 1.7502 0.6145 sec/batch\n",
      "Epoch 17/20  Iteration 2921/3560 Training loss: 1.7507 0.6283 sec/batch\n",
      "Epoch 17/20  Iteration 2922/3560 Training loss: 1.7504 0.6603 sec/batch\n",
      "Epoch 17/20  Iteration 2923/3560 Training loss: 1.7503 0.6649 sec/batch\n",
      "Epoch 17/20  Iteration 2924/3560 Training loss: 1.7505 0.6787 sec/batch\n",
      "Epoch 17/20  Iteration 2925/3560 Training loss: 1.7505 0.7302 sec/batch\n",
      "Epoch 17/20  Iteration 2926/3560 Training loss: 1.7507 0.5910 sec/batch\n",
      "Epoch 17/20  Iteration 2927/3560 Training loss: 1.7501 0.6870 sec/batch\n",
      "Epoch 17/20  Iteration 2928/3560 Training loss: 1.7500 0.6179 sec/batch\n",
      "Epoch 17/20  Iteration 2929/3560 Training loss: 1.7496 0.7719 sec/batch\n",
      "Epoch 17/20  Iteration 2930/3560 Training loss: 1.7497 0.5662 sec/batch\n",
      "Epoch 17/20  Iteration 2931/3560 Training loss: 1.7492 0.6162 sec/batch\n",
      "Epoch 17/20  Iteration 2932/3560 Training loss: 1.7493 0.6243 sec/batch\n",
      "Epoch 17/20  Iteration 2933/3560 Training loss: 1.7488 0.6536 sec/batch\n",
      "Epoch 17/20  Iteration 2934/3560 Training loss: 1.7485 0.6557 sec/batch\n",
      "Epoch 17/20  Iteration 2935/3560 Training loss: 1.7484 0.5813 sec/batch\n",
      "Epoch 17/20  Iteration 2936/3560 Training loss: 1.7480 0.7325 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 2937/3560 Training loss: 1.7477 0.6448 sec/batch\n",
      "Epoch 17/20  Iteration 2938/3560 Training loss: 1.7477 0.6024 sec/batch\n",
      "Epoch 17/20  Iteration 2939/3560 Training loss: 1.7474 0.7117 sec/batch\n",
      "Epoch 17/20  Iteration 2940/3560 Training loss: 1.7473 0.6736 sec/batch\n",
      "Epoch 17/20  Iteration 2941/3560 Training loss: 1.7470 0.6934 sec/batch\n",
      "Epoch 17/20  Iteration 2942/3560 Training loss: 1.7466 0.6334 sec/batch\n",
      "Epoch 17/20  Iteration 2943/3560 Training loss: 1.7462 0.6275 sec/batch\n",
      "Epoch 17/20  Iteration 2944/3560 Training loss: 1.7463 0.6066 sec/batch\n",
      "Epoch 17/20  Iteration 2945/3560 Training loss: 1.7462 0.6796 sec/batch\n",
      "Epoch 17/20  Iteration 2946/3560 Training loss: 1.7459 0.6083 sec/batch\n",
      "Epoch 17/20  Iteration 2947/3560 Training loss: 1.7456 0.6676 sec/batch\n",
      "Epoch 17/20  Iteration 2948/3560 Training loss: 1.7452 0.6544 sec/batch\n",
      "Epoch 17/20  Iteration 2949/3560 Training loss: 1.7453 0.6910 sec/batch\n",
      "Epoch 17/20  Iteration 2950/3560 Training loss: 1.7452 0.6153 sec/batch\n",
      "Epoch 17/20  Iteration 2951/3560 Training loss: 1.7450 0.6805 sec/batch\n",
      "Epoch 17/20  Iteration 2952/3560 Training loss: 1.7447 0.6904 sec/batch\n",
      "Epoch 17/20  Iteration 2953/3560 Training loss: 1.7445 0.7044 sec/batch\n",
      "Epoch 17/20  Iteration 2954/3560 Training loss: 1.7445 0.7194 sec/batch\n",
      "Epoch 17/20  Iteration 2955/3560 Training loss: 1.7445 0.6643 sec/batch\n",
      "Epoch 17/20  Iteration 2956/3560 Training loss: 1.7445 0.6260 sec/batch\n",
      "Epoch 17/20  Iteration 2957/3560 Training loss: 1.7445 0.6745 sec/batch\n",
      "Epoch 17/20  Iteration 2958/3560 Training loss: 1.7446 0.6026 sec/batch\n",
      "Epoch 17/20  Iteration 2959/3560 Training loss: 1.7445 0.6314 sec/batch\n",
      "Epoch 17/20  Iteration 2960/3560 Training loss: 1.7443 0.6133 sec/batch\n",
      "Epoch 17/20  Iteration 2961/3560 Training loss: 1.7442 0.6962 sec/batch\n",
      "Epoch 17/20  Iteration 2962/3560 Training loss: 1.7440 0.6255 sec/batch\n",
      "Epoch 17/20  Iteration 2963/3560 Training loss: 1.7439 0.6742 sec/batch\n",
      "Epoch 17/20  Iteration 2964/3560 Training loss: 1.7435 0.6118 sec/batch\n",
      "Epoch 17/20  Iteration 2965/3560 Training loss: 1.7435 0.7007 sec/batch\n",
      "Epoch 17/20  Iteration 2966/3560 Training loss: 1.7435 0.6856 sec/batch\n",
      "Epoch 17/20  Iteration 2967/3560 Training loss: 1.7435 0.6509 sec/batch\n",
      "Epoch 17/20  Iteration 2968/3560 Training loss: 1.7435 0.6236 sec/batch\n",
      "Epoch 17/20  Iteration 2969/3560 Training loss: 1.7435 0.7122 sec/batch\n",
      "Epoch 17/20  Iteration 2970/3560 Training loss: 1.7432 0.6509 sec/batch\n",
      "Epoch 17/20  Iteration 2971/3560 Training loss: 1.7429 0.5925 sec/batch\n",
      "Epoch 17/20  Iteration 2972/3560 Training loss: 1.7429 0.6666 sec/batch\n",
      "Epoch 17/20  Iteration 2973/3560 Training loss: 1.7429 0.6445 sec/batch\n",
      "Epoch 17/20  Iteration 2974/3560 Training loss: 1.7426 0.6814 sec/batch\n",
      "Epoch 17/20  Iteration 2975/3560 Training loss: 1.7428 0.6177 sec/batch\n",
      "Epoch 17/20  Iteration 2976/3560 Training loss: 1.7428 0.5939 sec/batch\n",
      "Epoch 17/20  Iteration 2977/3560 Training loss: 1.7427 0.6977 sec/batch\n",
      "Epoch 17/20  Iteration 2978/3560 Training loss: 1.7426 0.6589 sec/batch\n",
      "Epoch 17/20  Iteration 2979/3560 Training loss: 1.7423 0.6905 sec/batch\n",
      "Epoch 17/20  Iteration 2980/3560 Training loss: 1.7422 0.6217 sec/batch\n",
      "Epoch 17/20  Iteration 2981/3560 Training loss: 1.7423 0.6415 sec/batch\n",
      "Epoch 17/20  Iteration 2982/3560 Training loss: 1.7423 0.6601 sec/batch\n",
      "Epoch 17/20  Iteration 2983/3560 Training loss: 1.7423 0.6461 sec/batch\n",
      "Epoch 17/20  Iteration 2984/3560 Training loss: 1.7424 0.6596 sec/batch\n",
      "Epoch 17/20  Iteration 2985/3560 Training loss: 1.7426 0.6707 sec/batch\n",
      "Epoch 17/20  Iteration 2986/3560 Training loss: 1.7427 0.6949 sec/batch\n",
      "Epoch 17/20  Iteration 2987/3560 Training loss: 1.7428 0.5874 sec/batch\n",
      "Epoch 17/20  Iteration 2988/3560 Training loss: 1.7427 0.6000 sec/batch\n",
      "Epoch 17/20  Iteration 2989/3560 Training loss: 1.7430 0.6715 sec/batch\n",
      "Epoch 17/20  Iteration 2990/3560 Training loss: 1.7429 0.6157 sec/batch\n",
      "Epoch 17/20  Iteration 2991/3560 Training loss: 1.7429 0.7266 sec/batch\n",
      "Epoch 17/20  Iteration 2992/3560 Training loss: 1.7430 0.6063 sec/batch\n",
      "Epoch 17/20  Iteration 2993/3560 Training loss: 1.7430 0.6571 sec/batch\n",
      "Epoch 17/20  Iteration 2994/3560 Training loss: 1.7432 0.6105 sec/batch\n",
      "Epoch 17/20  Iteration 2995/3560 Training loss: 1.7432 0.5850 sec/batch\n",
      "Epoch 17/20  Iteration 2996/3560 Training loss: 1.7434 0.6390 sec/batch\n",
      "Epoch 17/20  Iteration 2997/3560 Training loss: 1.7435 0.5977 sec/batch\n",
      "Epoch 17/20  Iteration 2998/3560 Training loss: 1.7433 0.6876 sec/batch\n",
      "Epoch 17/20  Iteration 2999/3560 Training loss: 1.7431 0.6904 sec/batch\n",
      "Epoch 17/20  Iteration 3000/3560 Training loss: 1.7432 0.6721 sec/batch\n",
      "Epoch 17/20  Iteration 3001/3560 Training loss: 1.7432 0.6171 sec/batch\n",
      "Epoch 17/20  Iteration 3002/3560 Training loss: 1.7432 0.7028 sec/batch\n",
      "Epoch 17/20  Iteration 3003/3560 Training loss: 1.7432 0.6965 sec/batch\n",
      "Epoch 17/20  Iteration 3004/3560 Training loss: 1.7431 0.6660 sec/batch\n",
      "Epoch 17/20  Iteration 3005/3560 Training loss: 1.7432 0.7337 sec/batch\n",
      "Epoch 17/20  Iteration 3006/3560 Training loss: 1.7433 0.5754 sec/batch\n",
      "Epoch 17/20  Iteration 3007/3560 Training loss: 1.7430 0.5890 sec/batch\n",
      "Epoch 17/20  Iteration 3008/3560 Training loss: 1.7432 0.5789 sec/batch\n",
      "Epoch 17/20  Iteration 3009/3560 Training loss: 1.7434 0.6704 sec/batch\n",
      "Epoch 17/20  Iteration 3010/3560 Training loss: 1.7433 0.6582 sec/batch\n",
      "Epoch 17/20  Iteration 3011/3560 Training loss: 1.7434 0.6866 sec/batch\n",
      "Epoch 17/20  Iteration 3012/3560 Training loss: 1.7434 0.6220 sec/batch\n",
      "Epoch 17/20  Iteration 3013/3560 Training loss: 1.7434 0.7626 sec/batch\n",
      "Epoch 17/20  Iteration 3014/3560 Training loss: 1.7434 0.6611 sec/batch\n",
      "Epoch 17/20  Iteration 3015/3560 Training loss: 1.7434 0.6013 sec/batch\n",
      "Epoch 17/20  Iteration 3016/3560 Training loss: 1.7439 0.7081 sec/batch\n",
      "Epoch 17/20  Iteration 3017/3560 Training loss: 1.7439 0.7354 sec/batch\n",
      "Epoch 17/20  Iteration 3018/3560 Training loss: 1.7439 0.8864 sec/batch\n",
      "Epoch 17/20  Iteration 3019/3560 Training loss: 1.7438 0.5910 sec/batch\n",
      "Epoch 17/20  Iteration 3020/3560 Training loss: 1.7437 0.6207 sec/batch\n",
      "Epoch 17/20  Iteration 3021/3560 Training loss: 1.7438 0.6863 sec/batch\n",
      "Epoch 17/20  Iteration 3022/3560 Training loss: 1.7439 0.6147 sec/batch\n",
      "Epoch 17/20  Iteration 3023/3560 Training loss: 1.7439 0.6031 sec/batch\n",
      "Epoch 17/20  Iteration 3024/3560 Training loss: 1.7438 0.7240 sec/batch\n",
      "Epoch 17/20  Iteration 3025/3560 Training loss: 1.7438 0.5891 sec/batch\n",
      "Epoch 17/20  Iteration 3026/3560 Training loss: 1.7438 0.6782 sec/batch\n",
      "Epoch 18/20  Iteration 3027/3560 Training loss: 1.7940 0.7213 sec/batch\n",
      "Epoch 18/20  Iteration 3028/3560 Training loss: 1.7637 0.6348 sec/batch\n",
      "Epoch 18/20  Iteration 3029/3560 Training loss: 1.7552 0.6691 sec/batch\n",
      "Epoch 18/20  Iteration 3030/3560 Training loss: 1.7526 0.7785 sec/batch\n",
      "Epoch 18/20  Iteration 3031/3560 Training loss: 1.7474 0.5949 sec/batch\n",
      "Epoch 18/20  Iteration 3032/3560 Training loss: 1.7373 0.6482 sec/batch\n",
      "Epoch 18/20  Iteration 3033/3560 Training loss: 1.7376 0.6113 sec/batch\n",
      "Epoch 18/20  Iteration 3034/3560 Training loss: 1.7349 0.6102 sec/batch\n",
      "Epoch 18/20  Iteration 3035/3560 Training loss: 1.7372 0.6236 sec/batch\n",
      "Epoch 18/20  Iteration 3036/3560 Training loss: 1.7367 0.6502 sec/batch\n",
      "Epoch 18/20  Iteration 3037/3560 Training loss: 1.7342 0.6693 sec/batch\n",
      "Epoch 18/20  Iteration 3038/3560 Training loss: 1.7334 0.6029 sec/batch\n",
      "Epoch 18/20  Iteration 3039/3560 Training loss: 1.7337 0.6461 sec/batch\n",
      "Epoch 18/20  Iteration 3040/3560 Training loss: 1.7366 0.7165 sec/batch\n",
      "Epoch 18/20  Iteration 3041/3560 Training loss: 1.7364 0.6417 sec/batch\n",
      "Epoch 18/20  Iteration 3042/3560 Training loss: 1.7353 0.6912 sec/batch\n",
      "Epoch 18/20  Iteration 3043/3560 Training loss: 1.7354 0.6434 sec/batch\n",
      "Epoch 18/20  Iteration 3044/3560 Training loss: 1.7383 0.6588 sec/batch\n",
      "Epoch 18/20  Iteration 3045/3560 Training loss: 1.7387 0.7226 sec/batch\n",
      "Epoch 18/20  Iteration 3046/3560 Training loss: 1.7395 0.6818 sec/batch\n",
      "Epoch 18/20  Iteration 3047/3560 Training loss: 1.7386 0.6024 sec/batch\n",
      "Epoch 18/20  Iteration 3048/3560 Training loss: 1.7396 0.6918 sec/batch\n",
      "Epoch 18/20  Iteration 3049/3560 Training loss: 1.7391 0.6857 sec/batch\n",
      "Epoch 18/20  Iteration 3050/3560 Training loss: 1.7387 0.6177 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3051/3560 Training loss: 1.7381 0.6105 sec/batch\n",
      "Epoch 18/20  Iteration 3052/3560 Training loss: 1.7373 0.6469 sec/batch\n",
      "Epoch 18/20  Iteration 3053/3560 Training loss: 1.7367 0.6805 sec/batch\n",
      "Epoch 18/20  Iteration 3054/3560 Training loss: 1.7369 0.6416 sec/batch\n",
      "Epoch 18/20  Iteration 3055/3560 Training loss: 1.7376 0.6095 sec/batch\n",
      "Epoch 18/20  Iteration 3056/3560 Training loss: 1.7380 0.6344 sec/batch\n",
      "Epoch 18/20  Iteration 3057/3560 Training loss: 1.7381 0.6888 sec/batch\n",
      "Epoch 18/20  Iteration 3058/3560 Training loss: 1.7376 0.5987 sec/batch\n",
      "Epoch 18/20  Iteration 3059/3560 Training loss: 1.7377 0.6695 sec/batch\n",
      "Epoch 18/20  Iteration 3060/3560 Training loss: 1.7386 0.6661 sec/batch\n",
      "Epoch 18/20  Iteration 3061/3560 Training loss: 1.7385 0.6988 sec/batch\n",
      "Epoch 18/20  Iteration 3062/3560 Training loss: 1.7383 0.7177 sec/batch\n",
      "Epoch 18/20  Iteration 3063/3560 Training loss: 1.7379 0.6277 sec/batch\n",
      "Epoch 18/20  Iteration 3064/3560 Training loss: 1.7369 0.6516 sec/batch\n",
      "Epoch 18/20  Iteration 3065/3560 Training loss: 1.7355 0.6566 sec/batch\n",
      "Epoch 18/20  Iteration 3066/3560 Training loss: 1.7349 0.7382 sec/batch\n",
      "Epoch 18/20  Iteration 3067/3560 Training loss: 1.7346 0.6955 sec/batch\n",
      "Epoch 18/20  Iteration 3068/3560 Training loss: 1.7353 0.9089 sec/batch\n",
      "Epoch 18/20  Iteration 3069/3560 Training loss: 1.7349 0.8161 sec/batch\n",
      "Epoch 18/20  Iteration 3070/3560 Training loss: 1.7340 0.9632 sec/batch\n",
      "Epoch 18/20  Iteration 3071/3560 Training loss: 1.7345 0.5544 sec/batch\n",
      "Epoch 18/20  Iteration 3072/3560 Training loss: 1.7334 0.7980 sec/batch\n",
      "Epoch 18/20  Iteration 3073/3560 Training loss: 1.7335 0.9110 sec/batch\n",
      "Epoch 18/20  Iteration 3074/3560 Training loss: 1.7331 0.6708 sec/batch\n",
      "Epoch 18/20  Iteration 3075/3560 Training loss: 1.7330 0.7491 sec/batch\n",
      "Epoch 18/20  Iteration 3076/3560 Training loss: 1.7336 0.7075 sec/batch\n",
      "Epoch 18/20  Iteration 3077/3560 Training loss: 1.7331 0.6647 sec/batch\n",
      "Epoch 18/20  Iteration 3078/3560 Training loss: 1.7339 0.6670 sec/batch\n",
      "Epoch 18/20  Iteration 3079/3560 Training loss: 1.7340 0.5424 sec/batch\n",
      "Epoch 18/20  Iteration 3080/3560 Training loss: 1.7338 0.7172 sec/batch\n",
      "Epoch 18/20  Iteration 3081/3560 Training loss: 1.7334 0.7122 sec/batch\n",
      "Epoch 18/20  Iteration 3082/3560 Training loss: 1.7336 0.6778 sec/batch\n",
      "Epoch 18/20  Iteration 3083/3560 Training loss: 1.7339 0.5602 sec/batch\n",
      "Epoch 18/20  Iteration 3084/3560 Training loss: 1.7339 0.6180 sec/batch\n",
      "Epoch 18/20  Iteration 3085/3560 Training loss: 1.7336 0.7719 sec/batch\n",
      "Epoch 18/20  Iteration 3086/3560 Training loss: 1.7341 0.7271 sec/batch\n",
      "Epoch 18/20  Iteration 3087/3560 Training loss: 1.7341 0.6163 sec/batch\n",
      "Epoch 18/20  Iteration 3088/3560 Training loss: 1.7351 0.6677 sec/batch\n",
      "Epoch 18/20  Iteration 3089/3560 Training loss: 1.7356 0.5967 sec/batch\n",
      "Epoch 18/20  Iteration 3090/3560 Training loss: 1.7359 0.6867 sec/batch\n",
      "Epoch 18/20  Iteration 3091/3560 Training loss: 1.7356 0.6789 sec/batch\n",
      "Epoch 18/20  Iteration 3092/3560 Training loss: 1.7358 0.5720 sec/batch\n",
      "Epoch 18/20  Iteration 3093/3560 Training loss: 1.7360 0.6674 sec/batch\n",
      "Epoch 18/20  Iteration 3094/3560 Training loss: 1.7357 0.6576 sec/batch\n",
      "Epoch 18/20  Iteration 3095/3560 Training loss: 1.7356 0.5904 sec/batch\n",
      "Epoch 18/20  Iteration 3096/3560 Training loss: 1.7355 0.6045 sec/batch\n",
      "Epoch 18/20  Iteration 3097/3560 Training loss: 1.7361 0.6338 sec/batch\n",
      "Epoch 18/20  Iteration 3098/3560 Training loss: 1.7361 0.7457 sec/batch\n",
      "Epoch 18/20  Iteration 3099/3560 Training loss: 1.7366 0.6525 sec/batch\n",
      "Epoch 18/20  Iteration 3100/3560 Training loss: 1.7363 0.6388 sec/batch\n",
      "Epoch 18/20  Iteration 3101/3560 Training loss: 1.7362 0.6289 sec/batch\n",
      "Epoch 18/20  Iteration 3102/3560 Training loss: 1.7363 0.6263 sec/batch\n",
      "Epoch 18/20  Iteration 3103/3560 Training loss: 1.7364 0.5524 sec/batch\n",
      "Epoch 18/20  Iteration 3104/3560 Training loss: 1.7365 0.7852 sec/batch\n",
      "Epoch 18/20  Iteration 3105/3560 Training loss: 1.7360 0.6584 sec/batch\n",
      "Epoch 18/20  Iteration 3106/3560 Training loss: 1.7358 0.6147 sec/batch\n",
      "Epoch 18/20  Iteration 3107/3560 Training loss: 1.7353 0.6190 sec/batch\n",
      "Epoch 18/20  Iteration 3108/3560 Training loss: 1.7355 0.6231 sec/batch\n",
      "Epoch 18/20  Iteration 3109/3560 Training loss: 1.7350 0.6669 sec/batch\n",
      "Epoch 18/20  Iteration 3110/3560 Training loss: 1.7350 0.6697 sec/batch\n",
      "Epoch 18/20  Iteration 3111/3560 Training loss: 1.7345 0.6731 sec/batch\n",
      "Epoch 18/20  Iteration 3112/3560 Training loss: 1.7343 0.6859 sec/batch\n",
      "Epoch 18/20  Iteration 3113/3560 Training loss: 1.7341 0.6528 sec/batch\n",
      "Epoch 18/20  Iteration 3114/3560 Training loss: 1.7338 0.6516 sec/batch\n",
      "Epoch 18/20  Iteration 3115/3560 Training loss: 1.7333 0.6477 sec/batch\n",
      "Epoch 18/20  Iteration 3116/3560 Training loss: 1.7334 0.6684 sec/batch\n",
      "Epoch 18/20  Iteration 3117/3560 Training loss: 1.7333 0.5704 sec/batch\n",
      "Epoch 18/20  Iteration 3118/3560 Training loss: 1.7331 0.6422 sec/batch\n",
      "Epoch 18/20  Iteration 3119/3560 Training loss: 1.7326 0.6568 sec/batch\n",
      "Epoch 18/20  Iteration 3120/3560 Training loss: 1.7322 0.6594 sec/batch\n",
      "Epoch 18/20  Iteration 3121/3560 Training loss: 1.7318 0.6333 sec/batch\n",
      "Epoch 18/20  Iteration 3122/3560 Training loss: 1.7318 0.6063 sec/batch\n",
      "Epoch 18/20  Iteration 3123/3560 Training loss: 1.7317 0.6955 sec/batch\n",
      "Epoch 18/20  Iteration 3124/3560 Training loss: 1.7313 0.6832 sec/batch\n",
      "Epoch 18/20  Iteration 3125/3560 Training loss: 1.7311 0.6742 sec/batch\n",
      "Epoch 18/20  Iteration 3126/3560 Training loss: 1.7307 0.6321 sec/batch\n",
      "Epoch 18/20  Iteration 3127/3560 Training loss: 1.7307 0.5806 sec/batch\n",
      "Epoch 18/20  Iteration 3128/3560 Training loss: 1.7306 0.6515 sec/batch\n",
      "Epoch 18/20  Iteration 3129/3560 Training loss: 1.7303 0.6602 sec/batch\n",
      "Epoch 18/20  Iteration 3130/3560 Training loss: 1.7302 0.6227 sec/batch\n",
      "Epoch 18/20  Iteration 3131/3560 Training loss: 1.7300 0.6779 sec/batch\n",
      "Epoch 18/20  Iteration 3132/3560 Training loss: 1.7300 0.6517 sec/batch\n",
      "Epoch 18/20  Iteration 3133/3560 Training loss: 1.7300 0.6539 sec/batch\n",
      "Epoch 18/20  Iteration 3134/3560 Training loss: 1.7300 0.7303 sec/batch\n",
      "Epoch 18/20  Iteration 3135/3560 Training loss: 1.7301 0.6550 sec/batch\n",
      "Epoch 18/20  Iteration 3136/3560 Training loss: 1.7301 0.7204 sec/batch\n",
      "Epoch 18/20  Iteration 3137/3560 Training loss: 1.7300 0.6548 sec/batch\n",
      "Epoch 18/20  Iteration 3138/3560 Training loss: 1.7299 0.6467 sec/batch\n",
      "Epoch 18/20  Iteration 3139/3560 Training loss: 1.7298 0.6497 sec/batch\n",
      "Epoch 18/20  Iteration 3140/3560 Training loss: 1.7297 0.6020 sec/batch\n",
      "Epoch 18/20  Iteration 3141/3560 Training loss: 1.7295 0.6232 sec/batch\n",
      "Epoch 18/20  Iteration 3142/3560 Training loss: 1.7291 0.5993 sec/batch\n",
      "Epoch 18/20  Iteration 3143/3560 Training loss: 1.7290 0.5443 sec/batch\n",
      "Epoch 18/20  Iteration 3144/3560 Training loss: 1.7290 0.7237 sec/batch\n",
      "Epoch 18/20  Iteration 3145/3560 Training loss: 1.7290 0.6361 sec/batch\n",
      "Epoch 18/20  Iteration 3146/3560 Training loss: 1.7290 0.6636 sec/batch\n",
      "Epoch 18/20  Iteration 3147/3560 Training loss: 1.7290 0.6836 sec/batch\n",
      "Epoch 18/20  Iteration 3148/3560 Training loss: 1.7287 0.6839 sec/batch\n",
      "Epoch 18/20  Iteration 3149/3560 Training loss: 1.7284 0.6925 sec/batch\n",
      "Epoch 18/20  Iteration 3150/3560 Training loss: 1.7286 0.6896 sec/batch\n",
      "Epoch 18/20  Iteration 3151/3560 Training loss: 1.7285 0.6495 sec/batch\n",
      "Epoch 18/20  Iteration 3152/3560 Training loss: 1.7282 0.7261 sec/batch\n",
      "Epoch 18/20  Iteration 3153/3560 Training loss: 1.7283 0.6344 sec/batch\n",
      "Epoch 18/20  Iteration 3154/3560 Training loss: 1.7283 0.6805 sec/batch\n",
      "Epoch 18/20  Iteration 3155/3560 Training loss: 1.7282 0.5617 sec/batch\n",
      "Epoch 18/20  Iteration 3156/3560 Training loss: 1.7281 0.6464 sec/batch\n",
      "Epoch 18/20  Iteration 3157/3560 Training loss: 1.7278 0.6599 sec/batch\n",
      "Epoch 18/20  Iteration 3158/3560 Training loss: 1.7275 0.6385 sec/batch\n",
      "Epoch 18/20  Iteration 3159/3560 Training loss: 1.7276 0.6019 sec/batch\n",
      "Epoch 18/20  Iteration 3160/3560 Training loss: 1.7277 0.7402 sec/batch\n",
      "Epoch 18/20  Iteration 3161/3560 Training loss: 1.7277 0.6088 sec/batch\n",
      "Epoch 18/20  Iteration 3162/3560 Training loss: 1.7277 0.7684 sec/batch\n",
      "Epoch 18/20  Iteration 3163/3560 Training loss: 1.7278 0.6713 sec/batch\n",
      "Epoch 18/20  Iteration 3164/3560 Training loss: 1.7279 0.6421 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3165/3560 Training loss: 1.7281 0.6542 sec/batch\n",
      "Epoch 18/20  Iteration 3166/3560 Training loss: 1.7280 0.5859 sec/batch\n",
      "Epoch 18/20  Iteration 3167/3560 Training loss: 1.7284 0.6336 sec/batch\n",
      "Epoch 18/20  Iteration 3168/3560 Training loss: 1.7283 0.6223 sec/batch\n",
      "Epoch 18/20  Iteration 3169/3560 Training loss: 1.7282 0.6747 sec/batch\n",
      "Epoch 18/20  Iteration 3170/3560 Training loss: 1.7283 0.6604 sec/batch\n",
      "Epoch 18/20  Iteration 3171/3560 Training loss: 1.7282 0.6697 sec/batch\n",
      "Epoch 18/20  Iteration 3172/3560 Training loss: 1.7283 0.6115 sec/batch\n",
      "Epoch 18/20  Iteration 3173/3560 Training loss: 1.7283 0.7532 sec/batch\n",
      "Epoch 18/20  Iteration 3174/3560 Training loss: 1.7285 0.6439 sec/batch\n",
      "Epoch 18/20  Iteration 3175/3560 Training loss: 1.7285 0.7217 sec/batch\n",
      "Epoch 18/20  Iteration 3176/3560 Training loss: 1.7284 0.6587 sec/batch\n",
      "Epoch 18/20  Iteration 3177/3560 Training loss: 1.7282 0.6132 sec/batch\n",
      "Epoch 18/20  Iteration 3178/3560 Training loss: 1.7282 0.6467 sec/batch\n",
      "Epoch 18/20  Iteration 3179/3560 Training loss: 1.7282 0.6355 sec/batch\n",
      "Epoch 18/20  Iteration 3180/3560 Training loss: 1.7283 0.6228 sec/batch\n",
      "Epoch 18/20  Iteration 3181/3560 Training loss: 1.7283 0.6901 sec/batch\n",
      "Epoch 18/20  Iteration 3182/3560 Training loss: 1.7283 0.6664 sec/batch\n",
      "Epoch 18/20  Iteration 3183/3560 Training loss: 1.7283 0.6949 sec/batch\n",
      "Epoch 18/20  Iteration 3184/3560 Training loss: 1.7283 0.5938 sec/batch\n",
      "Epoch 18/20  Iteration 3185/3560 Training loss: 1.7281 0.6619 sec/batch\n",
      "Epoch 18/20  Iteration 3186/3560 Training loss: 1.7282 0.5658 sec/batch\n",
      "Epoch 18/20  Iteration 3187/3560 Training loss: 1.7284 0.7079 sec/batch\n",
      "Epoch 18/20  Iteration 3188/3560 Training loss: 1.7284 0.6391 sec/batch\n",
      "Epoch 18/20  Iteration 3189/3560 Training loss: 1.7284 0.6657 sec/batch\n",
      "Epoch 18/20  Iteration 3190/3560 Training loss: 1.7284 0.5787 sec/batch\n",
      "Epoch 18/20  Iteration 3191/3560 Training loss: 1.7284 0.6506 sec/batch\n",
      "Epoch 18/20  Iteration 3192/3560 Training loss: 1.7283 0.5927 sec/batch\n",
      "Epoch 18/20  Iteration 3193/3560 Training loss: 1.7284 0.7036 sec/batch\n",
      "Epoch 18/20  Iteration 3194/3560 Training loss: 1.7288 0.6664 sec/batch\n",
      "Epoch 18/20  Iteration 3195/3560 Training loss: 1.7288 0.6758 sec/batch\n",
      "Epoch 18/20  Iteration 3196/3560 Training loss: 1.7288 0.7001 sec/batch\n",
      "Epoch 18/20  Iteration 3197/3560 Training loss: 1.7286 0.6325 sec/batch\n",
      "Epoch 18/20  Iteration 3198/3560 Training loss: 1.7285 0.5715 sec/batch\n",
      "Epoch 18/20  Iteration 3199/3560 Training loss: 1.7286 0.6780 sec/batch\n",
      "Epoch 18/20  Iteration 3200/3560 Training loss: 1.7286 0.6317 sec/batch\n",
      "Epoch 18/20  Iteration 3201/3560 Training loss: 1.7286 0.7278 sec/batch\n",
      "Epoch 18/20  Iteration 3202/3560 Training loss: 1.7285 0.6689 sec/batch\n",
      "Epoch 18/20  Iteration 3203/3560 Training loss: 1.7284 0.6286 sec/batch\n",
      "Epoch 18/20  Iteration 3204/3560 Training loss: 1.7284 0.6468 sec/batch\n",
      "Epoch 19/20  Iteration 3205/3560 Training loss: 1.7779 0.5990 sec/batch\n",
      "Epoch 19/20  Iteration 3206/3560 Training loss: 1.7536 0.5734 sec/batch\n",
      "Epoch 19/20  Iteration 3207/3560 Training loss: 1.7402 0.6505 sec/batch\n",
      "Epoch 19/20  Iteration 3208/3560 Training loss: 1.7384 0.7132 sec/batch\n",
      "Epoch 19/20  Iteration 3209/3560 Training loss: 1.7318 0.6588 sec/batch\n",
      "Epoch 19/20  Iteration 3210/3560 Training loss: 1.7241 0.6639 sec/batch\n",
      "Epoch 19/20  Iteration 3211/3560 Training loss: 1.7237 0.5716 sec/batch\n",
      "Epoch 19/20  Iteration 3212/3560 Training loss: 1.7236 0.7378 sec/batch\n",
      "Epoch 19/20  Iteration 3213/3560 Training loss: 1.7260 0.6447 sec/batch\n",
      "Epoch 19/20  Iteration 3214/3560 Training loss: 1.7258 0.6035 sec/batch\n",
      "Epoch 19/20  Iteration 3215/3560 Training loss: 1.7220 0.6946 sec/batch\n",
      "Epoch 19/20  Iteration 3216/3560 Training loss: 1.7207 0.6045 sec/batch\n",
      "Epoch 19/20  Iteration 3217/3560 Training loss: 1.7204 0.5900 sec/batch\n",
      "Epoch 19/20  Iteration 3218/3560 Training loss: 1.7233 0.6130 sec/batch\n",
      "Epoch 19/20  Iteration 3219/3560 Training loss: 1.7229 0.6253 sec/batch\n",
      "Epoch 19/20  Iteration 3220/3560 Training loss: 1.7213 0.6169 sec/batch\n",
      "Epoch 19/20  Iteration 3221/3560 Training loss: 1.7218 0.6407 sec/batch\n",
      "Epoch 19/20  Iteration 3222/3560 Training loss: 1.7239 0.7095 sec/batch\n",
      "Epoch 19/20  Iteration 3223/3560 Training loss: 1.7237 0.6192 sec/batch\n",
      "Epoch 19/20  Iteration 3224/3560 Training loss: 1.7249 0.6157 sec/batch\n",
      "Epoch 19/20  Iteration 3225/3560 Training loss: 1.7246 0.7116 sec/batch\n",
      "Epoch 19/20  Iteration 3226/3560 Training loss: 1.7249 0.6052 sec/batch\n",
      "Epoch 19/20  Iteration 3227/3560 Training loss: 1.7242 0.6527 sec/batch\n",
      "Epoch 19/20  Iteration 3228/3560 Training loss: 1.7239 0.6173 sec/batch\n",
      "Epoch 19/20  Iteration 3229/3560 Training loss: 1.7237 0.6822 sec/batch\n",
      "Epoch 19/20  Iteration 3230/3560 Training loss: 1.7228 0.5869 sec/batch\n",
      "Epoch 19/20  Iteration 3231/3560 Training loss: 1.7221 0.6234 sec/batch\n",
      "Epoch 19/20  Iteration 3232/3560 Training loss: 1.7227 0.6609 sec/batch\n",
      "Epoch 19/20  Iteration 3233/3560 Training loss: 1.7233 0.6699 sec/batch\n",
      "Epoch 19/20  Iteration 3234/3560 Training loss: 1.7239 0.7289 sec/batch\n",
      "Epoch 19/20  Iteration 3235/3560 Training loss: 1.7238 0.6125 sec/batch\n",
      "Epoch 19/20  Iteration 3236/3560 Training loss: 1.7228 0.6399 sec/batch\n",
      "Epoch 19/20  Iteration 3237/3560 Training loss: 1.7230 0.6496 sec/batch\n",
      "Epoch 19/20  Iteration 3238/3560 Training loss: 1.7237 0.6317 sec/batch\n",
      "Epoch 19/20  Iteration 3239/3560 Training loss: 1.7236 0.6765 sec/batch\n",
      "Epoch 19/20  Iteration 3240/3560 Training loss: 1.7239 0.6383 sec/batch\n",
      "Epoch 19/20  Iteration 3241/3560 Training loss: 1.7231 0.6253 sec/batch\n",
      "Epoch 19/20  Iteration 3242/3560 Training loss: 1.7221 0.6416 sec/batch\n",
      "Epoch 19/20  Iteration 3243/3560 Training loss: 1.7211 0.5956 sec/batch\n",
      "Epoch 19/20  Iteration 3244/3560 Training loss: 1.7206 0.6545 sec/batch\n",
      "Epoch 19/20  Iteration 3245/3560 Training loss: 1.7203 0.6453 sec/batch\n",
      "Epoch 19/20  Iteration 3246/3560 Training loss: 1.7207 0.7040 sec/batch\n",
      "Epoch 19/20  Iteration 3247/3560 Training loss: 1.7202 0.6802 sec/batch\n",
      "Epoch 19/20  Iteration 3248/3560 Training loss: 1.7194 0.6708 sec/batch\n",
      "Epoch 19/20  Iteration 3249/3560 Training loss: 1.7200 0.6269 sec/batch\n",
      "Epoch 19/20  Iteration 3250/3560 Training loss: 1.7192 0.7088 sec/batch\n",
      "Epoch 19/20  Iteration 3251/3560 Training loss: 1.7192 0.7965 sec/batch\n",
      "Epoch 19/20  Iteration 3252/3560 Training loss: 1.7185 0.6299 sec/batch\n",
      "Epoch 19/20  Iteration 3253/3560 Training loss: 1.7185 0.6363 sec/batch\n",
      "Epoch 19/20  Iteration 3254/3560 Training loss: 1.7191 0.5978 sec/batch\n",
      "Epoch 19/20  Iteration 3255/3560 Training loss: 1.7187 0.6305 sec/batch\n",
      "Epoch 19/20  Iteration 3256/3560 Training loss: 1.7195 0.6676 sec/batch\n",
      "Epoch 19/20  Iteration 3257/3560 Training loss: 1.7194 0.6248 sec/batch\n",
      "Epoch 19/20  Iteration 3258/3560 Training loss: 1.7195 0.5961 sec/batch\n",
      "Epoch 19/20  Iteration 3259/3560 Training loss: 1.7190 0.7127 sec/batch\n",
      "Epoch 19/20  Iteration 3260/3560 Training loss: 1.7192 0.7504 sec/batch\n",
      "Epoch 19/20  Iteration 3261/3560 Training loss: 1.7196 0.7149 sec/batch\n",
      "Epoch 19/20  Iteration 3262/3560 Training loss: 1.7193 0.6666 sec/batch\n",
      "Epoch 19/20  Iteration 3263/3560 Training loss: 1.7188 0.6593 sec/batch\n",
      "Epoch 19/20  Iteration 3264/3560 Training loss: 1.7193 0.7787 sec/batch\n",
      "Epoch 19/20  Iteration 3265/3560 Training loss: 1.7192 0.6159 sec/batch\n",
      "Epoch 19/20  Iteration 3266/3560 Training loss: 1.7201 0.6136 sec/batch\n",
      "Epoch 19/20  Iteration 3267/3560 Training loss: 1.7205 0.5993 sec/batch\n",
      "Epoch 19/20  Iteration 3268/3560 Training loss: 1.7207 0.7256 sec/batch\n",
      "Epoch 19/20  Iteration 3269/3560 Training loss: 1.7207 0.8349 sec/batch\n",
      "Epoch 19/20  Iteration 3270/3560 Training loss: 1.7211 0.7151 sec/batch\n",
      "Epoch 19/20  Iteration 3271/3560 Training loss: 1.7214 0.8945 sec/batch\n",
      "Epoch 19/20  Iteration 3272/3560 Training loss: 1.7209 0.7800 sec/batch\n",
      "Epoch 19/20  Iteration 3273/3560 Training loss: 1.7208 0.8178 sec/batch\n",
      "Epoch 19/20  Iteration 3274/3560 Training loss: 1.7207 0.7168 sec/batch\n",
      "Epoch 19/20  Iteration 3275/3560 Training loss: 1.7211 0.6862 sec/batch\n",
      "Epoch 19/20  Iteration 3276/3560 Training loss: 1.7212 0.7213 sec/batch\n",
      "Epoch 19/20  Iteration 3277/3560 Training loss: 1.7217 0.7391 sec/batch\n",
      "Epoch 19/20  Iteration 3278/3560 Training loss: 1.7215 0.6959 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3279/3560 Training loss: 1.7214 0.6225 sec/batch\n",
      "Epoch 19/20  Iteration 3280/3560 Training loss: 1.7215 0.6105 sec/batch\n",
      "Epoch 19/20  Iteration 3281/3560 Training loss: 1.7214 0.6053 sec/batch\n",
      "Epoch 19/20  Iteration 3282/3560 Training loss: 1.7216 0.6110 sec/batch\n",
      "Epoch 19/20  Iteration 3283/3560 Training loss: 1.7211 0.6704 sec/batch\n",
      "Epoch 19/20  Iteration 3284/3560 Training loss: 1.7210 0.6812 sec/batch\n",
      "Epoch 19/20  Iteration 3285/3560 Training loss: 1.7205 0.5907 sec/batch\n",
      "Epoch 19/20  Iteration 3286/3560 Training loss: 1.7206 0.6759 sec/batch\n",
      "Epoch 19/20  Iteration 3287/3560 Training loss: 1.7201 0.7283 sec/batch\n",
      "Epoch 19/20  Iteration 3288/3560 Training loss: 1.7201 0.6775 sec/batch\n",
      "Epoch 19/20  Iteration 3289/3560 Training loss: 1.7197 0.6148 sec/batch\n",
      "Epoch 19/20  Iteration 3290/3560 Training loss: 1.7193 0.7173 sec/batch\n",
      "Epoch 19/20  Iteration 3291/3560 Training loss: 1.7192 0.6110 sec/batch\n",
      "Epoch 19/20  Iteration 3292/3560 Training loss: 1.7189 0.5336 sec/batch\n",
      "Epoch 19/20  Iteration 3293/3560 Training loss: 1.7184 0.6825 sec/batch\n",
      "Epoch 19/20  Iteration 3294/3560 Training loss: 1.7185 0.5785 sec/batch\n",
      "Epoch 19/20  Iteration 3295/3560 Training loss: 1.7183 0.6015 sec/batch\n",
      "Epoch 19/20  Iteration 3296/3560 Training loss: 1.7182 0.6231 sec/batch\n",
      "Epoch 19/20  Iteration 3297/3560 Training loss: 1.7178 0.7670 sec/batch\n",
      "Epoch 19/20  Iteration 3298/3560 Training loss: 1.7175 0.6060 sec/batch\n",
      "Epoch 19/20  Iteration 3299/3560 Training loss: 1.7173 0.6143 sec/batch\n",
      "Epoch 19/20  Iteration 3300/3560 Training loss: 1.7173 0.7049 sec/batch\n",
      "Epoch 19/20  Iteration 3301/3560 Training loss: 1.7173 0.6181 sec/batch\n",
      "Epoch 19/20  Iteration 3302/3560 Training loss: 1.7169 0.6650 sec/batch\n",
      "Epoch 19/20  Iteration 3303/3560 Training loss: 1.7166 0.7218 sec/batch\n",
      "Epoch 19/20  Iteration 3304/3560 Training loss: 1.7162 0.5966 sec/batch\n",
      "Epoch 19/20  Iteration 3305/3560 Training loss: 1.7161 0.6140 sec/batch\n",
      "Epoch 19/20  Iteration 3306/3560 Training loss: 1.7161 0.6735 sec/batch\n",
      "Epoch 19/20  Iteration 3307/3560 Training loss: 1.7159 0.5772 sec/batch\n",
      "Epoch 19/20  Iteration 3308/3560 Training loss: 1.7157 0.6365 sec/batch\n",
      "Epoch 19/20  Iteration 3309/3560 Training loss: 1.7155 0.7109 sec/batch\n",
      "Epoch 19/20  Iteration 3310/3560 Training loss: 1.7154 0.6511 sec/batch\n",
      "Epoch 19/20  Iteration 3311/3560 Training loss: 1.7154 0.6552 sec/batch\n",
      "Epoch 19/20  Iteration 3312/3560 Training loss: 1.7154 0.6060 sec/batch\n",
      "Epoch 19/20  Iteration 3313/3560 Training loss: 1.7154 0.7027 sec/batch\n",
      "Epoch 19/20  Iteration 3314/3560 Training loss: 1.7155 0.6588 sec/batch\n",
      "Epoch 19/20  Iteration 3315/3560 Training loss: 1.7154 0.6525 sec/batch\n",
      "Epoch 19/20  Iteration 3316/3560 Training loss: 1.7152 0.7779 sec/batch\n",
      "Epoch 19/20  Iteration 3317/3560 Training loss: 1.7152 0.7119 sec/batch\n",
      "Epoch 19/20  Iteration 3318/3560 Training loss: 1.7151 0.7439 sec/batch\n",
      "Epoch 19/20  Iteration 3319/3560 Training loss: 1.7149 0.7512 sec/batch\n",
      "Epoch 19/20  Iteration 3320/3560 Training loss: 1.7145 0.7108 sec/batch\n",
      "Epoch 19/20  Iteration 3321/3560 Training loss: 1.7145 0.7931 sec/batch\n",
      "Epoch 19/20  Iteration 3322/3560 Training loss: 1.7145 0.6230 sec/batch\n",
      "Epoch 19/20  Iteration 3323/3560 Training loss: 1.7144 0.7170 sec/batch\n",
      "Epoch 19/20  Iteration 3324/3560 Training loss: 1.7145 0.5736 sec/batch\n",
      "Epoch 19/20  Iteration 3325/3560 Training loss: 1.7145 0.6728 sec/batch\n",
      "Epoch 19/20  Iteration 3326/3560 Training loss: 1.7142 0.6774 sec/batch\n",
      "Epoch 19/20  Iteration 3327/3560 Training loss: 1.7139 0.6289 sec/batch\n",
      "Epoch 19/20  Iteration 3328/3560 Training loss: 1.7140 0.7608 sec/batch\n",
      "Epoch 19/20  Iteration 3329/3560 Training loss: 1.7139 0.6272 sec/batch\n",
      "Epoch 19/20  Iteration 3330/3560 Training loss: 1.7136 0.6475 sec/batch\n",
      "Epoch 19/20  Iteration 3331/3560 Training loss: 1.7137 0.6002 sec/batch\n",
      "Epoch 19/20  Iteration 3332/3560 Training loss: 1.7138 0.6221 sec/batch\n",
      "Epoch 19/20  Iteration 3333/3560 Training loss: 1.7137 0.6613 sec/batch\n",
      "Epoch 19/20  Iteration 3334/3560 Training loss: 1.7137 0.6511 sec/batch\n",
      "Epoch 19/20  Iteration 3335/3560 Training loss: 1.7134 0.7574 sec/batch\n",
      "Epoch 19/20  Iteration 3336/3560 Training loss: 1.7132 0.6662 sec/batch\n",
      "Epoch 19/20  Iteration 3337/3560 Training loss: 1.7133 0.6525 sec/batch\n",
      "Epoch 19/20  Iteration 3338/3560 Training loss: 1.7133 0.6912 sec/batch\n",
      "Epoch 19/20  Iteration 3339/3560 Training loss: 1.7133 0.7245 sec/batch\n",
      "Epoch 19/20  Iteration 3340/3560 Training loss: 1.7134 0.6928 sec/batch\n",
      "Epoch 19/20  Iteration 3341/3560 Training loss: 1.7136 0.7742 sec/batch\n",
      "Epoch 19/20  Iteration 3342/3560 Training loss: 1.7137 0.7444 sec/batch\n",
      "Epoch 19/20  Iteration 3343/3560 Training loss: 1.7138 0.6459 sec/batch\n",
      "Epoch 19/20  Iteration 3344/3560 Training loss: 1.7137 0.7718 sec/batch\n",
      "Epoch 19/20  Iteration 3345/3560 Training loss: 1.7140 0.6340 sec/batch\n",
      "Epoch 19/20  Iteration 3346/3560 Training loss: 1.7139 0.6377 sec/batch\n",
      "Epoch 19/20  Iteration 3347/3560 Training loss: 1.7139 0.7481 sec/batch\n",
      "Epoch 19/20  Iteration 3348/3560 Training loss: 1.7139 0.7753 sec/batch\n",
      "Epoch 19/20  Iteration 3349/3560 Training loss: 1.7139 0.7134 sec/batch\n",
      "Epoch 19/20  Iteration 3350/3560 Training loss: 1.7140 0.6543 sec/batch\n",
      "Epoch 19/20  Iteration 3351/3560 Training loss: 1.7141 0.7936 sec/batch\n",
      "Epoch 19/20  Iteration 3352/3560 Training loss: 1.7143 0.7403 sec/batch\n",
      "Epoch 19/20  Iteration 3353/3560 Training loss: 1.7143 0.7549 sec/batch\n",
      "Epoch 19/20  Iteration 3354/3560 Training loss: 1.7142 0.6387 sec/batch\n",
      "Epoch 19/20  Iteration 3355/3560 Training loss: 1.7140 0.6502 sec/batch\n",
      "Epoch 19/20  Iteration 3356/3560 Training loss: 1.7141 0.6089 sec/batch\n",
      "Epoch 19/20  Iteration 3357/3560 Training loss: 1.7141 0.6009 sec/batch\n",
      "Epoch 19/20  Iteration 3358/3560 Training loss: 1.7141 0.6386 sec/batch\n",
      "Epoch 19/20  Iteration 3359/3560 Training loss: 1.7142 0.6245 sec/batch\n",
      "Epoch 19/20  Iteration 3360/3560 Training loss: 1.7142 0.6841 sec/batch\n",
      "Epoch 19/20  Iteration 3361/3560 Training loss: 1.7143 0.6299 sec/batch\n",
      "Epoch 19/20  Iteration 3362/3560 Training loss: 1.7142 0.6126 sec/batch\n",
      "Epoch 19/20  Iteration 3363/3560 Training loss: 1.7140 0.7397 sec/batch\n",
      "Epoch 19/20  Iteration 3364/3560 Training loss: 1.7140 0.5797 sec/batch\n",
      "Epoch 19/20  Iteration 3365/3560 Training loss: 1.7143 0.7559 sec/batch\n",
      "Epoch 19/20  Iteration 3366/3560 Training loss: 1.7142 0.6895 sec/batch\n",
      "Epoch 19/20  Iteration 3367/3560 Training loss: 1.7142 0.6344 sec/batch\n",
      "Epoch 19/20  Iteration 3368/3560 Training loss: 1.7142 0.6202 sec/batch\n",
      "Epoch 19/20  Iteration 3369/3560 Training loss: 1.7142 0.5855 sec/batch\n",
      "Epoch 19/20  Iteration 3370/3560 Training loss: 1.7142 0.6571 sec/batch\n",
      "Epoch 19/20  Iteration 3371/3560 Training loss: 1.7143 0.6378 sec/batch\n",
      "Epoch 19/20  Iteration 3372/3560 Training loss: 1.7146 0.6083 sec/batch\n",
      "Epoch 19/20  Iteration 3373/3560 Training loss: 1.7146 0.7067 sec/batch\n",
      "Epoch 19/20  Iteration 3374/3560 Training loss: 1.7145 0.5976 sec/batch\n",
      "Epoch 19/20  Iteration 3375/3560 Training loss: 1.7144 0.6805 sec/batch\n",
      "Epoch 19/20  Iteration 3376/3560 Training loss: 1.7142 0.7217 sec/batch\n",
      "Epoch 19/20  Iteration 3377/3560 Training loss: 1.7143 0.6807 sec/batch\n",
      "Epoch 19/20  Iteration 3378/3560 Training loss: 1.7144 0.6398 sec/batch\n",
      "Epoch 19/20  Iteration 3379/3560 Training loss: 1.7144 0.6551 sec/batch\n",
      "Epoch 19/20  Iteration 3380/3560 Training loss: 1.7143 0.6115 sec/batch\n",
      "Epoch 19/20  Iteration 3381/3560 Training loss: 1.7142 0.6625 sec/batch\n",
      "Epoch 19/20  Iteration 3382/3560 Training loss: 1.7143 0.6282 sec/batch\n",
      "Epoch 20/20  Iteration 3383/3560 Training loss: 1.7808 0.7456 sec/batch\n",
      "Epoch 20/20  Iteration 3384/3560 Training loss: 1.7534 0.6155 sec/batch\n",
      "Epoch 20/20  Iteration 3385/3560 Training loss: 1.7354 0.6632 sec/batch\n",
      "Epoch 20/20  Iteration 3386/3560 Training loss: 1.7301 0.6884 sec/batch\n",
      "Epoch 20/20  Iteration 3387/3560 Training loss: 1.7249 0.6324 sec/batch\n",
      "Epoch 20/20  Iteration 3388/3560 Training loss: 1.7155 0.7453 sec/batch\n",
      "Epoch 20/20  Iteration 3389/3560 Training loss: 1.7141 0.7298 sec/batch\n",
      "Epoch 20/20  Iteration 3390/3560 Training loss: 1.7121 0.7539 sec/batch\n",
      "Epoch 20/20  Iteration 3391/3560 Training loss: 1.7145 0.5944 sec/batch\n",
      "Epoch 20/20  Iteration 3392/3560 Training loss: 1.7137 0.6122 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3393/3560 Training loss: 1.7114 0.7037 sec/batch\n",
      "Epoch 20/20  Iteration 3394/3560 Training loss: 1.7093 0.7485 sec/batch\n",
      "Epoch 20/20  Iteration 3395/3560 Training loss: 1.7083 0.6477 sec/batch\n",
      "Epoch 20/20  Iteration 3396/3560 Training loss: 1.7120 0.6324 sec/batch\n",
      "Epoch 20/20  Iteration 3397/3560 Training loss: 1.7116 0.6703 sec/batch\n",
      "Epoch 20/20  Iteration 3398/3560 Training loss: 1.7099 0.6630 sec/batch\n",
      "Epoch 20/20  Iteration 3399/3560 Training loss: 1.7098 0.6538 sec/batch\n",
      "Epoch 20/20  Iteration 3400/3560 Training loss: 1.7119 0.6408 sec/batch\n",
      "Epoch 20/20  Iteration 3401/3560 Training loss: 1.7117 0.6866 sec/batch\n",
      "Epoch 20/20  Iteration 3402/3560 Training loss: 1.7123 0.6082 sec/batch\n",
      "Epoch 20/20  Iteration 3403/3560 Training loss: 1.7116 0.7028 sec/batch\n",
      "Epoch 20/20  Iteration 3404/3560 Training loss: 1.7118 0.7686 sec/batch\n",
      "Epoch 20/20  Iteration 3405/3560 Training loss: 1.7114 0.6578 sec/batch\n",
      "Epoch 20/20  Iteration 3406/3560 Training loss: 1.7108 0.5965 sec/batch\n",
      "Epoch 20/20  Iteration 3407/3560 Training loss: 1.7106 0.6523 sec/batch\n",
      "Epoch 20/20  Iteration 3408/3560 Training loss: 1.7095 0.5918 sec/batch\n",
      "Epoch 20/20  Iteration 3409/3560 Training loss: 1.7086 0.6776 sec/batch\n",
      "Epoch 20/20  Iteration 3410/3560 Training loss: 1.7091 0.6335 sec/batch\n",
      "Epoch 20/20  Iteration 3411/3560 Training loss: 1.7100 0.7040 sec/batch\n",
      "Epoch 20/20  Iteration 3412/3560 Training loss: 1.7107 0.6629 sec/batch\n",
      "Epoch 20/20  Iteration 3413/3560 Training loss: 1.7103 0.6042 sec/batch\n",
      "Epoch 20/20  Iteration 3414/3560 Training loss: 1.7095 0.7478 sec/batch\n",
      "Epoch 20/20  Iteration 3415/3560 Training loss: 1.7099 0.6055 sec/batch\n",
      "Epoch 20/20  Iteration 3416/3560 Training loss: 1.7105 0.7029 sec/batch\n",
      "Epoch 20/20  Iteration 3417/3560 Training loss: 1.7099 0.6310 sec/batch\n",
      "Epoch 20/20  Iteration 3418/3560 Training loss: 1.7098 0.6765 sec/batch\n",
      "Epoch 20/20  Iteration 3419/3560 Training loss: 1.7093 0.6979 sec/batch\n",
      "Epoch 20/20  Iteration 3420/3560 Training loss: 1.7085 0.6274 sec/batch\n",
      "Epoch 20/20  Iteration 3421/3560 Training loss: 1.7070 0.6139 sec/batch\n",
      "Epoch 20/20  Iteration 3422/3560 Training loss: 1.7062 0.6580 sec/batch\n",
      "Epoch 20/20  Iteration 3423/3560 Training loss: 1.7058 0.7222 sec/batch\n",
      "Epoch 20/20  Iteration 3424/3560 Training loss: 1.7064 0.6453 sec/batch\n",
      "Epoch 20/20  Iteration 3425/3560 Training loss: 1.7059 0.6935 sec/batch\n",
      "Epoch 20/20  Iteration 3426/3560 Training loss: 1.7050 0.6742 sec/batch\n",
      "Epoch 20/20  Iteration 3427/3560 Training loss: 1.7054 0.6972 sec/batch\n",
      "Epoch 20/20  Iteration 3428/3560 Training loss: 1.7046 0.6659 sec/batch\n",
      "Epoch 20/20  Iteration 3429/3560 Training loss: 1.7044 0.6413 sec/batch\n",
      "Epoch 20/20  Iteration 3430/3560 Training loss: 1.7040 0.6549 sec/batch\n",
      "Epoch 20/20  Iteration 3431/3560 Training loss: 1.7035 0.6072 sec/batch\n",
      "Epoch 20/20  Iteration 3432/3560 Training loss: 1.7043 0.5778 sec/batch\n",
      "Epoch 20/20  Iteration 3433/3560 Training loss: 1.7039 0.5912 sec/batch\n",
      "Epoch 20/20  Iteration 3434/3560 Training loss: 1.7049 0.6764 sec/batch\n",
      "Epoch 20/20  Iteration 3435/3560 Training loss: 1.7050 0.6609 sec/batch\n",
      "Epoch 20/20  Iteration 3436/3560 Training loss: 1.7051 0.6874 sec/batch\n",
      "Epoch 20/20  Iteration 3437/3560 Training loss: 1.7047 0.6061 sec/batch\n",
      "Epoch 20/20  Iteration 3438/3560 Training loss: 1.7048 0.6986 sec/batch\n",
      "Epoch 20/20  Iteration 3439/3560 Training loss: 1.7051 0.7158 sec/batch\n",
      "Epoch 20/20  Iteration 3440/3560 Training loss: 1.7050 0.6858 sec/batch\n",
      "Epoch 20/20  Iteration 3441/3560 Training loss: 1.7045 0.6809 sec/batch\n",
      "Epoch 20/20  Iteration 3442/3560 Training loss: 1.7049 0.6463 sec/batch\n",
      "Epoch 20/20  Iteration 3443/3560 Training loss: 1.7050 0.6116 sec/batch\n",
      "Epoch 20/20  Iteration 3444/3560 Training loss: 1.7059 0.6550 sec/batch\n",
      "Epoch 20/20  Iteration 3445/3560 Training loss: 1.7064 0.6116 sec/batch\n",
      "Epoch 20/20  Iteration 3446/3560 Training loss: 1.7067 0.6356 sec/batch\n",
      "Epoch 20/20  Iteration 3447/3560 Training loss: 1.7068 0.6209 sec/batch\n",
      "Epoch 20/20  Iteration 3448/3560 Training loss: 1.7070 0.6479 sec/batch\n",
      "Epoch 20/20  Iteration 3449/3560 Training loss: 1.7073 0.7078 sec/batch\n",
      "Epoch 20/20  Iteration 3450/3560 Training loss: 1.7069 0.6125 sec/batch\n",
      "Epoch 20/20  Iteration 3451/3560 Training loss: 1.7067 0.6895 sec/batch\n",
      "Epoch 20/20  Iteration 3452/3560 Training loss: 1.7066 0.6566 sec/batch\n",
      "Epoch 20/20  Iteration 3453/3560 Training loss: 1.7070 0.6730 sec/batch\n",
      "Epoch 20/20  Iteration 3454/3560 Training loss: 1.7073 0.6269 sec/batch\n",
      "Epoch 20/20  Iteration 3455/3560 Training loss: 1.7078 0.6701 sec/batch\n",
      "Epoch 20/20  Iteration 3456/3560 Training loss: 1.7077 0.7110 sec/batch\n",
      "Epoch 20/20  Iteration 3457/3560 Training loss: 1.7076 0.5347 sec/batch\n",
      "Epoch 20/20  Iteration 3458/3560 Training loss: 1.7078 0.6238 sec/batch\n",
      "Epoch 20/20  Iteration 3459/3560 Training loss: 1.7077 0.5957 sec/batch\n",
      "Epoch 20/20  Iteration 3460/3560 Training loss: 1.7078 0.5818 sec/batch\n",
      "Epoch 20/20  Iteration 3461/3560 Training loss: 1.7073 0.6565 sec/batch\n",
      "Epoch 20/20  Iteration 3462/3560 Training loss: 1.7072 0.7495 sec/batch\n",
      "Epoch 20/20  Iteration 3463/3560 Training loss: 1.7067 0.6009 sec/batch\n",
      "Epoch 20/20  Iteration 3464/3560 Training loss: 1.7068 0.6644 sec/batch\n",
      "Epoch 20/20  Iteration 3465/3560 Training loss: 1.7064 0.6590 sec/batch\n",
      "Epoch 20/20  Iteration 3466/3560 Training loss: 1.7063 0.6320 sec/batch\n",
      "Epoch 20/20  Iteration 3467/3560 Training loss: 1.7059 0.5757 sec/batch\n",
      "Epoch 20/20  Iteration 3468/3560 Training loss: 1.7056 0.6490 sec/batch\n",
      "Epoch 20/20  Iteration 3469/3560 Training loss: 1.7054 0.6662 sec/batch\n",
      "Epoch 20/20  Iteration 3470/3560 Training loss: 1.7052 0.5782 sec/batch\n",
      "Epoch 20/20  Iteration 3471/3560 Training loss: 1.7048 0.5833 sec/batch\n",
      "Epoch 20/20  Iteration 3472/3560 Training loss: 1.7049 0.7151 sec/batch\n",
      "Epoch 20/20  Iteration 3473/3560 Training loss: 1.7046 0.6668 sec/batch\n",
      "Epoch 20/20  Iteration 3474/3560 Training loss: 1.7043 0.7132 sec/batch\n",
      "Epoch 20/20  Iteration 3475/3560 Training loss: 1.7040 0.6101 sec/batch\n",
      "Epoch 20/20  Iteration 3476/3560 Training loss: 1.7037 0.6756 sec/batch\n",
      "Epoch 20/20  Iteration 3477/3560 Training loss: 1.7033 0.6677 sec/batch\n",
      "Epoch 20/20  Iteration 3478/3560 Training loss: 1.7034 0.6752 sec/batch\n",
      "Epoch 20/20  Iteration 3479/3560 Training loss: 1.7033 0.6206 sec/batch\n",
      "Epoch 20/20  Iteration 3480/3560 Training loss: 1.7030 0.6622 sec/batch\n",
      "Epoch 20/20  Iteration 3481/3560 Training loss: 1.7026 0.6570 sec/batch\n",
      "Epoch 20/20  Iteration 3482/3560 Training loss: 1.7022 0.6749 sec/batch\n",
      "Epoch 20/20  Iteration 3483/3560 Training loss: 1.7023 0.6380 sec/batch\n",
      "Epoch 20/20  Iteration 3484/3560 Training loss: 1.7021 0.6471 sec/batch\n",
      "Epoch 20/20  Iteration 3485/3560 Training loss: 1.7019 0.7294 sec/batch\n",
      "Epoch 20/20  Iteration 3486/3560 Training loss: 1.7017 0.6188 sec/batch\n",
      "Epoch 20/20  Iteration 3487/3560 Training loss: 1.7015 0.6618 sec/batch\n",
      "Epoch 20/20  Iteration 3488/3560 Training loss: 1.7014 0.6842 sec/batch\n",
      "Epoch 20/20  Iteration 3489/3560 Training loss: 1.7013 0.7537 sec/batch\n",
      "Epoch 20/20  Iteration 3490/3560 Training loss: 1.7012 0.6530 sec/batch\n",
      "Epoch 20/20  Iteration 3491/3560 Training loss: 1.7012 0.6348 sec/batch\n",
      "Epoch 20/20  Iteration 3492/3560 Training loss: 1.7013 0.6158 sec/batch\n",
      "Epoch 20/20  Iteration 3493/3560 Training loss: 1.7012 0.6567 sec/batch\n",
      "Epoch 20/20  Iteration 3494/3560 Training loss: 1.7010 0.6739 sec/batch\n",
      "Epoch 20/20  Iteration 3495/3560 Training loss: 1.7009 0.6257 sec/batch\n",
      "Epoch 20/20  Iteration 3496/3560 Training loss: 1.7009 0.5781 sec/batch\n",
      "Epoch 20/20  Iteration 3497/3560 Training loss: 1.7008 0.6128 sec/batch\n",
      "Epoch 20/20  Iteration 3498/3560 Training loss: 1.7004 0.6889 sec/batch\n",
      "Epoch 20/20  Iteration 3499/3560 Training loss: 1.7005 0.6156 sec/batch\n",
      "Epoch 20/20  Iteration 3500/3560 Training loss: 1.7005 0.6700 sec/batch\n",
      "Epoch 20/20  Iteration 3501/3560 Training loss: 1.7005 0.7452 sec/batch\n",
      "Epoch 20/20  Iteration 3502/3560 Training loss: 1.7005 0.6884 sec/batch\n",
      "Epoch 20/20  Iteration 3503/3560 Training loss: 1.7005 0.7374 sec/batch\n",
      "Epoch 20/20  Iteration 3504/3560 Training loss: 1.7003 0.6250 sec/batch\n",
      "Epoch 20/20  Iteration 3505/3560 Training loss: 1.7000 0.6637 sec/batch\n",
      "Epoch 20/20  Iteration 3506/3560 Training loss: 1.7001 0.6235 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3507/3560 Training loss: 1.7001 0.6841 sec/batch\n",
      "Epoch 20/20  Iteration 3508/3560 Training loss: 1.6997 0.7984 sec/batch\n",
      "Epoch 20/20  Iteration 3509/3560 Training loss: 1.6999 0.7446 sec/batch\n",
      "Epoch 20/20  Iteration 3510/3560 Training loss: 1.7000 0.7577 sec/batch\n",
      "Epoch 20/20  Iteration 3511/3560 Training loss: 1.6999 0.9239 sec/batch\n",
      "Epoch 20/20  Iteration 3512/3560 Training loss: 1.6998 0.7362 sec/batch\n",
      "Epoch 20/20  Iteration 3513/3560 Training loss: 1.6996 0.7245 sec/batch\n",
      "Epoch 20/20  Iteration 3514/3560 Training loss: 1.6993 0.7982 sec/batch\n",
      "Epoch 20/20  Iteration 3515/3560 Training loss: 1.6994 0.7671 sec/batch\n",
      "Epoch 20/20  Iteration 3516/3560 Training loss: 1.6996 0.6814 sec/batch\n",
      "Epoch 20/20  Iteration 3517/3560 Training loss: 1.6996 0.6870 sec/batch\n",
      "Epoch 20/20  Iteration 3518/3560 Training loss: 1.6997 0.6768 sec/batch\n",
      "Epoch 20/20  Iteration 3519/3560 Training loss: 1.6998 0.6558 sec/batch\n",
      "Epoch 20/20  Iteration 3520/3560 Training loss: 1.6999 0.5930 sec/batch\n",
      "Epoch 20/20  Iteration 3521/3560 Training loss: 1.7001 0.6491 sec/batch\n",
      "Epoch 20/20  Iteration 3522/3560 Training loss: 1.7000 0.5901 sec/batch\n",
      "Epoch 20/20  Iteration 3523/3560 Training loss: 1.7003 0.7298 sec/batch\n",
      "Epoch 20/20  Iteration 3524/3560 Training loss: 1.7002 0.6597 sec/batch\n",
      "Epoch 20/20  Iteration 3525/3560 Training loss: 1.7002 0.6241 sec/batch\n",
      "Epoch 20/20  Iteration 3526/3560 Training loss: 1.7003 0.6855 sec/batch\n",
      "Epoch 20/20  Iteration 3527/3560 Training loss: 1.7002 0.5726 sec/batch\n",
      "Epoch 20/20  Iteration 3528/3560 Training loss: 1.7004 0.7187 sec/batch\n",
      "Epoch 20/20  Iteration 3529/3560 Training loss: 1.7005 0.5737 sec/batch\n",
      "Epoch 20/20  Iteration 3530/3560 Training loss: 1.7007 0.7965 sec/batch\n",
      "Epoch 20/20  Iteration 3531/3560 Training loss: 1.7007 0.5478 sec/batch\n",
      "Epoch 20/20  Iteration 3532/3560 Training loss: 1.7006 0.6446 sec/batch\n",
      "Epoch 20/20  Iteration 3533/3560 Training loss: 1.7004 0.6502 sec/batch\n",
      "Epoch 20/20  Iteration 3534/3560 Training loss: 1.7005 0.6750 sec/batch\n",
      "Epoch 20/20  Iteration 3535/3560 Training loss: 1.7006 0.5806 sec/batch\n",
      "Epoch 20/20  Iteration 3536/3560 Training loss: 1.7005 0.6934 sec/batch\n",
      "Epoch 20/20  Iteration 3537/3560 Training loss: 1.7006 0.6102 sec/batch\n",
      "Epoch 20/20  Iteration 3538/3560 Training loss: 1.7006 0.6502 sec/batch\n",
      "Epoch 20/20  Iteration 3539/3560 Training loss: 1.7007 0.6349 sec/batch\n",
      "Epoch 20/20  Iteration 3540/3560 Training loss: 1.7007 0.5938 sec/batch\n",
      "Epoch 20/20  Iteration 3541/3560 Training loss: 1.7004 0.6823 sec/batch\n",
      "Epoch 20/20  Iteration 3542/3560 Training loss: 1.7006 0.6444 sec/batch\n",
      "Epoch 20/20  Iteration 3543/3560 Training loss: 1.7008 0.7095 sec/batch\n",
      "Epoch 20/20  Iteration 3544/3560 Training loss: 1.7008 0.5249 sec/batch\n",
      "Epoch 20/20  Iteration 3545/3560 Training loss: 1.7009 0.6385 sec/batch\n",
      "Epoch 20/20  Iteration 3546/3560 Training loss: 1.7009 0.6519 sec/batch\n",
      "Epoch 20/20  Iteration 3547/3560 Training loss: 1.7009 0.6056 sec/batch\n",
      "Epoch 20/20  Iteration 3548/3560 Training loss: 1.7009 0.5840 sec/batch\n",
      "Epoch 20/20  Iteration 3549/3560 Training loss: 1.7010 0.7589 sec/batch\n",
      "Epoch 20/20  Iteration 3550/3560 Training loss: 1.7014 0.7502 sec/batch\n",
      "Epoch 20/20  Iteration 3551/3560 Training loss: 1.7014 0.6510 sec/batch\n",
      "Epoch 20/20  Iteration 3552/3560 Training loss: 1.7014 0.6105 sec/batch\n",
      "Epoch 20/20  Iteration 3553/3560 Training loss: 1.7013 0.6532 sec/batch\n",
      "Epoch 20/20  Iteration 3554/3560 Training loss: 1.7012 0.7057 sec/batch\n",
      "Epoch 20/20  Iteration 3555/3560 Training loss: 1.7013 0.6426 sec/batch\n",
      "Epoch 20/20  Iteration 3556/3560 Training loss: 1.7013 0.6967 sec/batch\n",
      "Epoch 20/20  Iteration 3557/3560 Training loss: 1.7014 0.5629 sec/batch\n",
      "Epoch 20/20  Iteration 3558/3560 Training loss: 1.7012 0.6192 sec/batch\n",
      "Epoch 20/20  Iteration 3559/3560 Training loss: 1.7011 0.6278 sec/batch\n",
      "Epoch 20/20  Iteration 3560/3560 Training loss: 1.7012 0.6459 sec/batch\n",
      "Epoch 1/20  Iteration 1/3560 Training loss: 4.4186 0.6192 sec/batch\n",
      "Epoch 1/20  Iteration 2/3560 Training loss: 4.4104 0.6186 sec/batch\n",
      "Epoch 1/20  Iteration 3/3560 Training loss: 4.4001 0.6105 sec/batch\n",
      "Epoch 1/20  Iteration 4/3560 Training loss: 4.3846 0.6467 sec/batch\n",
      "Epoch 1/20  Iteration 5/3560 Training loss: 4.3591 0.5854 sec/batch\n",
      "Epoch 1/20  Iteration 6/3560 Training loss: 4.3151 0.6196 sec/batch\n",
      "Epoch 1/20  Iteration 7/3560 Training loss: 4.2550 0.6610 sec/batch\n",
      "Epoch 1/20  Iteration 8/3560 Training loss: 4.1918 0.6495 sec/batch\n",
      "Epoch 1/20  Iteration 9/3560 Training loss: 4.1355 0.6311 sec/batch\n",
      "Epoch 1/20  Iteration 10/3560 Training loss: 4.0853 0.7164 sec/batch\n",
      "Epoch 1/20  Iteration 11/3560 Training loss: 4.0379 0.6042 sec/batch\n",
      "Epoch 1/20  Iteration 12/3560 Training loss: 3.9950 0.6319 sec/batch\n",
      "Epoch 1/20  Iteration 13/3560 Training loss: 3.9556 0.7158 sec/batch\n",
      "Epoch 1/20  Iteration 14/3560 Training loss: 3.9208 0.6424 sec/batch\n",
      "Epoch 1/20  Iteration 15/3560 Training loss: 3.8884 0.6046 sec/batch\n",
      "Epoch 1/20  Iteration 16/3560 Training loss: 3.8582 0.6371 sec/batch\n",
      "Epoch 1/20  Iteration 17/3560 Training loss: 3.8302 0.6193 sec/batch\n",
      "Epoch 1/20  Iteration 18/3560 Training loss: 3.8064 0.6166 sec/batch\n",
      "Epoch 1/20  Iteration 19/3560 Training loss: 3.7830 0.6646 sec/batch\n",
      "Epoch 1/20  Iteration 20/3560 Training loss: 3.7595 0.6277 sec/batch\n",
      "Epoch 1/20  Iteration 21/3560 Training loss: 3.7391 0.6104 sec/batch\n",
      "Epoch 1/20  Iteration 22/3560 Training loss: 3.7201 0.6225 sec/batch\n",
      "Epoch 1/20  Iteration 23/3560 Training loss: 3.7022 0.6088 sec/batch\n",
      "Epoch 1/20  Iteration 24/3560 Training loss: 3.6860 0.7224 sec/batch\n",
      "Epoch 1/20  Iteration 25/3560 Training loss: 3.6700 0.6395 sec/batch\n",
      "Epoch 1/20  Iteration 26/3560 Training loss: 3.6558 0.7156 sec/batch\n",
      "Epoch 1/20  Iteration 27/3560 Training loss: 3.6424 0.5708 sec/batch\n",
      "Epoch 1/20  Iteration 28/3560 Training loss: 3.6289 0.6106 sec/batch\n",
      "Epoch 1/20  Iteration 29/3560 Training loss: 3.6167 0.6923 sec/batch\n",
      "Epoch 1/20  Iteration 30/3560 Training loss: 3.6051 0.6060 sec/batch\n",
      "Epoch 1/20  Iteration 31/3560 Training loss: 3.5949 0.6295 sec/batch\n",
      "Epoch 1/20  Iteration 32/3560 Training loss: 3.5845 0.6142 sec/batch\n",
      "Epoch 1/20  Iteration 33/3560 Training loss: 3.5740 0.6665 sec/batch\n",
      "Epoch 1/20  Iteration 34/3560 Training loss: 3.5649 0.6937 sec/batch\n",
      "Epoch 1/20  Iteration 35/3560 Training loss: 3.5555 0.6199 sec/batch\n",
      "Epoch 1/20  Iteration 36/3560 Training loss: 3.5471 0.6824 sec/batch\n",
      "Epoch 1/20  Iteration 37/3560 Training loss: 3.5383 0.6474 sec/batch\n",
      "Epoch 1/20  Iteration 38/3560 Training loss: 3.5300 0.6248 sec/batch\n",
      "Epoch 1/20  Iteration 39/3560 Training loss: 3.5220 0.6350 sec/batch\n",
      "Epoch 1/20  Iteration 40/3560 Training loss: 3.5144 0.6480 sec/batch\n",
      "Epoch 1/20  Iteration 41/3560 Training loss: 3.5072 0.5779 sec/batch\n",
      "Epoch 1/20  Iteration 42/3560 Training loss: 3.5005 0.6189 sec/batch\n",
      "Epoch 1/20  Iteration 43/3560 Training loss: 3.4935 0.6669 sec/batch\n",
      "Epoch 1/20  Iteration 44/3560 Training loss: 3.4869 0.6326 sec/batch\n",
      "Epoch 1/20  Iteration 45/3560 Training loss: 3.4807 0.6694 sec/batch\n",
      "Epoch 1/20  Iteration 46/3560 Training loss: 3.4749 0.6612 sec/batch\n",
      "Epoch 1/20  Iteration 47/3560 Training loss: 3.4695 0.6744 sec/batch\n",
      "Epoch 1/20  Iteration 48/3560 Training loss: 3.4644 0.6735 sec/batch\n",
      "Epoch 1/20  Iteration 49/3560 Training loss: 3.4594 0.5981 sec/batch\n",
      "Epoch 1/20  Iteration 50/3560 Training loss: 3.4544 0.5788 sec/batch\n",
      "Epoch 1/20  Iteration 51/3560 Training loss: 3.4495 0.6555 sec/batch\n",
      "Epoch 1/20  Iteration 52/3560 Training loss: 3.4446 0.7303 sec/batch\n",
      "Epoch 1/20  Iteration 53/3560 Training loss: 3.4402 0.6677 sec/batch\n",
      "Epoch 1/20  Iteration 54/3560 Training loss: 3.4355 0.6515 sec/batch\n",
      "Epoch 1/20  Iteration 55/3560 Training loss: 3.4311 0.5637 sec/batch\n",
      "Epoch 1/20  Iteration 56/3560 Training loss: 3.4267 0.6201 sec/batch\n",
      "Epoch 1/20  Iteration 57/3560 Training loss: 3.4225 0.6443 sec/batch\n",
      "Epoch 1/20  Iteration 58/3560 Training loss: 3.4187 0.6502 sec/batch\n",
      "Epoch 1/20  Iteration 59/3560 Training loss: 3.4145 0.6572 sec/batch\n",
      "Epoch 1/20  Iteration 60/3560 Training loss: 3.4108 0.6206 sec/batch\n",
      "Epoch 1/20  Iteration 61/3560 Training loss: 3.4071 0.6020 sec/batch\n",
      "Epoch 1/20  Iteration 62/3560 Training loss: 3.4037 0.7209 sec/batch\n",
      "Epoch 1/20  Iteration 63/3560 Training loss: 3.4007 0.7339 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 64/3560 Training loss: 3.3969 0.6997 sec/batch\n",
      "Epoch 1/20  Iteration 65/3560 Training loss: 3.3935 0.5957 sec/batch\n",
      "Epoch 1/20  Iteration 66/3560 Training loss: 3.3905 0.6489 sec/batch\n",
      "Epoch 1/20  Iteration 67/3560 Training loss: 3.3875 0.6766 sec/batch\n",
      "Epoch 1/20  Iteration 68/3560 Training loss: 3.3840 0.6040 sec/batch\n",
      "Epoch 1/20  Iteration 69/3560 Training loss: 3.3808 0.5967 sec/batch\n",
      "Epoch 1/20  Iteration 70/3560 Training loss: 3.3780 0.6080 sec/batch\n",
      "Epoch 1/20  Iteration 71/3560 Training loss: 3.3751 0.6335 sec/batch\n",
      "Epoch 1/20  Iteration 72/3560 Training loss: 3.3725 0.6350 sec/batch\n",
      "Epoch 1/20  Iteration 73/3560 Training loss: 3.3698 0.7925 sec/batch\n",
      "Epoch 1/20  Iteration 74/3560 Training loss: 3.3673 0.6369 sec/batch\n",
      "Epoch 1/20  Iteration 75/3560 Training loss: 3.3649 0.6633 sec/batch\n",
      "Epoch 1/20  Iteration 76/3560 Training loss: 3.3625 0.5947 sec/batch\n",
      "Epoch 1/20  Iteration 77/3560 Training loss: 3.3600 0.6543 sec/batch\n",
      "Epoch 1/20  Iteration 78/3560 Training loss: 3.3577 0.6504 sec/batch\n",
      "Epoch 1/20  Iteration 79/3560 Training loss: 3.3553 0.6179 sec/batch\n",
      "Epoch 1/20  Iteration 80/3560 Training loss: 3.3528 0.6514 sec/batch\n",
      "Epoch 1/20  Iteration 81/3560 Training loss: 3.3504 0.6155 sec/batch\n",
      "Epoch 1/20  Iteration 82/3560 Training loss: 3.3483 0.6324 sec/batch\n",
      "Epoch 1/20  Iteration 83/3560 Training loss: 3.3462 0.6087 sec/batch\n",
      "Epoch 1/20  Iteration 84/3560 Training loss: 3.3441 0.6432 sec/batch\n",
      "Epoch 1/20  Iteration 85/3560 Training loss: 3.3419 0.6511 sec/batch\n",
      "Epoch 1/20  Iteration 86/3560 Training loss: 3.3399 0.6612 sec/batch\n",
      "Epoch 1/20  Iteration 87/3560 Training loss: 3.3378 0.6945 sec/batch\n",
      "Epoch 1/20  Iteration 88/3560 Training loss: 3.3358 0.6279 sec/batch\n",
      "Epoch 1/20  Iteration 89/3560 Training loss: 3.3340 0.6005 sec/batch\n",
      "Epoch 1/20  Iteration 90/3560 Training loss: 3.3322 0.7059 sec/batch\n",
      "Epoch 1/20  Iteration 91/3560 Training loss: 3.3304 0.5492 sec/batch\n",
      "Epoch 1/20  Iteration 92/3560 Training loss: 3.3285 0.6489 sec/batch\n",
      "Epoch 1/20  Iteration 93/3560 Training loss: 3.3267 0.5973 sec/batch\n",
      "Epoch 1/20  Iteration 94/3560 Training loss: 3.3251 0.6245 sec/batch\n",
      "Epoch 1/20  Iteration 95/3560 Training loss: 3.3233 0.5451 sec/batch\n",
      "Epoch 1/20  Iteration 96/3560 Training loss: 3.3216 0.6670 sec/batch\n",
      "Epoch 1/20  Iteration 97/3560 Training loss: 3.3199 0.7060 sec/batch\n",
      "Epoch 1/20  Iteration 98/3560 Training loss: 3.3183 0.5858 sec/batch\n",
      "Epoch 1/20  Iteration 99/3560 Training loss: 3.3167 0.6559 sec/batch\n",
      "Epoch 1/20  Iteration 100/3560 Training loss: 3.3151 0.6880 sec/batch\n",
      "Epoch 1/20  Iteration 101/3560 Training loss: 3.3136 0.6795 sec/batch\n",
      "Epoch 1/20  Iteration 102/3560 Training loss: 3.3122 0.6214 sec/batch\n",
      "Epoch 1/20  Iteration 103/3560 Training loss: 3.3107 0.5745 sec/batch\n",
      "Epoch 1/20  Iteration 104/3560 Training loss: 3.3092 0.6545 sec/batch\n",
      "Epoch 1/20  Iteration 105/3560 Training loss: 3.3078 0.5985 sec/batch\n",
      "Epoch 1/20  Iteration 106/3560 Training loss: 3.3064 0.6373 sec/batch\n",
      "Epoch 1/20  Iteration 107/3560 Training loss: 3.3048 0.6436 sec/batch\n",
      "Epoch 1/20  Iteration 108/3560 Training loss: 3.3033 0.6439 sec/batch\n",
      "Epoch 1/20  Iteration 109/3560 Training loss: 3.3020 0.6199 sec/batch\n",
      "Epoch 1/20  Iteration 110/3560 Training loss: 3.3004 0.5456 sec/batch\n",
      "Epoch 1/20  Iteration 111/3560 Training loss: 3.2991 0.7297 sec/batch\n",
      "Epoch 1/20  Iteration 112/3560 Training loss: 3.2978 0.5610 sec/batch\n",
      "Epoch 1/20  Iteration 113/3560 Training loss: 3.2964 0.7568 sec/batch\n",
      "Epoch 1/20  Iteration 114/3560 Training loss: 3.2950 0.5904 sec/batch\n",
      "Epoch 1/20  Iteration 115/3560 Training loss: 3.2936 0.6528 sec/batch\n",
      "Epoch 1/20  Iteration 116/3560 Training loss: 3.2922 0.5628 sec/batch\n",
      "Epoch 1/20  Iteration 117/3560 Training loss: 3.2909 0.6157 sec/batch\n",
      "Epoch 1/20  Iteration 118/3560 Training loss: 3.2898 0.5955 sec/batch\n",
      "Epoch 1/20  Iteration 119/3560 Training loss: 3.2886 0.5858 sec/batch\n",
      "Epoch 1/20  Iteration 120/3560 Training loss: 3.2873 0.6371 sec/batch\n",
      "Epoch 1/20  Iteration 121/3560 Training loss: 3.2863 0.5926 sec/batch\n",
      "Epoch 1/20  Iteration 122/3560 Training loss: 3.2852 0.7102 sec/batch\n",
      "Epoch 1/20  Iteration 123/3560 Training loss: 3.2841 0.6541 sec/batch\n",
      "Epoch 1/20  Iteration 124/3560 Training loss: 3.2831 0.5743 sec/batch\n",
      "Epoch 1/20  Iteration 125/3560 Training loss: 3.2819 0.6739 sec/batch\n",
      "Epoch 1/20  Iteration 126/3560 Training loss: 3.2806 0.6671 sec/batch\n",
      "Epoch 1/20  Iteration 127/3560 Training loss: 3.2795 0.6820 sec/batch\n",
      "Epoch 1/20  Iteration 128/3560 Training loss: 3.2785 0.6114 sec/batch\n",
      "Epoch 1/20  Iteration 129/3560 Training loss: 3.2773 0.5711 sec/batch\n",
      "Epoch 1/20  Iteration 130/3560 Training loss: 3.2762 0.6682 sec/batch\n",
      "Epoch 1/20  Iteration 131/3560 Training loss: 3.2753 0.6463 sec/batch\n",
      "Epoch 1/20  Iteration 132/3560 Training loss: 3.2742 0.5708 sec/batch\n",
      "Epoch 1/20  Iteration 133/3560 Training loss: 3.2732 0.6954 sec/batch\n",
      "Epoch 1/20  Iteration 134/3560 Training loss: 3.2722 0.5567 sec/batch\n",
      "Epoch 1/20  Iteration 135/3560 Training loss: 3.2709 0.7064 sec/batch\n",
      "Epoch 1/20  Iteration 136/3560 Training loss: 3.2698 0.6258 sec/batch\n",
      "Epoch 1/20  Iteration 137/3560 Training loss: 3.2687 0.7199 sec/batch\n",
      "Epoch 1/20  Iteration 138/3560 Training loss: 3.2676 0.6706 sec/batch\n",
      "Epoch 1/20  Iteration 139/3560 Training loss: 3.2667 0.6469 sec/batch\n",
      "Epoch 1/20  Iteration 140/3560 Training loss: 3.2657 0.7149 sec/batch\n",
      "Epoch 1/20  Iteration 141/3560 Training loss: 3.2647 0.6716 sec/batch\n",
      "Epoch 1/20  Iteration 142/3560 Training loss: 3.2635 0.5910 sec/batch\n",
      "Epoch 1/20  Iteration 143/3560 Training loss: 3.2625 0.6565 sec/batch\n",
      "Epoch 1/20  Iteration 144/3560 Training loss: 3.2614 0.6263 sec/batch\n",
      "Epoch 1/20  Iteration 145/3560 Training loss: 3.2604 0.6180 sec/batch\n",
      "Epoch 1/20  Iteration 146/3560 Training loss: 3.2595 0.5801 sec/batch\n",
      "Epoch 1/20  Iteration 147/3560 Training loss: 3.2586 0.6649 sec/batch\n",
      "Epoch 1/20  Iteration 148/3560 Training loss: 3.2579 0.6511 sec/batch\n",
      "Epoch 1/20  Iteration 149/3560 Training loss: 3.2568 0.6354 sec/batch\n",
      "Epoch 1/20  Iteration 150/3560 Training loss: 3.2559 0.6477 sec/batch\n",
      "Epoch 1/20  Iteration 151/3560 Training loss: 3.2551 0.6777 sec/batch\n",
      "Epoch 1/20  Iteration 152/3560 Training loss: 3.2543 0.6253 sec/batch\n",
      "Epoch 1/20  Iteration 153/3560 Training loss: 3.2534 0.7321 sec/batch\n",
      "Epoch 1/20  Iteration 154/3560 Training loss: 3.2525 0.6508 sec/batch\n",
      "Epoch 1/20  Iteration 155/3560 Training loss: 3.2515 0.5110 sec/batch\n",
      "Epoch 1/20  Iteration 156/3560 Training loss: 3.2506 0.6599 sec/batch\n",
      "Epoch 1/20  Iteration 157/3560 Training loss: 3.2496 0.6634 sec/batch\n",
      "Epoch 1/20  Iteration 158/3560 Training loss: 3.2486 0.6150 sec/batch\n",
      "Epoch 1/20  Iteration 159/3560 Training loss: 3.2475 0.5856 sec/batch\n",
      "Epoch 1/20  Iteration 160/3560 Training loss: 3.2465 0.6733 sec/batch\n",
      "Epoch 1/20  Iteration 161/3560 Training loss: 3.2456 0.6347 sec/batch\n",
      "Epoch 1/20  Iteration 162/3560 Training loss: 3.2445 0.6373 sec/batch\n",
      "Epoch 1/20  Iteration 163/3560 Training loss: 3.2434 0.5815 sec/batch\n",
      "Epoch 1/20  Iteration 164/3560 Training loss: 3.2425 0.5974 sec/batch\n",
      "Epoch 1/20  Iteration 165/3560 Training loss: 3.2415 0.7782 sec/batch\n",
      "Epoch 1/20  Iteration 166/3560 Training loss: 3.2404 0.4994 sec/batch\n",
      "Epoch 1/20  Iteration 167/3560 Training loss: 3.2394 0.6706 sec/batch\n",
      "Epoch 1/20  Iteration 168/3560 Training loss: 3.2385 0.5785 sec/batch\n",
      "Epoch 1/20  Iteration 169/3560 Training loss: 3.2375 0.6469 sec/batch\n",
      "Epoch 1/20  Iteration 170/3560 Training loss: 3.2364 0.6891 sec/batch\n",
      "Epoch 1/20  Iteration 171/3560 Training loss: 3.2354 0.6128 sec/batch\n",
      "Epoch 1/20  Iteration 172/3560 Training loss: 3.2346 0.6352 sec/batch\n",
      "Epoch 1/20  Iteration 173/3560 Training loss: 3.2339 0.7885 sec/batch\n",
      "Epoch 1/20  Iteration 174/3560 Training loss: 3.2331 0.5992 sec/batch\n",
      "Epoch 1/20  Iteration 175/3560 Training loss: 3.2322 0.6303 sec/batch\n",
      "Epoch 1/20  Iteration 176/3560 Training loss: 3.2313 0.5907 sec/batch\n",
      "Epoch 1/20  Iteration 177/3560 Training loss: 3.2303 0.6554 sec/batch\n",
      "Epoch 1/20  Iteration 178/3560 Training loss: 3.2291 0.5868 sec/batch\n",
      "Epoch 2/20  Iteration 179/3560 Training loss: 3.0840 0.6828 sec/batch\n",
      "Epoch 2/20  Iteration 180/3560 Training loss: 3.0504 0.6614 sec/batch\n",
      "Epoch 2/20  Iteration 181/3560 Training loss: 3.0382 0.5780 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 182/3560 Training loss: 3.0330 0.6000 sec/batch\n",
      "Epoch 2/20  Iteration 183/3560 Training loss: 3.0309 0.6555 sec/batch\n",
      "Epoch 2/20  Iteration 184/3560 Training loss: 3.0303 0.6399 sec/batch\n",
      "Epoch 2/20  Iteration 185/3560 Training loss: 3.0298 0.6870 sec/batch\n",
      "Epoch 2/20  Iteration 186/3560 Training loss: 3.0288 0.6962 sec/batch\n",
      "Epoch 2/20  Iteration 187/3560 Training loss: 3.0260 0.7146 sec/batch\n",
      "Epoch 2/20  Iteration 188/3560 Training loss: 3.0248 0.6040 sec/batch\n",
      "Epoch 2/20  Iteration 189/3560 Training loss: 3.0213 0.6638 sec/batch\n",
      "Epoch 2/20  Iteration 190/3560 Training loss: 3.0193 0.7337 sec/batch\n",
      "Epoch 2/20  Iteration 191/3560 Training loss: 3.0180 0.7859 sec/batch\n",
      "Epoch 2/20  Iteration 192/3560 Training loss: 3.0173 0.6992 sec/batch\n",
      "Epoch 2/20  Iteration 193/3560 Training loss: 3.0154 0.6595 sec/batch\n",
      "Epoch 2/20  Iteration 194/3560 Training loss: 3.0138 0.7524 sec/batch\n",
      "Epoch 2/20  Iteration 195/3560 Training loss: 3.0115 0.6592 sec/batch\n",
      "Epoch 2/20  Iteration 196/3560 Training loss: 3.0112 0.8125 sec/batch\n",
      "Epoch 2/20  Iteration 197/3560 Training loss: 3.0096 0.6886 sec/batch\n",
      "Epoch 2/20  Iteration 198/3560 Training loss: 3.0062 0.7619 sec/batch\n",
      "Epoch 2/20  Iteration 199/3560 Training loss: 3.0040 0.8796 sec/batch\n",
      "Epoch 2/20  Iteration 200/3560 Training loss: 3.0025 0.7333 sec/batch\n",
      "Epoch 2/20  Iteration 201/3560 Training loss: 3.0004 0.8247 sec/batch\n",
      "Epoch 2/20  Iteration 202/3560 Training loss: 2.9985 0.8810 sec/batch\n",
      "Epoch 2/20  Iteration 203/3560 Training loss: 2.9963 0.6940 sec/batch\n",
      "Epoch 2/20  Iteration 204/3560 Training loss: 2.9950 0.7987 sec/batch\n",
      "Epoch 2/20  Iteration 205/3560 Training loss: 2.9938 0.7268 sec/batch\n",
      "Epoch 2/20  Iteration 206/3560 Training loss: 2.9916 0.6608 sec/batch\n",
      "Epoch 2/20  Iteration 207/3560 Training loss: 2.9899 0.5978 sec/batch\n",
      "Epoch 2/20  Iteration 208/3560 Training loss: 2.9880 0.6441 sec/batch\n",
      "Epoch 2/20  Iteration 209/3560 Training loss: 2.9871 0.6108 sec/batch\n",
      "Epoch 2/20  Iteration 210/3560 Training loss: 2.9850 0.5787 sec/batch\n",
      "Epoch 2/20  Iteration 211/3560 Training loss: 2.9827 0.5974 sec/batch\n",
      "Epoch 2/20  Iteration 212/3560 Training loss: 2.9812 0.6773 sec/batch\n",
      "Epoch 2/20  Iteration 213/3560 Training loss: 2.9787 0.6788 sec/batch\n",
      "Epoch 2/20  Iteration 214/3560 Training loss: 2.9769 0.6350 sec/batch\n",
      "Epoch 2/20  Iteration 215/3560 Training loss: 2.9747 0.6648 sec/batch\n",
      "Epoch 2/20  Iteration 216/3560 Training loss: 2.9723 0.6974 sec/batch\n",
      "Epoch 2/20  Iteration 217/3560 Training loss: 2.9697 0.6398 sec/batch\n",
      "Epoch 2/20  Iteration 218/3560 Training loss: 2.9675 0.6542 sec/batch\n",
      "Epoch 2/20  Iteration 219/3560 Training loss: 2.9652 0.6242 sec/batch\n",
      "Epoch 2/20  Iteration 220/3560 Training loss: 2.9629 0.6459 sec/batch\n",
      "Epoch 2/20  Iteration 221/3560 Training loss: 2.9605 0.7052 sec/batch\n",
      "Epoch 2/20  Iteration 222/3560 Training loss: 2.9583 0.6839 sec/batch\n",
      "Epoch 2/20  Iteration 223/3560 Training loss: 2.9560 0.6269 sec/batch\n",
      "Epoch 2/20  Iteration 224/3560 Training loss: 2.9537 0.6536 sec/batch\n",
      "Epoch 2/20  Iteration 225/3560 Training loss: 2.9518 0.6709 sec/batch\n",
      "Epoch 2/20  Iteration 226/3560 Training loss: 2.9502 0.6658 sec/batch\n",
      "Epoch 2/20  Iteration 227/3560 Training loss: 2.9485 0.6173 sec/batch\n",
      "Epoch 2/20  Iteration 228/3560 Training loss: 2.9468 0.6641 sec/batch\n",
      "Epoch 2/20  Iteration 229/3560 Training loss: 2.9449 0.7681 sec/batch\n",
      "Epoch 2/20  Iteration 230/3560 Training loss: 2.9427 0.6994 sec/batch\n",
      "Epoch 2/20  Iteration 231/3560 Training loss: 2.9409 0.7268 sec/batch\n",
      "Epoch 2/20  Iteration 232/3560 Training loss: 2.9388 0.6647 sec/batch\n",
      "Epoch 2/20  Iteration 233/3560 Training loss: 2.9369 0.6476 sec/batch\n",
      "Epoch 2/20  Iteration 234/3560 Training loss: 2.9346 0.6028 sec/batch\n",
      "Epoch 2/20  Iteration 235/3560 Training loss: 2.9326 0.5206 sec/batch\n",
      "Epoch 2/20  Iteration 236/3560 Training loss: 2.9306 0.6963 sec/batch\n",
      "Epoch 2/20  Iteration 237/3560 Training loss: 2.9285 0.6467 sec/batch\n",
      "Epoch 2/20  Iteration 238/3560 Training loss: 2.9267 0.6206 sec/batch\n",
      "Epoch 2/20  Iteration 239/3560 Training loss: 2.9249 0.6340 sec/batch\n",
      "Epoch 2/20  Iteration 240/3560 Training loss: 2.9233 0.8490 sec/batch\n",
      "Epoch 2/20  Iteration 241/3560 Training loss: 2.9219 0.6546 sec/batch\n",
      "Epoch 2/20  Iteration 242/3560 Training loss: 2.9199 0.8302 sec/batch\n",
      "Epoch 2/20  Iteration 243/3560 Training loss: 2.9179 0.6916 sec/batch\n",
      "Epoch 2/20  Iteration 244/3560 Training loss: 2.9164 0.7909 sec/batch\n",
      "Epoch 2/20  Iteration 245/3560 Training loss: 2.9146 0.6249 sec/batch\n",
      "Epoch 2/20  Iteration 246/3560 Training loss: 2.9122 0.6936 sec/batch\n",
      "Epoch 2/20  Iteration 247/3560 Training loss: 2.9100 0.7231 sec/batch\n",
      "Epoch 2/20  Iteration 248/3560 Training loss: 2.9081 0.6448 sec/batch\n",
      "Epoch 2/20  Iteration 249/3560 Training loss: 2.9064 0.6701 sec/batch\n",
      "Epoch 2/20  Iteration 250/3560 Training loss: 2.9049 0.7177 sec/batch\n",
      "Epoch 2/20  Iteration 251/3560 Training loss: 2.9031 0.7942 sec/batch\n",
      "Epoch 2/20  Iteration 252/3560 Training loss: 2.9013 0.6755 sec/batch\n",
      "Epoch 2/20  Iteration 253/3560 Training loss: 2.8997 0.6870 sec/batch\n",
      "Epoch 2/20  Iteration 254/3560 Training loss: 2.8982 0.6228 sec/batch\n",
      "Epoch 2/20  Iteration 255/3560 Training loss: 2.8964 0.6571 sec/batch\n",
      "Epoch 2/20  Iteration 256/3560 Training loss: 2.8948 0.7712 sec/batch\n",
      "Epoch 2/20  Iteration 257/3560 Training loss: 2.8928 0.5788 sec/batch\n",
      "Epoch 2/20  Iteration 258/3560 Training loss: 2.8909 0.6040 sec/batch\n",
      "Epoch 2/20  Iteration 259/3560 Training loss: 2.8891 0.6612 sec/batch\n",
      "Epoch 2/20  Iteration 260/3560 Training loss: 2.8875 0.7118 sec/batch\n",
      "Epoch 2/20  Iteration 261/3560 Training loss: 2.8859 0.6555 sec/batch\n",
      "Epoch 2/20  Iteration 262/3560 Training loss: 2.8841 0.7143 sec/batch\n",
      "Epoch 2/20  Iteration 263/3560 Training loss: 2.8819 0.6225 sec/batch\n",
      "Epoch 2/20  Iteration 264/3560 Training loss: 2.8800 0.6657 sec/batch\n",
      "Epoch 2/20  Iteration 265/3560 Training loss: 2.8782 0.6163 sec/batch\n",
      "Epoch 2/20  Iteration 266/3560 Training loss: 2.8764 0.6319 sec/batch\n",
      "Epoch 2/20  Iteration 267/3560 Training loss: 2.8746 0.7159 sec/batch\n",
      "Epoch 2/20  Iteration 268/3560 Training loss: 2.8731 0.7252 sec/batch\n",
      "Epoch 2/20  Iteration 269/3560 Training loss: 2.8715 0.7156 sec/batch\n",
      "Epoch 2/20  Iteration 270/3560 Training loss: 2.8698 0.5958 sec/batch\n",
      "Epoch 2/20  Iteration 271/3560 Training loss: 2.8681 0.6323 sec/batch\n",
      "Epoch 2/20  Iteration 272/3560 Training loss: 2.8663 0.6782 sec/batch\n",
      "Epoch 2/20  Iteration 273/3560 Training loss: 2.8644 0.5950 sec/batch\n",
      "Epoch 2/20  Iteration 274/3560 Training loss: 2.8625 0.7000 sec/batch\n",
      "Epoch 2/20  Iteration 275/3560 Training loss: 2.8609 0.6422 sec/batch\n",
      "Epoch 2/20  Iteration 276/3560 Training loss: 2.8592 0.7017 sec/batch\n",
      "Epoch 2/20  Iteration 277/3560 Training loss: 2.8576 0.6402 sec/batch\n",
      "Epoch 2/20  Iteration 278/3560 Training loss: 2.8559 0.6352 sec/batch\n",
      "Epoch 2/20  Iteration 279/3560 Training loss: 2.8544 0.6483 sec/batch\n",
      "Epoch 2/20  Iteration 280/3560 Training loss: 2.8528 0.7091 sec/batch\n",
      "Epoch 2/20  Iteration 281/3560 Training loss: 2.8511 0.8236 sec/batch\n",
      "Epoch 2/20  Iteration 282/3560 Training loss: 2.8494 0.6460 sec/batch\n",
      "Epoch 2/20  Iteration 283/3560 Training loss: 2.8478 0.6182 sec/batch\n",
      "Epoch 2/20  Iteration 284/3560 Training loss: 2.8463 0.6384 sec/batch\n",
      "Epoch 2/20  Iteration 285/3560 Training loss: 2.8445 0.6565 sec/batch\n",
      "Epoch 2/20  Iteration 286/3560 Training loss: 2.8430 0.5574 sec/batch\n",
      "Epoch 2/20  Iteration 287/3560 Training loss: 2.8416 0.5981 sec/batch\n",
      "Epoch 2/20  Iteration 288/3560 Training loss: 2.8397 0.6923 sec/batch\n",
      "Epoch 2/20  Iteration 289/3560 Training loss: 2.8382 0.6933 sec/batch\n",
      "Epoch 2/20  Iteration 290/3560 Training loss: 2.8367 0.7157 sec/batch\n",
      "Epoch 2/20  Iteration 291/3560 Training loss: 2.8351 0.6406 sec/batch\n",
      "Epoch 2/20  Iteration 292/3560 Training loss: 2.8334 0.7010 sec/batch\n",
      "Epoch 2/20  Iteration 293/3560 Training loss: 2.8318 0.6608 sec/batch\n",
      "Epoch 2/20  Iteration 294/3560 Training loss: 2.8301 0.7367 sec/batch\n",
      "Epoch 2/20  Iteration 295/3560 Training loss: 2.8285 0.6661 sec/batch\n",
      "Epoch 2/20  Iteration 296/3560 Training loss: 2.8270 0.6163 sec/batch\n",
      "Epoch 2/20  Iteration 297/3560 Training loss: 2.8256 0.6238 sec/batch\n",
      "Epoch 2/20  Iteration 298/3560 Training loss: 2.8241 0.5944 sec/batch\n",
      "Epoch 2/20  Iteration 299/3560 Training loss: 2.8228 0.6250 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 300/3560 Training loss: 2.8214 0.5981 sec/batch\n",
      "Epoch 2/20  Iteration 301/3560 Training loss: 2.8199 0.7033 sec/batch\n",
      "Epoch 2/20  Iteration 302/3560 Training loss: 2.8186 0.7116 sec/batch\n",
      "Epoch 2/20  Iteration 303/3560 Training loss: 2.8171 0.6488 sec/batch\n",
      "Epoch 2/20  Iteration 304/3560 Training loss: 2.8155 0.6779 sec/batch\n",
      "Epoch 2/20  Iteration 305/3560 Training loss: 2.8142 0.7267 sec/batch\n",
      "Epoch 2/20  Iteration 306/3560 Training loss: 2.8129 0.6742 sec/batch\n",
      "Epoch 2/20  Iteration 307/3560 Training loss: 2.8115 0.8102 sec/batch\n",
      "Epoch 2/20  Iteration 308/3560 Training loss: 2.8102 0.6358 sec/batch\n",
      "Epoch 2/20  Iteration 309/3560 Training loss: 2.8089 0.6380 sec/batch\n",
      "Epoch 2/20  Iteration 310/3560 Training loss: 2.8075 0.6293 sec/batch\n",
      "Epoch 2/20  Iteration 311/3560 Training loss: 2.8063 0.6199 sec/batch\n",
      "Epoch 2/20  Iteration 312/3560 Training loss: 2.8050 0.6094 sec/batch\n",
      "Epoch 2/20  Iteration 313/3560 Training loss: 2.8035 0.6847 sec/batch\n",
      "Epoch 2/20  Iteration 314/3560 Training loss: 2.8021 0.6111 sec/batch\n",
      "Epoch 2/20  Iteration 315/3560 Training loss: 2.8007 0.6618 sec/batch\n",
      "Epoch 2/20  Iteration 316/3560 Training loss: 2.7994 0.5806 sec/batch\n",
      "Epoch 2/20  Iteration 317/3560 Training loss: 2.7983 0.6213 sec/batch\n",
      "Epoch 2/20  Iteration 318/3560 Training loss: 2.7969 0.6751 sec/batch\n",
      "Epoch 2/20  Iteration 319/3560 Training loss: 2.7957 0.6839 sec/batch\n",
      "Epoch 2/20  Iteration 320/3560 Training loss: 2.7943 0.7611 sec/batch\n",
      "Epoch 2/20  Iteration 321/3560 Training loss: 2.7930 0.6926 sec/batch\n",
      "Epoch 2/20  Iteration 322/3560 Training loss: 2.7917 0.5861 sec/batch\n",
      "Epoch 2/20  Iteration 323/3560 Training loss: 2.7904 0.7485 sec/batch\n",
      "Epoch 2/20  Iteration 324/3560 Training loss: 2.7893 0.5818 sec/batch\n",
      "Epoch 2/20  Iteration 325/3560 Training loss: 2.7880 0.6465 sec/batch\n",
      "Epoch 2/20  Iteration 326/3560 Training loss: 2.7870 0.6766 sec/batch\n",
      "Epoch 2/20  Iteration 327/3560 Training loss: 2.7857 0.6697 sec/batch\n",
      "Epoch 2/20  Iteration 328/3560 Training loss: 2.7844 0.6967 sec/batch\n",
      "Epoch 2/20  Iteration 329/3560 Training loss: 2.7835 0.6422 sec/batch\n",
      "Epoch 2/20  Iteration 330/3560 Training loss: 2.7826 0.5965 sec/batch\n",
      "Epoch 2/20  Iteration 331/3560 Training loss: 2.7813 0.7063 sec/batch\n",
      "Epoch 2/20  Iteration 332/3560 Training loss: 2.7802 0.6817 sec/batch\n",
      "Epoch 2/20  Iteration 333/3560 Training loss: 2.7790 0.7610 sec/batch\n",
      "Epoch 2/20  Iteration 334/3560 Training loss: 2.7778 0.6175 sec/batch\n",
      "Epoch 2/20  Iteration 335/3560 Training loss: 2.7766 0.6682 sec/batch\n",
      "Epoch 2/20  Iteration 336/3560 Training loss: 2.7754 0.6560 sec/batch\n",
      "Epoch 2/20  Iteration 337/3560 Training loss: 2.7741 0.8133 sec/batch\n",
      "Epoch 2/20  Iteration 338/3560 Training loss: 2.7730 0.6887 sec/batch\n",
      "Epoch 2/20  Iteration 339/3560 Training loss: 2.7718 0.7346 sec/batch\n",
      "Epoch 2/20  Iteration 340/3560 Training loss: 2.7704 0.7415 sec/batch\n",
      "Epoch 2/20  Iteration 341/3560 Training loss: 2.7691 0.6620 sec/batch\n",
      "Epoch 2/20  Iteration 342/3560 Training loss: 2.7680 0.7448 sec/batch\n",
      "Epoch 2/20  Iteration 343/3560 Training loss: 2.7669 0.6094 sec/batch\n",
      "Epoch 2/20  Iteration 344/3560 Training loss: 2.7658 0.6283 sec/batch\n",
      "Epoch 2/20  Iteration 345/3560 Training loss: 2.7647 0.6953 sec/batch\n",
      "Epoch 2/20  Iteration 346/3560 Training loss: 2.7636 0.6832 sec/batch\n",
      "Epoch 2/20  Iteration 347/3560 Training loss: 2.7626 0.5974 sec/batch\n",
      "Epoch 2/20  Iteration 348/3560 Training loss: 2.7614 0.6614 sec/batch\n",
      "Epoch 2/20  Iteration 349/3560 Training loss: 2.7603 0.6559 sec/batch\n",
      "Epoch 2/20  Iteration 350/3560 Training loss: 2.7594 0.7010 sec/batch\n",
      "Epoch 2/20  Iteration 351/3560 Training loss: 2.7586 0.6952 sec/batch\n",
      "Epoch 2/20  Iteration 352/3560 Training loss: 2.7578 0.7244 sec/batch\n",
      "Epoch 2/20  Iteration 353/3560 Training loss: 2.7569 0.7838 sec/batch\n",
      "Epoch 2/20  Iteration 354/3560 Training loss: 2.7559 0.7000 sec/batch\n",
      "Epoch 2/20  Iteration 355/3560 Training loss: 2.7547 0.6260 sec/batch\n",
      "Epoch 2/20  Iteration 356/3560 Training loss: 2.7536 0.7591 sec/batch\n",
      "Epoch 3/20  Iteration 357/3560 Training loss: 2.6300 0.6277 sec/batch\n",
      "Epoch 3/20  Iteration 358/3560 Training loss: 2.5825 0.8232 sec/batch\n",
      "Epoch 3/20  Iteration 359/3560 Training loss: 2.5722 0.6101 sec/batch\n",
      "Epoch 3/20  Iteration 360/3560 Training loss: 2.5690 0.5817 sec/batch\n",
      "Epoch 3/20  Iteration 361/3560 Training loss: 2.5643 0.6301 sec/batch\n",
      "Epoch 3/20  Iteration 362/3560 Training loss: 2.5611 0.6728 sec/batch\n",
      "Epoch 3/20  Iteration 363/3560 Training loss: 2.5608 0.6372 sec/batch\n",
      "Epoch 3/20  Iteration 364/3560 Training loss: 2.5606 0.6312 sec/batch\n",
      "Epoch 3/20  Iteration 365/3560 Training loss: 2.5608 0.6490 sec/batch\n",
      "Epoch 3/20  Iteration 366/3560 Training loss: 2.5587 0.7246 sec/batch\n",
      "Epoch 3/20  Iteration 367/3560 Training loss: 2.5566 0.7822 sec/batch\n",
      "Epoch 3/20  Iteration 368/3560 Training loss: 2.5566 0.6130 sec/batch\n",
      "Epoch 3/20  Iteration 369/3560 Training loss: 2.5562 0.6458 sec/batch\n",
      "Epoch 3/20  Iteration 370/3560 Training loss: 2.5580 0.6334 sec/batch\n",
      "Epoch 3/20  Iteration 371/3560 Training loss: 2.5580 0.6258 sec/batch\n",
      "Epoch 3/20  Iteration 372/3560 Training loss: 2.5583 0.6811 sec/batch\n",
      "Epoch 3/20  Iteration 373/3560 Training loss: 2.5577 0.6432 sec/batch\n",
      "Epoch 3/20  Iteration 374/3560 Training loss: 2.5589 0.6827 sec/batch\n",
      "Epoch 3/20  Iteration 375/3560 Training loss: 2.5585 0.6579 sec/batch\n",
      "Epoch 3/20  Iteration 376/3560 Training loss: 2.5566 0.6185 sec/batch\n",
      "Epoch 3/20  Iteration 377/3560 Training loss: 2.5557 0.7317 sec/batch\n",
      "Epoch 3/20  Iteration 378/3560 Training loss: 2.5562 0.7469 sec/batch\n",
      "Epoch 3/20  Iteration 379/3560 Training loss: 2.5554 0.7927 sec/batch\n",
      "Epoch 3/20  Iteration 380/3560 Training loss: 2.5545 0.6486 sec/batch\n",
      "Epoch 3/20  Iteration 381/3560 Training loss: 2.5533 0.6451 sec/batch\n",
      "Epoch 3/20  Iteration 382/3560 Training loss: 2.5532 0.6487 sec/batch\n",
      "Epoch 3/20  Iteration 383/3560 Training loss: 2.5529 0.6376 sec/batch\n",
      "Epoch 3/20  Iteration 384/3560 Training loss: 2.5521 0.6284 sec/batch\n",
      "Epoch 3/20  Iteration 385/3560 Training loss: 2.5518 0.6307 sec/batch\n",
      "Epoch 3/20  Iteration 386/3560 Training loss: 2.5513 0.7163 sec/batch\n",
      "Epoch 3/20  Iteration 387/3560 Training loss: 2.5515 0.5981 sec/batch\n",
      "Epoch 3/20  Iteration 388/3560 Training loss: 2.5507 0.6186 sec/batch\n",
      "Epoch 3/20  Iteration 389/3560 Training loss: 2.5496 0.7099 sec/batch\n",
      "Epoch 3/20  Iteration 390/3560 Training loss: 2.5495 0.6510 sec/batch\n",
      "Epoch 3/20  Iteration 391/3560 Training loss: 2.5486 0.6487 sec/batch\n",
      "Epoch 3/20  Iteration 392/3560 Training loss: 2.5481 0.7485 sec/batch\n",
      "Epoch 3/20  Iteration 393/3560 Training loss: 2.5473 0.6843 sec/batch\n",
      "Epoch 3/20  Iteration 394/3560 Training loss: 2.5460 0.7270 sec/batch\n",
      "Epoch 3/20  Iteration 395/3560 Training loss: 2.5450 0.6900 sec/batch\n",
      "Epoch 3/20  Iteration 396/3560 Training loss: 2.5439 0.6173 sec/batch\n",
      "Epoch 3/20  Iteration 397/3560 Training loss: 2.5432 0.6858 sec/batch\n",
      "Epoch 3/20  Iteration 398/3560 Training loss: 2.5423 0.6270 sec/batch\n",
      "Epoch 3/20  Iteration 399/3560 Training loss: 2.5414 0.6368 sec/batch\n",
      "Epoch 3/20  Iteration 400/3560 Training loss: 2.5406 0.7100 sec/batch\n",
      "Epoch 3/20  Iteration 401/3560 Training loss: 2.5396 0.6024 sec/batch\n",
      "Epoch 3/20  Iteration 402/3560 Training loss: 2.5382 0.6366 sec/batch\n",
      "Epoch 3/20  Iteration 403/3560 Training loss: 2.5378 0.6775 sec/batch\n",
      "Epoch 3/20  Iteration 404/3560 Training loss: 2.5372 0.7040 sec/batch\n",
      "Epoch 3/20  Iteration 405/3560 Training loss: 2.5364 0.5959 sec/batch\n",
      "Epoch 3/20  Iteration 406/3560 Training loss: 2.5362 0.6484 sec/batch\n",
      "Epoch 3/20  Iteration 407/3560 Training loss: 2.5354 0.5955 sec/batch\n",
      "Epoch 3/20  Iteration 408/3560 Training loss: 2.5348 0.6449 sec/batch\n",
      "Epoch 3/20  Iteration 409/3560 Training loss: 2.5343 0.6849 sec/batch\n",
      "Epoch 3/20  Iteration 410/3560 Training loss: 2.5337 0.5628 sec/batch\n",
      "Epoch 3/20  Iteration 411/3560 Training loss: 2.5330 0.7170 sec/batch\n",
      "Epoch 3/20  Iteration 412/3560 Training loss: 2.5325 0.5472 sec/batch\n",
      "Epoch 3/20  Iteration 413/3560 Training loss: 2.5320 0.7032 sec/batch\n",
      "Epoch 3/20  Iteration 414/3560 Training loss: 2.5312 0.6222 sec/batch\n",
      "Epoch 3/20  Iteration 415/3560 Training loss: 2.5306 0.7140 sec/batch\n",
      "Epoch 3/20  Iteration 416/3560 Training loss: 2.5303 0.6461 sec/batch\n",
      "Epoch 3/20  Iteration 417/3560 Training loss: 2.5297 0.6997 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 418/3560 Training loss: 2.5293 0.5907 sec/batch\n",
      "Epoch 3/20  Iteration 419/3560 Training loss: 2.5292 0.6765 sec/batch\n",
      "Epoch 3/20  Iteration 420/3560 Training loss: 2.5284 0.6066 sec/batch\n",
      "Epoch 3/20  Iteration 421/3560 Training loss: 2.5277 0.7824 sec/batch\n",
      "Epoch 3/20  Iteration 422/3560 Training loss: 2.5274 0.6183 sec/batch\n",
      "Epoch 3/20  Iteration 423/3560 Training loss: 2.5271 0.6377 sec/batch\n",
      "Epoch 3/20  Iteration 424/3560 Training loss: 2.5262 0.6478 sec/batch\n",
      "Epoch 3/20  Iteration 425/3560 Training loss: 2.5255 0.6577 sec/batch\n",
      "Epoch 3/20  Iteration 426/3560 Training loss: 2.5251 0.6423 sec/batch\n",
      "Epoch 3/20  Iteration 427/3560 Training loss: 2.5248 0.6778 sec/batch\n",
      "Epoch 3/20  Iteration 428/3560 Training loss: 2.5245 0.8689 sec/batch\n",
      "Epoch 3/20  Iteration 429/3560 Training loss: 2.5240 0.8339 sec/batch\n",
      "Epoch 3/20  Iteration 430/3560 Training loss: 2.5234 0.8631 sec/batch\n",
      "Epoch 3/20  Iteration 431/3560 Training loss: 2.5230 0.8935 sec/batch\n",
      "Epoch 3/20  Iteration 432/3560 Training loss: 2.5229 0.7807 sec/batch\n",
      "Epoch 3/20  Iteration 433/3560 Training loss: 2.5223 0.7173 sec/batch\n",
      "Epoch 3/20  Iteration 434/3560 Training loss: 2.5221 0.6219 sec/batch\n",
      "Epoch 3/20  Iteration 435/3560 Training loss: 2.5214 0.7544 sec/batch\n",
      "Epoch 3/20  Iteration 436/3560 Training loss: 2.5209 0.7621 sec/batch\n",
      "Epoch 3/20  Iteration 437/3560 Training loss: 2.5203 0.7682 sec/batch\n",
      "Epoch 3/20  Iteration 438/3560 Training loss: 2.5200 0.7138 sec/batch\n",
      "Epoch 3/20  Iteration 439/3560 Training loss: 2.5195 0.6904 sec/batch\n",
      "Epoch 3/20  Iteration 440/3560 Training loss: 2.5189 0.6823 sec/batch\n",
      "Epoch 3/20  Iteration 441/3560 Training loss: 2.5180 0.7351 sec/batch\n",
      "Epoch 3/20  Iteration 442/3560 Training loss: 2.5174 0.7443 sec/batch\n",
      "Epoch 3/20  Iteration 443/3560 Training loss: 2.5170 0.6210 sec/batch\n",
      "Epoch 3/20  Iteration 444/3560 Training loss: 2.5164 0.6426 sec/batch\n",
      "Epoch 3/20  Iteration 445/3560 Training loss: 2.5159 0.6806 sec/batch\n",
      "Epoch 3/20  Iteration 446/3560 Training loss: 2.5155 0.4733 sec/batch\n",
      "Epoch 3/20  Iteration 447/3560 Training loss: 2.5150 0.7200 sec/batch\n",
      "Epoch 3/20  Iteration 448/3560 Training loss: 2.5145 0.6459 sec/batch\n",
      "Epoch 3/20  Iteration 449/3560 Training loss: 2.5141 0.7232 sec/batch\n",
      "Epoch 3/20  Iteration 450/3560 Training loss: 2.5135 0.6790 sec/batch\n",
      "Epoch 3/20  Iteration 451/3560 Training loss: 2.5128 0.6222 sec/batch\n",
      "Epoch 3/20  Iteration 452/3560 Training loss: 2.5123 0.6232 sec/batch\n",
      "Epoch 3/20  Iteration 453/3560 Training loss: 2.5119 0.6814 sec/batch\n",
      "Epoch 3/20  Iteration 454/3560 Training loss: 2.5114 0.6912 sec/batch\n",
      "Epoch 3/20  Iteration 455/3560 Training loss: 2.5109 0.7939 sec/batch\n",
      "Epoch 3/20  Iteration 456/3560 Training loss: 2.5104 0.7501 sec/batch\n",
      "Epoch 3/20  Iteration 457/3560 Training loss: 2.5102 0.6699 sec/batch\n",
      "Epoch 3/20  Iteration 458/3560 Training loss: 2.5098 0.5663 sec/batch\n",
      "Epoch 3/20  Iteration 459/3560 Training loss: 2.5092 0.5930 sec/batch\n",
      "Epoch 3/20  Iteration 460/3560 Training loss: 2.5087 0.7241 sec/batch\n",
      "Epoch 3/20  Iteration 461/3560 Training loss: 2.5082 0.6895 sec/batch\n",
      "Epoch 3/20  Iteration 462/3560 Training loss: 2.5079 0.5149 sec/batch\n",
      "Epoch 3/20  Iteration 463/3560 Training loss: 2.5073 0.7028 sec/batch\n",
      "Epoch 3/20  Iteration 464/3560 Training loss: 2.5069 0.6325 sec/batch\n",
      "Epoch 3/20  Iteration 465/3560 Training loss: 2.5065 0.6936 sec/batch\n",
      "Epoch 3/20  Iteration 466/3560 Training loss: 2.5058 0.6704 sec/batch\n",
      "Epoch 3/20  Iteration 467/3560 Training loss: 2.5054 0.6124 sec/batch\n",
      "Epoch 3/20  Iteration 468/3560 Training loss: 2.5051 0.6788 sec/batch\n",
      "Epoch 3/20  Iteration 469/3560 Training loss: 2.5046 0.7075 sec/batch\n",
      "Epoch 3/20  Iteration 470/3560 Training loss: 2.5041 0.5930 sec/batch\n",
      "Epoch 3/20  Iteration 471/3560 Training loss: 2.5037 0.6564 sec/batch\n",
      "Epoch 3/20  Iteration 472/3560 Training loss: 2.5030 0.6082 sec/batch\n",
      "Epoch 3/20  Iteration 473/3560 Training loss: 2.5026 0.6552 sec/batch\n",
      "Epoch 3/20  Iteration 474/3560 Training loss: 2.5021 0.6148 sec/batch\n",
      "Epoch 3/20  Iteration 475/3560 Training loss: 2.5020 0.6189 sec/batch\n",
      "Epoch 3/20  Iteration 476/3560 Training loss: 2.5017 0.6163 sec/batch\n",
      "Epoch 3/20  Iteration 477/3560 Training loss: 2.5015 0.6586 sec/batch\n",
      "Epoch 3/20  Iteration 478/3560 Training loss: 2.5012 0.6987 sec/batch\n",
      "Epoch 3/20  Iteration 479/3560 Training loss: 2.5008 0.6401 sec/batch\n",
      "Epoch 3/20  Iteration 480/3560 Training loss: 2.5004 0.6435 sec/batch\n",
      "Epoch 3/20  Iteration 481/3560 Training loss: 2.5000 0.6168 sec/batch\n",
      "Epoch 3/20  Iteration 482/3560 Training loss: 2.4995 0.6155 sec/batch\n",
      "Epoch 3/20  Iteration 483/3560 Training loss: 2.4992 0.6812 sec/batch\n",
      "Epoch 3/20  Iteration 484/3560 Training loss: 2.4989 0.5455 sec/batch\n",
      "Epoch 3/20  Iteration 485/3560 Training loss: 2.4986 0.7232 sec/batch\n",
      "Epoch 3/20  Iteration 486/3560 Training loss: 2.4982 0.6450 sec/batch\n",
      "Epoch 3/20  Iteration 487/3560 Training loss: 2.4979 0.6096 sec/batch\n",
      "Epoch 3/20  Iteration 488/3560 Training loss: 2.4974 0.6666 sec/batch\n",
      "Epoch 3/20  Iteration 489/3560 Training loss: 2.4971 0.6716 sec/batch\n",
      "Epoch 3/20  Iteration 490/3560 Training loss: 2.4968 0.6361 sec/batch\n",
      "Epoch 3/20  Iteration 491/3560 Training loss: 2.4964 0.6510 sec/batch\n",
      "Epoch 3/20  Iteration 492/3560 Training loss: 2.4959 0.6747 sec/batch\n",
      "Epoch 3/20  Iteration 493/3560 Training loss: 2.4956 0.6507 sec/batch\n",
      "Epoch 3/20  Iteration 494/3560 Training loss: 2.4952 0.5970 sec/batch\n",
      "Epoch 3/20  Iteration 495/3560 Training loss: 2.4950 0.6174 sec/batch\n",
      "Epoch 3/20  Iteration 496/3560 Training loss: 2.4946 0.6146 sec/batch\n",
      "Epoch 3/20  Iteration 497/3560 Training loss: 2.4943 0.6794 sec/batch\n",
      "Epoch 3/20  Iteration 498/3560 Training loss: 2.4938 0.6662 sec/batch\n",
      "Epoch 3/20  Iteration 499/3560 Training loss: 2.4935 0.6598 sec/batch\n",
      "Epoch 3/20  Iteration 500/3560 Training loss: 2.4930 0.6001 sec/batch\n",
      "Epoch 3/20  Iteration 501/3560 Training loss: 2.4926 0.6372 sec/batch\n",
      "Epoch 3/20  Iteration 502/3560 Training loss: 2.4924 0.6080 sec/batch\n",
      "Epoch 3/20  Iteration 503/3560 Training loss: 2.4920 0.8005 sec/batch\n",
      "Epoch 3/20  Iteration 504/3560 Training loss: 2.4918 0.6160 sec/batch\n",
      "Epoch 3/20  Iteration 505/3560 Training loss: 2.4913 0.6056 sec/batch\n",
      "Epoch 3/20  Iteration 506/3560 Training loss: 2.4909 0.6162 sec/batch\n",
      "Epoch 3/20  Iteration 507/3560 Training loss: 2.4908 0.6663 sec/batch\n",
      "Epoch 3/20  Iteration 508/3560 Training loss: 2.4906 0.5849 sec/batch\n",
      "Epoch 3/20  Iteration 509/3560 Training loss: 2.4903 0.6197 sec/batch\n",
      "Epoch 3/20  Iteration 510/3560 Training loss: 2.4900 0.6044 sec/batch\n",
      "Epoch 3/20  Iteration 511/3560 Training loss: 2.4896 0.7022 sec/batch\n",
      "Epoch 3/20  Iteration 512/3560 Training loss: 2.4892 0.6338 sec/batch\n",
      "Epoch 3/20  Iteration 513/3560 Training loss: 2.4888 0.5760 sec/batch\n",
      "Epoch 3/20  Iteration 514/3560 Training loss: 2.4884 0.6671 sec/batch\n",
      "Epoch 3/20  Iteration 515/3560 Training loss: 2.4879 0.6130 sec/batch\n",
      "Epoch 3/20  Iteration 516/3560 Training loss: 2.4877 0.6723 sec/batch\n",
      "Epoch 3/20  Iteration 517/3560 Training loss: 2.4874 0.7100 sec/batch\n",
      "Epoch 3/20  Iteration 518/3560 Training loss: 2.4868 0.5971 sec/batch\n",
      "Epoch 3/20  Iteration 519/3560 Training loss: 2.4864 0.6251 sec/batch\n",
      "Epoch 3/20  Iteration 520/3560 Training loss: 2.4860 0.6818 sec/batch\n",
      "Epoch 3/20  Iteration 521/3560 Training loss: 2.4856 0.5775 sec/batch\n",
      "Epoch 3/20  Iteration 522/3560 Training loss: 2.4853 0.6133 sec/batch\n",
      "Epoch 3/20  Iteration 523/3560 Training loss: 2.4850 0.6853 sec/batch\n",
      "Epoch 3/20  Iteration 524/3560 Training loss: 2.4847 0.6795 sec/batch\n",
      "Epoch 3/20  Iteration 525/3560 Training loss: 2.4844 0.6193 sec/batch\n",
      "Epoch 3/20  Iteration 526/3560 Training loss: 2.4839 0.6393 sec/batch\n",
      "Epoch 3/20  Iteration 527/3560 Training loss: 2.4837 0.5754 sec/batch\n",
      "Epoch 3/20  Iteration 528/3560 Training loss: 2.4835 0.7102 sec/batch\n",
      "Epoch 3/20  Iteration 529/3560 Training loss: 2.4834 0.6427 sec/batch\n",
      "Epoch 3/20  Iteration 530/3560 Training loss: 2.4832 0.6632 sec/batch\n",
      "Epoch 3/20  Iteration 531/3560 Training loss: 2.4831 0.6266 sec/batch\n",
      "Epoch 3/20  Iteration 532/3560 Training loss: 2.4828 0.6843 sec/batch\n",
      "Epoch 3/20  Iteration 533/3560 Training loss: 2.4824 0.5967 sec/batch\n",
      "Epoch 3/20  Iteration 534/3560 Training loss: 2.4819 0.5285 sec/batch\n",
      "Epoch 4/20  Iteration 535/3560 Training loss: 2.4864 0.7066 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 536/3560 Training loss: 2.4395 0.6615 sec/batch\n",
      "Epoch 4/20  Iteration 537/3560 Training loss: 2.4243 0.6107 sec/batch\n",
      "Epoch 4/20  Iteration 538/3560 Training loss: 2.4197 0.6508 sec/batch\n",
      "Epoch 4/20  Iteration 539/3560 Training loss: 2.4161 0.5670 sec/batch\n",
      "Epoch 4/20  Iteration 540/3560 Training loss: 2.4146 0.6503 sec/batch\n",
      "Epoch 4/20  Iteration 541/3560 Training loss: 2.4143 0.7064 sec/batch\n",
      "Epoch 4/20  Iteration 542/3560 Training loss: 2.4170 0.6934 sec/batch\n",
      "Epoch 4/20  Iteration 543/3560 Training loss: 2.4171 0.5819 sec/batch\n",
      "Epoch 4/20  Iteration 544/3560 Training loss: 2.4158 0.7641 sec/batch\n",
      "Epoch 4/20  Iteration 545/3560 Training loss: 2.4142 0.5876 sec/batch\n",
      "Epoch 4/20  Iteration 546/3560 Training loss: 2.4139 0.6211 sec/batch\n",
      "Epoch 4/20  Iteration 547/3560 Training loss: 2.4138 0.6473 sec/batch\n",
      "Epoch 4/20  Iteration 548/3560 Training loss: 2.4157 0.6421 sec/batch\n",
      "Epoch 4/20  Iteration 549/3560 Training loss: 2.4152 0.7097 sec/batch\n",
      "Epoch 4/20  Iteration 550/3560 Training loss: 2.4155 0.6028 sec/batch\n",
      "Epoch 4/20  Iteration 551/3560 Training loss: 2.4153 0.6429 sec/batch\n",
      "Epoch 4/20  Iteration 552/3560 Training loss: 2.4169 0.6107 sec/batch\n",
      "Epoch 4/20  Iteration 553/3560 Training loss: 2.4167 0.6756 sec/batch\n",
      "Epoch 4/20  Iteration 554/3560 Training loss: 2.4153 0.7128 sec/batch\n",
      "Epoch 4/20  Iteration 555/3560 Training loss: 2.4147 0.5919 sec/batch\n",
      "Epoch 4/20  Iteration 556/3560 Training loss: 2.4150 0.6238 sec/batch\n",
      "Epoch 4/20  Iteration 557/3560 Training loss: 2.4147 0.5972 sec/batch\n",
      "Epoch 4/20  Iteration 558/3560 Training loss: 2.4140 0.6407 sec/batch\n",
      "Epoch 4/20  Iteration 559/3560 Training loss: 2.4130 0.6523 sec/batch\n",
      "Epoch 4/20  Iteration 560/3560 Training loss: 2.4129 0.6649 sec/batch\n",
      "Epoch 4/20  Iteration 561/3560 Training loss: 2.4123 0.5799 sec/batch\n",
      "Epoch 4/20  Iteration 562/3560 Training loss: 2.4121 0.6377 sec/batch\n",
      "Epoch 4/20  Iteration 563/3560 Training loss: 2.4124 0.6648 sec/batch\n",
      "Epoch 4/20  Iteration 564/3560 Training loss: 2.4118 0.6448 sec/batch\n",
      "Epoch 4/20  Iteration 565/3560 Training loss: 2.4122 0.6356 sec/batch\n",
      "Epoch 4/20  Iteration 566/3560 Training loss: 2.4114 0.5938 sec/batch\n",
      "Epoch 4/20  Iteration 567/3560 Training loss: 2.4108 0.6415 sec/batch\n",
      "Epoch 4/20  Iteration 568/3560 Training loss: 2.4109 0.6817 sec/batch\n",
      "Epoch 4/20  Iteration 569/3560 Training loss: 2.4103 0.6354 sec/batch\n",
      "Epoch 4/20  Iteration 570/3560 Training loss: 2.4102 0.5865 sec/batch\n",
      "Epoch 4/20  Iteration 571/3560 Training loss: 2.4095 0.6109 sec/batch\n",
      "Epoch 4/20  Iteration 572/3560 Training loss: 2.4084 0.6331 sec/batch\n",
      "Epoch 4/20  Iteration 573/3560 Training loss: 2.4078 0.7006 sec/batch\n",
      "Epoch 4/20  Iteration 574/3560 Training loss: 2.4067 0.6058 sec/batch\n",
      "Epoch 4/20  Iteration 575/3560 Training loss: 2.4061 0.6848 sec/batch\n",
      "Epoch 4/20  Iteration 576/3560 Training loss: 2.4053 0.5851 sec/batch\n",
      "Epoch 4/20  Iteration 577/3560 Training loss: 2.4045 0.6055 sec/batch\n",
      "Epoch 4/20  Iteration 578/3560 Training loss: 2.4037 0.6675 sec/batch\n",
      "Epoch 4/20  Iteration 579/3560 Training loss: 2.4030 0.6976 sec/batch\n",
      "Epoch 4/20  Iteration 580/3560 Training loss: 2.4018 0.6491 sec/batch\n",
      "Epoch 4/20  Iteration 581/3560 Training loss: 2.4017 0.6233 sec/batch\n",
      "Epoch 4/20  Iteration 582/3560 Training loss: 2.4012 0.6018 sec/batch\n",
      "Epoch 4/20  Iteration 583/3560 Training loss: 2.4008 0.6544 sec/batch\n",
      "Epoch 4/20  Iteration 584/3560 Training loss: 2.4009 0.5724 sec/batch\n",
      "Epoch 4/20  Iteration 585/3560 Training loss: 2.4003 0.6374 sec/batch\n",
      "Epoch 4/20  Iteration 586/3560 Training loss: 2.3999 0.6799 sec/batch\n",
      "Epoch 4/20  Iteration 587/3560 Training loss: 2.3995 0.6531 sec/batch\n",
      "Epoch 4/20  Iteration 588/3560 Training loss: 2.3991 0.6582 sec/batch\n",
      "Epoch 4/20  Iteration 589/3560 Training loss: 2.3987 0.6439 sec/batch\n",
      "Epoch 4/20  Iteration 590/3560 Training loss: 2.3985 0.5778 sec/batch\n",
      "Epoch 4/20  Iteration 591/3560 Training loss: 2.3982 0.6842 sec/batch\n",
      "Epoch 4/20  Iteration 592/3560 Training loss: 2.3978 0.6361 sec/batch\n",
      "Epoch 4/20  Iteration 593/3560 Training loss: 2.3973 0.6688 sec/batch\n",
      "Epoch 4/20  Iteration 594/3560 Training loss: 2.3973 0.5860 sec/batch\n",
      "Epoch 4/20  Iteration 595/3560 Training loss: 2.3969 0.6486 sec/batch\n",
      "Epoch 4/20  Iteration 596/3560 Training loss: 2.3968 0.6380 sec/batch\n",
      "Epoch 4/20  Iteration 597/3560 Training loss: 2.3968 0.5804 sec/batch\n",
      "Epoch 4/20  Iteration 598/3560 Training loss: 2.3962 0.6256 sec/batch\n",
      "Epoch 4/20  Iteration 599/3560 Training loss: 2.3956 0.6863 sec/batch\n",
      "Epoch 4/20  Iteration 600/3560 Training loss: 2.3957 0.6018 sec/batch\n",
      "Epoch 4/20  Iteration 601/3560 Training loss: 2.3953 0.6808 sec/batch\n",
      "Epoch 4/20  Iteration 602/3560 Training loss: 2.3946 0.6648 sec/batch\n",
      "Epoch 4/20  Iteration 603/3560 Training loss: 2.3940 0.6024 sec/batch\n",
      "Epoch 4/20  Iteration 604/3560 Training loss: 2.3939 0.6402 sec/batch\n",
      "Epoch 4/20  Iteration 605/3560 Training loss: 2.3938 0.7138 sec/batch\n",
      "Epoch 4/20  Iteration 606/3560 Training loss: 2.3938 0.6365 sec/batch\n",
      "Epoch 4/20  Iteration 607/3560 Training loss: 2.3937 0.6423 sec/batch\n",
      "Epoch 4/20  Iteration 608/3560 Training loss: 2.3933 0.6593 sec/batch\n",
      "Epoch 4/20  Iteration 609/3560 Training loss: 2.3930 0.5973 sec/batch\n",
      "Epoch 4/20  Iteration 610/3560 Training loss: 2.3933 0.5600 sec/batch\n",
      "Epoch 4/20  Iteration 611/3560 Training loss: 2.3929 0.6999 sec/batch\n",
      "Epoch 4/20  Iteration 612/3560 Training loss: 2.3929 0.6743 sec/batch\n",
      "Epoch 4/20  Iteration 613/3560 Training loss: 2.3925 0.6632 sec/batch\n",
      "Epoch 4/20  Iteration 614/3560 Training loss: 2.3921 0.5802 sec/batch\n",
      "Epoch 4/20  Iteration 615/3560 Training loss: 2.3917 0.6216 sec/batch\n",
      "Epoch 4/20  Iteration 616/3560 Training loss: 2.3915 0.6646 sec/batch\n",
      "Epoch 4/20  Iteration 617/3560 Training loss: 2.3912 0.6506 sec/batch\n",
      "Epoch 4/20  Iteration 618/3560 Training loss: 2.3908 0.6867 sec/batch\n",
      "Epoch 4/20  Iteration 619/3560 Training loss: 2.3900 0.6781 sec/batch\n",
      "Epoch 4/20  Iteration 620/3560 Training loss: 2.3896 0.5702 sec/batch\n",
      "Epoch 4/20  Iteration 621/3560 Training loss: 2.3893 0.7199 sec/batch\n",
      "Epoch 4/20  Iteration 622/3560 Training loss: 2.3889 0.6477 sec/batch\n",
      "Epoch 4/20  Iteration 623/3560 Training loss: 2.3885 0.7275 sec/batch\n",
      "Epoch 4/20  Iteration 624/3560 Training loss: 2.3883 0.7481 sec/batch\n",
      "Epoch 4/20  Iteration 625/3560 Training loss: 2.3879 0.7482 sec/batch\n",
      "Epoch 4/20  Iteration 626/3560 Training loss: 2.3878 0.7211 sec/batch\n",
      "Epoch 4/20  Iteration 627/3560 Training loss: 2.3873 0.7017 sec/batch\n",
      "Epoch 4/20  Iteration 628/3560 Training loss: 2.3869 0.7116 sec/batch\n",
      "Epoch 4/20  Iteration 629/3560 Training loss: 2.3864 0.6631 sec/batch\n",
      "Epoch 4/20  Iteration 630/3560 Training loss: 2.3861 0.6075 sec/batch\n",
      "Epoch 4/20  Iteration 631/3560 Training loss: 2.3858 0.6695 sec/batch\n",
      "Epoch 4/20  Iteration 632/3560 Training loss: 2.3856 0.6724 sec/batch\n",
      "Epoch 4/20  Iteration 633/3560 Training loss: 2.3853 0.5786 sec/batch\n",
      "Epoch 4/20  Iteration 634/3560 Training loss: 2.3849 0.6134 sec/batch\n",
      "Epoch 4/20  Iteration 635/3560 Training loss: 2.3847 0.5983 sec/batch\n",
      "Epoch 4/20  Iteration 636/3560 Training loss: 2.3845 0.6617 sec/batch\n",
      "Epoch 4/20  Iteration 637/3560 Training loss: 2.3842 0.6734 sec/batch\n",
      "Epoch 4/20  Iteration 638/3560 Training loss: 2.3839 0.6868 sec/batch\n",
      "Epoch 4/20  Iteration 639/3560 Training loss: 2.3835 0.6036 sec/batch\n",
      "Epoch 4/20  Iteration 640/3560 Training loss: 2.3833 0.5868 sec/batch\n",
      "Epoch 4/20  Iteration 641/3560 Training loss: 2.3830 0.6923 sec/batch\n",
      "Epoch 4/20  Iteration 642/3560 Training loss: 2.3827 0.7245 sec/batch\n",
      "Epoch 4/20  Iteration 643/3560 Training loss: 2.3826 0.6040 sec/batch\n",
      "Epoch 4/20  Iteration 644/3560 Training loss: 2.3821 0.6157 sec/batch\n",
      "Epoch 4/20  Iteration 645/3560 Training loss: 2.3819 0.6894 sec/batch\n",
      "Epoch 4/20  Iteration 646/3560 Training loss: 2.3817 0.5702 sec/batch\n",
      "Epoch 4/20  Iteration 647/3560 Training loss: 2.3814 0.6150 sec/batch\n",
      "Epoch 4/20  Iteration 648/3560 Training loss: 2.3811 0.5568 sec/batch\n",
      "Epoch 4/20  Iteration 649/3560 Training loss: 2.3807 0.6631 sec/batch\n",
      "Epoch 4/20  Iteration 650/3560 Training loss: 2.3802 0.6684 sec/batch\n",
      "Epoch 4/20  Iteration 651/3560 Training loss: 2.3799 0.6808 sec/batch\n",
      "Epoch 4/20  Iteration 652/3560 Training loss: 2.3797 0.6284 sec/batch\n",
      "Epoch 4/20  Iteration 653/3560 Training loss: 2.3795 0.6121 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 654/3560 Training loss: 2.3793 0.6271 sec/batch\n",
      "Epoch 4/20  Iteration 655/3560 Training loss: 2.3792 0.6330 sec/batch\n",
      "Epoch 4/20  Iteration 656/3560 Training loss: 2.3790 0.7311 sec/batch\n",
      "Epoch 4/20  Iteration 657/3560 Training loss: 2.3787 0.6553 sec/batch\n",
      "Epoch 4/20  Iteration 658/3560 Training loss: 2.3786 0.6084 sec/batch\n",
      "Epoch 4/20  Iteration 659/3560 Training loss: 2.3783 0.6239 sec/batch\n",
      "Epoch 4/20  Iteration 660/3560 Training loss: 2.3779 0.6390 sec/batch\n",
      "Epoch 4/20  Iteration 661/3560 Training loss: 2.3777 0.5972 sec/batch\n",
      "Epoch 4/20  Iteration 662/3560 Training loss: 2.3776 0.7217 sec/batch\n",
      "Epoch 4/20  Iteration 663/3560 Training loss: 2.3773 0.6227 sec/batch\n",
      "Epoch 4/20  Iteration 664/3560 Training loss: 2.3772 0.6326 sec/batch\n",
      "Epoch 4/20  Iteration 665/3560 Training loss: 2.3770 0.6675 sec/batch\n",
      "Epoch 4/20  Iteration 666/3560 Training loss: 2.3767 0.6360 sec/batch\n",
      "Epoch 4/20  Iteration 667/3560 Training loss: 2.3765 0.5597 sec/batch\n",
      "Epoch 4/20  Iteration 668/3560 Training loss: 2.3764 0.7640 sec/batch\n",
      "Epoch 4/20  Iteration 669/3560 Training loss: 2.3761 0.6250 sec/batch\n",
      "Epoch 4/20  Iteration 670/3560 Training loss: 2.3758 0.6464 sec/batch\n",
      "Epoch 4/20  Iteration 671/3560 Training loss: 2.3756 0.6984 sec/batch\n",
      "Epoch 4/20  Iteration 672/3560 Training loss: 2.3754 0.6027 sec/batch\n",
      "Epoch 4/20  Iteration 673/3560 Training loss: 2.3754 0.7664 sec/batch\n",
      "Epoch 4/20  Iteration 674/3560 Training loss: 2.3751 0.7518 sec/batch\n",
      "Epoch 4/20  Iteration 675/3560 Training loss: 2.3751 0.9232 sec/batch\n",
      "Epoch 4/20  Iteration 676/3560 Training loss: 2.3747 0.9923 sec/batch\n",
      "Epoch 4/20  Iteration 677/3560 Training loss: 2.3744 0.7195 sec/batch\n",
      "Epoch 4/20  Iteration 678/3560 Training loss: 2.3741 0.6852 sec/batch\n",
      "Epoch 4/20  Iteration 679/3560 Training loss: 2.3739 0.6738 sec/batch\n",
      "Epoch 4/20  Iteration 680/3560 Training loss: 2.3739 0.7475 sec/batch\n",
      "Epoch 4/20  Iteration 681/3560 Training loss: 2.3737 0.9693 sec/batch\n",
      "Epoch 4/20  Iteration 682/3560 Training loss: 2.3737 0.8344 sec/batch\n",
      "Epoch 4/20  Iteration 683/3560 Training loss: 2.3735 0.7185 sec/batch\n",
      "Epoch 4/20  Iteration 684/3560 Training loss: 2.3732 0.6249 sec/batch\n",
      "Epoch 4/20  Iteration 685/3560 Training loss: 2.3731 0.6841 sec/batch\n",
      "Epoch 4/20  Iteration 686/3560 Training loss: 2.3731 0.5551 sec/batch\n",
      "Epoch 4/20  Iteration 687/3560 Training loss: 2.3730 0.7050 sec/batch\n",
      "Epoch 4/20  Iteration 688/3560 Training loss: 2.3729 0.6250 sec/batch\n",
      "Epoch 4/20  Iteration 689/3560 Training loss: 2.3726 0.6778 sec/batch\n",
      "Epoch 4/20  Iteration 690/3560 Training loss: 2.3723 0.5814 sec/batch\n",
      "Epoch 4/20  Iteration 691/3560 Training loss: 2.3720 0.6720 sec/batch\n",
      "Epoch 4/20  Iteration 692/3560 Training loss: 2.3718 0.5418 sec/batch\n",
      "Epoch 4/20  Iteration 693/3560 Training loss: 2.3714 0.7386 sec/batch\n",
      "Epoch 4/20  Iteration 694/3560 Training loss: 2.3713 0.6850 sec/batch\n",
      "Epoch 4/20  Iteration 695/3560 Training loss: 2.3711 0.6960 sec/batch\n",
      "Epoch 4/20  Iteration 696/3560 Training loss: 2.3708 0.6012 sec/batch\n",
      "Epoch 4/20  Iteration 697/3560 Training loss: 2.3705 0.6766 sec/batch\n",
      "Epoch 4/20  Iteration 698/3560 Training loss: 2.3703 0.5927 sec/batch\n",
      "Epoch 4/20  Iteration 699/3560 Training loss: 2.3701 0.5985 sec/batch\n",
      "Epoch 4/20  Iteration 700/3560 Training loss: 2.3699 0.7258 sec/batch\n",
      "Epoch 4/20  Iteration 701/3560 Training loss: 2.3697 0.7192 sec/batch\n",
      "Epoch 4/20  Iteration 702/3560 Training loss: 2.3695 0.6367 sec/batch\n",
      "Epoch 4/20  Iteration 703/3560 Training loss: 2.3693 0.6195 sec/batch\n",
      "Epoch 4/20  Iteration 704/3560 Training loss: 2.3690 0.6665 sec/batch\n",
      "Epoch 4/20  Iteration 705/3560 Training loss: 2.3688 0.6549 sec/batch\n",
      "Epoch 4/20  Iteration 706/3560 Training loss: 2.3687 0.6942 sec/batch\n",
      "Epoch 4/20  Iteration 707/3560 Training loss: 2.3687 0.6466 sec/batch\n",
      "Epoch 4/20  Iteration 708/3560 Training loss: 2.3686 0.6046 sec/batch\n",
      "Epoch 4/20  Iteration 709/3560 Training loss: 2.3686 0.6541 sec/batch\n",
      "Epoch 4/20  Iteration 710/3560 Training loss: 2.3685 0.5920 sec/batch\n",
      "Epoch 4/20  Iteration 711/3560 Training loss: 2.3682 0.5754 sec/batch\n",
      "Epoch 4/20  Iteration 712/3560 Training loss: 2.3679 0.6409 sec/batch\n",
      "Epoch 5/20  Iteration 713/3560 Training loss: 2.3874 0.6913 sec/batch\n",
      "Epoch 5/20  Iteration 714/3560 Training loss: 2.3470 0.5320 sec/batch\n",
      "Epoch 5/20  Iteration 715/3560 Training loss: 2.3329 0.6993 sec/batch\n",
      "Epoch 5/20  Iteration 716/3560 Training loss: 2.3284 0.6218 sec/batch\n",
      "Epoch 5/20  Iteration 717/3560 Training loss: 2.3274 0.6728 sec/batch\n",
      "Epoch 5/20  Iteration 718/3560 Training loss: 2.3269 0.7066 sec/batch\n",
      "Epoch 5/20  Iteration 719/3560 Training loss: 2.3276 0.6455 sec/batch\n",
      "Epoch 5/20  Iteration 720/3560 Training loss: 2.3285 0.6580 sec/batch\n",
      "Epoch 5/20  Iteration 721/3560 Training loss: 2.3305 0.5907 sec/batch\n",
      "Epoch 5/20  Iteration 722/3560 Training loss: 2.3293 0.7002 sec/batch\n",
      "Epoch 5/20  Iteration 723/3560 Training loss: 2.3277 0.6216 sec/batch\n",
      "Epoch 5/20  Iteration 724/3560 Training loss: 2.3271 0.6795 sec/batch\n",
      "Epoch 5/20  Iteration 725/3560 Training loss: 2.3268 0.5779 sec/batch\n",
      "Epoch 5/20  Iteration 726/3560 Training loss: 2.3293 0.7016 sec/batch\n",
      "Epoch 5/20  Iteration 727/3560 Training loss: 2.3297 0.6239 sec/batch\n",
      "Epoch 5/20  Iteration 728/3560 Training loss: 2.3293 0.6615 sec/batch\n",
      "Epoch 5/20  Iteration 729/3560 Training loss: 2.3287 0.6375 sec/batch\n",
      "Epoch 5/20  Iteration 730/3560 Training loss: 2.3302 0.6139 sec/batch\n",
      "Epoch 5/20  Iteration 731/3560 Training loss: 2.3305 0.7008 sec/batch\n",
      "Epoch 5/20  Iteration 732/3560 Training loss: 2.3299 0.7002 sec/batch\n",
      "Epoch 5/20  Iteration 733/3560 Training loss: 2.3290 0.6847 sec/batch\n",
      "Epoch 5/20  Iteration 734/3560 Training loss: 2.3298 0.6156 sec/batch\n",
      "Epoch 5/20  Iteration 735/3560 Training loss: 2.3295 0.5911 sec/batch\n",
      "Epoch 5/20  Iteration 736/3560 Training loss: 2.3288 0.5994 sec/batch\n",
      "Epoch 5/20  Iteration 737/3560 Training loss: 2.3278 0.6039 sec/batch\n",
      "Epoch 5/20  Iteration 738/3560 Training loss: 2.3277 0.6875 sec/batch\n",
      "Epoch 5/20  Iteration 739/3560 Training loss: 2.3272 0.7026 sec/batch\n",
      "Epoch 5/20  Iteration 740/3560 Training loss: 2.3268 0.6697 sec/batch\n",
      "Epoch 5/20  Iteration 741/3560 Training loss: 2.3272 0.5792 sec/batch\n",
      "Epoch 5/20  Iteration 742/3560 Training loss: 2.3270 0.6459 sec/batch\n",
      "Epoch 5/20  Iteration 743/3560 Training loss: 2.3274 0.7280 sec/batch\n",
      "Epoch 5/20  Iteration 744/3560 Training loss: 2.3267 0.6236 sec/batch\n",
      "Epoch 5/20  Iteration 745/3560 Training loss: 2.3264 0.6358 sec/batch\n",
      "Epoch 5/20  Iteration 746/3560 Training loss: 2.3267 0.6601 sec/batch\n",
      "Epoch 5/20  Iteration 747/3560 Training loss: 2.3260 0.6382 sec/batch\n",
      "Epoch 5/20  Iteration 748/3560 Training loss: 2.3259 0.6640 sec/batch\n",
      "Epoch 5/20  Iteration 749/3560 Training loss: 2.3252 0.6558 sec/batch\n",
      "Epoch 5/20  Iteration 750/3560 Training loss: 2.3242 0.6590 sec/batch\n",
      "Epoch 5/20  Iteration 751/3560 Training loss: 2.3234 0.7092 sec/batch\n",
      "Epoch 5/20  Iteration 752/3560 Training loss: 2.3226 0.6124 sec/batch\n",
      "Epoch 5/20  Iteration 753/3560 Training loss: 2.3220 0.6240 sec/batch\n",
      "Epoch 5/20  Iteration 754/3560 Training loss: 2.3214 0.6007 sec/batch\n",
      "Epoch 5/20  Iteration 755/3560 Training loss: 2.3208 0.6166 sec/batch\n",
      "Epoch 5/20  Iteration 756/3560 Training loss: 2.3201 0.7156 sec/batch\n",
      "Epoch 5/20  Iteration 757/3560 Training loss: 2.3194 0.6584 sec/batch\n",
      "Epoch 5/20  Iteration 758/3560 Training loss: 2.3183 0.6314 sec/batch\n",
      "Epoch 5/20  Iteration 759/3560 Training loss: 2.3182 0.6278 sec/batch\n",
      "Epoch 5/20  Iteration 760/3560 Training loss: 2.3178 0.6118 sec/batch\n",
      "Epoch 5/20  Iteration 761/3560 Training loss: 2.3173 0.5885 sec/batch\n",
      "Epoch 5/20  Iteration 762/3560 Training loss: 2.3176 0.6101 sec/batch\n",
      "Epoch 5/20  Iteration 763/3560 Training loss: 2.3171 0.7156 sec/batch\n",
      "Epoch 5/20  Iteration 764/3560 Training loss: 2.3170 0.7010 sec/batch\n",
      "Epoch 5/20  Iteration 765/3560 Training loss: 2.3165 0.6195 sec/batch\n",
      "Epoch 5/20  Iteration 766/3560 Training loss: 2.3161 0.5752 sec/batch\n",
      "Epoch 5/20  Iteration 767/3560 Training loss: 2.3157 0.6171 sec/batch\n",
      "Epoch 5/20  Iteration 768/3560 Training loss: 2.3154 0.7092 sec/batch\n",
      "Epoch 5/20  Iteration 769/3560 Training loss: 2.3153 0.7242 sec/batch\n",
      "Epoch 5/20  Iteration 770/3560 Training loss: 2.3148 0.5792 sec/batch\n",
      "Epoch 5/20  Iteration 771/3560 Training loss: 2.3144 0.6450 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 772/3560 Training loss: 2.3144 0.6045 sec/batch\n",
      "Epoch 5/20  Iteration 773/3560 Training loss: 2.3141 0.6268 sec/batch\n",
      "Epoch 5/20  Iteration 774/3560 Training loss: 2.3141 0.5921 sec/batch\n",
      "Epoch 5/20  Iteration 775/3560 Training loss: 2.3142 0.6438 sec/batch\n",
      "Epoch 5/20  Iteration 776/3560 Training loss: 2.3139 0.6376 sec/batch\n",
      "Epoch 5/20  Iteration 777/3560 Training loss: 2.3133 0.6443 sec/batch\n",
      "Epoch 5/20  Iteration 778/3560 Training loss: 2.3134 0.7340 sec/batch\n",
      "Epoch 5/20  Iteration 779/3560 Training loss: 2.3134 0.5832 sec/batch\n",
      "Epoch 5/20  Iteration 780/3560 Training loss: 2.3127 0.6515 sec/batch\n",
      "Epoch 5/20  Iteration 781/3560 Training loss: 2.3122 0.6421 sec/batch\n",
      "Epoch 5/20  Iteration 782/3560 Training loss: 2.3121 0.6942 sec/batch\n",
      "Epoch 5/20  Iteration 783/3560 Training loss: 2.3120 0.6119 sec/batch\n",
      "Epoch 5/20  Iteration 784/3560 Training loss: 2.3120 0.6365 sec/batch\n",
      "Epoch 5/20  Iteration 785/3560 Training loss: 2.3120 0.6617 sec/batch\n",
      "Epoch 5/20  Iteration 786/3560 Training loss: 2.3118 0.6279 sec/batch\n",
      "Epoch 5/20  Iteration 787/3560 Training loss: 2.3116 0.5853 sec/batch\n",
      "Epoch 5/20  Iteration 788/3560 Training loss: 2.3118 0.6793 sec/batch\n",
      "Epoch 5/20  Iteration 789/3560 Training loss: 2.3115 0.6711 sec/batch\n",
      "Epoch 5/20  Iteration 790/3560 Training loss: 2.3115 0.6926 sec/batch\n",
      "Epoch 5/20  Iteration 791/3560 Training loss: 2.3111 0.6833 sec/batch\n",
      "Epoch 5/20  Iteration 792/3560 Training loss: 2.3107 0.5736 sec/batch\n",
      "Epoch 5/20  Iteration 793/3560 Training loss: 2.3104 0.6668 sec/batch\n",
      "Epoch 5/20  Iteration 794/3560 Training loss: 2.3104 0.6687 sec/batch\n",
      "Epoch 5/20  Iteration 795/3560 Training loss: 2.3101 0.6973 sec/batch\n",
      "Epoch 5/20  Iteration 796/3560 Training loss: 2.3096 0.6870 sec/batch\n",
      "Epoch 5/20  Iteration 797/3560 Training loss: 2.3090 0.6436 sec/batch\n",
      "Epoch 5/20  Iteration 798/3560 Training loss: 2.3087 0.5619 sec/batch\n",
      "Epoch 5/20  Iteration 799/3560 Training loss: 2.3083 0.6150 sec/batch\n",
      "Epoch 5/20  Iteration 800/3560 Training loss: 2.3080 0.6026 sec/batch\n",
      "Epoch 5/20  Iteration 801/3560 Training loss: 2.3076 0.7294 sec/batch\n",
      "Epoch 5/20  Iteration 802/3560 Training loss: 2.3076 0.6845 sec/batch\n",
      "Epoch 5/20  Iteration 803/3560 Training loss: 2.3072 0.6100 sec/batch\n",
      "Epoch 5/20  Iteration 804/3560 Training loss: 2.3071 0.6443 sec/batch\n",
      "Epoch 5/20  Iteration 805/3560 Training loss: 2.3068 0.6373 sec/batch\n",
      "Epoch 5/20  Iteration 806/3560 Training loss: 2.3063 0.6800 sec/batch\n",
      "Epoch 5/20  Iteration 807/3560 Training loss: 2.3058 0.6607 sec/batch\n",
      "Epoch 5/20  Iteration 808/3560 Training loss: 2.3056 0.6463 sec/batch\n",
      "Epoch 5/20  Iteration 809/3560 Training loss: 2.3054 0.5977 sec/batch\n",
      "Epoch 5/20  Iteration 810/3560 Training loss: 2.3051 0.6223 sec/batch\n",
      "Epoch 5/20  Iteration 811/3560 Training loss: 2.3048 0.6706 sec/batch\n",
      "Epoch 5/20  Iteration 812/3560 Training loss: 2.3045 0.6353 sec/batch\n",
      "Epoch 5/20  Iteration 813/3560 Training loss: 2.3045 0.6668 sec/batch\n",
      "Epoch 5/20  Iteration 814/3560 Training loss: 2.3044 0.6328 sec/batch\n",
      "Epoch 5/20  Iteration 815/3560 Training loss: 2.3040 0.6592 sec/batch\n",
      "Epoch 5/20  Iteration 816/3560 Training loss: 2.3037 0.6743 sec/batch\n",
      "Epoch 5/20  Iteration 817/3560 Training loss: 2.3034 0.5806 sec/batch\n",
      "Epoch 5/20  Iteration 818/3560 Training loss: 2.3033 0.6570 sec/batch\n",
      "Epoch 5/20  Iteration 819/3560 Training loss: 2.3030 0.6302 sec/batch\n",
      "Epoch 5/20  Iteration 820/3560 Training loss: 2.3029 0.6206 sec/batch\n",
      "Epoch 5/20  Iteration 821/3560 Training loss: 2.3029 0.6809 sec/batch\n",
      "Epoch 5/20  Iteration 822/3560 Training loss: 2.3025 0.6379 sec/batch\n",
      "Epoch 5/20  Iteration 823/3560 Training loss: 2.3023 0.5824 sec/batch\n",
      "Epoch 5/20  Iteration 824/3560 Training loss: 2.3023 0.6563 sec/batch\n",
      "Epoch 5/20  Iteration 825/3560 Training loss: 2.3020 0.5870 sec/batch\n",
      "Epoch 5/20  Iteration 826/3560 Training loss: 2.3017 0.6646 sec/batch\n",
      "Epoch 5/20  Iteration 827/3560 Training loss: 2.3015 0.6618 sec/batch\n",
      "Epoch 5/20  Iteration 828/3560 Training loss: 2.3010 0.6827 sec/batch\n",
      "Epoch 5/20  Iteration 829/3560 Training loss: 2.3008 0.6428 sec/batch\n",
      "Epoch 5/20  Iteration 830/3560 Training loss: 2.3007 0.6155 sec/batch\n",
      "Epoch 5/20  Iteration 831/3560 Training loss: 2.3006 0.6109 sec/batch\n",
      "Epoch 5/20  Iteration 832/3560 Training loss: 2.3004 0.6711 sec/batch\n",
      "Epoch 5/20  Iteration 833/3560 Training loss: 2.3003 0.6405 sec/batch\n",
      "Epoch 5/20  Iteration 834/3560 Training loss: 2.3002 0.6905 sec/batch\n",
      "Epoch 5/20  Iteration 835/3560 Training loss: 2.3000 0.5771 sec/batch\n",
      "Epoch 5/20  Iteration 836/3560 Training loss: 2.2999 0.6467 sec/batch\n",
      "Epoch 5/20  Iteration 837/3560 Training loss: 2.2997 0.5825 sec/batch\n",
      "Epoch 5/20  Iteration 838/3560 Training loss: 2.2993 0.7257 sec/batch\n",
      "Epoch 5/20  Iteration 839/3560 Training loss: 2.2993 0.6465 sec/batch\n",
      "Epoch 5/20  Iteration 840/3560 Training loss: 2.2992 0.7398 sec/batch\n",
      "Epoch 5/20  Iteration 841/3560 Training loss: 2.2990 0.6346 sec/batch\n",
      "Epoch 5/20  Iteration 842/3560 Training loss: 2.2988 0.6607 sec/batch\n",
      "Epoch 5/20  Iteration 843/3560 Training loss: 2.2986 0.6875 sec/batch\n",
      "Epoch 5/20  Iteration 844/3560 Training loss: 2.2983 0.6350 sec/batch\n",
      "Epoch 5/20  Iteration 845/3560 Training loss: 2.2982 0.5879 sec/batch\n",
      "Epoch 5/20  Iteration 846/3560 Training loss: 2.2982 0.6169 sec/batch\n",
      "Epoch 5/20  Iteration 847/3560 Training loss: 2.2979 0.6938 sec/batch\n",
      "Epoch 5/20  Iteration 848/3560 Training loss: 2.2977 0.6113 sec/batch\n",
      "Epoch 5/20  Iteration 849/3560 Training loss: 2.2976 0.6744 sec/batch\n",
      "Epoch 5/20  Iteration 850/3560 Training loss: 2.2974 0.6103 sec/batch\n",
      "Epoch 5/20  Iteration 851/3560 Training loss: 2.2974 0.7003 sec/batch\n",
      "Epoch 5/20  Iteration 852/3560 Training loss: 2.2972 0.6999 sec/batch\n",
      "Epoch 5/20  Iteration 853/3560 Training loss: 2.2971 0.6241 sec/batch\n",
      "Epoch 5/20  Iteration 854/3560 Training loss: 2.2969 0.6592 sec/batch\n",
      "Epoch 5/20  Iteration 855/3560 Training loss: 2.2967 0.8012 sec/batch\n",
      "Epoch 5/20  Iteration 856/3560 Training loss: 2.2965 0.7063 sec/batch\n",
      "Epoch 5/20  Iteration 857/3560 Training loss: 2.2962 0.7128 sec/batch\n",
      "Epoch 5/20  Iteration 858/3560 Training loss: 2.2963 0.6523 sec/batch\n",
      "Epoch 5/20  Iteration 859/3560 Training loss: 2.2962 0.6159 sec/batch\n",
      "Epoch 5/20  Iteration 860/3560 Training loss: 2.2962 0.6829 sec/batch\n",
      "Epoch 5/20  Iteration 861/3560 Training loss: 2.2959 0.6391 sec/batch\n",
      "Epoch 5/20  Iteration 862/3560 Training loss: 2.2958 0.5562 sec/batch\n",
      "Epoch 5/20  Iteration 863/3560 Training loss: 2.2957 0.6792 sec/batch\n",
      "Epoch 5/20  Iteration 864/3560 Training loss: 2.2957 0.6962 sec/batch\n",
      "Epoch 5/20  Iteration 865/3560 Training loss: 2.2956 0.6601 sec/batch\n",
      "Epoch 5/20  Iteration 866/3560 Training loss: 2.2956 0.7155 sec/batch\n",
      "Epoch 5/20  Iteration 867/3560 Training loss: 2.2954 0.5867 sec/batch\n",
      "Epoch 5/20  Iteration 868/3560 Training loss: 2.2952 0.6486 sec/batch\n",
      "Epoch 5/20  Iteration 869/3560 Training loss: 2.2950 0.5962 sec/batch\n",
      "Epoch 5/20  Iteration 870/3560 Training loss: 2.2948 0.6829 sec/batch\n",
      "Epoch 5/20  Iteration 871/3560 Training loss: 2.2944 0.6648 sec/batch\n",
      "Epoch 5/20  Iteration 872/3560 Training loss: 2.2944 0.6541 sec/batch\n",
      "Epoch 5/20  Iteration 873/3560 Training loss: 2.2943 0.5947 sec/batch\n",
      "Epoch 5/20  Iteration 874/3560 Training loss: 2.2941 0.6732 sec/batch\n",
      "Epoch 5/20  Iteration 875/3560 Training loss: 2.2939 0.6876 sec/batch\n",
      "Epoch 5/20  Iteration 876/3560 Training loss: 2.2937 0.6719 sec/batch\n",
      "Epoch 5/20  Iteration 877/3560 Training loss: 2.2936 0.6573 sec/batch\n",
      "Epoch 5/20  Iteration 878/3560 Training loss: 2.2935 0.7701 sec/batch\n",
      "Epoch 5/20  Iteration 879/3560 Training loss: 2.2933 0.5993 sec/batch\n",
      "Epoch 5/20  Iteration 880/3560 Training loss: 2.2932 0.6016 sec/batch\n",
      "Epoch 5/20  Iteration 881/3560 Training loss: 2.2931 0.6638 sec/batch\n",
      "Epoch 5/20  Iteration 882/3560 Training loss: 2.2928 0.6192 sec/batch\n",
      "Epoch 5/20  Iteration 883/3560 Training loss: 2.2926 0.6834 sec/batch\n",
      "Epoch 5/20  Iteration 884/3560 Training loss: 2.2924 0.5973 sec/batch\n",
      "Epoch 5/20  Iteration 885/3560 Training loss: 2.2924 0.6026 sec/batch\n",
      "Epoch 5/20  Iteration 886/3560 Training loss: 2.2924 0.7235 sec/batch\n",
      "Epoch 5/20  Iteration 887/3560 Training loss: 2.2923 0.7156 sec/batch\n",
      "Epoch 5/20  Iteration 888/3560 Training loss: 2.2922 0.5875 sec/batch\n",
      "Epoch 5/20  Iteration 889/3560 Training loss: 2.2920 0.7711 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 890/3560 Training loss: 2.2918 0.7386 sec/batch\n",
      "Epoch 6/20  Iteration 891/3560 Training loss: 2.3281 0.6884 sec/batch\n",
      "Epoch 6/20  Iteration 892/3560 Training loss: 2.2817 0.6439 sec/batch\n",
      "Epoch 6/20  Iteration 893/3560 Training loss: 2.2650 0.6062 sec/batch\n",
      "Epoch 6/20  Iteration 894/3560 Training loss: 2.2599 0.6499 sec/batch\n",
      "Epoch 6/20  Iteration 895/3560 Training loss: 2.2579 0.6286 sec/batch\n",
      "Epoch 6/20  Iteration 896/3560 Training loss: 2.2556 0.6361 sec/batch\n",
      "Epoch 6/20  Iteration 897/3560 Training loss: 2.2568 0.6522 sec/batch\n",
      "Epoch 6/20  Iteration 898/3560 Training loss: 2.2582 0.5822 sec/batch\n",
      "Epoch 6/20  Iteration 899/3560 Training loss: 2.2601 0.6120 sec/batch\n",
      "Epoch 6/20  Iteration 900/3560 Training loss: 2.2600 0.5748 sec/batch\n",
      "Epoch 6/20  Iteration 901/3560 Training loss: 2.2590 0.5960 sec/batch\n",
      "Epoch 6/20  Iteration 902/3560 Training loss: 2.2582 0.6993 sec/batch\n",
      "Epoch 6/20  Iteration 903/3560 Training loss: 2.2580 0.6853 sec/batch\n",
      "Epoch 6/20  Iteration 904/3560 Training loss: 2.2607 0.7385 sec/batch\n",
      "Epoch 6/20  Iteration 905/3560 Training loss: 2.2609 0.5924 sec/batch\n",
      "Epoch 6/20  Iteration 906/3560 Training loss: 2.2601 0.6592 sec/batch\n",
      "Epoch 6/20  Iteration 907/3560 Training loss: 2.2601 0.6435 sec/batch\n",
      "Epoch 6/20  Iteration 908/3560 Training loss: 2.2621 0.6012 sec/batch\n",
      "Epoch 6/20  Iteration 909/3560 Training loss: 2.2620 0.6365 sec/batch\n",
      "Epoch 6/20  Iteration 910/3560 Training loss: 2.2611 0.7261 sec/batch\n",
      "Epoch 6/20  Iteration 911/3560 Training loss: 2.2607 0.6486 sec/batch\n",
      "Epoch 6/20  Iteration 912/3560 Training loss: 2.2616 0.6673 sec/batch\n",
      "Epoch 6/20  Iteration 913/3560 Training loss: 2.2614 0.6105 sec/batch\n",
      "Epoch 6/20  Iteration 914/3560 Training loss: 2.2608 0.6705 sec/batch\n",
      "Epoch 6/20  Iteration 915/3560 Training loss: 2.2600 0.6296 sec/batch\n",
      "Epoch 6/20  Iteration 916/3560 Training loss: 2.2600 0.6575 sec/batch\n",
      "Epoch 6/20  Iteration 917/3560 Training loss: 2.2596 0.6530 sec/batch\n",
      "Epoch 6/20  Iteration 918/3560 Training loss: 2.2595 0.6496 sec/batch\n",
      "Epoch 6/20  Iteration 919/3560 Training loss: 2.2597 0.7408 sec/batch\n",
      "Epoch 6/20  Iteration 920/3560 Training loss: 2.2602 0.7888 sec/batch\n",
      "Epoch 6/20  Iteration 921/3560 Training loss: 2.2603 0.7659 sec/batch\n",
      "Epoch 6/20  Iteration 922/3560 Training loss: 2.2600 0.7378 sec/batch\n",
      "Epoch 6/20  Iteration 923/3560 Training loss: 2.2598 0.8278 sec/batch\n",
      "Epoch 6/20  Iteration 924/3560 Training loss: 2.2601 0.7930 sec/batch\n",
      "Epoch 6/20  Iteration 925/3560 Training loss: 2.2599 0.7720 sec/batch\n",
      "Epoch 6/20  Iteration 926/3560 Training loss: 2.2599 0.7565 sec/batch\n",
      "Epoch 6/20  Iteration 927/3560 Training loss: 2.2596 0.7258 sec/batch\n",
      "Epoch 6/20  Iteration 928/3560 Training loss: 2.2587 0.7277 sec/batch\n",
      "Epoch 6/20  Iteration 929/3560 Training loss: 2.2580 0.7293 sec/batch\n",
      "Epoch 6/20  Iteration 930/3560 Training loss: 2.2571 0.6829 sec/batch\n",
      "Epoch 6/20  Iteration 931/3560 Training loss: 2.2566 0.6126 sec/batch\n",
      "Epoch 6/20  Iteration 932/3560 Training loss: 2.2560 0.6116 sec/batch\n",
      "Epoch 6/20  Iteration 933/3560 Training loss: 2.2554 0.6947 sec/batch\n",
      "Epoch 6/20  Iteration 934/3560 Training loss: 2.2548 0.5201 sec/batch\n",
      "Epoch 6/20  Iteration 935/3560 Training loss: 2.2542 0.6988 sec/batch\n",
      "Epoch 6/20  Iteration 936/3560 Training loss: 2.2529 0.7066 sec/batch\n",
      "Epoch 6/20  Iteration 937/3560 Training loss: 2.2529 0.6520 sec/batch\n",
      "Epoch 6/20  Iteration 938/3560 Training loss: 2.2523 0.6317 sec/batch\n",
      "Epoch 6/20  Iteration 939/3560 Training loss: 2.2523 0.6014 sec/batch\n",
      "Epoch 6/20  Iteration 940/3560 Training loss: 2.2528 0.6605 sec/batch\n",
      "Epoch 6/20  Iteration 941/3560 Training loss: 2.2522 0.6369 sec/batch\n",
      "Epoch 6/20  Iteration 942/3560 Training loss: 2.2522 0.6800 sec/batch\n",
      "Epoch 6/20  Iteration 943/3560 Training loss: 2.2520 0.6649 sec/batch\n",
      "Epoch 6/20  Iteration 944/3560 Training loss: 2.2516 0.6274 sec/batch\n",
      "Epoch 6/20  Iteration 945/3560 Training loss: 2.2512 0.6192 sec/batch\n",
      "Epoch 6/20  Iteration 946/3560 Training loss: 2.2511 0.6416 sec/batch\n",
      "Epoch 6/20  Iteration 947/3560 Training loss: 2.2511 0.7319 sec/batch\n",
      "Epoch 6/20  Iteration 948/3560 Training loss: 2.2507 0.6460 sec/batch\n",
      "Epoch 6/20  Iteration 949/3560 Training loss: 2.2502 0.6906 sec/batch\n",
      "Epoch 6/20  Iteration 950/3560 Training loss: 2.2502 0.6372 sec/batch\n",
      "Epoch 6/20  Iteration 951/3560 Training loss: 2.2507 0.6523 sec/batch\n",
      "Epoch 6/20  Iteration 952/3560 Training loss: 2.2514 0.6212 sec/batch\n",
      "Epoch 6/20  Iteration 953/3560 Training loss: 2.2522 0.6889 sec/batch\n",
      "Epoch 6/20  Iteration 954/3560 Training loss: 2.2525 0.7277 sec/batch\n",
      "Epoch 6/20  Iteration 955/3560 Training loss: 2.2527 0.6148 sec/batch\n",
      "Epoch 6/20  Iteration 956/3560 Training loss: 2.2534 0.6295 sec/batch\n",
      "Epoch 6/20  Iteration 957/3560 Training loss: 2.2538 0.6614 sec/batch\n",
      "Epoch 6/20  Iteration 958/3560 Training loss: 2.2536 0.6406 sec/batch\n",
      "Epoch 6/20  Iteration 959/3560 Training loss: 2.2535 0.6609 sec/batch\n",
      "Epoch 6/20  Iteration 960/3560 Training loss: 2.2540 0.6415 sec/batch\n",
      "Epoch 6/20  Iteration 961/3560 Training loss: 2.2543 0.6223 sec/batch\n",
      "Epoch 6/20  Iteration 962/3560 Training loss: 2.2547 0.6417 sec/batch\n",
      "Epoch 6/20  Iteration 963/3560 Training loss: 2.2549 0.6640 sec/batch\n",
      "Epoch 6/20  Iteration 964/3560 Training loss: 2.2549 0.7211 sec/batch\n",
      "Epoch 6/20  Iteration 965/3560 Training loss: 2.2549 0.7562 sec/batch\n",
      "Epoch 6/20  Iteration 966/3560 Training loss: 2.2554 0.7279 sec/batch\n",
      "Epoch 6/20  Iteration 967/3560 Training loss: 2.2551 0.7094 sec/batch\n",
      "Epoch 6/20  Iteration 968/3560 Training loss: 2.2551 0.6775 sec/batch\n",
      "Epoch 6/20  Iteration 969/3560 Training loss: 2.2549 0.6051 sec/batch\n",
      "Epoch 6/20  Iteration 970/3560 Training loss: 2.2546 0.6758 sec/batch\n",
      "Epoch 6/20  Iteration 971/3560 Training loss: 2.2543 0.6001 sec/batch\n",
      "Epoch 6/20  Iteration 972/3560 Training loss: 2.2543 0.6869 sec/batch\n",
      "Epoch 6/20  Iteration 973/3560 Training loss: 2.2540 0.6199 sec/batch\n",
      "Epoch 6/20  Iteration 974/3560 Training loss: 2.2538 0.6480 sec/batch\n",
      "Epoch 6/20  Iteration 975/3560 Training loss: 2.2533 0.6420 sec/batch\n",
      "Epoch 6/20  Iteration 976/3560 Training loss: 2.2529 0.6807 sec/batch\n",
      "Epoch 6/20  Iteration 977/3560 Training loss: 2.2527 0.6075 sec/batch\n",
      "Epoch 6/20  Iteration 978/3560 Training loss: 2.2523 0.7116 sec/batch\n",
      "Epoch 6/20  Iteration 979/3560 Training loss: 2.2520 0.6663 sec/batch\n",
      "Epoch 6/20  Iteration 980/3560 Training loss: 2.2520 0.7123 sec/batch\n",
      "Epoch 6/20  Iteration 981/3560 Training loss: 2.2518 0.6425 sec/batch\n",
      "Epoch 6/20  Iteration 982/3560 Training loss: 2.2517 0.6249 sec/batch\n",
      "Epoch 6/20  Iteration 983/3560 Training loss: 2.2513 0.7021 sec/batch\n",
      "Epoch 6/20  Iteration 984/3560 Training loss: 2.2509 0.6504 sec/batch\n",
      "Epoch 6/20  Iteration 985/3560 Training loss: 2.2505 0.7182 sec/batch\n",
      "Epoch 6/20  Iteration 986/3560 Training loss: 2.2502 0.6506 sec/batch\n",
      "Epoch 6/20  Iteration 987/3560 Training loss: 2.2501 0.7367 sec/batch\n",
      "Epoch 6/20  Iteration 988/3560 Training loss: 2.2499 0.6726 sec/batch\n",
      "Epoch 6/20  Iteration 989/3560 Training loss: 2.2496 0.6125 sec/batch\n",
      "Epoch 6/20  Iteration 990/3560 Training loss: 2.2493 0.6538 sec/batch\n",
      "Epoch 6/20  Iteration 991/3560 Training loss: 2.2492 0.6977 sec/batch\n",
      "Epoch 6/20  Iteration 992/3560 Training loss: 2.2491 0.6688 sec/batch\n",
      "Epoch 6/20  Iteration 993/3560 Training loss: 2.2487 0.7708 sec/batch\n",
      "Epoch 6/20  Iteration 994/3560 Training loss: 2.2485 0.6706 sec/batch\n",
      "Epoch 6/20  Iteration 995/3560 Training loss: 2.2481 0.6803 sec/batch\n",
      "Epoch 6/20  Iteration 996/3560 Training loss: 2.2480 0.5933 sec/batch\n",
      "Epoch 6/20  Iteration 997/3560 Training loss: 2.2477 0.6169 sec/batch\n",
      "Epoch 6/20  Iteration 998/3560 Training loss: 2.2477 0.7526 sec/batch\n",
      "Epoch 6/20  Iteration 999/3560 Training loss: 2.2477 0.7482 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3560 Training loss: 2.2474 0.6746 sec/batch\n",
      "Epoch 6/20  Iteration 1001/3560 Training loss: 2.2472 0.6069 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3560 Training loss: 2.2471 0.6575 sec/batch\n",
      "Epoch 6/20  Iteration 1003/3560 Training loss: 2.2469 0.6299 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3560 Training loss: 2.2466 0.6538 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3560 Training loss: 2.2462 0.7291 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3560 Training loss: 2.2457 0.6523 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 1007/3560 Training loss: 2.2455 0.6025 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3560 Training loss: 2.2454 0.6608 sec/batch\n",
      "Epoch 6/20  Iteration 1009/3560 Training loss: 2.2454 0.6560 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3560 Training loss: 2.2452 0.6942 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3560 Training loss: 2.2451 0.6657 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3560 Training loss: 2.2448 0.7570 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3560 Training loss: 2.2446 0.6640 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3560 Training loss: 2.2445 0.6583 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3560 Training loss: 2.2444 0.6560 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3560 Training loss: 2.2440 0.6224 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3560 Training loss: 2.2439 0.6751 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3560 Training loss: 2.2439 0.6809 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3560 Training loss: 2.2436 0.6300 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3560 Training loss: 2.2435 0.6567 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3560 Training loss: 2.2433 0.6473 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3560 Training loss: 2.2429 0.5860 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3560 Training loss: 2.2429 0.6875 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3560 Training loss: 2.2428 0.8005 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3560 Training loss: 2.2426 1.5834 sec/batch\n",
      "Epoch 6/20  Iteration 1026/3560 Training loss: 2.2424 0.9828 sec/batch\n",
      "Epoch 6/20  Iteration 1027/3560 Training loss: 2.2423 1.2825 sec/batch\n",
      "Epoch 6/20  Iteration 1028/3560 Training loss: 2.2421 0.8512 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3560 Training loss: 2.2422 0.8155 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3560 Training loss: 2.2419 1.0022 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3560 Training loss: 2.2419 0.8671 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3560 Training loss: 2.2416 0.8146 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3560 Training loss: 2.2415 0.8328 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3560 Training loss: 2.2413 0.7789 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3560 Training loss: 2.2410 0.6559 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3560 Training loss: 2.2410 0.8199 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3560 Training loss: 2.2409 0.6899 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3560 Training loss: 2.2409 0.7505 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3560 Training loss: 2.2407 0.6604 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3560 Training loss: 2.2405 0.7807 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3560 Training loss: 2.2404 0.6904 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3560 Training loss: 2.2404 0.7674 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3560 Training loss: 2.2403 0.8642 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3560 Training loss: 2.2402 0.8467 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3560 Training loss: 2.2400 0.6835 sec/batch\n",
      "Epoch 6/20  Iteration 1046/3560 Training loss: 2.2399 0.7364 sec/batch\n",
      "Epoch 6/20  Iteration 1047/3560 Training loss: 2.2397 0.7123 sec/batch\n",
      "Epoch 6/20  Iteration 1048/3560 Training loss: 2.2395 0.7517 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3560 Training loss: 2.2392 0.7498 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3560 Training loss: 2.2392 0.7232 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3560 Training loss: 2.2391 0.6943 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3560 Training loss: 2.2389 0.6401 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3560 Training loss: 2.2387 0.8937 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3560 Training loss: 2.2385 0.8808 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3560 Training loss: 2.2383 0.8153 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3560 Training loss: 2.2381 0.7481 sec/batch\n",
      "Epoch 6/20  Iteration 1057/3560 Training loss: 2.2380 0.6741 sec/batch\n",
      "Epoch 6/20  Iteration 1058/3560 Training loss: 2.2380 0.6816 sec/batch\n",
      "Epoch 6/20  Iteration 1059/3560 Training loss: 2.2379 0.7483 sec/batch\n",
      "Epoch 6/20  Iteration 1060/3560 Training loss: 2.2376 0.6355 sec/batch\n",
      "Epoch 6/20  Iteration 1061/3560 Training loss: 2.2374 0.7576 sec/batch\n",
      "Epoch 6/20  Iteration 1062/3560 Training loss: 2.2372 0.6682 sec/batch\n",
      "Epoch 6/20  Iteration 1063/3560 Training loss: 2.2371 0.6883 sec/batch\n",
      "Epoch 6/20  Iteration 1064/3560 Training loss: 2.2370 0.6741 sec/batch\n",
      "Epoch 6/20  Iteration 1065/3560 Training loss: 2.2369 0.5640 sec/batch\n",
      "Epoch 6/20  Iteration 1066/3560 Training loss: 2.2368 0.5733 sec/batch\n",
      "Epoch 6/20  Iteration 1067/3560 Training loss: 2.2366 0.7486 sec/batch\n",
      "Epoch 6/20  Iteration 1068/3560 Training loss: 2.2363 0.6213 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3560 Training loss: 2.2799 0.7155 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3560 Training loss: 2.2365 0.6373 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3560 Training loss: 2.2185 0.5425 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3560 Training loss: 2.2088 0.6272 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3560 Training loss: 2.2062 0.6514 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3560 Training loss: 2.2019 0.6483 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3560 Training loss: 2.2031 0.6390 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3560 Training loss: 2.2055 0.6811 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3560 Training loss: 2.2078 0.6300 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3560 Training loss: 2.2068 0.6452 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3560 Training loss: 2.2043 0.7129 sec/batch\n",
      "Epoch 7/20  Iteration 1080/3560 Training loss: 2.2030 0.6512 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3560 Training loss: 2.2035 0.6410 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3560 Training loss: 2.2056 0.7383 sec/batch\n",
      "Epoch 7/20  Iteration 1083/3560 Training loss: 2.2060 0.7263 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3560 Training loss: 2.2053 0.5341 sec/batch\n",
      "Epoch 7/20  Iteration 1085/3560 Training loss: 2.2053 0.6376 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3560 Training loss: 2.2077 0.6296 sec/batch\n",
      "Epoch 7/20  Iteration 1087/3560 Training loss: 2.2077 0.6660 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3560 Training loss: 2.2068 0.6415 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3560 Training loss: 2.2061 0.7046 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3560 Training loss: 2.2069 0.6697 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3560 Training loss: 2.2066 0.5969 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3560 Training loss: 2.2058 0.6464 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3560 Training loss: 2.2048 0.6820 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3560 Training loss: 2.2043 0.7607 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3560 Training loss: 2.2037 0.6291 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3560 Training loss: 2.2038 0.6593 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3560 Training loss: 2.2043 0.5793 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3560 Training loss: 2.2045 0.6894 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3560 Training loss: 2.2046 0.6715 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3560 Training loss: 2.2040 0.6296 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3560 Training loss: 2.2037 0.6965 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3560 Training loss: 2.2042 0.6199 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3560 Training loss: 2.2040 0.5900 sec/batch\n",
      "Epoch 7/20  Iteration 1104/3560 Training loss: 2.2041 0.6196 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3560 Training loss: 2.2036 0.6989 sec/batch\n",
      "Epoch 7/20  Iteration 1106/3560 Training loss: 2.2027 0.6630 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3560 Training loss: 2.2017 0.6519 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3560 Training loss: 2.2011 0.6679 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3560 Training loss: 2.2003 0.6249 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3560 Training loss: 2.2000 0.6504 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3560 Training loss: 2.1992 0.5540 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3560 Training loss: 2.1985 0.6429 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3560 Training loss: 2.1982 0.7324 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3560 Training loss: 2.1969 0.7005 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3560 Training loss: 2.1971 0.6029 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3560 Training loss: 2.1968 0.6801 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3560 Training loss: 2.1965 0.6466 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3560 Training loss: 2.1970 0.6815 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3560 Training loss: 2.1963 0.6968 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3560 Training loss: 2.1965 0.6657 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3560 Training loss: 2.1962 0.7920 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3560 Training loss: 2.1960 0.6876 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1123/3560 Training loss: 2.1957 0.6595 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3560 Training loss: 2.1956 0.7124 sec/batch\n",
      "Epoch 7/20  Iteration 1125/3560 Training loss: 2.1955 0.6774 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3560 Training loss: 2.1951 0.7502 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3560 Training loss: 2.1947 0.6253 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3560 Training loss: 2.1949 0.6206 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3560 Training loss: 2.1949 0.5545 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3560 Training loss: 2.1951 0.7190 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3560 Training loss: 2.1953 0.7453 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3560 Training loss: 2.1952 0.6813 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3560 Training loss: 2.1950 0.7407 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3560 Training loss: 2.1951 0.6118 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3560 Training loss: 2.1953 0.7469 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3560 Training loss: 2.1948 0.6320 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3560 Training loss: 2.1947 0.6469 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3560 Training loss: 2.1948 0.7395 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3560 Training loss: 2.1951 0.6811 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3560 Training loss: 2.1951 0.6063 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3560 Training loss: 2.1953 0.6822 sec/batch\n",
      "Epoch 7/20  Iteration 1142/3560 Training loss: 2.1950 0.6467 sec/batch\n",
      "Epoch 7/20  Iteration 1143/3560 Training loss: 2.1950 0.6949 sec/batch\n",
      "Epoch 7/20  Iteration 1144/3560 Training loss: 2.1953 0.6945 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3560 Training loss: 2.1951 0.7144 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3560 Training loss: 2.1950 0.6284 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3560 Training loss: 2.1948 0.5948 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3560 Training loss: 2.1946 0.6325 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3560 Training loss: 2.1942 0.6998 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3560 Training loss: 2.1942 0.7255 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3560 Training loss: 2.1940 0.8846 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3560 Training loss: 2.1937 0.8960 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3560 Training loss: 2.1932 0.9437 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3560 Training loss: 2.1930 0.7602 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3560 Training loss: 2.1929 0.8400 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3560 Training loss: 2.1927 0.7099 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3560 Training loss: 2.1923 0.8034 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3560 Training loss: 2.1924 0.8166 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3560 Training loss: 2.1921 0.6942 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3560 Training loss: 2.1920 0.8043 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3560 Training loss: 2.1916 0.7145 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3560 Training loss: 2.1913 0.7761 sec/batch\n",
      "Epoch 7/20  Iteration 1163/3560 Training loss: 2.1909 0.6341 sec/batch\n",
      "Epoch 7/20  Iteration 1164/3560 Training loss: 2.1907 0.7015 sec/batch\n",
      "Epoch 7/20  Iteration 1165/3560 Training loss: 2.1906 0.6159 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3560 Training loss: 2.1904 0.5785 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3560 Training loss: 2.1902 0.6640 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3560 Training loss: 2.1899 0.7707 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3560 Training loss: 2.1899 0.6059 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3560 Training loss: 2.1899 0.6425 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3560 Training loss: 2.1895 0.5821 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3560 Training loss: 2.1892 0.6535 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3560 Training loss: 2.1889 0.5952 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3560 Training loss: 2.1887 0.6419 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3560 Training loss: 2.1885 0.6457 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3560 Training loss: 2.1884 0.6895 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3560 Training loss: 2.1885 0.6333 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3560 Training loss: 2.1882 0.6915 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3560 Training loss: 2.1881 0.6471 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3560 Training loss: 2.1880 0.6418 sec/batch\n",
      "Epoch 7/20  Iteration 1181/3560 Training loss: 2.1878 0.6523 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3560 Training loss: 2.1876 0.6001 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3560 Training loss: 2.1873 0.7073 sec/batch\n",
      "Epoch 7/20  Iteration 1184/3560 Training loss: 2.1869 0.6583 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3560 Training loss: 2.1867 0.6060 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3560 Training loss: 2.1865 0.6119 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3560 Training loss: 2.1865 0.5989 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3560 Training loss: 2.1864 0.6862 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3560 Training loss: 2.1864 0.6641 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3560 Training loss: 2.1862 0.5639 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3560 Training loss: 2.1859 0.7113 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3560 Training loss: 2.1860 0.5623 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3560 Training loss: 2.1859 0.6655 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3560 Training loss: 2.1855 0.7471 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3560 Training loss: 2.1856 0.6295 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3560 Training loss: 2.1855 0.6274 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3560 Training loss: 2.1854 0.6431 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3560 Training loss: 2.1852 0.6101 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3560 Training loss: 2.1851 0.6960 sec/batch\n",
      "Epoch 7/20  Iteration 1200/3560 Training loss: 2.1847 0.6083 sec/batch\n",
      "Epoch 7/20  Iteration 1201/3560 Training loss: 2.1847 0.7447 sec/batch\n",
      "Epoch 7/20  Iteration 1202/3560 Training loss: 2.1846 0.6537 sec/batch\n",
      "Epoch 7/20  Iteration 1203/3560 Training loss: 2.1844 0.6315 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3560 Training loss: 2.1843 0.5586 sec/batch\n",
      "Epoch 7/20  Iteration 1205/3560 Training loss: 2.1842 0.6738 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3560 Training loss: 2.1841 0.6550 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3560 Training loss: 2.1842 0.6537 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3560 Training loss: 2.1840 0.6245 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3560 Training loss: 2.1840 0.6232 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3560 Training loss: 2.1837 0.6234 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3560 Training loss: 2.1836 0.6332 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3560 Training loss: 2.1835 0.6050 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3560 Training loss: 2.1832 0.6854 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3560 Training loss: 2.1833 0.5965 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3560 Training loss: 2.1833 0.6166 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3560 Training loss: 2.1833 0.6582 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3560 Training loss: 2.1832 0.5900 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3560 Training loss: 2.1830 0.6226 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3560 Training loss: 2.1829 0.7551 sec/batch\n",
      "Epoch 7/20  Iteration 1220/3560 Training loss: 2.1831 0.6398 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3560 Training loss: 2.1829 0.6379 sec/batch\n",
      "Epoch 7/20  Iteration 1222/3560 Training loss: 2.1829 0.5927 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3560 Training loss: 2.1828 0.6936 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3560 Training loss: 2.1826 0.5884 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3560 Training loss: 2.1825 0.6276 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3560 Training loss: 2.1824 0.6340 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3560 Training loss: 2.1822 0.5932 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3560 Training loss: 2.1822 0.6982 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3560 Training loss: 2.1822 0.5847 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3560 Training loss: 2.1820 0.6784 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3560 Training loss: 2.1819 0.6972 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3560 Training loss: 2.1817 0.6725 sec/batch\n",
      "Epoch 7/20  Iteration 1233/3560 Training loss: 2.1816 0.6202 sec/batch\n",
      "Epoch 7/20  Iteration 1234/3560 Training loss: 2.1815 0.6486 sec/batch\n",
      "Epoch 7/20  Iteration 1235/3560 Training loss: 2.1815 0.6521 sec/batch\n",
      "Epoch 7/20  Iteration 1236/3560 Training loss: 2.1815 0.6066 sec/batch\n",
      "Epoch 7/20  Iteration 1237/3560 Training loss: 2.1814 0.6334 sec/batch\n",
      "Epoch 7/20  Iteration 1238/3560 Training loss: 2.1812 0.6470 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1239/3560 Training loss: 2.1810 0.6438 sec/batch\n",
      "Epoch 7/20  Iteration 1240/3560 Training loss: 2.1809 0.6969 sec/batch\n",
      "Epoch 7/20  Iteration 1241/3560 Training loss: 2.1809 0.6670 sec/batch\n",
      "Epoch 7/20  Iteration 1242/3560 Training loss: 2.1809 0.5938 sec/batch\n",
      "Epoch 7/20  Iteration 1243/3560 Training loss: 2.1809 0.6108 sec/batch\n",
      "Epoch 7/20  Iteration 1244/3560 Training loss: 2.1808 0.7057 sec/batch\n",
      "Epoch 7/20  Iteration 1245/3560 Training loss: 2.1807 0.6756 sec/batch\n",
      "Epoch 7/20  Iteration 1246/3560 Training loss: 2.1805 0.6660 sec/batch\n",
      "Epoch 8/20  Iteration 1247/3560 Training loss: 2.2270 0.6780 sec/batch\n",
      "Epoch 8/20  Iteration 1248/3560 Training loss: 2.1853 0.6569 sec/batch\n",
      "Epoch 8/20  Iteration 1249/3560 Training loss: 2.1699 0.5408 sec/batch\n",
      "Epoch 8/20  Iteration 1250/3560 Training loss: 2.1637 0.6441 sec/batch\n",
      "Epoch 8/20  Iteration 1251/3560 Training loss: 2.1619 0.6738 sec/batch\n",
      "Epoch 8/20  Iteration 1252/3560 Training loss: 2.1557 0.6257 sec/batch\n",
      "Epoch 8/20  Iteration 1253/3560 Training loss: 2.1568 0.6765 sec/batch\n",
      "Epoch 8/20  Iteration 1254/3560 Training loss: 2.1578 0.7106 sec/batch\n",
      "Epoch 8/20  Iteration 1255/3560 Training loss: 2.1591 0.6318 sec/batch\n",
      "Epoch 8/20  Iteration 1256/3560 Training loss: 2.1576 0.6592 sec/batch\n",
      "Epoch 8/20  Iteration 1257/3560 Training loss: 2.1546 0.7641 sec/batch\n",
      "Epoch 8/20  Iteration 1258/3560 Training loss: 2.1540 0.6057 sec/batch\n",
      "Epoch 8/20  Iteration 1259/3560 Training loss: 2.1543 0.6069 sec/batch\n",
      "Epoch 8/20  Iteration 1260/3560 Training loss: 2.1564 0.6986 sec/batch\n",
      "Epoch 8/20  Iteration 1261/3560 Training loss: 2.1556 0.5787 sec/batch\n",
      "Epoch 8/20  Iteration 1262/3560 Training loss: 2.1548 0.6527 sec/batch\n",
      "Epoch 8/20  Iteration 1263/3560 Training loss: 2.1549 0.5945 sec/batch\n",
      "Epoch 8/20  Iteration 1264/3560 Training loss: 2.1573 0.6385 sec/batch\n",
      "Epoch 8/20  Iteration 1265/3560 Training loss: 2.1576 0.6725 sec/batch\n",
      "Epoch 8/20  Iteration 1266/3560 Training loss: 2.1568 0.6762 sec/batch\n",
      "Epoch 8/20  Iteration 1267/3560 Training loss: 2.1559 0.6335 sec/batch\n",
      "Epoch 8/20  Iteration 1268/3560 Training loss: 2.1570 0.6288 sec/batch\n",
      "Epoch 8/20  Iteration 1269/3560 Training loss: 2.1564 0.6664 sec/batch\n",
      "Epoch 8/20  Iteration 1270/3560 Training loss: 2.1563 0.6177 sec/batch\n",
      "Epoch 8/20  Iteration 1271/3560 Training loss: 2.1555 0.7217 sec/batch\n",
      "Epoch 8/20  Iteration 1272/3560 Training loss: 2.1552 0.6713 sec/batch\n",
      "Epoch 8/20  Iteration 1273/3560 Training loss: 2.1548 0.5926 sec/batch\n",
      "Epoch 8/20  Iteration 1274/3560 Training loss: 2.1552 0.6107 sec/batch\n",
      "Epoch 8/20  Iteration 1275/3560 Training loss: 2.1557 0.6085 sec/batch\n",
      "Epoch 8/20  Iteration 1276/3560 Training loss: 2.1563 0.6380 sec/batch\n",
      "Epoch 8/20  Iteration 1277/3560 Training loss: 2.1567 0.6886 sec/batch\n",
      "Epoch 8/20  Iteration 1278/3560 Training loss: 2.1560 0.6624 sec/batch\n",
      "Epoch 8/20  Iteration 1279/3560 Training loss: 2.1558 0.5969 sec/batch\n",
      "Epoch 8/20  Iteration 1280/3560 Training loss: 2.1567 0.6830 sec/batch\n",
      "Epoch 8/20  Iteration 1281/3560 Training loss: 2.1561 0.6039 sec/batch\n",
      "Epoch 8/20  Iteration 1282/3560 Training loss: 2.1559 0.6505 sec/batch\n",
      "Epoch 8/20  Iteration 1283/3560 Training loss: 2.1556 0.7068 sec/batch\n",
      "Epoch 8/20  Iteration 1284/3560 Training loss: 2.1549 0.6293 sec/batch\n",
      "Epoch 8/20  Iteration 1285/3560 Training loss: 2.1540 0.6261 sec/batch\n",
      "Epoch 8/20  Iteration 1286/3560 Training loss: 2.1532 0.6162 sec/batch\n",
      "Epoch 8/20  Iteration 1287/3560 Training loss: 2.1527 0.6275 sec/batch\n",
      "Epoch 8/20  Iteration 1288/3560 Training loss: 2.1524 0.6284 sec/batch\n",
      "Epoch 8/20  Iteration 1289/3560 Training loss: 2.1518 0.5901 sec/batch\n",
      "Epoch 8/20  Iteration 1290/3560 Training loss: 2.1510 0.7158 sec/batch\n",
      "Epoch 8/20  Iteration 1291/3560 Training loss: 2.1509 0.6710 sec/batch\n",
      "Epoch 8/20  Iteration 1292/3560 Training loss: 2.1496 0.6572 sec/batch\n",
      "Epoch 8/20  Iteration 1293/3560 Training loss: 2.1497 0.6623 sec/batch\n",
      "Epoch 8/20  Iteration 1294/3560 Training loss: 2.1493 0.6509 sec/batch\n",
      "Epoch 8/20  Iteration 1295/3560 Training loss: 2.1492 0.6487 sec/batch\n",
      "Epoch 8/20  Iteration 1296/3560 Training loss: 2.1497 0.6814 sec/batch\n",
      "Epoch 8/20  Iteration 1297/3560 Training loss: 2.1490 0.6296 sec/batch\n",
      "Epoch 8/20  Iteration 1298/3560 Training loss: 2.1493 0.6181 sec/batch\n",
      "Epoch 8/20  Iteration 1299/3560 Training loss: 2.1490 0.6641 sec/batch\n",
      "Epoch 8/20  Iteration 1300/3560 Training loss: 2.1487 0.6756 sec/batch\n",
      "Epoch 8/20  Iteration 1301/3560 Training loss: 2.1483 0.5284 sec/batch\n",
      "Epoch 8/20  Iteration 1302/3560 Training loss: 2.1484 0.7201 sec/batch\n",
      "Epoch 8/20  Iteration 1303/3560 Training loss: 2.1483 0.7091 sec/batch\n",
      "Epoch 8/20  Iteration 1304/3560 Training loss: 2.1481 0.6746 sec/batch\n",
      "Epoch 8/20  Iteration 1305/3560 Training loss: 2.1478 0.5956 sec/batch\n",
      "Epoch 8/20  Iteration 1306/3560 Training loss: 2.1479 0.6078 sec/batch\n",
      "Epoch 8/20  Iteration 1307/3560 Training loss: 2.1476 0.6998 sec/batch\n",
      "Epoch 8/20  Iteration 1308/3560 Training loss: 2.1479 0.5956 sec/batch\n",
      "Epoch 8/20  Iteration 1309/3560 Training loss: 2.1481 0.7334 sec/batch\n",
      "Epoch 8/20  Iteration 1310/3560 Training loss: 2.1481 0.6541 sec/batch\n",
      "Epoch 8/20  Iteration 1311/3560 Training loss: 2.1478 0.6137 sec/batch\n",
      "Epoch 8/20  Iteration 1312/3560 Training loss: 2.1481 0.5983 sec/batch\n",
      "Epoch 8/20  Iteration 1313/3560 Training loss: 2.1480 0.6397 sec/batch\n",
      "Epoch 8/20  Iteration 1314/3560 Training loss: 2.1474 0.6389 sec/batch\n",
      "Epoch 8/20  Iteration 1315/3560 Training loss: 2.1471 0.5845 sec/batch\n",
      "Epoch 8/20  Iteration 1316/3560 Training loss: 2.1471 0.6828 sec/batch\n",
      "Epoch 8/20  Iteration 1317/3560 Training loss: 2.1473 0.7199 sec/batch\n",
      "Epoch 8/20  Iteration 1318/3560 Training loss: 2.1474 0.6079 sec/batch\n",
      "Epoch 8/20  Iteration 1319/3560 Training loss: 2.1475 0.6812 sec/batch\n",
      "Epoch 8/20  Iteration 1320/3560 Training loss: 2.1472 0.6321 sec/batch\n",
      "Epoch 8/20  Iteration 1321/3560 Training loss: 2.1471 0.6273 sec/batch\n",
      "Epoch 8/20  Iteration 1322/3560 Training loss: 2.1475 0.7597 sec/batch\n",
      "Epoch 8/20  Iteration 1323/3560 Training loss: 2.1473 0.6133 sec/batch\n",
      "Epoch 8/20  Iteration 1324/3560 Training loss: 2.1472 0.5968 sec/batch\n",
      "Epoch 8/20  Iteration 1325/3560 Training loss: 2.1469 0.6387 sec/batch\n",
      "Epoch 8/20  Iteration 1326/3560 Training loss: 2.1465 0.6185 sec/batch\n",
      "Epoch 8/20  Iteration 1327/3560 Training loss: 2.1461 0.6410 sec/batch\n",
      "Epoch 8/20  Iteration 1328/3560 Training loss: 2.1462 0.7132 sec/batch\n",
      "Epoch 8/20  Iteration 1329/3560 Training loss: 2.1458 0.5983 sec/batch\n",
      "Epoch 8/20  Iteration 1330/3560 Training loss: 2.1455 0.7219 sec/batch\n",
      "Epoch 8/20  Iteration 1331/3560 Training loss: 2.1448 0.5816 sec/batch\n",
      "Epoch 8/20  Iteration 1332/3560 Training loss: 2.1446 0.6695 sec/batch\n",
      "Epoch 8/20  Iteration 1333/3560 Training loss: 2.1445 0.6693 sec/batch\n",
      "Epoch 8/20  Iteration 1334/3560 Training loss: 2.1442 0.6404 sec/batch\n",
      "Epoch 8/20  Iteration 1335/3560 Training loss: 2.1438 0.6756 sec/batch\n",
      "Epoch 8/20  Iteration 1336/3560 Training loss: 2.1437 0.6318 sec/batch\n",
      "Epoch 8/20  Iteration 1337/3560 Training loss: 2.1435 0.7005 sec/batch\n",
      "Epoch 8/20  Iteration 1338/3560 Training loss: 2.1435 0.6597 sec/batch\n",
      "Epoch 8/20  Iteration 1339/3560 Training loss: 2.1431 0.7644 sec/batch\n",
      "Epoch 8/20  Iteration 1340/3560 Training loss: 2.1427 0.7609 sec/batch\n",
      "Epoch 8/20  Iteration 1341/3560 Training loss: 2.1423 0.7022 sec/batch\n",
      "Epoch 8/20  Iteration 1342/3560 Training loss: 2.1419 0.7240 sec/batch\n",
      "Epoch 8/20  Iteration 1343/3560 Training loss: 2.1418 0.6017 sec/batch\n",
      "Epoch 8/20  Iteration 1344/3560 Training loss: 2.1415 0.6610 sec/batch\n",
      "Epoch 8/20  Iteration 1345/3560 Training loss: 2.1412 0.6670 sec/batch\n",
      "Epoch 8/20  Iteration 1346/3560 Training loss: 2.1408 0.7803 sec/batch\n",
      "Epoch 8/20  Iteration 1347/3560 Training loss: 2.1408 0.6255 sec/batch\n",
      "Epoch 8/20  Iteration 1348/3560 Training loss: 2.1406 0.6600 sec/batch\n",
      "Epoch 8/20  Iteration 1349/3560 Training loss: 2.1403 0.6003 sec/batch\n",
      "Epoch 8/20  Iteration 1350/3560 Training loss: 2.1400 0.6628 sec/batch\n",
      "Epoch 8/20  Iteration 1351/3560 Training loss: 2.1398 0.6750 sec/batch\n",
      "Epoch 8/20  Iteration 1352/3560 Training loss: 2.1398 0.6502 sec/batch\n",
      "Epoch 8/20  Iteration 1353/3560 Training loss: 2.1395 0.8337 sec/batch\n",
      "Epoch 8/20  Iteration 1354/3560 Training loss: 2.1394 0.6335 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1355/3560 Training loss: 2.1395 0.7761 sec/batch\n",
      "Epoch 8/20  Iteration 1356/3560 Training loss: 2.1392 0.6435 sec/batch\n",
      "Epoch 8/20  Iteration 1357/3560 Training loss: 2.1391 0.7659 sec/batch\n",
      "Epoch 8/20  Iteration 1358/3560 Training loss: 2.1391 0.7032 sec/batch\n",
      "Epoch 8/20  Iteration 1359/3560 Training loss: 2.1389 0.6737 sec/batch\n",
      "Epoch 8/20  Iteration 1360/3560 Training loss: 2.1387 0.6812 sec/batch\n",
      "Epoch 8/20  Iteration 1361/3560 Training loss: 2.1385 0.6630 sec/batch\n",
      "Epoch 8/20  Iteration 1362/3560 Training loss: 2.1381 0.6106 sec/batch\n",
      "Epoch 8/20  Iteration 1363/3560 Training loss: 2.1380 0.6365 sec/batch\n",
      "Epoch 8/20  Iteration 1364/3560 Training loss: 2.1379 0.6508 sec/batch\n",
      "Epoch 8/20  Iteration 1365/3560 Training loss: 2.1379 0.6252 sec/batch\n",
      "Epoch 8/20  Iteration 1366/3560 Training loss: 2.1378 0.6408 sec/batch\n",
      "Epoch 8/20  Iteration 1367/3560 Training loss: 2.1378 0.6616 sec/batch\n",
      "Epoch 8/20  Iteration 1368/3560 Training loss: 2.1377 0.7561 sec/batch\n",
      "Epoch 8/20  Iteration 1369/3560 Training loss: 2.1374 0.7637 sec/batch\n",
      "Epoch 8/20  Iteration 1370/3560 Training loss: 2.1375 0.6438 sec/batch\n",
      "Epoch 8/20  Iteration 1371/3560 Training loss: 2.1374 0.6650 sec/batch\n",
      "Epoch 8/20  Iteration 1372/3560 Training loss: 2.1371 0.7564 sec/batch\n",
      "Epoch 8/20  Iteration 1373/3560 Training loss: 2.1370 0.6869 sec/batch\n",
      "Epoch 8/20  Iteration 1374/3560 Training loss: 2.1371 0.6251 sec/batch\n",
      "Epoch 8/20  Iteration 1375/3560 Training loss: 2.1370 0.6752 sec/batch\n",
      "Epoch 8/20  Iteration 1376/3560 Training loss: 2.1369 0.6669 sec/batch\n",
      "Epoch 8/20  Iteration 1377/3560 Training loss: 2.1367 0.7280 sec/batch\n",
      "Epoch 8/20  Iteration 1378/3560 Training loss: 2.1363 0.7371 sec/batch\n",
      "Epoch 8/20  Iteration 1379/3560 Training loss: 2.1363 0.6645 sec/batch\n",
      "Epoch 8/20  Iteration 1380/3560 Training loss: 2.1363 0.6315 sec/batch\n",
      "Epoch 8/20  Iteration 1381/3560 Training loss: 2.1361 0.6959 sec/batch\n",
      "Epoch 8/20  Iteration 1382/3560 Training loss: 2.1361 0.7022 sec/batch\n",
      "Epoch 8/20  Iteration 1383/3560 Training loss: 2.1361 0.7622 sec/batch\n",
      "Epoch 8/20  Iteration 1384/3560 Training loss: 2.1361 0.6483 sec/batch\n",
      "Epoch 8/20  Iteration 1385/3560 Training loss: 2.1362 0.7102 sec/batch\n",
      "Epoch 8/20  Iteration 1386/3560 Training loss: 2.1359 0.6419 sec/batch\n",
      "Epoch 8/20  Iteration 1387/3560 Training loss: 2.1359 0.6388 sec/batch\n",
      "Epoch 8/20  Iteration 1388/3560 Training loss: 2.1358 0.6300 sec/batch\n",
      "Epoch 8/20  Iteration 1389/3560 Training loss: 2.1357 0.6483 sec/batch\n",
      "Epoch 8/20  Iteration 1390/3560 Training loss: 2.1356 0.8667 sec/batch\n",
      "Epoch 8/20  Iteration 1391/3560 Training loss: 2.1354 0.9649 sec/batch\n",
      "Epoch 8/20  Iteration 1392/3560 Training loss: 2.1355 0.9643 sec/batch\n",
      "Epoch 8/20  Iteration 1393/3560 Training loss: 2.1354 0.7941 sec/batch\n",
      "Epoch 8/20  Iteration 1394/3560 Training loss: 2.1356 0.6635 sec/batch\n",
      "Epoch 8/20  Iteration 1395/3560 Training loss: 2.1355 0.9160 sec/batch\n",
      "Epoch 8/20  Iteration 1396/3560 Training loss: 2.1353 0.9355 sec/batch\n",
      "Epoch 8/20  Iteration 1397/3560 Training loss: 2.1353 0.9825 sec/batch\n",
      "Epoch 8/20  Iteration 1398/3560 Training loss: 2.1355 1.2804 sec/batch\n",
      "Epoch 8/20  Iteration 1399/3560 Training loss: 2.1353 1.1090 sec/batch\n",
      "Epoch 8/20  Iteration 1400/3560 Training loss: 2.1353 1.0382 sec/batch\n",
      "Epoch 8/20  Iteration 1401/3560 Training loss: 2.1352 0.8915 sec/batch\n",
      "Epoch 8/20  Iteration 1402/3560 Training loss: 2.1351 0.9314 sec/batch\n",
      "Epoch 8/20  Iteration 1403/3560 Training loss: 2.1350 0.9136 sec/batch\n",
      "Epoch 8/20  Iteration 1404/3560 Training loss: 2.1349 0.8437 sec/batch\n",
      "Epoch 8/20  Iteration 1405/3560 Training loss: 2.1347 0.7547 sec/batch\n",
      "Epoch 8/20  Iteration 1406/3560 Training loss: 2.1348 0.8172 sec/batch\n",
      "Epoch 8/20  Iteration 1407/3560 Training loss: 2.1348 0.6402 sec/batch\n",
      "Epoch 8/20  Iteration 1408/3560 Training loss: 2.1346 0.7695 sec/batch\n",
      "Epoch 8/20  Iteration 1409/3560 Training loss: 2.1346 0.8750 sec/batch\n",
      "Epoch 8/20  Iteration 1410/3560 Training loss: 2.1345 0.6688 sec/batch\n",
      "Epoch 8/20  Iteration 1411/3560 Training loss: 2.1344 0.6363 sec/batch\n",
      "Epoch 8/20  Iteration 1412/3560 Training loss: 2.1343 0.6280 sec/batch\n",
      "Epoch 8/20  Iteration 1413/3560 Training loss: 2.1342 0.6112 sec/batch\n",
      "Epoch 8/20  Iteration 1414/3560 Training loss: 2.1344 0.6502 sec/batch\n",
      "Epoch 8/20  Iteration 1415/3560 Training loss: 2.1343 0.6353 sec/batch\n",
      "Epoch 8/20  Iteration 1416/3560 Training loss: 2.1341 0.6965 sec/batch\n",
      "Epoch 8/20  Iteration 1417/3560 Training loss: 2.1340 0.6801 sec/batch\n",
      "Epoch 8/20  Iteration 1418/3560 Training loss: 2.1338 0.6473 sec/batch\n",
      "Epoch 8/20  Iteration 1419/3560 Training loss: 2.1339 0.7186 sec/batch\n",
      "Epoch 8/20  Iteration 1420/3560 Training loss: 2.1339 0.7323 sec/batch\n",
      "Epoch 8/20  Iteration 1421/3560 Training loss: 2.1339 0.8129 sec/batch\n",
      "Epoch 8/20  Iteration 1422/3560 Training loss: 2.1338 0.6102 sec/batch\n",
      "Epoch 8/20  Iteration 1423/3560 Training loss: 2.1336 0.6784 sec/batch\n",
      "Epoch 8/20  Iteration 1424/3560 Training loss: 2.1334 0.6330 sec/batch\n",
      "Epoch 9/20  Iteration 1425/3560 Training loss: 2.1857 0.6371 sec/batch\n",
      "Epoch 9/20  Iteration 1426/3560 Training loss: 2.1396 0.6164 sec/batch\n",
      "Epoch 9/20  Iteration 1427/3560 Training loss: 2.1222 0.6356 sec/batch\n",
      "Epoch 9/20  Iteration 1428/3560 Training loss: 2.1169 0.6381 sec/batch\n",
      "Epoch 9/20  Iteration 1429/3560 Training loss: 2.1137 0.6732 sec/batch\n",
      "Epoch 9/20  Iteration 1430/3560 Training loss: 2.1095 0.6500 sec/batch\n",
      "Epoch 9/20  Iteration 1431/3560 Training loss: 2.1117 0.6765 sec/batch\n",
      "Epoch 9/20  Iteration 1432/3560 Training loss: 2.1135 0.7335 sec/batch\n",
      "Epoch 9/20  Iteration 1433/3560 Training loss: 2.1160 0.5778 sec/batch\n",
      "Epoch 9/20  Iteration 1434/3560 Training loss: 2.1163 0.7610 sec/batch\n",
      "Epoch 9/20  Iteration 1435/3560 Training loss: 2.1140 0.5611 sec/batch\n",
      "Epoch 9/20  Iteration 1436/3560 Training loss: 2.1137 0.6996 sec/batch\n",
      "Epoch 9/20  Iteration 1437/3560 Training loss: 2.1139 0.6718 sec/batch\n",
      "Epoch 9/20  Iteration 1438/3560 Training loss: 2.1159 0.6101 sec/batch\n",
      "Epoch 9/20  Iteration 1439/3560 Training loss: 2.1165 0.5895 sec/batch\n",
      "Epoch 9/20  Iteration 1440/3560 Training loss: 2.1156 0.6151 sec/batch\n",
      "Epoch 9/20  Iteration 1441/3560 Training loss: 2.1157 0.6749 sec/batch\n",
      "Epoch 9/20  Iteration 1442/3560 Training loss: 2.1177 0.6478 sec/batch\n",
      "Epoch 9/20  Iteration 1443/3560 Training loss: 2.1183 0.6459 sec/batch\n",
      "Epoch 9/20  Iteration 1444/3560 Training loss: 2.1176 0.6514 sec/batch\n",
      "Epoch 9/20  Iteration 1445/3560 Training loss: 2.1166 0.6235 sec/batch\n",
      "Epoch 9/20  Iteration 1446/3560 Training loss: 2.1179 0.6648 sec/batch\n",
      "Epoch 9/20  Iteration 1447/3560 Training loss: 2.1171 0.6409 sec/batch\n",
      "Epoch 9/20  Iteration 1448/3560 Training loss: 2.1166 0.6242 sec/batch\n",
      "Epoch 9/20  Iteration 1449/3560 Training loss: 2.1165 0.6141 sec/batch\n",
      "Epoch 9/20  Iteration 1450/3560 Training loss: 2.1160 0.6011 sec/batch\n",
      "Epoch 9/20  Iteration 1451/3560 Training loss: 2.1156 0.6538 sec/batch\n",
      "Epoch 9/20  Iteration 1452/3560 Training loss: 2.1160 0.6173 sec/batch\n",
      "Epoch 9/20  Iteration 1453/3560 Training loss: 2.1165 0.6959 sec/batch\n",
      "Epoch 9/20  Iteration 1454/3560 Training loss: 2.1170 0.6549 sec/batch\n",
      "Epoch 9/20  Iteration 1455/3560 Training loss: 2.1173 0.6945 sec/batch\n",
      "Epoch 9/20  Iteration 1456/3560 Training loss: 2.1163 0.6525 sec/batch\n",
      "Epoch 9/20  Iteration 1457/3560 Training loss: 2.1164 0.6580 sec/batch\n",
      "Epoch 9/20  Iteration 1458/3560 Training loss: 2.1171 0.6119 sec/batch\n",
      "Epoch 9/20  Iteration 1459/3560 Training loss: 2.1167 0.7340 sec/batch\n",
      "Epoch 9/20  Iteration 1460/3560 Training loss: 2.1166 0.6291 sec/batch\n",
      "Epoch 9/20  Iteration 1461/3560 Training loss: 2.1164 0.6431 sec/batch\n",
      "Epoch 9/20  Iteration 1462/3560 Training loss: 2.1155 0.5155 sec/batch\n",
      "Epoch 9/20  Iteration 1463/3560 Training loss: 2.1145 0.6451 sec/batch\n",
      "Epoch 9/20  Iteration 1464/3560 Training loss: 2.1141 0.6105 sec/batch\n",
      "Epoch 9/20  Iteration 1465/3560 Training loss: 2.1136 0.5707 sec/batch\n",
      "Epoch 9/20  Iteration 1466/3560 Training loss: 2.1133 0.7124 sec/batch\n",
      "Epoch 9/20  Iteration 1467/3560 Training loss: 2.1127 0.6165 sec/batch\n",
      "Epoch 9/20  Iteration 1468/3560 Training loss: 2.1121 0.6764 sec/batch\n",
      "Epoch 9/20  Iteration 1469/3560 Training loss: 2.1121 0.6623 sec/batch\n",
      "Epoch 9/20  Iteration 1470/3560 Training loss: 2.1107 0.7105 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1471/3560 Training loss: 2.1107 0.6348 sec/batch\n",
      "Epoch 9/20  Iteration 1472/3560 Training loss: 2.1104 0.6779 sec/batch\n",
      "Epoch 9/20  Iteration 1473/3560 Training loss: 2.1102 0.6385 sec/batch\n",
      "Epoch 9/20  Iteration 1474/3560 Training loss: 2.1105 0.6695 sec/batch\n",
      "Epoch 9/20  Iteration 1475/3560 Training loss: 2.1100 0.6266 sec/batch\n",
      "Epoch 9/20  Iteration 1476/3560 Training loss: 2.1105 0.5912 sec/batch\n",
      "Epoch 9/20  Iteration 1477/3560 Training loss: 2.1101 0.6083 sec/batch\n",
      "Epoch 9/20  Iteration 1478/3560 Training loss: 2.1097 0.5862 sec/batch\n",
      "Epoch 9/20  Iteration 1479/3560 Training loss: 2.1090 0.7037 sec/batch\n",
      "Epoch 9/20  Iteration 1480/3560 Training loss: 2.1091 0.6535 sec/batch\n",
      "Epoch 9/20  Iteration 1481/3560 Training loss: 2.1091 0.6627 sec/batch\n",
      "Epoch 9/20  Iteration 1482/3560 Training loss: 2.1088 0.6220 sec/batch\n",
      "Epoch 9/20  Iteration 1483/3560 Training loss: 2.1087 0.7225 sec/batch\n",
      "Epoch 9/20  Iteration 1484/3560 Training loss: 2.1089 0.6443 sec/batch\n",
      "Epoch 9/20  Iteration 1485/3560 Training loss: 2.1087 0.7455 sec/batch\n",
      "Epoch 9/20  Iteration 1486/3560 Training loss: 2.1089 0.5500 sec/batch\n",
      "Epoch 9/20  Iteration 1487/3560 Training loss: 2.1091 0.6253 sec/batch\n",
      "Epoch 9/20  Iteration 1488/3560 Training loss: 2.1090 0.6090 sec/batch\n",
      "Epoch 9/20  Iteration 1489/3560 Training loss: 2.1087 0.6425 sec/batch\n",
      "Epoch 9/20  Iteration 1490/3560 Training loss: 2.1089 0.6063 sec/batch\n",
      "Epoch 9/20  Iteration 1491/3560 Training loss: 2.1090 0.6363 sec/batch\n",
      "Epoch 9/20  Iteration 1492/3560 Training loss: 2.1084 0.6827 sec/batch\n",
      "Epoch 9/20  Iteration 1493/3560 Training loss: 2.1081 0.6492 sec/batch\n",
      "Epoch 9/20  Iteration 1494/3560 Training loss: 2.1079 0.7219 sec/batch\n",
      "Epoch 9/20  Iteration 1495/3560 Training loss: 2.1082 0.6339 sec/batch\n",
      "Epoch 9/20  Iteration 1496/3560 Training loss: 2.1083 0.6695 sec/batch\n",
      "Epoch 9/20  Iteration 1497/3560 Training loss: 2.1084 0.7401 sec/batch\n",
      "Epoch 9/20  Iteration 1498/3560 Training loss: 2.1081 0.5956 sec/batch\n",
      "Epoch 9/20  Iteration 1499/3560 Training loss: 2.1081 0.6756 sec/batch\n",
      "Epoch 9/20  Iteration 1500/3560 Training loss: 2.1082 0.6102 sec/batch\n",
      "Epoch 9/20  Iteration 1501/3560 Training loss: 2.1081 0.6453 sec/batch\n",
      "Epoch 9/20  Iteration 1502/3560 Training loss: 2.1081 0.6043 sec/batch\n",
      "Epoch 9/20  Iteration 1503/3560 Training loss: 2.1077 0.6377 sec/batch\n",
      "Epoch 9/20  Iteration 1504/3560 Training loss: 2.1074 0.6257 sec/batch\n",
      "Epoch 9/20  Iteration 1505/3560 Training loss: 2.1071 0.6186 sec/batch\n",
      "Epoch 9/20  Iteration 1506/3560 Training loss: 2.1072 0.7049 sec/batch\n",
      "Epoch 9/20  Iteration 1507/3560 Training loss: 2.1068 0.6758 sec/batch\n",
      "Epoch 9/20  Iteration 1508/3560 Training loss: 2.1066 0.5969 sec/batch\n",
      "Epoch 9/20  Iteration 1509/3560 Training loss: 2.1060 0.7007 sec/batch\n",
      "Epoch 9/20  Iteration 1510/3560 Training loss: 2.1057 0.6840 sec/batch\n",
      "Epoch 9/20  Iteration 1511/3560 Training loss: 2.1056 0.6265 sec/batch\n",
      "Epoch 9/20  Iteration 1512/3560 Training loss: 2.1055 0.6330 sec/batch\n",
      "Epoch 9/20  Iteration 1513/3560 Training loss: 2.1052 0.6636 sec/batch\n",
      "Epoch 9/20  Iteration 1514/3560 Training loss: 2.1052 0.6002 sec/batch\n",
      "Epoch 9/20  Iteration 1515/3560 Training loss: 2.1048 0.6523 sec/batch\n",
      "Epoch 9/20  Iteration 1516/3560 Training loss: 2.1047 0.6281 sec/batch\n",
      "Epoch 9/20  Iteration 1517/3560 Training loss: 2.1044 0.6720 sec/batch\n",
      "Epoch 9/20  Iteration 1518/3560 Training loss: 2.1040 0.7012 sec/batch\n",
      "Epoch 9/20  Iteration 1519/3560 Training loss: 2.1035 0.6066 sec/batch\n",
      "Epoch 9/20  Iteration 1520/3560 Training loss: 2.1033 0.6387 sec/batch\n",
      "Epoch 9/20  Iteration 1521/3560 Training loss: 2.1031 0.6451 sec/batch\n",
      "Epoch 9/20  Iteration 1522/3560 Training loss: 2.1028 0.6672 sec/batch\n",
      "Epoch 9/20  Iteration 1523/3560 Training loss: 2.1025 0.6649 sec/batch\n",
      "Epoch 9/20  Iteration 1524/3560 Training loss: 2.1021 0.5931 sec/batch\n",
      "Epoch 9/20  Iteration 1525/3560 Training loss: 2.1021 0.6604 sec/batch\n",
      "Epoch 9/20  Iteration 1526/3560 Training loss: 2.1020 0.6112 sec/batch\n",
      "Epoch 9/20  Iteration 1527/3560 Training loss: 2.1016 0.6170 sec/batch\n",
      "Epoch 9/20  Iteration 1528/3560 Training loss: 2.1013 0.5776 sec/batch\n",
      "Epoch 9/20  Iteration 1529/3560 Training loss: 2.1010 0.7044 sec/batch\n",
      "Epoch 9/20  Iteration 1530/3560 Training loss: 2.1009 0.6866 sec/batch\n",
      "Epoch 9/20  Iteration 1531/3560 Training loss: 2.1008 0.6335 sec/batch\n",
      "Epoch 9/20  Iteration 1532/3560 Training loss: 2.1007 0.5867 sec/batch\n",
      "Epoch 9/20  Iteration 1533/3560 Training loss: 2.1007 0.7455 sec/batch\n",
      "Epoch 9/20  Iteration 1534/3560 Training loss: 2.1006 0.6346 sec/batch\n",
      "Epoch 9/20  Iteration 1535/3560 Training loss: 2.1004 0.6232 sec/batch\n",
      "Epoch 9/20  Iteration 1536/3560 Training loss: 2.1004 0.7675 sec/batch\n",
      "Epoch 9/20  Iteration 1537/3560 Training loss: 2.1003 0.5880 sec/batch\n",
      "Epoch 9/20  Iteration 1538/3560 Training loss: 2.1002 0.6083 sec/batch\n",
      "Epoch 9/20  Iteration 1539/3560 Training loss: 2.1000 0.6379 sec/batch\n",
      "Epoch 9/20  Iteration 1540/3560 Training loss: 2.0996 0.5455 sec/batch\n",
      "Epoch 9/20  Iteration 1541/3560 Training loss: 2.0995 0.6601 sec/batch\n",
      "Epoch 9/20  Iteration 1542/3560 Training loss: 2.0994 0.6482 sec/batch\n",
      "Epoch 9/20  Iteration 1543/3560 Training loss: 2.0993 0.6232 sec/batch\n",
      "Epoch 9/20  Iteration 1544/3560 Training loss: 2.0993 0.6605 sec/batch\n",
      "Epoch 9/20  Iteration 1545/3560 Training loss: 2.0993 0.6232 sec/batch\n",
      "Epoch 9/20  Iteration 1546/3560 Training loss: 2.0992 0.7879 sec/batch\n",
      "Epoch 9/20  Iteration 1547/3560 Training loss: 2.0989 0.5927 sec/batch\n",
      "Epoch 9/20  Iteration 1548/3560 Training loss: 2.0989 0.6834 sec/batch\n",
      "Epoch 9/20  Iteration 1549/3560 Training loss: 2.0988 0.6406 sec/batch\n",
      "Epoch 9/20  Iteration 1550/3560 Training loss: 2.0985 0.6760 sec/batch\n",
      "Epoch 9/20  Iteration 1551/3560 Training loss: 2.0985 0.5795 sec/batch\n",
      "Epoch 9/20  Iteration 1552/3560 Training loss: 2.0985 0.6225 sec/batch\n",
      "Epoch 9/20  Iteration 1553/3560 Training loss: 2.0984 0.5658 sec/batch\n",
      "Epoch 9/20  Iteration 1554/3560 Training loss: 2.0983 0.6324 sec/batch\n",
      "Epoch 9/20  Iteration 1555/3560 Training loss: 2.0982 0.6347 sec/batch\n",
      "Epoch 9/20  Iteration 1556/3560 Training loss: 2.0979 0.6373 sec/batch\n",
      "Epoch 9/20  Iteration 1557/3560 Training loss: 2.0979 0.6905 sec/batch\n",
      "Epoch 9/20  Iteration 1558/3560 Training loss: 2.0979 0.6593 sec/batch\n",
      "Epoch 9/20  Iteration 1559/3560 Training loss: 2.0977 0.6896 sec/batch\n",
      "Epoch 9/20  Iteration 1560/3560 Training loss: 2.0977 0.6806 sec/batch\n",
      "Epoch 9/20  Iteration 1561/3560 Training loss: 2.0978 0.6868 sec/batch\n",
      "Epoch 9/20  Iteration 1562/3560 Training loss: 2.0977 0.6661 sec/batch\n",
      "Epoch 9/20  Iteration 1563/3560 Training loss: 2.0979 0.5959 sec/batch\n",
      "Epoch 9/20  Iteration 1564/3560 Training loss: 2.0976 0.6221 sec/batch\n",
      "Epoch 9/20  Iteration 1565/3560 Training loss: 2.0977 0.5926 sec/batch\n",
      "Epoch 9/20  Iteration 1566/3560 Training loss: 2.0976 0.6033 sec/batch\n",
      "Epoch 9/20  Iteration 1567/3560 Training loss: 2.0975 0.6849 sec/batch\n",
      "Epoch 9/20  Iteration 1568/3560 Training loss: 2.0974 0.6647 sec/batch\n",
      "Epoch 9/20  Iteration 1569/3560 Training loss: 2.0972 0.6321 sec/batch\n",
      "Epoch 9/20  Iteration 1570/3560 Training loss: 2.0972 0.6540 sec/batch\n",
      "Epoch 9/20  Iteration 1571/3560 Training loss: 2.0972 0.6363 sec/batch\n",
      "Epoch 9/20  Iteration 1572/3560 Training loss: 2.0973 0.7123 sec/batch\n",
      "Epoch 9/20  Iteration 1573/3560 Training loss: 2.0972 0.7046 sec/batch\n",
      "Epoch 9/20  Iteration 1574/3560 Training loss: 2.0971 0.5730 sec/batch\n",
      "Epoch 9/20  Iteration 1575/3560 Training loss: 2.0969 0.6363 sec/batch\n",
      "Epoch 9/20  Iteration 1576/3560 Training loss: 2.0971 0.5823 sec/batch\n",
      "Epoch 9/20  Iteration 1577/3560 Training loss: 2.0971 0.6423 sec/batch\n",
      "Epoch 9/20  Iteration 1578/3560 Training loss: 2.0971 0.7422 sec/batch\n",
      "Epoch 9/20  Iteration 1579/3560 Training loss: 2.0970 0.5544 sec/batch\n",
      "Epoch 9/20  Iteration 1580/3560 Training loss: 2.0968 0.6163 sec/batch\n",
      "Epoch 9/20  Iteration 1581/3560 Training loss: 2.0968 0.7070 sec/batch\n",
      "Epoch 9/20  Iteration 1582/3560 Training loss: 2.0967 0.6710 sec/batch\n",
      "Epoch 9/20  Iteration 1583/3560 Training loss: 2.0965 0.6643 sec/batch\n",
      "Epoch 9/20  Iteration 1584/3560 Training loss: 2.0966 0.6675 sec/batch\n",
      "Epoch 9/20  Iteration 1585/3560 Training loss: 2.0966 0.6302 sec/batch\n",
      "Epoch 9/20  Iteration 1586/3560 Training loss: 2.0965 0.6611 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1587/3560 Training loss: 2.0964 0.6518 sec/batch\n",
      "Epoch 9/20  Iteration 1588/3560 Training loss: 2.0964 0.6803 sec/batch\n",
      "Epoch 9/20  Iteration 1589/3560 Training loss: 2.0963 0.5603 sec/batch\n",
      "Epoch 9/20  Iteration 1590/3560 Training loss: 2.0962 0.6965 sec/batch\n",
      "Epoch 9/20  Iteration 1591/3560 Training loss: 2.0961 0.6338 sec/batch\n",
      "Epoch 9/20  Iteration 1592/3560 Training loss: 2.0962 0.5929 sec/batch\n",
      "Epoch 9/20  Iteration 1593/3560 Training loss: 2.0962 0.7351 sec/batch\n",
      "Epoch 9/20  Iteration 1594/3560 Training loss: 2.0960 0.6649 sec/batch\n",
      "Epoch 9/20  Iteration 1595/3560 Training loss: 2.0958 0.6891 sec/batch\n",
      "Epoch 9/20  Iteration 1596/3560 Training loss: 2.0956 0.6120 sec/batch\n",
      "Epoch 9/20  Iteration 1597/3560 Training loss: 2.0957 0.6764 sec/batch\n",
      "Epoch 9/20  Iteration 1598/3560 Training loss: 2.0957 0.6421 sec/batch\n",
      "Epoch 9/20  Iteration 1599/3560 Training loss: 2.0957 0.7104 sec/batch\n",
      "Epoch 9/20  Iteration 1600/3560 Training loss: 2.0956 0.6311 sec/batch\n",
      "Epoch 9/20  Iteration 1601/3560 Training loss: 2.0954 0.6496 sec/batch\n",
      "Epoch 9/20  Iteration 1602/3560 Training loss: 2.0953 0.6490 sec/batch\n",
      "Epoch 10/20  Iteration 1603/3560 Training loss: 2.1531 0.5824 sec/batch\n",
      "Epoch 10/20  Iteration 1604/3560 Training loss: 2.1087 0.6384 sec/batch\n",
      "Epoch 10/20  Iteration 1605/3560 Training loss: 2.0916 0.6523 sec/batch\n",
      "Epoch 10/20  Iteration 1606/3560 Training loss: 2.0858 0.5972 sec/batch\n",
      "Epoch 10/20  Iteration 1607/3560 Training loss: 2.0824 0.7215 sec/batch\n",
      "Epoch 10/20  Iteration 1608/3560 Training loss: 2.0764 0.6201 sec/batch\n",
      "Epoch 10/20  Iteration 1609/3560 Training loss: 2.0764 0.6836 sec/batch\n",
      "Epoch 10/20  Iteration 1610/3560 Training loss: 2.0769 0.6541 sec/batch\n",
      "Epoch 10/20  Iteration 1611/3560 Training loss: 2.0791 0.7120 sec/batch\n",
      "Epoch 10/20  Iteration 1612/3560 Training loss: 2.0787 0.6416 sec/batch\n",
      "Epoch 10/20  Iteration 1613/3560 Training loss: 2.0754 0.5529 sec/batch\n",
      "Epoch 10/20  Iteration 1614/3560 Training loss: 2.0745 0.6289 sec/batch\n",
      "Epoch 10/20  Iteration 1615/3560 Training loss: 2.0754 0.6017 sec/batch\n",
      "Epoch 10/20  Iteration 1616/3560 Training loss: 2.0780 0.5907 sec/batch\n",
      "Epoch 10/20  Iteration 1617/3560 Training loss: 2.0783 0.6696 sec/batch\n",
      "Epoch 10/20  Iteration 1618/3560 Training loss: 2.0766 0.6802 sec/batch\n",
      "Epoch 10/20  Iteration 1619/3560 Training loss: 2.0765 0.6994 sec/batch\n",
      "Epoch 10/20  Iteration 1620/3560 Training loss: 2.0783 0.6231 sec/batch\n",
      "Epoch 10/20  Iteration 1621/3560 Training loss: 2.0783 0.6389 sec/batch\n",
      "Epoch 10/20  Iteration 1622/3560 Training loss: 2.0780 0.7111 sec/batch\n",
      "Epoch 10/20  Iteration 1623/3560 Training loss: 2.0771 0.6635 sec/batch\n",
      "Epoch 10/20  Iteration 1624/3560 Training loss: 2.0781 0.6643 sec/batch\n",
      "Epoch 10/20  Iteration 1625/3560 Training loss: 2.0775 0.7267 sec/batch\n",
      "Epoch 10/20  Iteration 1626/3560 Training loss: 2.0769 0.6114 sec/batch\n",
      "Epoch 10/20  Iteration 1627/3560 Training loss: 2.0765 0.6810 sec/batch\n",
      "Epoch 10/20  Iteration 1628/3560 Training loss: 2.0764 0.6317 sec/batch\n",
      "Epoch 10/20  Iteration 1629/3560 Training loss: 2.0754 0.6763 sec/batch\n",
      "Epoch 10/20  Iteration 1630/3560 Training loss: 2.0761 0.6262 sec/batch\n",
      "Epoch 10/20  Iteration 1631/3560 Training loss: 2.0764 0.8420 sec/batch\n",
      "Epoch 10/20  Iteration 1632/3560 Training loss: 2.0766 0.7561 sec/batch\n",
      "Epoch 10/20  Iteration 1633/3560 Training loss: 2.0770 0.8737 sec/batch\n",
      "Epoch 10/20  Iteration 1634/3560 Training loss: 2.0763 0.8996 sec/batch\n",
      "Epoch 10/20  Iteration 1635/3560 Training loss: 2.0766 0.9200 sec/batch\n",
      "Epoch 10/20  Iteration 1636/3560 Training loss: 2.0772 0.7796 sec/batch\n",
      "Epoch 10/20  Iteration 1637/3560 Training loss: 2.0769 0.7413 sec/batch\n",
      "Epoch 10/20  Iteration 1638/3560 Training loss: 2.0769 0.8222 sec/batch\n",
      "Epoch 10/20  Iteration 1639/3560 Training loss: 2.0766 0.7793 sec/batch\n",
      "Epoch 10/20  Iteration 1640/3560 Training loss: 2.0756 0.7703 sec/batch\n",
      "Epoch 10/20  Iteration 1641/3560 Training loss: 2.0748 0.8088 sec/batch\n",
      "Epoch 10/20  Iteration 1642/3560 Training loss: 2.0741 0.5643 sec/batch\n",
      "Epoch 10/20  Iteration 1643/3560 Training loss: 2.0736 0.6947 sec/batch\n",
      "Epoch 10/20  Iteration 1644/3560 Training loss: 2.0734 0.6821 sec/batch\n",
      "Epoch 10/20  Iteration 1645/3560 Training loss: 2.0729 0.6213 sec/batch\n",
      "Epoch 10/20  Iteration 1646/3560 Training loss: 2.0722 0.6122 sec/batch\n",
      "Epoch 10/20  Iteration 1647/3560 Training loss: 2.0722 0.6689 sec/batch\n",
      "Epoch 10/20  Iteration 1648/3560 Training loss: 2.0711 0.6503 sec/batch\n",
      "Epoch 10/20  Iteration 1649/3560 Training loss: 2.0711 0.6726 sec/batch\n",
      "Epoch 10/20  Iteration 1650/3560 Training loss: 2.0709 0.6358 sec/batch\n",
      "Epoch 10/20  Iteration 1651/3560 Training loss: 2.0705 0.6354 sec/batch\n",
      "Epoch 10/20  Iteration 1652/3560 Training loss: 2.0711 0.6177 sec/batch\n",
      "Epoch 10/20  Iteration 1653/3560 Training loss: 2.0706 0.6452 sec/batch\n",
      "Epoch 10/20  Iteration 1654/3560 Training loss: 2.0712 0.6015 sec/batch\n",
      "Epoch 10/20  Iteration 1655/3560 Training loss: 2.0710 0.6626 sec/batch\n",
      "Epoch 10/20  Iteration 1656/3560 Training loss: 2.0707 0.6066 sec/batch\n",
      "Epoch 10/20  Iteration 1657/3560 Training loss: 2.0704 0.7197 sec/batch\n",
      "Epoch 10/20  Iteration 1658/3560 Training loss: 2.0705 0.5954 sec/batch\n",
      "Epoch 10/20  Iteration 1659/3560 Training loss: 2.0706 0.6130 sec/batch\n",
      "Epoch 10/20  Iteration 1660/3560 Training loss: 2.0703 0.6746 sec/batch\n",
      "Epoch 10/20  Iteration 1661/3560 Training loss: 2.0699 0.6221 sec/batch\n",
      "Epoch 10/20  Iteration 1662/3560 Training loss: 2.0700 0.7083 sec/batch\n",
      "Epoch 10/20  Iteration 1663/3560 Training loss: 2.0698 0.6158 sec/batch\n",
      "Epoch 10/20  Iteration 1664/3560 Training loss: 2.0703 0.6755 sec/batch\n",
      "Epoch 10/20  Iteration 1665/3560 Training loss: 2.0706 0.6289 sec/batch\n",
      "Epoch 10/20  Iteration 1666/3560 Training loss: 2.0707 0.6548 sec/batch\n",
      "Epoch 10/20  Iteration 1667/3560 Training loss: 2.0705 0.6419 sec/batch\n",
      "Epoch 10/20  Iteration 1668/3560 Training loss: 2.0709 0.6964 sec/batch\n",
      "Epoch 10/20  Iteration 1669/3560 Training loss: 2.0709 0.7282 sec/batch\n",
      "Epoch 10/20  Iteration 1670/3560 Training loss: 2.0704 0.6647 sec/batch\n",
      "Epoch 10/20  Iteration 1671/3560 Training loss: 2.0702 0.6548 sec/batch\n",
      "Epoch 10/20  Iteration 1672/3560 Training loss: 2.0701 0.6352 sec/batch\n",
      "Epoch 10/20  Iteration 1673/3560 Training loss: 2.0704 0.7057 sec/batch\n",
      "Epoch 10/20  Iteration 1674/3560 Training loss: 2.0705 0.7007 sec/batch\n",
      "Epoch 10/20  Iteration 1675/3560 Training loss: 2.0708 0.6640 sec/batch\n",
      "Epoch 10/20  Iteration 1676/3560 Training loss: 2.0706 0.6292 sec/batch\n",
      "Epoch 10/20  Iteration 1677/3560 Training loss: 2.0705 0.6122 sec/batch\n",
      "Epoch 10/20  Iteration 1678/3560 Training loss: 2.0708 0.6041 sec/batch\n",
      "Epoch 10/20  Iteration 1679/3560 Training loss: 2.0706 0.6572 sec/batch\n",
      "Epoch 10/20  Iteration 1680/3560 Training loss: 2.0706 0.6106 sec/batch\n",
      "Epoch 10/20  Iteration 1681/3560 Training loss: 2.0702 0.6550 sec/batch\n",
      "Epoch 10/20  Iteration 1682/3560 Training loss: 2.0699 0.6441 sec/batch\n",
      "Epoch 10/20  Iteration 1683/3560 Training loss: 2.0695 0.6858 sec/batch\n",
      "Epoch 10/20  Iteration 1684/3560 Training loss: 2.0696 0.7103 sec/batch\n",
      "Epoch 10/20  Iteration 1685/3560 Training loss: 2.0691 0.6902 sec/batch\n",
      "Epoch 10/20  Iteration 1686/3560 Training loss: 2.0689 0.6580 sec/batch\n",
      "Epoch 10/20  Iteration 1687/3560 Training loss: 2.0683 0.6534 sec/batch\n",
      "Epoch 10/20  Iteration 1688/3560 Training loss: 2.0680 0.6540 sec/batch\n",
      "Epoch 10/20  Iteration 1689/3560 Training loss: 2.0679 0.6327 sec/batch\n",
      "Epoch 10/20  Iteration 1690/3560 Training loss: 2.0676 0.6112 sec/batch\n",
      "Epoch 10/20  Iteration 1691/3560 Training loss: 2.0672 0.5943 sec/batch\n",
      "Epoch 10/20  Iteration 1692/3560 Training loss: 2.0673 0.5925 sec/batch\n",
      "Epoch 10/20  Iteration 1693/3560 Training loss: 2.0671 0.6624 sec/batch\n",
      "Epoch 10/20  Iteration 1694/3560 Training loss: 2.0669 0.6796 sec/batch\n",
      "Epoch 10/20  Iteration 1695/3560 Training loss: 2.0665 0.6864 sec/batch\n",
      "Epoch 10/20  Iteration 1696/3560 Training loss: 2.0661 0.6426 sec/batch\n",
      "Epoch 10/20  Iteration 1697/3560 Training loss: 2.0657 0.7321 sec/batch\n",
      "Epoch 10/20  Iteration 1698/3560 Training loss: 2.0656 0.6065 sec/batch\n",
      "Epoch 10/20  Iteration 1699/3560 Training loss: 2.0655 0.7591 sec/batch\n",
      "Epoch 10/20  Iteration 1700/3560 Training loss: 2.0652 0.7483 sec/batch\n",
      "Epoch 10/20  Iteration 1701/3560 Training loss: 2.0649 0.6812 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1702/3560 Training loss: 2.0645 0.5890 sec/batch\n",
      "Epoch 10/20  Iteration 1703/3560 Training loss: 2.0645 0.6595 sec/batch\n",
      "Epoch 10/20  Iteration 1704/3560 Training loss: 2.0644 0.5653 sec/batch\n",
      "Epoch 10/20  Iteration 1705/3560 Training loss: 2.0641 0.6591 sec/batch\n",
      "Epoch 10/20  Iteration 1706/3560 Training loss: 2.0638 0.6239 sec/batch\n",
      "Epoch 10/20  Iteration 1707/3560 Training loss: 2.0636 0.6384 sec/batch\n",
      "Epoch 10/20  Iteration 1708/3560 Training loss: 2.0635 0.6799 sec/batch\n",
      "Epoch 10/20  Iteration 1709/3560 Training loss: 2.0633 0.6296 sec/batch\n",
      "Epoch 10/20  Iteration 1710/3560 Training loss: 2.0634 0.7023 sec/batch\n",
      "Epoch 10/20  Iteration 1711/3560 Training loss: 2.0634 0.6149 sec/batch\n",
      "Epoch 10/20  Iteration 1712/3560 Training loss: 2.0633 0.8605 sec/batch\n",
      "Epoch 10/20  Iteration 1713/3560 Training loss: 2.0632 0.6293 sec/batch\n",
      "Epoch 10/20  Iteration 1714/3560 Training loss: 2.0632 0.6902 sec/batch\n",
      "Epoch 10/20  Iteration 1715/3560 Training loss: 2.0630 0.5893 sec/batch\n",
      "Epoch 10/20  Iteration 1716/3560 Training loss: 2.0628 0.6513 sec/batch\n",
      "Epoch 10/20  Iteration 1717/3560 Training loss: 2.0626 0.6076 sec/batch\n",
      "Epoch 10/20  Iteration 1718/3560 Training loss: 2.0622 0.6017 sec/batch\n",
      "Epoch 10/20  Iteration 1719/3560 Training loss: 2.0621 0.6438 sec/batch\n",
      "Epoch 10/20  Iteration 1720/3560 Training loss: 2.0621 0.7057 sec/batch\n",
      "Epoch 10/20  Iteration 1721/3560 Training loss: 2.0621 0.6169 sec/batch\n",
      "Epoch 10/20  Iteration 1722/3560 Training loss: 2.0620 0.6260 sec/batch\n",
      "Epoch 10/20  Iteration 1723/3560 Training loss: 2.0620 0.7565 sec/batch\n",
      "Epoch 10/20  Iteration 1724/3560 Training loss: 2.0618 0.6433 sec/batch\n",
      "Epoch 10/20  Iteration 1725/3560 Training loss: 2.0617 0.6896 sec/batch\n",
      "Epoch 10/20  Iteration 1726/3560 Training loss: 2.0617 0.5841 sec/batch\n",
      "Epoch 10/20  Iteration 1727/3560 Training loss: 2.0617 0.6450 sec/batch\n",
      "Epoch 10/20  Iteration 1728/3560 Training loss: 2.0613 0.6012 sec/batch\n",
      "Epoch 10/20  Iteration 1729/3560 Training loss: 2.0613 0.7095 sec/batch\n",
      "Epoch 10/20  Iteration 1730/3560 Training loss: 2.0613 0.6466 sec/batch\n",
      "Epoch 10/20  Iteration 1731/3560 Training loss: 2.0612 0.6421 sec/batch\n",
      "Epoch 10/20  Iteration 1732/3560 Training loss: 2.0611 0.6785 sec/batch\n",
      "Epoch 10/20  Iteration 1733/3560 Training loss: 2.0609 0.6906 sec/batch\n",
      "Epoch 10/20  Iteration 1734/3560 Training loss: 2.0606 0.5647 sec/batch\n",
      "Epoch 10/20  Iteration 1735/3560 Training loss: 2.0606 0.7305 sec/batch\n",
      "Epoch 10/20  Iteration 1736/3560 Training loss: 2.0607 0.6075 sec/batch\n",
      "Epoch 10/20  Iteration 1737/3560 Training loss: 2.0606 0.7880 sec/batch\n",
      "Epoch 10/20  Iteration 1738/3560 Training loss: 2.0606 0.6271 sec/batch\n",
      "Epoch 10/20  Iteration 1739/3560 Training loss: 2.0606 0.6645 sec/batch\n",
      "Epoch 10/20  Iteration 1740/3560 Training loss: 2.0605 0.6478 sec/batch\n",
      "Epoch 10/20  Iteration 1741/3560 Training loss: 2.0607 0.6249 sec/batch\n",
      "Epoch 10/20  Iteration 1742/3560 Training loss: 2.0605 0.6408 sec/batch\n",
      "Epoch 10/20  Iteration 1743/3560 Training loss: 2.0606 0.6405 sec/batch\n",
      "Epoch 10/20  Iteration 1744/3560 Training loss: 2.0606 0.6521 sec/batch\n",
      "Epoch 10/20  Iteration 1745/3560 Training loss: 2.0605 0.7326 sec/batch\n",
      "Epoch 10/20  Iteration 1746/3560 Training loss: 2.0605 0.6166 sec/batch\n",
      "Epoch 10/20  Iteration 1747/3560 Training loss: 2.0604 0.6060 sec/batch\n",
      "Epoch 10/20  Iteration 1748/3560 Training loss: 2.0604 0.6522 sec/batch\n",
      "Epoch 10/20  Iteration 1749/3560 Training loss: 2.0604 0.6893 sec/batch\n",
      "Epoch 10/20  Iteration 1750/3560 Training loss: 2.0605 0.7410 sec/batch\n",
      "Epoch 10/20  Iteration 1751/3560 Training loss: 2.0605 0.5671 sec/batch\n",
      "Epoch 10/20  Iteration 1752/3560 Training loss: 2.0603 0.6379 sec/batch\n",
      "Epoch 10/20  Iteration 1753/3560 Training loss: 2.0602 0.5702 sec/batch\n",
      "Epoch 10/20  Iteration 1754/3560 Training loss: 2.0604 0.5576 sec/batch\n",
      "Epoch 10/20  Iteration 1755/3560 Training loss: 2.0604 0.6654 sec/batch\n",
      "Epoch 10/20  Iteration 1756/3560 Training loss: 2.0604 0.5906 sec/batch\n",
      "Epoch 10/20  Iteration 1757/3560 Training loss: 2.0602 0.6711 sec/batch\n",
      "Epoch 10/20  Iteration 1758/3560 Training loss: 2.0602 0.7125 sec/batch\n",
      "Epoch 10/20  Iteration 1759/3560 Training loss: 2.0601 0.6710 sec/batch\n",
      "Epoch 10/20  Iteration 1760/3560 Training loss: 2.0600 0.6492 sec/batch\n",
      "Epoch 10/20  Iteration 1761/3560 Training loss: 2.0598 0.6747 sec/batch\n",
      "Epoch 10/20  Iteration 1762/3560 Training loss: 2.0599 0.7287 sec/batch\n",
      "Epoch 10/20  Iteration 1763/3560 Training loss: 2.0599 0.6879 sec/batch\n",
      "Epoch 10/20  Iteration 1764/3560 Training loss: 2.0599 0.6601 sec/batch\n",
      "Epoch 10/20  Iteration 1765/3560 Training loss: 2.0599 0.6534 sec/batch\n",
      "Epoch 10/20  Iteration 1766/3560 Training loss: 2.0598 0.6481 sec/batch\n",
      "Epoch 10/20  Iteration 1767/3560 Training loss: 2.0598 0.5966 sec/batch\n",
      "Epoch 10/20  Iteration 1768/3560 Training loss: 2.0597 0.6630 sec/batch\n",
      "Epoch 10/20  Iteration 1769/3560 Training loss: 2.0596 0.6677 sec/batch\n",
      "Epoch 10/20  Iteration 1770/3560 Training loss: 2.0598 0.6700 sec/batch\n",
      "Epoch 10/20  Iteration 1771/3560 Training loss: 2.0598 0.6651 sec/batch\n",
      "Epoch 10/20  Iteration 1772/3560 Training loss: 2.0597 0.6977 sec/batch\n",
      "Epoch 10/20  Iteration 1773/3560 Training loss: 2.0595 0.7039 sec/batch\n",
      "Epoch 10/20  Iteration 1774/3560 Training loss: 2.0594 0.6892 sec/batch\n",
      "Epoch 10/20  Iteration 1775/3560 Training loss: 2.0594 0.7719 sec/batch\n",
      "Epoch 10/20  Iteration 1776/3560 Training loss: 2.0594 0.6848 sec/batch\n",
      "Epoch 10/20  Iteration 1777/3560 Training loss: 2.0594 0.5800 sec/batch\n",
      "Epoch 10/20  Iteration 1778/3560 Training loss: 2.0594 0.6738 sec/batch\n",
      "Epoch 10/20  Iteration 1779/3560 Training loss: 2.0593 0.6083 sec/batch\n",
      "Epoch 10/20  Iteration 1780/3560 Training loss: 2.0593 0.5915 sec/batch\n",
      "Epoch 11/20  Iteration 1781/3560 Training loss: 2.1125 0.5976 sec/batch\n",
      "Epoch 11/20  Iteration 1782/3560 Training loss: 2.0712 0.6802 sec/batch\n",
      "Epoch 11/20  Iteration 1783/3560 Training loss: 2.0541 0.6992 sec/batch\n",
      "Epoch 11/20  Iteration 1784/3560 Training loss: 2.0477 0.6233 sec/batch\n",
      "Epoch 11/20  Iteration 1785/3560 Training loss: 2.0450 0.6077 sec/batch\n",
      "Epoch 11/20  Iteration 1786/3560 Training loss: 2.0393 0.7510 sec/batch\n",
      "Epoch 11/20  Iteration 1787/3560 Training loss: 2.0408 0.6881 sec/batch\n",
      "Epoch 11/20  Iteration 1788/3560 Training loss: 2.0423 0.7013 sec/batch\n",
      "Epoch 11/20  Iteration 1789/3560 Training loss: 2.0459 0.6802 sec/batch\n",
      "Epoch 11/20  Iteration 1790/3560 Training loss: 2.0457 0.6012 sec/batch\n",
      "Epoch 11/20  Iteration 1791/3560 Training loss: 2.0422 0.6318 sec/batch\n",
      "Epoch 11/20  Iteration 1792/3560 Training loss: 2.0411 0.6417 sec/batch\n",
      "Epoch 11/20  Iteration 1793/3560 Training loss: 2.0412 0.5862 sec/batch\n",
      "Epoch 11/20  Iteration 1794/3560 Training loss: 2.0441 0.6340 sec/batch\n",
      "Epoch 11/20  Iteration 1795/3560 Training loss: 2.0437 0.6977 sec/batch\n",
      "Epoch 11/20  Iteration 1796/3560 Training loss: 2.0428 0.6322 sec/batch\n",
      "Epoch 11/20  Iteration 1797/3560 Training loss: 2.0437 0.7115 sec/batch\n",
      "Epoch 11/20  Iteration 1798/3560 Training loss: 2.0455 0.6930 sec/batch\n",
      "Epoch 11/20  Iteration 1799/3560 Training loss: 2.0452 0.6998 sec/batch\n",
      "Epoch 11/20  Iteration 1800/3560 Training loss: 2.0453 0.6623 sec/batch\n",
      "Epoch 11/20  Iteration 1801/3560 Training loss: 2.0446 0.6485 sec/batch\n",
      "Epoch 11/20  Iteration 1802/3560 Training loss: 2.0460 0.6571 sec/batch\n",
      "Epoch 11/20  Iteration 1803/3560 Training loss: 2.0456 0.6043 sec/batch\n",
      "Epoch 11/20  Iteration 1804/3560 Training loss: 2.0451 0.6579 sec/batch\n",
      "Epoch 11/20  Iteration 1805/3560 Training loss: 2.0448 0.6347 sec/batch\n",
      "Epoch 11/20  Iteration 1806/3560 Training loss: 2.0445 0.6566 sec/batch\n",
      "Epoch 11/20  Iteration 1807/3560 Training loss: 2.0435 0.6617 sec/batch\n",
      "Epoch 11/20  Iteration 1808/3560 Training loss: 2.0437 0.6750 sec/batch\n",
      "Epoch 11/20  Iteration 1809/3560 Training loss: 2.0441 0.6108 sec/batch\n",
      "Epoch 11/20  Iteration 1810/3560 Training loss: 2.0444 0.6389 sec/batch\n",
      "Epoch 11/20  Iteration 1811/3560 Training loss: 2.0445 0.6995 sec/batch\n",
      "Epoch 11/20  Iteration 1812/3560 Training loss: 2.0436 0.7082 sec/batch\n",
      "Epoch 11/20  Iteration 1813/3560 Training loss: 2.0438 0.6797 sec/batch\n",
      "Epoch 11/20  Iteration 1814/3560 Training loss: 2.0447 0.6485 sec/batch\n",
      "Epoch 11/20  Iteration 1815/3560 Training loss: 2.0444 0.6128 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1816/3560 Training loss: 2.0443 0.5573 sec/batch\n",
      "Epoch 11/20  Iteration 1817/3560 Training loss: 2.0439 0.6813 sec/batch\n",
      "Epoch 11/20  Iteration 1818/3560 Training loss: 2.0426 0.6288 sec/batch\n",
      "Epoch 11/20  Iteration 1819/3560 Training loss: 2.0415 0.6551 sec/batch\n",
      "Epoch 11/20  Iteration 1820/3560 Training loss: 2.0411 0.6171 sec/batch\n",
      "Epoch 11/20  Iteration 1821/3560 Training loss: 2.0407 0.7445 sec/batch\n",
      "Epoch 11/20  Iteration 1822/3560 Training loss: 2.0406 0.6599 sec/batch\n",
      "Epoch 11/20  Iteration 1823/3560 Training loss: 2.0399 0.6655 sec/batch\n",
      "Epoch 11/20  Iteration 1824/3560 Training loss: 2.0390 0.6617 sec/batch\n",
      "Epoch 11/20  Iteration 1825/3560 Training loss: 2.0390 0.7447 sec/batch\n",
      "Epoch 11/20  Iteration 1826/3560 Training loss: 2.0376 0.6438 sec/batch\n",
      "Epoch 11/20  Iteration 1827/3560 Training loss: 2.0375 0.5971 sec/batch\n",
      "Epoch 11/20  Iteration 1828/3560 Training loss: 2.0372 0.6063 sec/batch\n",
      "Epoch 11/20  Iteration 1829/3560 Training loss: 2.0368 0.5963 sec/batch\n",
      "Epoch 11/20  Iteration 1830/3560 Training loss: 2.0375 0.6259 sec/batch\n",
      "Epoch 11/20  Iteration 1831/3560 Training loss: 2.0370 0.6087 sec/batch\n",
      "Epoch 11/20  Iteration 1832/3560 Training loss: 2.0375 0.5950 sec/batch\n",
      "Epoch 11/20  Iteration 1833/3560 Training loss: 2.0372 0.7197 sec/batch\n",
      "Epoch 11/20  Iteration 1834/3560 Training loss: 2.0368 0.6570 sec/batch\n",
      "Epoch 11/20  Iteration 1835/3560 Training loss: 2.0364 0.6466 sec/batch\n",
      "Epoch 11/20  Iteration 1836/3560 Training loss: 2.0365 0.7034 sec/batch\n",
      "Epoch 11/20  Iteration 1837/3560 Training loss: 2.0366 0.6507 sec/batch\n",
      "Epoch 11/20  Iteration 1838/3560 Training loss: 2.0362 0.6973 sec/batch\n",
      "Epoch 11/20  Iteration 1839/3560 Training loss: 2.0360 0.6202 sec/batch\n",
      "Epoch 11/20  Iteration 1840/3560 Training loss: 2.0364 0.6802 sec/batch\n",
      "Epoch 11/20  Iteration 1841/3560 Training loss: 2.0363 0.6336 sec/batch\n",
      "Epoch 11/20  Iteration 1842/3560 Training loss: 2.0370 0.5876 sec/batch\n",
      "Epoch 11/20  Iteration 1843/3560 Training loss: 2.0373 0.6228 sec/batch\n",
      "Epoch 11/20  Iteration 1844/3560 Training loss: 2.0373 0.6186 sec/batch\n",
      "Epoch 11/20  Iteration 1845/3560 Training loss: 2.0373 0.5750 sec/batch\n",
      "Epoch 11/20  Iteration 1846/3560 Training loss: 2.0375 0.6711 sec/batch\n",
      "Epoch 11/20  Iteration 1847/3560 Training loss: 2.0374 0.6611 sec/batch\n",
      "Epoch 11/20  Iteration 1848/3560 Training loss: 2.0369 0.7274 sec/batch\n",
      "Epoch 11/20  Iteration 1849/3560 Training loss: 2.0367 0.6720 sec/batch\n",
      "Epoch 11/20  Iteration 1850/3560 Training loss: 2.0366 0.6868 sec/batch\n",
      "Epoch 11/20  Iteration 1851/3560 Training loss: 2.0369 0.6871 sec/batch\n",
      "Epoch 11/20  Iteration 1852/3560 Training loss: 2.0370 0.6725 sec/batch\n",
      "Epoch 11/20  Iteration 1853/3560 Training loss: 2.0373 0.5900 sec/batch\n",
      "Epoch 11/20  Iteration 1854/3560 Training loss: 2.0369 0.6246 sec/batch\n",
      "Epoch 11/20  Iteration 1855/3560 Training loss: 2.0369 0.5923 sec/batch\n",
      "Epoch 11/20  Iteration 1856/3560 Training loss: 2.0372 0.5919 sec/batch\n",
      "Epoch 11/20  Iteration 1857/3560 Training loss: 2.0370 0.6233 sec/batch\n",
      "Epoch 11/20  Iteration 1858/3560 Training loss: 2.0369 0.6166 sec/batch\n",
      "Epoch 11/20  Iteration 1859/3560 Training loss: 2.0365 0.6651 sec/batch\n",
      "Epoch 11/20  Iteration 1860/3560 Training loss: 2.0364 0.6396 sec/batch\n",
      "Epoch 11/20  Iteration 1861/3560 Training loss: 2.0359 0.6878 sec/batch\n",
      "Epoch 11/20  Iteration 1862/3560 Training loss: 2.0360 0.6882 sec/batch\n",
      "Epoch 11/20  Iteration 1863/3560 Training loss: 2.0356 0.7046 sec/batch\n",
      "Epoch 11/20  Iteration 1864/3560 Training loss: 2.0354 0.7428 sec/batch\n",
      "Epoch 11/20  Iteration 1865/3560 Training loss: 2.0347 0.6453 sec/batch\n",
      "Epoch 11/20  Iteration 1866/3560 Training loss: 2.0345 0.6840 sec/batch\n",
      "Epoch 11/20  Iteration 1867/3560 Training loss: 2.0344 0.6752 sec/batch\n",
      "Epoch 11/20  Iteration 1868/3560 Training loss: 2.0342 0.6121 sec/batch\n",
      "Epoch 11/20  Iteration 1869/3560 Training loss: 2.0336 0.6485 sec/batch\n",
      "Epoch 11/20  Iteration 1870/3560 Training loss: 2.0336 0.6057 sec/batch\n",
      "Epoch 11/20  Iteration 1871/3560 Training loss: 2.0334 0.6865 sec/batch\n",
      "Epoch 11/20  Iteration 1872/3560 Training loss: 2.0332 0.8342 sec/batch\n",
      "Epoch 11/20  Iteration 1873/3560 Training loss: 2.0327 0.6465 sec/batch\n",
      "Epoch 11/20  Iteration 1874/3560 Training loss: 2.0323 0.8807 sec/batch\n",
      "Epoch 11/20  Iteration 1875/3560 Training loss: 2.0319 1.0097 sec/batch\n",
      "Epoch 11/20  Iteration 1876/3560 Training loss: 2.0317 0.6963 sec/batch\n",
      "Epoch 11/20  Iteration 1877/3560 Training loss: 2.0316 1.0497 sec/batch\n",
      "Epoch 11/20  Iteration 1878/3560 Training loss: 2.0313 0.6910 sec/batch\n",
      "Epoch 11/20  Iteration 1879/3560 Training loss: 2.0310 0.6794 sec/batch\n",
      "Epoch 11/20  Iteration 1880/3560 Training loss: 2.0306 0.6958 sec/batch\n",
      "Epoch 11/20  Iteration 1881/3560 Training loss: 2.0306 0.6593 sec/batch\n",
      "Epoch 11/20  Iteration 1882/3560 Training loss: 2.0306 0.7732 sec/batch\n",
      "Epoch 11/20  Iteration 1883/3560 Training loss: 2.0304 0.8334 sec/batch\n",
      "Epoch 11/20  Iteration 1884/3560 Training loss: 2.0302 0.7980 sec/batch\n",
      "Epoch 11/20  Iteration 1885/3560 Training loss: 2.0298 0.7023 sec/batch\n",
      "Epoch 11/20  Iteration 1886/3560 Training loss: 2.0296 0.8597 sec/batch\n",
      "Epoch 11/20  Iteration 1887/3560 Training loss: 2.0296 0.7906 sec/batch\n",
      "Epoch 11/20  Iteration 1888/3560 Training loss: 2.0296 0.6083 sec/batch\n",
      "Epoch 11/20  Iteration 1889/3560 Training loss: 2.0296 0.6423 sec/batch\n",
      "Epoch 11/20  Iteration 1890/3560 Training loss: 2.0294 0.6265 sec/batch\n",
      "Epoch 11/20  Iteration 1891/3560 Training loss: 2.0293 0.6028 sec/batch\n",
      "Epoch 11/20  Iteration 1892/3560 Training loss: 2.0293 0.6348 sec/batch\n",
      "Epoch 11/20  Iteration 1893/3560 Training loss: 2.0293 0.6751 sec/batch\n",
      "Epoch 11/20  Iteration 1894/3560 Training loss: 2.0292 0.6456 sec/batch\n",
      "Epoch 11/20  Iteration 1895/3560 Training loss: 2.0291 0.6631 sec/batch\n",
      "Epoch 11/20  Iteration 1896/3560 Training loss: 2.0287 0.7155 sec/batch\n",
      "Epoch 11/20  Iteration 1897/3560 Training loss: 2.0286 0.6936 sec/batch\n",
      "Epoch 11/20  Iteration 1898/3560 Training loss: 2.0285 0.6695 sec/batch\n",
      "Epoch 11/20  Iteration 1899/3560 Training loss: 2.0286 0.6541 sec/batch\n",
      "Epoch 11/20  Iteration 1900/3560 Training loss: 2.0286 0.7852 sec/batch\n",
      "Epoch 11/20  Iteration 1901/3560 Training loss: 2.0286 0.6353 sec/batch\n",
      "Epoch 11/20  Iteration 1902/3560 Training loss: 2.0284 0.6801 sec/batch\n",
      "Epoch 11/20  Iteration 1903/3560 Training loss: 2.0282 0.6243 sec/batch\n",
      "Epoch 11/20  Iteration 1904/3560 Training loss: 2.0284 0.6610 sec/batch\n",
      "Epoch 11/20  Iteration 1905/3560 Training loss: 2.0284 0.5612 sec/batch\n",
      "Epoch 11/20  Iteration 1906/3560 Training loss: 2.0281 0.6293 sec/batch\n",
      "Epoch 11/20  Iteration 1907/3560 Training loss: 2.0282 0.6339 sec/batch\n",
      "Epoch 11/20  Iteration 1908/3560 Training loss: 2.0282 0.5838 sec/batch\n",
      "Epoch 11/20  Iteration 1909/3560 Training loss: 2.0281 0.6299 sec/batch\n",
      "Epoch 11/20  Iteration 1910/3560 Training loss: 2.0281 0.6321 sec/batch\n",
      "Epoch 11/20  Iteration 1911/3560 Training loss: 2.0278 0.7120 sec/batch\n",
      "Epoch 11/20  Iteration 1912/3560 Training loss: 2.0276 0.7282 sec/batch\n",
      "Epoch 11/20  Iteration 1913/3560 Training loss: 2.0276 0.7813 sec/batch\n",
      "Epoch 11/20  Iteration 1914/3560 Training loss: 2.0276 0.6885 sec/batch\n",
      "Epoch 11/20  Iteration 1915/3560 Training loss: 2.0275 0.6414 sec/batch\n",
      "Epoch 11/20  Iteration 1916/3560 Training loss: 2.0275 0.6491 sec/batch\n",
      "Epoch 11/20  Iteration 1917/3560 Training loss: 2.0275 0.5922 sec/batch\n",
      "Epoch 11/20  Iteration 1918/3560 Training loss: 2.0275 0.6847 sec/batch\n",
      "Epoch 11/20  Iteration 1919/3560 Training loss: 2.0276 0.5449 sec/batch\n",
      "Epoch 11/20  Iteration 1920/3560 Training loss: 2.0275 0.6979 sec/batch\n",
      "Epoch 11/20  Iteration 1921/3560 Training loss: 2.0276 0.6088 sec/batch\n",
      "Epoch 11/20  Iteration 1922/3560 Training loss: 2.0275 0.6726 sec/batch\n",
      "Epoch 11/20  Iteration 1923/3560 Training loss: 2.0274 0.6671 sec/batch\n",
      "Epoch 11/20  Iteration 1924/3560 Training loss: 2.0273 0.6830 sec/batch\n",
      "Epoch 11/20  Iteration 1925/3560 Training loss: 2.0272 0.7146 sec/batch\n",
      "Epoch 11/20  Iteration 1926/3560 Training loss: 2.0273 0.7015 sec/batch\n",
      "Epoch 11/20  Iteration 1927/3560 Training loss: 2.0272 0.7370 sec/batch\n",
      "Epoch 11/20  Iteration 1928/3560 Training loss: 2.0273 0.5865 sec/batch\n",
      "Epoch 11/20  Iteration 1929/3560 Training loss: 2.0273 0.5902 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1930/3560 Training loss: 2.0271 0.7184 sec/batch\n",
      "Epoch 11/20  Iteration 1931/3560 Training loss: 2.0270 0.6298 sec/batch\n",
      "Epoch 11/20  Iteration 1932/3560 Training loss: 2.0272 0.6189 sec/batch\n",
      "Epoch 11/20  Iteration 1933/3560 Training loss: 2.0272 0.6387 sec/batch\n",
      "Epoch 11/20  Iteration 1934/3560 Training loss: 2.0272 0.6248 sec/batch\n",
      "Epoch 11/20  Iteration 1935/3560 Training loss: 2.0271 0.6366 sec/batch\n",
      "Epoch 11/20  Iteration 1936/3560 Training loss: 2.0270 0.6876 sec/batch\n",
      "Epoch 11/20  Iteration 1937/3560 Training loss: 2.0269 0.7179 sec/batch\n",
      "Epoch 11/20  Iteration 1938/3560 Training loss: 2.0268 0.7002 sec/batch\n",
      "Epoch 11/20  Iteration 1939/3560 Training loss: 2.0266 0.7923 sec/batch\n",
      "Epoch 11/20  Iteration 1940/3560 Training loss: 2.0267 0.7662 sec/batch\n",
      "Epoch 11/20  Iteration 1941/3560 Training loss: 2.0267 0.5936 sec/batch\n",
      "Epoch 11/20  Iteration 1942/3560 Training loss: 2.0267 0.6508 sec/batch\n",
      "Epoch 11/20  Iteration 1943/3560 Training loss: 2.0267 0.7045 sec/batch\n",
      "Epoch 11/20  Iteration 1944/3560 Training loss: 2.0266 0.5147 sec/batch\n",
      "Epoch 11/20  Iteration 1945/3560 Training loss: 2.0266 0.6062 sec/batch\n",
      "Epoch 11/20  Iteration 1946/3560 Training loss: 2.0265 0.6765 sec/batch\n",
      "Epoch 11/20  Iteration 1947/3560 Training loss: 2.0265 0.7047 sec/batch\n",
      "Epoch 11/20  Iteration 1948/3560 Training loss: 2.0267 0.6122 sec/batch\n",
      "Epoch 11/20  Iteration 1949/3560 Training loss: 2.0267 0.6643 sec/batch\n",
      "Epoch 11/20  Iteration 1950/3560 Training loss: 2.0266 0.7226 sec/batch\n",
      "Epoch 11/20  Iteration 1951/3560 Training loss: 2.0265 0.7236 sec/batch\n",
      "Epoch 11/20  Iteration 1952/3560 Training loss: 2.0263 0.6429 sec/batch\n",
      "Epoch 11/20  Iteration 1953/3560 Training loss: 2.0264 0.6871 sec/batch\n",
      "Epoch 11/20  Iteration 1954/3560 Training loss: 2.0263 0.6107 sec/batch\n",
      "Epoch 11/20  Iteration 1955/3560 Training loss: 2.0264 0.6298 sec/batch\n",
      "Epoch 11/20  Iteration 1956/3560 Training loss: 2.0263 0.6547 sec/batch\n",
      "Epoch 11/20  Iteration 1957/3560 Training loss: 2.0262 0.6481 sec/batch\n",
      "Epoch 11/20  Iteration 1958/3560 Training loss: 2.0261 0.5961 sec/batch\n",
      "Epoch 12/20  Iteration 1959/3560 Training loss: 2.0789 0.6149 sec/batch\n",
      "Epoch 12/20  Iteration 1960/3560 Training loss: 2.0349 0.7033 sec/batch\n",
      "Epoch 12/20  Iteration 1961/3560 Training loss: 2.0225 0.6281 sec/batch\n",
      "Epoch 12/20  Iteration 1962/3560 Training loss: 2.0160 0.7499 sec/batch\n",
      "Epoch 12/20  Iteration 1963/3560 Training loss: 2.0165 0.6787 sec/batch\n",
      "Epoch 12/20  Iteration 1964/3560 Training loss: 2.0109 0.6816 sec/batch\n",
      "Epoch 12/20  Iteration 1965/3560 Training loss: 2.0125 0.6640 sec/batch\n",
      "Epoch 12/20  Iteration 1966/3560 Training loss: 2.0116 0.6192 sec/batch\n",
      "Epoch 12/20  Iteration 1967/3560 Training loss: 2.0140 0.6503 sec/batch\n",
      "Epoch 12/20  Iteration 1968/3560 Training loss: 2.0140 0.6180 sec/batch\n",
      "Epoch 12/20  Iteration 1969/3560 Training loss: 2.0122 0.6162 sec/batch\n",
      "Epoch 12/20  Iteration 1970/3560 Training loss: 2.0115 0.6032 sec/batch\n",
      "Epoch 12/20  Iteration 1971/3560 Training loss: 2.0106 0.5801 sec/batch\n",
      "Epoch 12/20  Iteration 1972/3560 Training loss: 2.0133 0.6792 sec/batch\n",
      "Epoch 12/20  Iteration 1973/3560 Training loss: 2.0127 0.6886 sec/batch\n",
      "Epoch 12/20  Iteration 1974/3560 Training loss: 2.0117 0.6988 sec/batch\n",
      "Epoch 12/20  Iteration 1975/3560 Training loss: 2.0115 0.7292 sec/batch\n",
      "Epoch 12/20  Iteration 1976/3560 Training loss: 2.0134 0.6781 sec/batch\n",
      "Epoch 12/20  Iteration 1977/3560 Training loss: 2.0134 0.6556 sec/batch\n",
      "Epoch 12/20  Iteration 1978/3560 Training loss: 2.0139 0.7689 sec/batch\n",
      "Epoch 12/20  Iteration 1979/3560 Training loss: 2.0131 0.5999 sec/batch\n",
      "Epoch 12/20  Iteration 1980/3560 Training loss: 2.0144 0.6141 sec/batch\n",
      "Epoch 12/20  Iteration 1981/3560 Training loss: 2.0137 0.6391 sec/batch\n",
      "Epoch 12/20  Iteration 1982/3560 Training loss: 2.0129 0.6071 sec/batch\n",
      "Epoch 12/20  Iteration 1983/3560 Training loss: 2.0124 0.5898 sec/batch\n",
      "Epoch 12/20  Iteration 1984/3560 Training loss: 2.0119 0.6350 sec/batch\n",
      "Epoch 12/20  Iteration 1985/3560 Training loss: 2.0114 0.6583 sec/batch\n",
      "Epoch 12/20  Iteration 1986/3560 Training loss: 2.0116 0.6606 sec/batch\n",
      "Epoch 12/20  Iteration 1987/3560 Training loss: 2.0126 0.7191 sec/batch\n",
      "Epoch 12/20  Iteration 1988/3560 Training loss: 2.0128 0.7169 sec/batch\n",
      "Epoch 12/20  Iteration 1989/3560 Training loss: 2.0131 0.6691 sec/batch\n",
      "Epoch 12/20  Iteration 1990/3560 Training loss: 2.0122 0.7886 sec/batch\n",
      "Epoch 12/20  Iteration 1991/3560 Training loss: 2.0126 0.6814 sec/batch\n",
      "Epoch 12/20  Iteration 1992/3560 Training loss: 2.0133 0.6174 sec/batch\n",
      "Epoch 12/20  Iteration 1993/3560 Training loss: 2.0130 0.5405 sec/batch\n",
      "Epoch 12/20  Iteration 1994/3560 Training loss: 2.0129 0.6734 sec/batch\n",
      "Epoch 12/20  Iteration 1995/3560 Training loss: 2.0124 0.5800 sec/batch\n",
      "Epoch 12/20  Iteration 1996/3560 Training loss: 2.0115 0.6145 sec/batch\n",
      "Epoch 12/20  Iteration 1997/3560 Training loss: 2.0108 0.6244 sec/batch\n",
      "Epoch 12/20  Iteration 1998/3560 Training loss: 2.0101 0.7224 sec/batch\n",
      "Epoch 12/20  Iteration 1999/3560 Training loss: 2.0097 0.6584 sec/batch\n",
      "Epoch 12/20  Iteration 2000/3560 Training loss: 2.0095 0.6509 sec/batch\n",
      "Epoch 12/20  Iteration 2001/3560 Training loss: 2.0088 0.6808 sec/batch\n",
      "Epoch 12/20  Iteration 2002/3560 Training loss: 2.0080 0.7118 sec/batch\n",
      "Epoch 12/20  Iteration 2003/3560 Training loss: 2.0080 0.6925 sec/batch\n",
      "Epoch 12/20  Iteration 2004/3560 Training loss: 2.0067 0.6663 sec/batch\n",
      "Epoch 12/20  Iteration 2005/3560 Training loss: 2.0067 0.6523 sec/batch\n",
      "Epoch 12/20  Iteration 2006/3560 Training loss: 2.0062 0.6631 sec/batch\n",
      "Epoch 12/20  Iteration 2007/3560 Training loss: 2.0060 0.5642 sec/batch\n",
      "Epoch 12/20  Iteration 2008/3560 Training loss: 2.0066 0.6067 sec/batch\n",
      "Epoch 12/20  Iteration 2009/3560 Training loss: 2.0061 0.6588 sec/batch\n",
      "Epoch 12/20  Iteration 2010/3560 Training loss: 2.0067 0.6509 sec/batch\n",
      "Epoch 12/20  Iteration 2011/3560 Training loss: 2.0065 0.7089 sec/batch\n",
      "Epoch 12/20  Iteration 2012/3560 Training loss: 2.0065 0.6288 sec/batch\n",
      "Epoch 12/20  Iteration 2013/3560 Training loss: 2.0060 0.7493 sec/batch\n",
      "Epoch 12/20  Iteration 2014/3560 Training loss: 2.0060 0.6608 sec/batch\n",
      "Epoch 12/20  Iteration 2015/3560 Training loss: 2.0061 0.7523 sec/batch\n",
      "Epoch 12/20  Iteration 2016/3560 Training loss: 2.0058 0.5591 sec/batch\n",
      "Epoch 12/20  Iteration 2017/3560 Training loss: 2.0056 0.6849 sec/batch\n",
      "Epoch 12/20  Iteration 2018/3560 Training loss: 2.0057 0.7131 sec/batch\n",
      "Epoch 12/20  Iteration 2019/3560 Training loss: 2.0056 0.6229 sec/batch\n",
      "Epoch 12/20  Iteration 2020/3560 Training loss: 2.0062 0.6022 sec/batch\n",
      "Epoch 12/20  Iteration 2021/3560 Training loss: 2.0066 0.6243 sec/batch\n",
      "Epoch 12/20  Iteration 2022/3560 Training loss: 2.0068 0.6700 sec/batch\n",
      "Epoch 12/20  Iteration 2023/3560 Training loss: 2.0066 0.6064 sec/batch\n",
      "Epoch 12/20  Iteration 2024/3560 Training loss: 2.0069 0.6598 sec/batch\n",
      "Epoch 12/20  Iteration 2025/3560 Training loss: 2.0069 0.6250 sec/batch\n",
      "Epoch 12/20  Iteration 2026/3560 Training loss: 2.0065 0.6840 sec/batch\n",
      "Epoch 12/20  Iteration 2027/3560 Training loss: 2.0062 0.6452 sec/batch\n",
      "Epoch 12/20  Iteration 2028/3560 Training loss: 2.0061 0.7262 sec/batch\n",
      "Epoch 12/20  Iteration 2029/3560 Training loss: 2.0066 0.6223 sec/batch\n",
      "Epoch 12/20  Iteration 2030/3560 Training loss: 2.0067 0.6068 sec/batch\n",
      "Epoch 12/20  Iteration 2031/3560 Training loss: 2.0070 0.6543 sec/batch\n",
      "Epoch 12/20  Iteration 2032/3560 Training loss: 2.0068 0.5465 sec/batch\n",
      "Epoch 12/20  Iteration 2033/3560 Training loss: 2.0068 0.6446 sec/batch\n",
      "Epoch 12/20  Iteration 2034/3560 Training loss: 2.0070 0.7098 sec/batch\n",
      "Epoch 12/20  Iteration 2035/3560 Training loss: 2.0069 0.7527 sec/batch\n",
      "Epoch 12/20  Iteration 2036/3560 Training loss: 2.0071 0.6776 sec/batch\n",
      "Epoch 12/20  Iteration 2037/3560 Training loss: 2.0067 0.5922 sec/batch\n",
      "Epoch 12/20  Iteration 2038/3560 Training loss: 2.0064 0.7269 sec/batch\n",
      "Epoch 12/20  Iteration 2039/3560 Training loss: 2.0060 0.6208 sec/batch\n",
      "Epoch 12/20  Iteration 2040/3560 Training loss: 2.0062 0.7035 sec/batch\n",
      "Epoch 12/20  Iteration 2041/3560 Training loss: 2.0058 0.7132 sec/batch\n",
      "Epoch 12/20  Iteration 2042/3560 Training loss: 2.0056 0.5960 sec/batch\n",
      "Epoch 12/20  Iteration 2043/3560 Training loss: 2.0050 0.7550 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 2044/3560 Training loss: 2.0048 0.6246 sec/batch\n",
      "Epoch 12/20  Iteration 2045/3560 Training loss: 2.0046 0.6662 sec/batch\n",
      "Epoch 12/20  Iteration 2046/3560 Training loss: 2.0044 0.6083 sec/batch\n",
      "Epoch 12/20  Iteration 2047/3560 Training loss: 2.0039 0.6176 sec/batch\n",
      "Epoch 12/20  Iteration 2048/3560 Training loss: 2.0039 0.7917 sec/batch\n",
      "Epoch 12/20  Iteration 2049/3560 Training loss: 2.0037 0.7076 sec/batch\n",
      "Epoch 12/20  Iteration 2050/3560 Training loss: 2.0035 0.6877 sec/batch\n",
      "Epoch 12/20  Iteration 2051/3560 Training loss: 2.0030 0.6842 sec/batch\n",
      "Epoch 12/20  Iteration 2052/3560 Training loss: 2.0027 0.7047 sec/batch\n",
      "Epoch 12/20  Iteration 2053/3560 Training loss: 2.0024 0.6742 sec/batch\n",
      "Epoch 12/20  Iteration 2054/3560 Training loss: 2.0023 0.6357 sec/batch\n",
      "Epoch 12/20  Iteration 2055/3560 Training loss: 2.0022 0.6248 sec/batch\n",
      "Epoch 12/20  Iteration 2056/3560 Training loss: 2.0019 0.5750 sec/batch\n",
      "Epoch 12/20  Iteration 2057/3560 Training loss: 2.0015 0.5840 sec/batch\n",
      "Epoch 12/20  Iteration 2058/3560 Training loss: 2.0011 0.6395 sec/batch\n",
      "Epoch 12/20  Iteration 2059/3560 Training loss: 2.0011 0.5562 sec/batch\n",
      "Epoch 12/20  Iteration 2060/3560 Training loss: 2.0011 0.7137 sec/batch\n",
      "Epoch 12/20  Iteration 2061/3560 Training loss: 2.0008 0.5992 sec/batch\n",
      "Epoch 12/20  Iteration 2062/3560 Training loss: 2.0006 0.7225 sec/batch\n",
      "Epoch 12/20  Iteration 2063/3560 Training loss: 2.0002 0.6331 sec/batch\n",
      "Epoch 12/20  Iteration 2064/3560 Training loss: 2.0002 0.7077 sec/batch\n",
      "Epoch 12/20  Iteration 2065/3560 Training loss: 2.0001 0.6607 sec/batch\n",
      "Epoch 12/20  Iteration 2066/3560 Training loss: 2.0001 0.6351 sec/batch\n",
      "Epoch 12/20  Iteration 2067/3560 Training loss: 2.0001 0.7415 sec/batch\n",
      "Epoch 12/20  Iteration 2068/3560 Training loss: 2.0000 0.5810 sec/batch\n",
      "Epoch 12/20  Iteration 2069/3560 Training loss: 2.0001 0.6367 sec/batch\n",
      "Epoch 12/20  Iteration 2070/3560 Training loss: 2.0000 0.5909 sec/batch\n",
      "Epoch 12/20  Iteration 2071/3560 Training loss: 1.9998 0.6577 sec/batch\n",
      "Epoch 12/20  Iteration 2072/3560 Training loss: 1.9997 0.5970 sec/batch\n",
      "Epoch 12/20  Iteration 2073/3560 Training loss: 1.9995 0.6493 sec/batch\n",
      "Epoch 12/20  Iteration 2074/3560 Training loss: 1.9991 0.6635 sec/batch\n",
      "Epoch 12/20  Iteration 2075/3560 Training loss: 1.9991 0.6331 sec/batch\n",
      "Epoch 12/20  Iteration 2076/3560 Training loss: 1.9990 0.7531 sec/batch\n",
      "Epoch 12/20  Iteration 2077/3560 Training loss: 1.9989 0.8732 sec/batch\n",
      "Epoch 12/20  Iteration 2078/3560 Training loss: 1.9989 0.7162 sec/batch\n",
      "Epoch 12/20  Iteration 2079/3560 Training loss: 1.9989 0.6482 sec/batch\n",
      "Epoch 12/20  Iteration 2080/3560 Training loss: 1.9987 0.6280 sec/batch\n",
      "Epoch 12/20  Iteration 2081/3560 Training loss: 1.9985 0.6891 sec/batch\n",
      "Epoch 12/20  Iteration 2082/3560 Training loss: 1.9987 0.5857 sec/batch\n",
      "Epoch 12/20  Iteration 2083/3560 Training loss: 1.9987 0.6055 sec/batch\n",
      "Epoch 12/20  Iteration 2084/3560 Training loss: 1.9983 0.6430 sec/batch\n",
      "Epoch 12/20  Iteration 2085/3560 Training loss: 1.9984 0.6367 sec/batch\n",
      "Epoch 12/20  Iteration 2086/3560 Training loss: 1.9985 0.6791 sec/batch\n",
      "Epoch 12/20  Iteration 2087/3560 Training loss: 1.9984 0.6728 sec/batch\n",
      "Epoch 12/20  Iteration 2088/3560 Training loss: 1.9984 0.7890 sec/batch\n",
      "Epoch 12/20  Iteration 2089/3560 Training loss: 1.9983 0.6174 sec/batch\n",
      "Epoch 12/20  Iteration 2090/3560 Training loss: 1.9980 0.6889 sec/batch\n",
      "Epoch 12/20  Iteration 2091/3560 Training loss: 1.9980 0.6777 sec/batch\n",
      "Epoch 12/20  Iteration 2092/3560 Training loss: 1.9981 0.6483 sec/batch\n",
      "Epoch 12/20  Iteration 2093/3560 Training loss: 1.9980 0.6569 sec/batch\n",
      "Epoch 12/20  Iteration 2094/3560 Training loss: 1.9979 0.6192 sec/batch\n",
      "Epoch 12/20  Iteration 2095/3560 Training loss: 1.9980 0.6751 sec/batch\n",
      "Epoch 12/20  Iteration 2096/3560 Training loss: 1.9981 0.6318 sec/batch\n",
      "Epoch 12/20  Iteration 2097/3560 Training loss: 1.9981 0.6152 sec/batch\n",
      "Epoch 12/20  Iteration 2098/3560 Training loss: 1.9979 0.6347 sec/batch\n",
      "Epoch 12/20  Iteration 2099/3560 Training loss: 1.9981 0.6595 sec/batch\n",
      "Epoch 12/20  Iteration 2100/3560 Training loss: 1.9981 0.7259 sec/batch\n",
      "Epoch 12/20  Iteration 2101/3560 Training loss: 1.9981 0.6463 sec/batch\n",
      "Epoch 12/20  Iteration 2102/3560 Training loss: 1.9981 0.6684 sec/batch\n",
      "Epoch 12/20  Iteration 2103/3560 Training loss: 1.9980 0.7011 sec/batch\n",
      "Epoch 12/20  Iteration 2104/3560 Training loss: 1.9980 0.6755 sec/batch\n",
      "Epoch 12/20  Iteration 2105/3560 Training loss: 1.9980 0.6559 sec/batch\n",
      "Epoch 12/20  Iteration 2106/3560 Training loss: 1.9981 0.6233 sec/batch\n",
      "Epoch 12/20  Iteration 2107/3560 Training loss: 1.9980 0.6787 sec/batch\n",
      "Epoch 12/20  Iteration 2108/3560 Training loss: 1.9979 0.5716 sec/batch\n",
      "Epoch 12/20  Iteration 2109/3560 Training loss: 1.9978 0.5519 sec/batch\n",
      "Epoch 12/20  Iteration 2110/3560 Training loss: 1.9980 0.6111 sec/batch\n",
      "Epoch 12/20  Iteration 2111/3560 Training loss: 1.9979 0.7225 sec/batch\n",
      "Epoch 12/20  Iteration 2112/3560 Training loss: 1.9979 0.7427 sec/batch\n",
      "Epoch 12/20  Iteration 2113/3560 Training loss: 1.9978 0.5584 sec/batch\n",
      "Epoch 12/20  Iteration 2114/3560 Training loss: 1.9977 0.8442 sec/batch\n",
      "Epoch 12/20  Iteration 2115/3560 Training loss: 1.9977 0.6987 sec/batch\n",
      "Epoch 12/20  Iteration 2116/3560 Training loss: 1.9976 0.6435 sec/batch\n",
      "Epoch 12/20  Iteration 2117/3560 Training loss: 1.9974 0.5828 sec/batch\n",
      "Epoch 12/20  Iteration 2118/3560 Training loss: 1.9975 0.6107 sec/batch\n",
      "Epoch 12/20  Iteration 2119/3560 Training loss: 1.9977 0.6180 sec/batch\n",
      "Epoch 12/20  Iteration 2120/3560 Training loss: 1.9976 0.6107 sec/batch\n",
      "Epoch 12/20  Iteration 2121/3560 Training loss: 1.9976 0.5924 sec/batch\n",
      "Epoch 12/20  Iteration 2122/3560 Training loss: 1.9975 0.6403 sec/batch\n",
      "Epoch 12/20  Iteration 2123/3560 Training loss: 1.9975 0.6482 sec/batch\n",
      "Epoch 12/20  Iteration 2124/3560 Training loss: 1.9974 0.6879 sec/batch\n",
      "Epoch 12/20  Iteration 2125/3560 Training loss: 1.9973 0.6646 sec/batch\n",
      "Epoch 12/20  Iteration 2126/3560 Training loss: 1.9976 0.6729 sec/batch\n",
      "Epoch 12/20  Iteration 2127/3560 Training loss: 1.9975 0.6913 sec/batch\n",
      "Epoch 12/20  Iteration 2128/3560 Training loss: 1.9974 0.7257 sec/batch\n",
      "Epoch 12/20  Iteration 2129/3560 Training loss: 1.9973 0.7149 sec/batch\n",
      "Epoch 12/20  Iteration 2130/3560 Training loss: 1.9971 0.5960 sec/batch\n",
      "Epoch 12/20  Iteration 2131/3560 Training loss: 1.9972 0.6553 sec/batch\n",
      "Epoch 12/20  Iteration 2132/3560 Training loss: 1.9972 0.6239 sec/batch\n",
      "Epoch 12/20  Iteration 2133/3560 Training loss: 1.9972 0.6170 sec/batch\n",
      "Epoch 12/20  Iteration 2134/3560 Training loss: 1.9971 0.6550 sec/batch\n",
      "Epoch 12/20  Iteration 2135/3560 Training loss: 1.9969 0.6556 sec/batch\n",
      "Epoch 12/20  Iteration 2136/3560 Training loss: 1.9969 0.6087 sec/batch\n",
      "Epoch 13/20  Iteration 2137/3560 Training loss: 2.0556 0.7611 sec/batch\n",
      "Epoch 13/20  Iteration 2138/3560 Training loss: 2.0123 0.7122 sec/batch\n",
      "Epoch 13/20  Iteration 2139/3560 Training loss: 1.9994 0.6225 sec/batch\n",
      "Epoch 13/20  Iteration 2140/3560 Training loss: 1.9942 0.7391 sec/batch\n",
      "Epoch 13/20  Iteration 2141/3560 Training loss: 1.9918 0.6471 sec/batch\n",
      "Epoch 13/20  Iteration 2142/3560 Training loss: 1.9851 0.6716 sec/batch\n",
      "Epoch 13/20  Iteration 2143/3560 Training loss: 1.9859 0.6493 sec/batch\n",
      "Epoch 13/20  Iteration 2144/3560 Training loss: 1.9852 0.6307 sec/batch\n",
      "Epoch 13/20  Iteration 2145/3560 Training loss: 1.9875 0.6625 sec/batch\n",
      "Epoch 13/20  Iteration 2146/3560 Training loss: 1.9879 0.6618 sec/batch\n",
      "Epoch 13/20  Iteration 2147/3560 Training loss: 1.9854 0.5818 sec/batch\n",
      "Epoch 13/20  Iteration 2148/3560 Training loss: 1.9848 0.6032 sec/batch\n",
      "Epoch 13/20  Iteration 2149/3560 Training loss: 1.9845 0.6965 sec/batch\n",
      "Epoch 13/20  Iteration 2150/3560 Training loss: 1.9870 0.6477 sec/batch\n",
      "Epoch 13/20  Iteration 2151/3560 Training loss: 1.9864 0.6764 sec/batch\n",
      "Epoch 13/20  Iteration 2152/3560 Training loss: 1.9849 0.6454 sec/batch\n",
      "Epoch 13/20  Iteration 2153/3560 Training loss: 1.9850 0.7273 sec/batch\n",
      "Epoch 13/20  Iteration 2154/3560 Training loss: 1.9871 0.6508 sec/batch\n",
      "Epoch 13/20  Iteration 2155/3560 Training loss: 1.9862 0.5893 sec/batch\n",
      "Epoch 13/20  Iteration 2156/3560 Training loss: 1.9861 0.6394 sec/batch\n",
      "Epoch 13/20  Iteration 2157/3560 Training loss: 1.9856 0.6383 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2158/3560 Training loss: 1.9867 0.6408 sec/batch\n",
      "Epoch 13/20  Iteration 2159/3560 Training loss: 1.9860 0.6944 sec/batch\n",
      "Epoch 13/20  Iteration 2160/3560 Training loss: 1.9854 0.6311 sec/batch\n",
      "Epoch 13/20  Iteration 2161/3560 Training loss: 1.9848 0.5713 sec/batch\n",
      "Epoch 13/20  Iteration 2162/3560 Training loss: 1.9839 0.6431 sec/batch\n",
      "Epoch 13/20  Iteration 2163/3560 Training loss: 1.9835 0.6791 sec/batch\n",
      "Epoch 13/20  Iteration 2164/3560 Training loss: 1.9837 0.6732 sec/batch\n",
      "Epoch 13/20  Iteration 2165/3560 Training loss: 1.9845 0.6344 sec/batch\n",
      "Epoch 13/20  Iteration 2166/3560 Training loss: 1.9848 0.6038 sec/batch\n",
      "Epoch 13/20  Iteration 2167/3560 Training loss: 1.9849 0.7118 sec/batch\n",
      "Epoch 13/20  Iteration 2168/3560 Training loss: 1.9842 0.6287 sec/batch\n",
      "Epoch 13/20  Iteration 2169/3560 Training loss: 1.9846 0.6244 sec/batch\n",
      "Epoch 13/20  Iteration 2170/3560 Training loss: 1.9853 0.5801 sec/batch\n",
      "Epoch 13/20  Iteration 2171/3560 Training loss: 1.9849 0.6675 sec/batch\n",
      "Epoch 13/20  Iteration 2172/3560 Training loss: 1.9849 0.6323 sec/batch\n",
      "Epoch 13/20  Iteration 2173/3560 Training loss: 1.9843 0.6377 sec/batch\n",
      "Epoch 13/20  Iteration 2174/3560 Training loss: 1.9833 0.6991 sec/batch\n",
      "Epoch 13/20  Iteration 2175/3560 Training loss: 1.9822 0.6209 sec/batch\n",
      "Epoch 13/20  Iteration 2176/3560 Training loss: 1.9817 0.5617 sec/batch\n",
      "Epoch 13/20  Iteration 2177/3560 Training loss: 1.9813 0.6969 sec/batch\n",
      "Epoch 13/20  Iteration 2178/3560 Training loss: 1.9812 0.6679 sec/batch\n",
      "Epoch 13/20  Iteration 2179/3560 Training loss: 1.9808 0.6934 sec/batch\n",
      "Epoch 13/20  Iteration 2180/3560 Training loss: 1.9799 0.7713 sec/batch\n",
      "Epoch 13/20  Iteration 2181/3560 Training loss: 1.9799 0.6004 sec/batch\n",
      "Epoch 13/20  Iteration 2182/3560 Training loss: 1.9786 0.6118 sec/batch\n",
      "Epoch 13/20  Iteration 2183/3560 Training loss: 1.9785 0.6907 sec/batch\n",
      "Epoch 13/20  Iteration 2184/3560 Training loss: 1.9783 0.6906 sec/batch\n",
      "Epoch 13/20  Iteration 2185/3560 Training loss: 1.9781 0.6308 sec/batch\n",
      "Epoch 13/20  Iteration 2186/3560 Training loss: 1.9789 0.6566 sec/batch\n",
      "Epoch 13/20  Iteration 2187/3560 Training loss: 1.9783 0.6183 sec/batch\n",
      "Epoch 13/20  Iteration 2188/3560 Training loss: 1.9791 0.7069 sec/batch\n",
      "Epoch 13/20  Iteration 2189/3560 Training loss: 1.9791 0.6579 sec/batch\n",
      "Epoch 13/20  Iteration 2190/3560 Training loss: 1.9789 0.7744 sec/batch\n",
      "Epoch 13/20  Iteration 2191/3560 Training loss: 1.9786 0.6570 sec/batch\n",
      "Epoch 13/20  Iteration 2192/3560 Training loss: 1.9786 0.6471 sec/batch\n",
      "Epoch 13/20  Iteration 2193/3560 Training loss: 1.9788 0.6526 sec/batch\n",
      "Epoch 13/20  Iteration 2194/3560 Training loss: 1.9784 0.6066 sec/batch\n",
      "Epoch 13/20  Iteration 2195/3560 Training loss: 1.9780 0.6986 sec/batch\n",
      "Epoch 13/20  Iteration 2196/3560 Training loss: 1.9782 0.6310 sec/batch\n",
      "Epoch 13/20  Iteration 2197/3560 Training loss: 1.9780 0.6519 sec/batch\n",
      "Epoch 13/20  Iteration 2198/3560 Training loss: 1.9785 0.7177 sec/batch\n",
      "Epoch 13/20  Iteration 2199/3560 Training loss: 1.9788 0.6222 sec/batch\n",
      "Epoch 13/20  Iteration 2200/3560 Training loss: 1.9789 0.6740 sec/batch\n",
      "Epoch 13/20  Iteration 2201/3560 Training loss: 1.9788 0.6077 sec/batch\n",
      "Epoch 13/20  Iteration 2202/3560 Training loss: 1.9791 0.7000 sec/batch\n",
      "Epoch 13/20  Iteration 2203/3560 Training loss: 1.9792 0.7072 sec/batch\n",
      "Epoch 13/20  Iteration 2204/3560 Training loss: 1.9786 0.6965 sec/batch\n",
      "Epoch 13/20  Iteration 2205/3560 Training loss: 1.9783 0.6885 sec/batch\n",
      "Epoch 13/20  Iteration 2206/3560 Training loss: 1.9783 0.6245 sec/batch\n",
      "Epoch 13/20  Iteration 2207/3560 Training loss: 1.9786 0.6429 sec/batch\n",
      "Epoch 13/20  Iteration 2208/3560 Training loss: 1.9788 0.6170 sec/batch\n",
      "Epoch 13/20  Iteration 2209/3560 Training loss: 1.9791 0.6173 sec/batch\n",
      "Epoch 13/20  Iteration 2210/3560 Training loss: 1.9789 0.6244 sec/batch\n",
      "Epoch 13/20  Iteration 2211/3560 Training loss: 1.9790 0.6770 sec/batch\n",
      "Epoch 13/20  Iteration 2212/3560 Training loss: 1.9792 0.6156 sec/batch\n",
      "Epoch 13/20  Iteration 2213/3560 Training loss: 1.9792 0.6249 sec/batch\n",
      "Epoch 13/20  Iteration 2214/3560 Training loss: 1.9792 0.5876 sec/batch\n",
      "Epoch 13/20  Iteration 2215/3560 Training loss: 1.9788 0.7516 sec/batch\n",
      "Epoch 13/20  Iteration 2216/3560 Training loss: 1.9786 0.6831 sec/batch\n",
      "Epoch 13/20  Iteration 2217/3560 Training loss: 1.9782 0.6388 sec/batch\n",
      "Epoch 13/20  Iteration 2218/3560 Training loss: 1.9784 0.6799 sec/batch\n",
      "Epoch 13/20  Iteration 2219/3560 Training loss: 1.9779 0.6333 sec/batch\n",
      "Epoch 13/20  Iteration 2220/3560 Training loss: 1.9777 0.6608 sec/batch\n",
      "Epoch 13/20  Iteration 2221/3560 Training loss: 1.9773 0.6902 sec/batch\n",
      "Epoch 13/20  Iteration 2222/3560 Training loss: 1.9771 0.6376 sec/batch\n",
      "Epoch 13/20  Iteration 2223/3560 Training loss: 1.9770 0.6226 sec/batch\n",
      "Epoch 13/20  Iteration 2224/3560 Training loss: 1.9768 0.6212 sec/batch\n",
      "Epoch 13/20  Iteration 2225/3560 Training loss: 1.9764 0.6490 sec/batch\n",
      "Epoch 13/20  Iteration 2226/3560 Training loss: 1.9764 0.6584 sec/batch\n",
      "Epoch 13/20  Iteration 2227/3560 Training loss: 1.9761 0.6599 sec/batch\n",
      "Epoch 13/20  Iteration 2228/3560 Training loss: 1.9761 0.5904 sec/batch\n",
      "Epoch 13/20  Iteration 2229/3560 Training loss: 1.9756 0.7034 sec/batch\n",
      "Epoch 13/20  Iteration 2230/3560 Training loss: 1.9753 0.7057 sec/batch\n",
      "Epoch 13/20  Iteration 2231/3560 Training loss: 1.9750 0.6443 sec/batch\n",
      "Epoch 13/20  Iteration 2232/3560 Training loss: 1.9750 0.6173 sec/batch\n",
      "Epoch 13/20  Iteration 2233/3560 Training loss: 1.9748 0.6324 sec/batch\n",
      "Epoch 13/20  Iteration 2234/3560 Training loss: 1.9745 0.6852 sec/batch\n",
      "Epoch 13/20  Iteration 2235/3560 Training loss: 1.9742 0.6107 sec/batch\n",
      "Epoch 13/20  Iteration 2236/3560 Training loss: 1.9737 0.6990 sec/batch\n",
      "Epoch 13/20  Iteration 2237/3560 Training loss: 1.9737 0.6536 sec/batch\n",
      "Epoch 13/20  Iteration 2238/3560 Training loss: 1.9736 0.5918 sec/batch\n",
      "Epoch 13/20  Iteration 2239/3560 Training loss: 1.9733 0.6717 sec/batch\n",
      "Epoch 13/20  Iteration 2240/3560 Training loss: 1.9730 0.6887 sec/batch\n",
      "Epoch 13/20  Iteration 2241/3560 Training loss: 1.9728 0.7540 sec/batch\n",
      "Epoch 13/20  Iteration 2242/3560 Training loss: 1.9728 0.6936 sec/batch\n",
      "Epoch 13/20  Iteration 2243/3560 Training loss: 1.9727 0.6036 sec/batch\n",
      "Epoch 13/20  Iteration 2244/3560 Training loss: 1.9726 0.7121 sec/batch\n",
      "Epoch 13/20  Iteration 2245/3560 Training loss: 1.9727 0.6430 sec/batch\n",
      "Epoch 13/20  Iteration 2246/3560 Training loss: 1.9726 0.6091 sec/batch\n",
      "Epoch 13/20  Iteration 2247/3560 Training loss: 1.9726 0.6593 sec/batch\n",
      "Epoch 13/20  Iteration 2248/3560 Training loss: 1.9726 0.6420 sec/batch\n",
      "Epoch 13/20  Iteration 2249/3560 Training loss: 1.9725 0.6282 sec/batch\n",
      "Epoch 13/20  Iteration 2250/3560 Training loss: 1.9724 0.7082 sec/batch\n",
      "Epoch 13/20  Iteration 2251/3560 Training loss: 1.9722 0.6075 sec/batch\n",
      "Epoch 13/20  Iteration 2252/3560 Training loss: 1.9718 0.7778 sec/batch\n",
      "Epoch 13/20  Iteration 2253/3560 Training loss: 1.9718 0.7119 sec/batch\n",
      "Epoch 13/20  Iteration 2254/3560 Training loss: 1.9716 1.1179 sec/batch\n",
      "Epoch 13/20  Iteration 2255/3560 Training loss: 1.9716 0.6505 sec/batch\n",
      "Epoch 13/20  Iteration 2256/3560 Training loss: 1.9715 0.6423 sec/batch\n",
      "Epoch 13/20  Iteration 2257/3560 Training loss: 1.9715 0.6696 sec/batch\n",
      "Epoch 13/20  Iteration 2258/3560 Training loss: 1.9714 0.5892 sec/batch\n",
      "Epoch 13/20  Iteration 2259/3560 Training loss: 1.9712 0.6120 sec/batch\n",
      "Epoch 13/20  Iteration 2260/3560 Training loss: 1.9713 0.6888 sec/batch\n",
      "Epoch 13/20  Iteration 2261/3560 Training loss: 1.9713 0.6777 sec/batch\n",
      "Epoch 13/20  Iteration 2262/3560 Training loss: 1.9709 0.6436 sec/batch\n",
      "Epoch 13/20  Iteration 2263/3560 Training loss: 1.9710 0.6438 sec/batch\n",
      "Epoch 13/20  Iteration 2264/3560 Training loss: 1.9711 0.7364 sec/batch\n",
      "Epoch 13/20  Iteration 2265/3560 Training loss: 1.9710 0.7385 sec/batch\n",
      "Epoch 13/20  Iteration 2266/3560 Training loss: 1.9710 0.8010 sec/batch\n",
      "Epoch 13/20  Iteration 2267/3560 Training loss: 1.9708 0.6200 sec/batch\n",
      "Epoch 13/20  Iteration 2268/3560 Training loss: 1.9706 0.6905 sec/batch\n",
      "Epoch 13/20  Iteration 2269/3560 Training loss: 1.9706 0.6518 sec/batch\n",
      "Epoch 13/20  Iteration 2270/3560 Training loss: 1.9707 0.5743 sec/batch\n",
      "Epoch 13/20  Iteration 2271/3560 Training loss: 1.9707 0.6329 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2272/3560 Training loss: 1.9707 0.5748 sec/batch\n",
      "Epoch 13/20  Iteration 2273/3560 Training loss: 1.9708 0.6692 sec/batch\n",
      "Epoch 13/20  Iteration 2274/3560 Training loss: 1.9708 0.6240 sec/batch\n",
      "Epoch 13/20  Iteration 2275/3560 Training loss: 1.9709 0.6183 sec/batch\n",
      "Epoch 13/20  Iteration 2276/3560 Training loss: 1.9707 0.6561 sec/batch\n",
      "Epoch 13/20  Iteration 2277/3560 Training loss: 1.9709 0.6356 sec/batch\n",
      "Epoch 13/20  Iteration 2278/3560 Training loss: 1.9707 0.6763 sec/batch\n",
      "Epoch 13/20  Iteration 2279/3560 Training loss: 1.9707 0.7280 sec/batch\n",
      "Epoch 13/20  Iteration 2280/3560 Training loss: 1.9707 0.6057 sec/batch\n",
      "Epoch 13/20  Iteration 2281/3560 Training loss: 1.9706 0.6710 sec/batch\n",
      "Epoch 13/20  Iteration 2282/3560 Training loss: 1.9706 0.6207 sec/batch\n",
      "Epoch 13/20  Iteration 2283/3560 Training loss: 1.9706 0.6257 sec/batch\n",
      "Epoch 13/20  Iteration 2284/3560 Training loss: 1.9707 0.6020 sec/batch\n",
      "Epoch 13/20  Iteration 2285/3560 Training loss: 1.9707 0.6483 sec/batch\n",
      "Epoch 13/20  Iteration 2286/3560 Training loss: 1.9705 0.6077 sec/batch\n",
      "Epoch 13/20  Iteration 2287/3560 Training loss: 1.9704 0.6101 sec/batch\n",
      "Epoch 13/20  Iteration 2288/3560 Training loss: 1.9705 0.5898 sec/batch\n",
      "Epoch 13/20  Iteration 2289/3560 Training loss: 1.9705 0.6127 sec/batch\n",
      "Epoch 13/20  Iteration 2290/3560 Training loss: 1.9705 0.7319 sec/batch\n",
      "Epoch 13/20  Iteration 2291/3560 Training loss: 1.9703 0.7673 sec/batch\n",
      "Epoch 13/20  Iteration 2292/3560 Training loss: 1.9703 0.6751 sec/batch\n",
      "Epoch 13/20  Iteration 2293/3560 Training loss: 1.9703 0.6546 sec/batch\n",
      "Epoch 13/20  Iteration 2294/3560 Training loss: 1.9702 0.6131 sec/batch\n",
      "Epoch 13/20  Iteration 2295/3560 Training loss: 1.9700 0.6197 sec/batch\n",
      "Epoch 13/20  Iteration 2296/3560 Training loss: 1.9702 0.6657 sec/batch\n",
      "Epoch 13/20  Iteration 2297/3560 Training loss: 1.9704 0.6160 sec/batch\n",
      "Epoch 13/20  Iteration 2298/3560 Training loss: 1.9703 0.6293 sec/batch\n",
      "Epoch 13/20  Iteration 2299/3560 Training loss: 1.9703 0.6003 sec/batch\n",
      "Epoch 13/20  Iteration 2300/3560 Training loss: 1.9702 0.6472 sec/batch\n",
      "Epoch 13/20  Iteration 2301/3560 Training loss: 1.9702 0.6621 sec/batch\n",
      "Epoch 13/20  Iteration 2302/3560 Training loss: 1.9701 0.6390 sec/batch\n",
      "Epoch 13/20  Iteration 2303/3560 Training loss: 1.9701 0.7471 sec/batch\n",
      "Epoch 13/20  Iteration 2304/3560 Training loss: 1.9704 0.7703 sec/batch\n",
      "Epoch 13/20  Iteration 2305/3560 Training loss: 1.9703 0.7320 sec/batch\n",
      "Epoch 13/20  Iteration 2306/3560 Training loss: 1.9703 0.9012 sec/batch\n",
      "Epoch 13/20  Iteration 2307/3560 Training loss: 1.9701 0.6981 sec/batch\n",
      "Epoch 13/20  Iteration 2308/3560 Training loss: 1.9700 0.6595 sec/batch\n",
      "Epoch 13/20  Iteration 2309/3560 Training loss: 1.9701 0.6847 sec/batch\n",
      "Epoch 13/20  Iteration 2310/3560 Training loss: 1.9701 0.6492 sec/batch\n",
      "Epoch 13/20  Iteration 2311/3560 Training loss: 1.9701 0.6423 sec/batch\n",
      "Epoch 13/20  Iteration 2312/3560 Training loss: 1.9701 0.7128 sec/batch\n",
      "Epoch 13/20  Iteration 2313/3560 Training loss: 1.9700 0.5890 sec/batch\n",
      "Epoch 13/20  Iteration 2314/3560 Training loss: 1.9700 0.7127 sec/batch\n",
      "Epoch 14/20  Iteration 2315/3560 Training loss: 2.0280 0.6217 sec/batch\n",
      "Epoch 14/20  Iteration 2316/3560 Training loss: 1.9871 0.7265 sec/batch\n",
      "Epoch 14/20  Iteration 2317/3560 Training loss: 1.9687 0.7257 sec/batch\n",
      "Epoch 14/20  Iteration 2318/3560 Training loss: 1.9666 0.6499 sec/batch\n",
      "Epoch 14/20  Iteration 2319/3560 Training loss: 1.9647 0.6547 sec/batch\n",
      "Epoch 14/20  Iteration 2320/3560 Training loss: 1.9585 0.6256 sec/batch\n",
      "Epoch 14/20  Iteration 2321/3560 Training loss: 1.9579 0.6424 sec/batch\n",
      "Epoch 14/20  Iteration 2322/3560 Training loss: 1.9584 0.6306 sec/batch\n",
      "Epoch 14/20  Iteration 2323/3560 Training loss: 1.9619 0.6341 sec/batch\n",
      "Epoch 14/20  Iteration 2324/3560 Training loss: 1.9626 0.6016 sec/batch\n",
      "Epoch 14/20  Iteration 2325/3560 Training loss: 1.9603 0.7445 sec/batch\n",
      "Epoch 14/20  Iteration 2326/3560 Training loss: 1.9595 0.6405 sec/batch\n",
      "Epoch 14/20  Iteration 2327/3560 Training loss: 1.9597 0.6538 sec/batch\n",
      "Epoch 14/20  Iteration 2328/3560 Training loss: 1.9623 0.7045 sec/batch\n",
      "Epoch 14/20  Iteration 2329/3560 Training loss: 1.9619 0.7381 sec/batch\n",
      "Epoch 14/20  Iteration 2330/3560 Training loss: 1.9607 0.6358 sec/batch\n",
      "Epoch 14/20  Iteration 2331/3560 Training loss: 1.9607 0.7655 sec/batch\n",
      "Epoch 14/20  Iteration 2332/3560 Training loss: 1.9624 0.5786 sec/batch\n",
      "Epoch 14/20  Iteration 2333/3560 Training loss: 1.9616 0.6391 sec/batch\n",
      "Epoch 14/20  Iteration 2334/3560 Training loss: 1.9618 0.5420 sec/batch\n",
      "Epoch 14/20  Iteration 2335/3560 Training loss: 1.9609 0.6748 sec/batch\n",
      "Epoch 14/20  Iteration 2336/3560 Training loss: 1.9623 0.6349 sec/batch\n",
      "Epoch 14/20  Iteration 2337/3560 Training loss: 1.9620 0.5949 sec/batch\n",
      "Epoch 14/20  Iteration 2338/3560 Training loss: 1.9612 0.6414 sec/batch\n",
      "Epoch 14/20  Iteration 2339/3560 Training loss: 1.9608 0.6062 sec/batch\n",
      "Epoch 14/20  Iteration 2340/3560 Training loss: 1.9600 0.6793 sec/batch\n",
      "Epoch 14/20  Iteration 2341/3560 Training loss: 1.9596 0.7098 sec/batch\n",
      "Epoch 14/20  Iteration 2342/3560 Training loss: 1.9601 0.7775 sec/batch\n",
      "Epoch 14/20  Iteration 2343/3560 Training loss: 1.9610 0.7550 sec/batch\n",
      "Epoch 14/20  Iteration 2344/3560 Training loss: 1.9611 0.7050 sec/batch\n",
      "Epoch 14/20  Iteration 2345/3560 Training loss: 1.9612 0.5863 sec/batch\n",
      "Epoch 14/20  Iteration 2346/3560 Training loss: 1.9604 0.7478 sec/batch\n",
      "Epoch 14/20  Iteration 2347/3560 Training loss: 1.9601 0.5432 sec/batch\n",
      "Epoch 14/20  Iteration 2348/3560 Training loss: 1.9611 0.5957 sec/batch\n",
      "Epoch 14/20  Iteration 2349/3560 Training loss: 1.9608 0.6649 sec/batch\n",
      "Epoch 14/20  Iteration 2350/3560 Training loss: 1.9604 0.5803 sec/batch\n",
      "Epoch 14/20  Iteration 2351/3560 Training loss: 1.9599 0.6622 sec/batch\n",
      "Epoch 14/20  Iteration 2352/3560 Training loss: 1.9589 0.6786 sec/batch\n",
      "Epoch 14/20  Iteration 2353/3560 Training loss: 1.9581 0.6717 sec/batch\n",
      "Epoch 14/20  Iteration 2354/3560 Training loss: 1.9575 0.7419 sec/batch\n",
      "Epoch 14/20  Iteration 2355/3560 Training loss: 1.9569 0.7665 sec/batch\n",
      "Epoch 14/20  Iteration 2356/3560 Training loss: 1.9566 0.6961 sec/batch\n",
      "Epoch 14/20  Iteration 2357/3560 Training loss: 1.9558 0.6578 sec/batch\n",
      "Epoch 14/20  Iteration 2358/3560 Training loss: 1.9551 0.6145 sec/batch\n",
      "Epoch 14/20  Iteration 2359/3560 Training loss: 1.9553 0.6953 sec/batch\n",
      "Epoch 14/20  Iteration 2360/3560 Training loss: 1.9542 0.5960 sec/batch\n",
      "Epoch 14/20  Iteration 2361/3560 Training loss: 1.9541 0.6245 sec/batch\n",
      "Epoch 14/20  Iteration 2362/3560 Training loss: 1.9537 0.6187 sec/batch\n",
      "Epoch 14/20  Iteration 2363/3560 Training loss: 1.9537 0.6477 sec/batch\n",
      "Epoch 14/20  Iteration 2364/3560 Training loss: 1.9545 0.6039 sec/batch\n",
      "Epoch 14/20  Iteration 2365/3560 Training loss: 1.9541 0.6192 sec/batch\n",
      "Epoch 14/20  Iteration 2366/3560 Training loss: 1.9549 0.6978 sec/batch\n",
      "Epoch 14/20  Iteration 2367/3560 Training loss: 1.9545 0.7063 sec/batch\n",
      "Epoch 14/20  Iteration 2368/3560 Training loss: 1.9543 0.6732 sec/batch\n",
      "Epoch 14/20  Iteration 2369/3560 Training loss: 1.9537 0.7030 sec/batch\n",
      "Epoch 14/20  Iteration 2370/3560 Training loss: 1.9538 0.6373 sec/batch\n",
      "Epoch 14/20  Iteration 2371/3560 Training loss: 1.9540 0.6728 sec/batch\n",
      "Epoch 14/20  Iteration 2372/3560 Training loss: 1.9537 0.6052 sec/batch\n",
      "Epoch 14/20  Iteration 2373/3560 Training loss: 1.9534 0.6368 sec/batch\n",
      "Epoch 14/20  Iteration 2374/3560 Training loss: 1.9536 0.6642 sec/batch\n",
      "Epoch 14/20  Iteration 2375/3560 Training loss: 1.9534 0.6299 sec/batch\n",
      "Epoch 14/20  Iteration 2376/3560 Training loss: 1.9540 0.6307 sec/batch\n",
      "Epoch 14/20  Iteration 2377/3560 Training loss: 1.9542 0.5693 sec/batch\n",
      "Epoch 14/20  Iteration 2378/3560 Training loss: 1.9543 0.6784 sec/batch\n",
      "Epoch 14/20  Iteration 2379/3560 Training loss: 1.9542 0.6542 sec/batch\n",
      "Epoch 14/20  Iteration 2380/3560 Training loss: 1.9546 0.7389 sec/batch\n",
      "Epoch 14/20  Iteration 2381/3560 Training loss: 1.9546 0.6234 sec/batch\n",
      "Epoch 14/20  Iteration 2382/3560 Training loss: 1.9541 0.6328 sec/batch\n",
      "Epoch 14/20  Iteration 2383/3560 Training loss: 1.9540 0.6375 sec/batch\n",
      "Epoch 14/20  Iteration 2384/3560 Training loss: 1.9540 0.6652 sec/batch\n",
      "Epoch 14/20  Iteration 2385/3560 Training loss: 1.9544 0.5765 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2386/3560 Training loss: 1.9545 0.6468 sec/batch\n",
      "Epoch 14/20  Iteration 2387/3560 Training loss: 1.9548 0.6376 sec/batch\n",
      "Epoch 14/20  Iteration 2388/3560 Training loss: 1.9545 0.6335 sec/batch\n",
      "Epoch 14/20  Iteration 2389/3560 Training loss: 1.9545 0.6194 sec/batch\n",
      "Epoch 14/20  Iteration 2390/3560 Training loss: 1.9548 0.6338 sec/batch\n",
      "Epoch 14/20  Iteration 2391/3560 Training loss: 1.9548 0.6823 sec/batch\n",
      "Epoch 14/20  Iteration 2392/3560 Training loss: 1.9549 0.6853 sec/batch\n",
      "Epoch 14/20  Iteration 2393/3560 Training loss: 1.9545 0.7561 sec/batch\n",
      "Epoch 14/20  Iteration 2394/3560 Training loss: 1.9544 0.6482 sec/batch\n",
      "Epoch 14/20  Iteration 2395/3560 Training loss: 1.9539 0.6360 sec/batch\n",
      "Epoch 14/20  Iteration 2396/3560 Training loss: 1.9540 0.6758 sec/batch\n",
      "Epoch 14/20  Iteration 2397/3560 Training loss: 1.9535 0.5447 sec/batch\n",
      "Epoch 14/20  Iteration 2398/3560 Training loss: 1.9534 0.6724 sec/batch\n",
      "Epoch 14/20  Iteration 2399/3560 Training loss: 1.9529 0.6331 sec/batch\n",
      "Epoch 14/20  Iteration 2400/3560 Training loss: 1.9526 0.6198 sec/batch\n",
      "Epoch 14/20  Iteration 2401/3560 Training loss: 1.9525 0.5809 sec/batch\n",
      "Epoch 14/20  Iteration 2402/3560 Training loss: 1.9521 0.6495 sec/batch\n",
      "Epoch 14/20  Iteration 2403/3560 Training loss: 1.9516 0.6654 sec/batch\n",
      "Epoch 14/20  Iteration 2404/3560 Training loss: 1.9517 0.7354 sec/batch\n",
      "Epoch 14/20  Iteration 2405/3560 Training loss: 1.9515 0.6483 sec/batch\n",
      "Epoch 14/20  Iteration 2406/3560 Training loss: 1.9514 0.7000 sec/batch\n",
      "Epoch 14/20  Iteration 2407/3560 Training loss: 1.9509 0.6719 sec/batch\n",
      "Epoch 14/20  Iteration 2408/3560 Training loss: 1.9505 0.6570 sec/batch\n",
      "Epoch 14/20  Iteration 2409/3560 Training loss: 1.9502 0.5890 sec/batch\n",
      "Epoch 14/20  Iteration 2410/3560 Training loss: 1.9501 0.7303 sec/batch\n",
      "Epoch 14/20  Iteration 2411/3560 Training loss: 1.9500 0.6617 sec/batch\n",
      "Epoch 14/20  Iteration 2412/3560 Training loss: 1.9497 0.6794 sec/batch\n",
      "Epoch 14/20  Iteration 2413/3560 Training loss: 1.9492 0.6137 sec/batch\n",
      "Epoch 14/20  Iteration 2414/3560 Training loss: 1.9487 0.6464 sec/batch\n",
      "Epoch 14/20  Iteration 2415/3560 Training loss: 1.9487 0.5991 sec/batch\n",
      "Epoch 14/20  Iteration 2416/3560 Training loss: 1.9487 0.7930 sec/batch\n",
      "Epoch 14/20  Iteration 2417/3560 Training loss: 1.9484 0.7435 sec/batch\n",
      "Epoch 14/20  Iteration 2418/3560 Training loss: 1.9482 0.7472 sec/batch\n",
      "Epoch 14/20  Iteration 2419/3560 Training loss: 1.9481 0.7285 sec/batch\n",
      "Epoch 14/20  Iteration 2420/3560 Training loss: 1.9481 0.7395 sec/batch\n",
      "Epoch 14/20  Iteration 2421/3560 Training loss: 1.9480 0.7331 sec/batch\n",
      "Epoch 14/20  Iteration 2422/3560 Training loss: 1.9480 0.6256 sec/batch\n",
      "Epoch 14/20  Iteration 2423/3560 Training loss: 1.9479 0.7184 sec/batch\n",
      "Epoch 14/20  Iteration 2424/3560 Training loss: 1.9479 0.5915 sec/batch\n",
      "Epoch 14/20  Iteration 2425/3560 Training loss: 1.9479 0.7256 sec/batch\n",
      "Epoch 14/20  Iteration 2426/3560 Training loss: 1.9478 0.6109 sec/batch\n",
      "Epoch 14/20  Iteration 2427/3560 Training loss: 1.9476 0.6325 sec/batch\n",
      "Epoch 14/20  Iteration 2428/3560 Training loss: 1.9475 0.6915 sec/batch\n",
      "Epoch 14/20  Iteration 2429/3560 Training loss: 1.9474 0.7914 sec/batch\n",
      "Epoch 14/20  Iteration 2430/3560 Training loss: 1.9470 0.7543 sec/batch\n",
      "Epoch 14/20  Iteration 2431/3560 Training loss: 1.9469 0.7722 sec/batch\n",
      "Epoch 14/20  Iteration 2432/3560 Training loss: 1.9469 0.5538 sec/batch\n",
      "Epoch 14/20  Iteration 2433/3560 Training loss: 1.9469 0.6182 sec/batch\n",
      "Epoch 14/20  Iteration 2434/3560 Training loss: 1.9468 0.6569 sec/batch\n",
      "Epoch 14/20  Iteration 2435/3560 Training loss: 1.9468 0.6382 sec/batch\n",
      "Epoch 14/20  Iteration 2436/3560 Training loss: 1.9467 0.6132 sec/batch\n",
      "Epoch 14/20  Iteration 2437/3560 Training loss: 1.9465 0.6617 sec/batch\n",
      "Epoch 14/20  Iteration 2438/3560 Training loss: 1.9467 0.5737 sec/batch\n",
      "Epoch 14/20  Iteration 2439/3560 Training loss: 1.9468 0.6070 sec/batch\n",
      "Epoch 14/20  Iteration 2440/3560 Training loss: 1.9464 0.6064 sec/batch\n",
      "Epoch 14/20  Iteration 2441/3560 Training loss: 1.9465 0.6645 sec/batch\n",
      "Epoch 14/20  Iteration 2442/3560 Training loss: 1.9465 0.6391 sec/batch\n",
      "Epoch 14/20  Iteration 2443/3560 Training loss: 1.9465 0.7747 sec/batch\n",
      "Epoch 14/20  Iteration 2444/3560 Training loss: 1.9465 0.6222 sec/batch\n",
      "Epoch 14/20  Iteration 2445/3560 Training loss: 1.9463 0.6641 sec/batch\n",
      "Epoch 14/20  Iteration 2446/3560 Training loss: 1.9459 0.6051 sec/batch\n",
      "Epoch 14/20  Iteration 2447/3560 Training loss: 1.9459 0.6454 sec/batch\n",
      "Epoch 14/20  Iteration 2448/3560 Training loss: 1.9460 0.6196 sec/batch\n",
      "Epoch 14/20  Iteration 2449/3560 Training loss: 1.9459 0.6329 sec/batch\n",
      "Epoch 14/20  Iteration 2450/3560 Training loss: 1.9460 0.5900 sec/batch\n",
      "Epoch 14/20  Iteration 2451/3560 Training loss: 1.9460 0.6606 sec/batch\n",
      "Epoch 14/20  Iteration 2452/3560 Training loss: 1.9460 0.6016 sec/batch\n",
      "Epoch 14/20  Iteration 2453/3560 Training loss: 1.9463 0.6128 sec/batch\n",
      "Epoch 14/20  Iteration 2454/3560 Training loss: 1.9461 0.7168 sec/batch\n",
      "Epoch 14/20  Iteration 2455/3560 Training loss: 1.9463 0.6994 sec/batch\n",
      "Epoch 14/20  Iteration 2456/3560 Training loss: 1.9462 0.7182 sec/batch\n",
      "Epoch 14/20  Iteration 2457/3560 Training loss: 1.9462 0.6897 sec/batch\n",
      "Epoch 14/20  Iteration 2458/3560 Training loss: 1.9462 0.5971 sec/batch\n",
      "Epoch 14/20  Iteration 2459/3560 Training loss: 1.9461 0.6395 sec/batch\n",
      "Epoch 14/20  Iteration 2460/3560 Training loss: 1.9462 0.6709 sec/batch\n",
      "Epoch 14/20  Iteration 2461/3560 Training loss: 1.9461 0.5710 sec/batch\n",
      "Epoch 14/20  Iteration 2462/3560 Training loss: 1.9463 0.6380 sec/batch\n",
      "Epoch 14/20  Iteration 2463/3560 Training loss: 1.9462 0.6015 sec/batch\n",
      "Epoch 14/20  Iteration 2464/3560 Training loss: 1.9461 0.7617 sec/batch\n",
      "Epoch 14/20  Iteration 2465/3560 Training loss: 1.9459 0.6304 sec/batch\n",
      "Epoch 14/20  Iteration 2466/3560 Training loss: 1.9461 0.6868 sec/batch\n",
      "Epoch 14/20  Iteration 2467/3560 Training loss: 1.9460 0.7119 sec/batch\n",
      "Epoch 14/20  Iteration 2468/3560 Training loss: 1.9460 0.6880 sec/batch\n",
      "Epoch 14/20  Iteration 2469/3560 Training loss: 1.9459 0.6965 sec/batch\n",
      "Epoch 14/20  Iteration 2470/3560 Training loss: 1.9459 0.6627 sec/batch\n",
      "Epoch 14/20  Iteration 2471/3560 Training loss: 1.9459 0.6198 sec/batch\n",
      "Epoch 14/20  Iteration 2472/3560 Training loss: 1.9458 0.6571 sec/batch\n",
      "Epoch 14/20  Iteration 2473/3560 Training loss: 1.9456 0.6165 sec/batch\n",
      "Epoch 14/20  Iteration 2474/3560 Training loss: 1.9458 0.6107 sec/batch\n",
      "Epoch 14/20  Iteration 2475/3560 Training loss: 1.9459 0.6597 sec/batch\n",
      "Epoch 14/20  Iteration 2476/3560 Training loss: 1.9459 0.6293 sec/batch\n",
      "Epoch 14/20  Iteration 2477/3560 Training loss: 1.9459 0.6614 sec/batch\n",
      "Epoch 14/20  Iteration 2478/3560 Training loss: 1.9459 0.6480 sec/batch\n",
      "Epoch 14/20  Iteration 2479/3560 Training loss: 1.9459 0.6542 sec/batch\n",
      "Epoch 14/20  Iteration 2480/3560 Training loss: 1.9458 0.7313 sec/batch\n",
      "Epoch 14/20  Iteration 2481/3560 Training loss: 1.9458 0.7374 sec/batch\n",
      "Epoch 14/20  Iteration 2482/3560 Training loss: 1.9462 0.6536 sec/batch\n",
      "Epoch 14/20  Iteration 2483/3560 Training loss: 1.9461 0.8155 sec/batch\n",
      "Epoch 14/20  Iteration 2484/3560 Training loss: 1.9460 0.6910 sec/batch\n",
      "Epoch 14/20  Iteration 2485/3560 Training loss: 1.9459 0.7602 sec/batch\n",
      "Epoch 14/20  Iteration 2486/3560 Training loss: 1.9458 0.6352 sec/batch\n",
      "Epoch 14/20  Iteration 2487/3560 Training loss: 1.9459 0.6750 sec/batch\n",
      "Epoch 14/20  Iteration 2488/3560 Training loss: 1.9459 0.7283 sec/batch\n",
      "Epoch 14/20  Iteration 2489/3560 Training loss: 1.9459 0.7551 sec/batch\n",
      "Epoch 14/20  Iteration 2490/3560 Training loss: 1.9459 0.6289 sec/batch\n",
      "Epoch 14/20  Iteration 2491/3560 Training loss: 1.9458 0.8357 sec/batch\n",
      "Epoch 14/20  Iteration 2492/3560 Training loss: 1.9458 0.8026 sec/batch\n",
      "Epoch 15/20  Iteration 2493/3560 Training loss: 2.0024 0.7978 sec/batch\n",
      "Epoch 15/20  Iteration 2494/3560 Training loss: 1.9598 0.7615 sec/batch\n",
      "Epoch 15/20  Iteration 2495/3560 Training loss: 1.9471 0.8573 sec/batch\n",
      "Epoch 15/20  Iteration 2496/3560 Training loss: 1.9412 0.7416 sec/batch\n",
      "Epoch 15/20  Iteration 2497/3560 Training loss: 1.9388 0.7189 sec/batch\n",
      "Epoch 15/20  Iteration 2498/3560 Training loss: 1.9326 0.8572 sec/batch\n",
      "Epoch 15/20  Iteration 2499/3560 Training loss: 1.9338 0.7124 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2500/3560 Training loss: 1.9344 0.7412 sec/batch\n",
      "Epoch 15/20  Iteration 2501/3560 Training loss: 1.9375 0.7675 sec/batch\n",
      "Epoch 15/20  Iteration 2502/3560 Training loss: 1.9386 0.7522 sec/batch\n",
      "Epoch 15/20  Iteration 2503/3560 Training loss: 1.9354 0.6660 sec/batch\n",
      "Epoch 15/20  Iteration 2504/3560 Training loss: 1.9340 0.7336 sec/batch\n",
      "Epoch 15/20  Iteration 2505/3560 Training loss: 1.9332 0.7859 sec/batch\n",
      "Epoch 15/20  Iteration 2506/3560 Training loss: 1.9364 0.8550 sec/batch\n",
      "Epoch 15/20  Iteration 2507/3560 Training loss: 1.9362 0.8191 sec/batch\n",
      "Epoch 15/20  Iteration 2508/3560 Training loss: 1.9349 0.7057 sec/batch\n",
      "Epoch 15/20  Iteration 2509/3560 Training loss: 1.9345 0.7699 sec/batch\n",
      "Epoch 15/20  Iteration 2510/3560 Training loss: 1.9363 0.6964 sec/batch\n",
      "Epoch 15/20  Iteration 2511/3560 Training loss: 1.9358 0.6416 sec/batch\n",
      "Epoch 15/20  Iteration 2512/3560 Training loss: 1.9357 0.7736 sec/batch\n",
      "Epoch 15/20  Iteration 2513/3560 Training loss: 1.9349 0.6832 sec/batch\n",
      "Epoch 15/20  Iteration 2514/3560 Training loss: 1.9368 0.6567 sec/batch\n",
      "Epoch 15/20  Iteration 2515/3560 Training loss: 1.9359 0.7169 sec/batch\n",
      "Epoch 15/20  Iteration 2516/3560 Training loss: 1.9351 0.7593 sec/batch\n",
      "Epoch 15/20  Iteration 2517/3560 Training loss: 1.9344 0.7750 sec/batch\n",
      "Epoch 15/20  Iteration 2518/3560 Training loss: 1.9336 0.8763 sec/batch\n",
      "Epoch 15/20  Iteration 2519/3560 Training loss: 1.9329 0.7794 sec/batch\n",
      "Epoch 15/20  Iteration 2520/3560 Training loss: 1.9336 0.8426 sec/batch\n",
      "Epoch 15/20  Iteration 2521/3560 Training loss: 1.9344 0.9570 sec/batch\n",
      "Epoch 15/20  Iteration 2522/3560 Training loss: 1.9350 0.7747 sec/batch\n",
      "Epoch 15/20  Iteration 2523/3560 Training loss: 1.9351 0.7254 sec/batch\n",
      "Epoch 15/20  Iteration 2524/3560 Training loss: 1.9344 0.7950 sec/batch\n",
      "Epoch 15/20  Iteration 2525/3560 Training loss: 1.9347 0.6775 sec/batch\n",
      "Epoch 15/20  Iteration 2526/3560 Training loss: 1.9356 0.7902 sec/batch\n",
      "Epoch 15/20  Iteration 2527/3560 Training loss: 1.9354 0.6515 sec/batch\n",
      "Epoch 15/20  Iteration 2528/3560 Training loss: 1.9351 0.8387 sec/batch\n",
      "Epoch 15/20  Iteration 2529/3560 Training loss: 1.9348 0.6899 sec/batch\n",
      "Epoch 15/20  Iteration 2530/3560 Training loss: 1.9338 0.7958 sec/batch\n",
      "Epoch 15/20  Iteration 2531/3560 Training loss: 1.9331 0.6979 sec/batch\n",
      "Epoch 15/20  Iteration 2532/3560 Training loss: 1.9324 0.8431 sec/batch\n",
      "Epoch 15/20  Iteration 2533/3560 Training loss: 1.9321 0.6175 sec/batch\n",
      "Epoch 15/20  Iteration 2534/3560 Training loss: 1.9319 0.6516 sec/batch\n",
      "Epoch 15/20  Iteration 2535/3560 Training loss: 1.9314 0.6350 sec/batch\n",
      "Epoch 15/20  Iteration 2536/3560 Training loss: 1.9307 0.6336 sec/batch\n",
      "Epoch 15/20  Iteration 2537/3560 Training loss: 1.9308 0.6552 sec/batch\n",
      "Epoch 15/20  Iteration 2538/3560 Training loss: 1.9298 0.6126 sec/batch\n",
      "Epoch 15/20  Iteration 2539/3560 Training loss: 1.9299 0.6017 sec/batch\n",
      "Epoch 15/20  Iteration 2540/3560 Training loss: 1.9295 0.6242 sec/batch\n",
      "Epoch 15/20  Iteration 2541/3560 Training loss: 1.9294 0.7328 sec/batch\n",
      "Epoch 15/20  Iteration 2542/3560 Training loss: 1.9301 0.8254 sec/batch\n",
      "Epoch 15/20  Iteration 2543/3560 Training loss: 1.9296 0.7887 sec/batch\n",
      "Epoch 15/20  Iteration 2544/3560 Training loss: 1.9303 0.6828 sec/batch\n",
      "Epoch 15/20  Iteration 2545/3560 Training loss: 1.9301 0.7947 sec/batch\n",
      "Epoch 15/20  Iteration 2546/3560 Training loss: 1.9298 0.7018 sec/batch\n",
      "Epoch 15/20  Iteration 2547/3560 Training loss: 1.9294 0.6668 sec/batch\n",
      "Epoch 15/20  Iteration 2548/3560 Training loss: 1.9296 0.5901 sec/batch\n",
      "Epoch 15/20  Iteration 2549/3560 Training loss: 1.9300 0.6643 sec/batch\n",
      "Epoch 15/20  Iteration 2550/3560 Training loss: 1.9299 0.6708 sec/batch\n",
      "Epoch 15/20  Iteration 2551/3560 Training loss: 1.9295 0.6315 sec/batch\n",
      "Epoch 15/20  Iteration 2552/3560 Training loss: 1.9298 0.6020 sec/batch\n",
      "Epoch 15/20  Iteration 2553/3560 Training loss: 1.9297 0.6099 sec/batch\n",
      "Epoch 15/20  Iteration 2554/3560 Training loss: 1.9303 0.6725 sec/batch\n",
      "Epoch 15/20  Iteration 2555/3560 Training loss: 1.9306 0.6351 sec/batch\n",
      "Epoch 15/20  Iteration 2556/3560 Training loss: 1.9307 0.7255 sec/batch\n",
      "Epoch 15/20  Iteration 2557/3560 Training loss: 1.9307 0.7175 sec/batch\n",
      "Epoch 15/20  Iteration 2558/3560 Training loss: 1.9311 0.6393 sec/batch\n",
      "Epoch 15/20  Iteration 2559/3560 Training loss: 1.9312 0.6164 sec/batch\n",
      "Epoch 15/20  Iteration 2560/3560 Training loss: 1.9307 0.6529 sec/batch\n",
      "Epoch 15/20  Iteration 2561/3560 Training loss: 1.9306 0.5363 sec/batch\n",
      "Epoch 15/20  Iteration 2562/3560 Training loss: 1.9307 0.6468 sec/batch\n",
      "Epoch 15/20  Iteration 2563/3560 Training loss: 1.9311 0.6154 sec/batch\n",
      "Epoch 15/20  Iteration 2564/3560 Training loss: 1.9313 0.6360 sec/batch\n",
      "Epoch 15/20  Iteration 2565/3560 Training loss: 1.9315 0.6706 sec/batch\n",
      "Epoch 15/20  Iteration 2566/3560 Training loss: 1.9313 0.5795 sec/batch\n",
      "Epoch 15/20  Iteration 2567/3560 Training loss: 1.9312 0.6763 sec/batch\n",
      "Epoch 15/20  Iteration 2568/3560 Training loss: 1.9313 0.6747 sec/batch\n",
      "Epoch 15/20  Iteration 2569/3560 Training loss: 1.9314 0.6772 sec/batch\n",
      "Epoch 15/20  Iteration 2570/3560 Training loss: 1.9316 0.7107 sec/batch\n",
      "Epoch 15/20  Iteration 2571/3560 Training loss: 1.9312 0.6059 sec/batch\n",
      "Epoch 15/20  Iteration 2572/3560 Training loss: 1.9310 0.6364 sec/batch\n",
      "Epoch 15/20  Iteration 2573/3560 Training loss: 1.9305 0.6525 sec/batch\n",
      "Epoch 15/20  Iteration 2574/3560 Training loss: 1.9307 0.7215 sec/batch\n",
      "Epoch 15/20  Iteration 2575/3560 Training loss: 1.9303 0.5238 sec/batch\n",
      "Epoch 15/20  Iteration 2576/3560 Training loss: 1.9301 0.6905 sec/batch\n",
      "Epoch 15/20  Iteration 2577/3560 Training loss: 1.9296 0.5817 sec/batch\n",
      "Epoch 15/20  Iteration 2578/3560 Training loss: 1.9292 0.6154 sec/batch\n",
      "Epoch 15/20  Iteration 2579/3560 Training loss: 1.9291 0.7048 sec/batch\n",
      "Epoch 15/20  Iteration 2580/3560 Training loss: 1.9288 0.6646 sec/batch\n",
      "Epoch 15/20  Iteration 2581/3560 Training loss: 1.9283 0.6932 sec/batch\n",
      "Epoch 15/20  Iteration 2582/3560 Training loss: 1.9283 0.7753 sec/batch\n",
      "Epoch 15/20  Iteration 2583/3560 Training loss: 1.9280 0.7002 sec/batch\n",
      "Epoch 15/20  Iteration 2584/3560 Training loss: 1.9279 0.5903 sec/batch\n",
      "Epoch 15/20  Iteration 2585/3560 Training loss: 1.9274 0.7022 sec/batch\n",
      "Epoch 15/20  Iteration 2586/3560 Training loss: 1.9271 0.5792 sec/batch\n",
      "Epoch 15/20  Iteration 2587/3560 Training loss: 1.9268 0.6885 sec/batch\n",
      "Epoch 15/20  Iteration 2588/3560 Training loss: 1.9268 0.5860 sec/batch\n",
      "Epoch 15/20  Iteration 2589/3560 Training loss: 1.9268 0.6204 sec/batch\n",
      "Epoch 15/20  Iteration 2590/3560 Training loss: 1.9265 0.5916 sec/batch\n",
      "Epoch 15/20  Iteration 2591/3560 Training loss: 1.9262 0.6837 sec/batch\n",
      "Epoch 15/20  Iteration 2592/3560 Training loss: 1.9258 0.6365 sec/batch\n",
      "Epoch 15/20  Iteration 2593/3560 Training loss: 1.9258 0.8282 sec/batch\n",
      "Epoch 15/20  Iteration 2594/3560 Training loss: 1.9256 0.7576 sec/batch\n",
      "Epoch 15/20  Iteration 2595/3560 Training loss: 1.9254 0.6170 sec/batch\n",
      "Epoch 15/20  Iteration 2596/3560 Training loss: 1.9251 0.7698 sec/batch\n",
      "Epoch 15/20  Iteration 2597/3560 Training loss: 1.9249 0.7381 sec/batch\n",
      "Epoch 15/20  Iteration 2598/3560 Training loss: 1.9248 0.6522 sec/batch\n",
      "Epoch 15/20  Iteration 2599/3560 Training loss: 1.9247 0.5559 sec/batch\n",
      "Epoch 15/20  Iteration 2600/3560 Training loss: 1.9246 0.6448 sec/batch\n",
      "Epoch 15/20  Iteration 2601/3560 Training loss: 1.9245 0.6217 sec/batch\n",
      "Epoch 15/20  Iteration 2602/3560 Training loss: 1.9246 0.5828 sec/batch\n",
      "Epoch 15/20  Iteration 2603/3560 Training loss: 1.9245 0.7323 sec/batch\n",
      "Epoch 15/20  Iteration 2604/3560 Training loss: 1.9244 0.7813 sec/batch\n",
      "Epoch 15/20  Iteration 2605/3560 Training loss: 1.9243 0.6627 sec/batch\n",
      "Epoch 15/20  Iteration 2606/3560 Training loss: 1.9241 0.7056 sec/batch\n",
      "Epoch 15/20  Iteration 2607/3560 Training loss: 1.9239 0.7238 sec/batch\n",
      "Epoch 15/20  Iteration 2608/3560 Training loss: 1.9236 0.6825 sec/batch\n",
      "Epoch 15/20  Iteration 2609/3560 Training loss: 1.9236 0.7174 sec/batch\n",
      "Epoch 15/20  Iteration 2610/3560 Training loss: 1.9234 0.8206 sec/batch\n",
      "Epoch 15/20  Iteration 2611/3560 Training loss: 1.9235 0.6693 sec/batch\n",
      "Epoch 15/20  Iteration 2612/3560 Training loss: 1.9233 0.6034 sec/batch\n",
      "Epoch 15/20  Iteration 2613/3560 Training loss: 1.9234 0.6635 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2614/3560 Training loss: 1.9232 0.6089 sec/batch\n",
      "Epoch 15/20  Iteration 2615/3560 Training loss: 1.9231 0.6174 sec/batch\n",
      "Epoch 15/20  Iteration 2616/3560 Training loss: 1.9232 0.5941 sec/batch\n",
      "Epoch 15/20  Iteration 2617/3560 Training loss: 1.9233 0.6684 sec/batch\n",
      "Epoch 15/20  Iteration 2618/3560 Training loss: 1.9228 0.7236 sec/batch\n",
      "Epoch 15/20  Iteration 2619/3560 Training loss: 1.9230 0.6841 sec/batch\n",
      "Epoch 15/20  Iteration 2620/3560 Training loss: 1.9231 0.7221 sec/batch\n",
      "Epoch 15/20  Iteration 2621/3560 Training loss: 1.9230 0.6450 sec/batch\n",
      "Epoch 15/20  Iteration 2622/3560 Training loss: 1.9230 0.7196 sec/batch\n",
      "Epoch 15/20  Iteration 2623/3560 Training loss: 1.9227 0.5849 sec/batch\n",
      "Epoch 15/20  Iteration 2624/3560 Training loss: 1.9224 0.6284 sec/batch\n",
      "Epoch 15/20  Iteration 2625/3560 Training loss: 1.9223 0.5749 sec/batch\n",
      "Epoch 15/20  Iteration 2626/3560 Training loss: 1.9223 0.6680 sec/batch\n",
      "Epoch 15/20  Iteration 2627/3560 Training loss: 1.9222 0.6257 sec/batch\n",
      "Epoch 15/20  Iteration 2628/3560 Training loss: 1.9222 0.6378 sec/batch\n",
      "Epoch 15/20  Iteration 2629/3560 Training loss: 1.9223 0.6130 sec/batch\n",
      "Epoch 15/20  Iteration 2630/3560 Training loss: 1.9224 0.6749 sec/batch\n",
      "Epoch 15/20  Iteration 2631/3560 Training loss: 1.9225 0.6173 sec/batch\n",
      "Epoch 15/20  Iteration 2632/3560 Training loss: 1.9223 0.6634 sec/batch\n",
      "Epoch 15/20  Iteration 2633/3560 Training loss: 1.9224 0.6546 sec/batch\n",
      "Epoch 15/20  Iteration 2634/3560 Training loss: 1.9223 0.6280 sec/batch\n",
      "Epoch 15/20  Iteration 2635/3560 Training loss: 1.9223 0.6920 sec/batch\n",
      "Epoch 15/20  Iteration 2636/3560 Training loss: 1.9223 0.6600 sec/batch\n",
      "Epoch 15/20  Iteration 2637/3560 Training loss: 1.9222 0.5912 sec/batch\n",
      "Epoch 15/20  Iteration 2638/3560 Training loss: 1.9222 0.5812 sec/batch\n",
      "Epoch 15/20  Iteration 2639/3560 Training loss: 1.9222 0.7627 sec/batch\n",
      "Epoch 15/20  Iteration 2640/3560 Training loss: 1.9224 0.5799 sec/batch\n",
      "Epoch 15/20  Iteration 2641/3560 Training loss: 1.9224 0.5866 sec/batch\n",
      "Epoch 15/20  Iteration 2642/3560 Training loss: 1.9223 0.6295 sec/batch\n",
      "Epoch 15/20  Iteration 2643/3560 Training loss: 1.9221 0.6295 sec/batch\n",
      "Epoch 15/20  Iteration 2644/3560 Training loss: 1.9223 0.6925 sec/batch\n",
      "Epoch 15/20  Iteration 2645/3560 Training loss: 1.9223 0.6137 sec/batch\n",
      "Epoch 15/20  Iteration 2646/3560 Training loss: 1.9223 0.7621 sec/batch\n",
      "Epoch 15/20  Iteration 2647/3560 Training loss: 1.9222 0.6362 sec/batch\n",
      "Epoch 15/20  Iteration 2648/3560 Training loss: 1.9221 0.6293 sec/batch\n",
      "Epoch 15/20  Iteration 2649/3560 Training loss: 1.9221 0.5997 sec/batch\n",
      "Epoch 15/20  Iteration 2650/3560 Training loss: 1.9221 0.6414 sec/batch\n",
      "Epoch 15/20  Iteration 2651/3560 Training loss: 1.9219 0.6114 sec/batch\n",
      "Epoch 15/20  Iteration 2652/3560 Training loss: 1.9222 0.6698 sec/batch\n",
      "Epoch 15/20  Iteration 2653/3560 Training loss: 1.9223 0.6539 sec/batch\n",
      "Epoch 15/20  Iteration 2654/3560 Training loss: 1.9223 0.6899 sec/batch\n",
      "Epoch 15/20  Iteration 2655/3560 Training loss: 1.9223 0.6399 sec/batch\n",
      "Epoch 15/20  Iteration 2656/3560 Training loss: 1.9223 0.6637 sec/batch\n",
      "Epoch 15/20  Iteration 2657/3560 Training loss: 1.9223 0.6790 sec/batch\n",
      "Epoch 15/20  Iteration 2658/3560 Training loss: 1.9223 0.5677 sec/batch\n",
      "Epoch 15/20  Iteration 2659/3560 Training loss: 1.9223 0.6846 sec/batch\n",
      "Epoch 15/20  Iteration 2660/3560 Training loss: 1.9226 0.6486 sec/batch\n",
      "Epoch 15/20  Iteration 2661/3560 Training loss: 1.9226 0.6326 sec/batch\n",
      "Epoch 15/20  Iteration 2662/3560 Training loss: 1.9225 0.6785 sec/batch\n",
      "Epoch 15/20  Iteration 2663/3560 Training loss: 1.9224 0.6017 sec/batch\n",
      "Epoch 15/20  Iteration 2664/3560 Training loss: 1.9223 0.6241 sec/batch\n",
      "Epoch 15/20  Iteration 2665/3560 Training loss: 1.9223 0.5918 sec/batch\n",
      "Epoch 15/20  Iteration 2666/3560 Training loss: 1.9223 0.6219 sec/batch\n",
      "Epoch 15/20  Iteration 2667/3560 Training loss: 1.9224 0.6930 sec/batch\n",
      "Epoch 15/20  Iteration 2668/3560 Training loss: 1.9224 0.6403 sec/batch\n",
      "Epoch 15/20  Iteration 2669/3560 Training loss: 1.9223 0.7017 sec/batch\n",
      "Epoch 15/20  Iteration 2670/3560 Training loss: 1.9222 0.6042 sec/batch\n",
      "Epoch 16/20  Iteration 2671/3560 Training loss: 1.9852 0.7310 sec/batch\n",
      "Epoch 16/20  Iteration 2672/3560 Training loss: 1.9438 0.6676 sec/batch\n",
      "Epoch 16/20  Iteration 2673/3560 Training loss: 1.9295 0.6323 sec/batch\n",
      "Epoch 16/20  Iteration 2674/3560 Training loss: 1.9233 0.6338 sec/batch\n",
      "Epoch 16/20  Iteration 2675/3560 Training loss: 1.9238 0.6497 sec/batch\n",
      "Epoch 16/20  Iteration 2676/3560 Training loss: 1.9161 0.6481 sec/batch\n",
      "Epoch 16/20  Iteration 2677/3560 Training loss: 1.9152 0.5703 sec/batch\n",
      "Epoch 16/20  Iteration 2678/3560 Training loss: 1.9136 0.6135 sec/batch\n",
      "Epoch 16/20  Iteration 2679/3560 Training loss: 1.9164 0.6408 sec/batch\n",
      "Epoch 16/20  Iteration 2680/3560 Training loss: 1.9171 0.6202 sec/batch\n",
      "Epoch 16/20  Iteration 2681/3560 Training loss: 1.9145 0.7142 sec/batch\n",
      "Epoch 16/20  Iteration 2682/3560 Training loss: 1.9132 0.6820 sec/batch\n",
      "Epoch 16/20  Iteration 2683/3560 Training loss: 1.9136 0.6944 sec/batch\n",
      "Epoch 16/20  Iteration 2684/3560 Training loss: 1.9160 0.6474 sec/batch\n",
      "Epoch 16/20  Iteration 2685/3560 Training loss: 1.9155 0.6822 sec/batch\n",
      "Epoch 16/20  Iteration 2686/3560 Training loss: 1.9137 0.6400 sec/batch\n",
      "Epoch 16/20  Iteration 2687/3560 Training loss: 1.9138 0.6675 sec/batch\n",
      "Epoch 16/20  Iteration 2688/3560 Training loss: 1.9166 0.5885 sec/batch\n",
      "Epoch 16/20  Iteration 2689/3560 Training loss: 1.9163 0.6314 sec/batch\n",
      "Epoch 16/20  Iteration 2690/3560 Training loss: 1.9166 0.6300 sec/batch\n",
      "Epoch 16/20  Iteration 2691/3560 Training loss: 1.9155 0.6261 sec/batch\n",
      "Epoch 16/20  Iteration 2692/3560 Training loss: 1.9170 0.6270 sec/batch\n",
      "Epoch 16/20  Iteration 2693/3560 Training loss: 1.9164 0.7835 sec/batch\n",
      "Epoch 16/20  Iteration 2694/3560 Training loss: 1.9157 0.5966 sec/batch\n",
      "Epoch 16/20  Iteration 2695/3560 Training loss: 1.9151 0.6941 sec/batch\n",
      "Epoch 16/20  Iteration 2696/3560 Training loss: 1.9144 0.6580 sec/batch\n",
      "Epoch 16/20  Iteration 2697/3560 Training loss: 1.9137 0.6712 sec/batch\n",
      "Epoch 16/20  Iteration 2698/3560 Training loss: 1.9141 0.6680 sec/batch\n",
      "Epoch 16/20  Iteration 2699/3560 Training loss: 1.9152 0.6995 sec/batch\n",
      "Epoch 16/20  Iteration 2700/3560 Training loss: 1.9153 0.5857 sec/batch\n",
      "Epoch 16/20  Iteration 2701/3560 Training loss: 1.9153 0.6717 sec/batch\n",
      "Epoch 16/20  Iteration 2702/3560 Training loss: 1.9146 0.6300 sec/batch\n",
      "Epoch 16/20  Iteration 2703/3560 Training loss: 1.9152 0.6218 sec/batch\n",
      "Epoch 16/20  Iteration 2704/3560 Training loss: 1.9161 0.6248 sec/batch\n",
      "Epoch 16/20  Iteration 2705/3560 Training loss: 1.9155 0.6067 sec/batch\n",
      "Epoch 16/20  Iteration 2706/3560 Training loss: 1.9151 0.6509 sec/batch\n",
      "Epoch 16/20  Iteration 2707/3560 Training loss: 1.9147 0.6783 sec/batch\n",
      "Epoch 16/20  Iteration 2708/3560 Training loss: 1.9138 0.6519 sec/batch\n",
      "Epoch 16/20  Iteration 2709/3560 Training loss: 1.9127 0.6625 sec/batch\n",
      "Epoch 16/20  Iteration 2710/3560 Training loss: 1.9121 0.6710 sec/batch\n",
      "Epoch 16/20  Iteration 2711/3560 Training loss: 1.9115 0.6207 sec/batch\n",
      "Epoch 16/20  Iteration 2712/3560 Training loss: 1.9116 0.5917 sec/batch\n",
      "Epoch 16/20  Iteration 2713/3560 Training loss: 1.9112 0.6033 sec/batch\n",
      "Epoch 16/20  Iteration 2714/3560 Training loss: 1.9103 0.6981 sec/batch\n",
      "Epoch 16/20  Iteration 2715/3560 Training loss: 1.9103 0.5501 sec/batch\n",
      "Epoch 16/20  Iteration 2716/3560 Training loss: 1.9091 0.7017 sec/batch\n",
      "Epoch 16/20  Iteration 2717/3560 Training loss: 1.9090 0.5815 sec/batch\n",
      "Epoch 16/20  Iteration 2718/3560 Training loss: 1.9087 0.6591 sec/batch\n",
      "Epoch 16/20  Iteration 2719/3560 Training loss: 1.9083 0.6835 sec/batch\n",
      "Epoch 16/20  Iteration 2720/3560 Training loss: 1.9091 0.6352 sec/batch\n",
      "Epoch 16/20  Iteration 2721/3560 Training loss: 1.9086 0.6737 sec/batch\n",
      "Epoch 16/20  Iteration 2722/3560 Training loss: 1.9094 0.6295 sec/batch\n",
      "Epoch 16/20  Iteration 2723/3560 Training loss: 1.9093 0.6963 sec/batch\n",
      "Epoch 16/20  Iteration 2724/3560 Training loss: 1.9093 0.6392 sec/batch\n",
      "Epoch 16/20  Iteration 2725/3560 Training loss: 1.9089 0.5449 sec/batch\n",
      "Epoch 16/20  Iteration 2726/3560 Training loss: 1.9092 0.6349 sec/batch\n",
      "Epoch 16/20  Iteration 2727/3560 Training loss: 1.9093 0.6429 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2728/3560 Training loss: 1.9091 0.5911 sec/batch\n",
      "Epoch 16/20  Iteration 2729/3560 Training loss: 1.9087 0.6084 sec/batch\n",
      "Epoch 16/20  Iteration 2730/3560 Training loss: 1.9091 0.6295 sec/batch\n",
      "Epoch 16/20  Iteration 2731/3560 Training loss: 1.9088 0.7390 sec/batch\n",
      "Epoch 16/20  Iteration 2732/3560 Training loss: 1.9096 0.6435 sec/batch\n",
      "Epoch 16/20  Iteration 2733/3560 Training loss: 1.9096 0.6999 sec/batch\n",
      "Epoch 16/20  Iteration 2734/3560 Training loss: 1.9099 0.6445 sec/batch\n",
      "Epoch 16/20  Iteration 2735/3560 Training loss: 1.9100 0.6939 sec/batch\n",
      "Epoch 16/20  Iteration 2736/3560 Training loss: 1.9101 0.7033 sec/batch\n",
      "Epoch 16/20  Iteration 2737/3560 Training loss: 1.9104 0.6238 sec/batch\n",
      "Epoch 16/20  Iteration 2738/3560 Training loss: 1.9099 0.6517 sec/batch\n",
      "Epoch 16/20  Iteration 2739/3560 Training loss: 1.9097 0.6166 sec/batch\n",
      "Epoch 16/20  Iteration 2740/3560 Training loss: 1.9096 0.6109 sec/batch\n",
      "Epoch 16/20  Iteration 2741/3560 Training loss: 1.9100 0.6255 sec/batch\n",
      "Epoch 16/20  Iteration 2742/3560 Training loss: 1.9103 0.5857 sec/batch\n",
      "Epoch 16/20  Iteration 2743/3560 Training loss: 1.9107 0.6471 sec/batch\n",
      "Epoch 16/20  Iteration 2744/3560 Training loss: 1.9103 0.7078 sec/batch\n",
      "Epoch 16/20  Iteration 2745/3560 Training loss: 1.9102 0.6510 sec/batch\n",
      "Epoch 16/20  Iteration 2746/3560 Training loss: 1.9104 0.6607 sec/batch\n",
      "Epoch 16/20  Iteration 2747/3560 Training loss: 1.9103 0.7393 sec/batch\n",
      "Epoch 16/20  Iteration 2748/3560 Training loss: 1.9105 0.6569 sec/batch\n",
      "Epoch 16/20  Iteration 2749/3560 Training loss: 1.9101 0.6159 sec/batch\n",
      "Epoch 16/20  Iteration 2750/3560 Training loss: 1.9099 0.6165 sec/batch\n",
      "Epoch 16/20  Iteration 2751/3560 Training loss: 1.9094 0.6335 sec/batch\n",
      "Epoch 16/20  Iteration 2752/3560 Training loss: 1.9097 0.5899 sec/batch\n",
      "Epoch 16/20  Iteration 2753/3560 Training loss: 1.9092 0.6336 sec/batch\n",
      "Epoch 16/20  Iteration 2754/3560 Training loss: 1.9090 0.6295 sec/batch\n",
      "Epoch 16/20  Iteration 2755/3560 Training loss: 1.9084 0.5806 sec/batch\n",
      "Epoch 16/20  Iteration 2756/3560 Training loss: 1.9080 0.6851 sec/batch\n",
      "Epoch 16/20  Iteration 2757/3560 Training loss: 1.9078 0.6458 sec/batch\n",
      "Epoch 16/20  Iteration 2758/3560 Training loss: 1.9076 0.6456 sec/batch\n",
      "Epoch 16/20  Iteration 2759/3560 Training loss: 1.9071 0.7833 sec/batch\n",
      "Epoch 16/20  Iteration 2760/3560 Training loss: 1.9072 0.7601 sec/batch\n",
      "Epoch 16/20  Iteration 2761/3560 Training loss: 1.9069 0.5748 sec/batch\n",
      "Epoch 16/20  Iteration 2762/3560 Training loss: 1.9067 0.6317 sec/batch\n",
      "Epoch 16/20  Iteration 2763/3560 Training loss: 1.9063 0.6509 sec/batch\n",
      "Epoch 16/20  Iteration 2764/3560 Training loss: 1.9060 0.6820 sec/batch\n",
      "Epoch 16/20  Iteration 2765/3560 Training loss: 1.9058 0.6076 sec/batch\n",
      "Epoch 16/20  Iteration 2766/3560 Training loss: 1.9057 0.5779 sec/batch\n",
      "Epoch 16/20  Iteration 2767/3560 Training loss: 1.9056 0.6220 sec/batch\n",
      "Epoch 16/20  Iteration 2768/3560 Training loss: 1.9053 0.6509 sec/batch\n",
      "Epoch 16/20  Iteration 2769/3560 Training loss: 1.9049 0.6491 sec/batch\n",
      "Epoch 16/20  Iteration 2770/3560 Training loss: 1.9046 0.6750 sec/batch\n",
      "Epoch 16/20  Iteration 2771/3560 Training loss: 1.9046 0.6880 sec/batch\n",
      "Epoch 16/20  Iteration 2772/3560 Training loss: 1.9045 0.6389 sec/batch\n",
      "Epoch 16/20  Iteration 2773/3560 Training loss: 1.9043 0.6407 sec/batch\n",
      "Epoch 16/20  Iteration 2774/3560 Training loss: 1.9040 0.6229 sec/batch\n",
      "Epoch 16/20  Iteration 2775/3560 Training loss: 1.9038 0.6233 sec/batch\n",
      "Epoch 16/20  Iteration 2776/3560 Training loss: 1.9037 0.6625 sec/batch\n",
      "Epoch 16/20  Iteration 2777/3560 Training loss: 1.9036 0.5916 sec/batch\n",
      "Epoch 16/20  Iteration 2778/3560 Training loss: 1.9036 0.6820 sec/batch\n",
      "Epoch 16/20  Iteration 2779/3560 Training loss: 1.9036 0.5434 sec/batch\n",
      "Epoch 16/20  Iteration 2780/3560 Training loss: 1.9036 0.6408 sec/batch\n",
      "Epoch 16/20  Iteration 2781/3560 Training loss: 1.9037 0.6998 sec/batch\n",
      "Epoch 16/20  Iteration 2782/3560 Training loss: 1.9035 0.7315 sec/batch\n",
      "Epoch 16/20  Iteration 2783/3560 Training loss: 1.9035 0.6060 sec/batch\n",
      "Epoch 16/20  Iteration 2784/3560 Training loss: 1.9034 0.6485 sec/batch\n",
      "Epoch 16/20  Iteration 2785/3560 Training loss: 1.9032 0.6941 sec/batch\n",
      "Epoch 16/20  Iteration 2786/3560 Training loss: 1.9029 0.6448 sec/batch\n",
      "Epoch 16/20  Iteration 2787/3560 Training loss: 1.9028 0.6323 sec/batch\n",
      "Epoch 16/20  Iteration 2788/3560 Training loss: 1.9028 0.5792 sec/batch\n",
      "Epoch 16/20  Iteration 2789/3560 Training loss: 1.9027 0.6425 sec/batch\n",
      "Epoch 16/20  Iteration 2790/3560 Training loss: 1.9027 0.5810 sec/batch\n",
      "Epoch 16/20  Iteration 2791/3560 Training loss: 1.9027 0.6269 sec/batch\n",
      "Epoch 16/20  Iteration 2792/3560 Training loss: 1.9025 0.6053 sec/batch\n",
      "Epoch 16/20  Iteration 2793/3560 Training loss: 1.9022 0.6820 sec/batch\n",
      "Epoch 16/20  Iteration 2794/3560 Training loss: 1.9024 0.6326 sec/batch\n",
      "Epoch 16/20  Iteration 2795/3560 Training loss: 1.9024 0.6692 sec/batch\n",
      "Epoch 16/20  Iteration 2796/3560 Training loss: 1.9020 0.6650 sec/batch\n",
      "Epoch 16/20  Iteration 2797/3560 Training loss: 1.9021 0.7064 sec/batch\n",
      "Epoch 16/20  Iteration 2798/3560 Training loss: 1.9021 0.7070 sec/batch\n",
      "Epoch 16/20  Iteration 2799/3560 Training loss: 1.9022 0.7316 sec/batch\n",
      "Epoch 16/20  Iteration 2800/3560 Training loss: 1.9022 0.5716 sec/batch\n",
      "Epoch 16/20  Iteration 2801/3560 Training loss: 1.9019 0.6173 sec/batch\n",
      "Epoch 16/20  Iteration 2802/3560 Training loss: 1.9017 0.6049 sec/batch\n",
      "Epoch 16/20  Iteration 2803/3560 Training loss: 1.9017 0.5997 sec/batch\n",
      "Epoch 16/20  Iteration 2804/3560 Training loss: 1.9017 0.5988 sec/batch\n",
      "Epoch 16/20  Iteration 2805/3560 Training loss: 1.9016 0.5975 sec/batch\n",
      "Epoch 16/20  Iteration 2806/3560 Training loss: 1.9017 0.6739 sec/batch\n",
      "Epoch 16/20  Iteration 2807/3560 Training loss: 1.9017 0.7154 sec/batch\n",
      "Epoch 16/20  Iteration 2808/3560 Training loss: 1.9017 0.6559 sec/batch\n",
      "Epoch 16/20  Iteration 2809/3560 Training loss: 1.9019 0.6411 sec/batch\n",
      "Epoch 16/20  Iteration 2810/3560 Training loss: 1.9017 0.7351 sec/batch\n",
      "Epoch 16/20  Iteration 2811/3560 Training loss: 1.9019 0.7273 sec/batch\n",
      "Epoch 16/20  Iteration 2812/3560 Training loss: 1.9019 0.6086 sec/batch\n",
      "Epoch 16/20  Iteration 2813/3560 Training loss: 1.9018 0.6866 sec/batch\n",
      "Epoch 16/20  Iteration 2814/3560 Training loss: 1.9018 0.6839 sec/batch\n",
      "Epoch 16/20  Iteration 2815/3560 Training loss: 1.9017 0.6440 sec/batch\n",
      "Epoch 16/20  Iteration 2816/3560 Training loss: 1.9018 0.6610 sec/batch\n",
      "Epoch 16/20  Iteration 2817/3560 Training loss: 1.9019 0.5913 sec/batch\n",
      "Epoch 16/20  Iteration 2818/3560 Training loss: 1.9021 0.7092 sec/batch\n",
      "Epoch 16/20  Iteration 2819/3560 Training loss: 1.9021 0.6348 sec/batch\n",
      "Epoch 16/20  Iteration 2820/3560 Training loss: 1.9020 0.7221 sec/batch\n",
      "Epoch 16/20  Iteration 2821/3560 Training loss: 1.9018 0.6337 sec/batch\n",
      "Epoch 16/20  Iteration 2822/3560 Training loss: 1.9019 0.6684 sec/batch\n",
      "Epoch 16/20  Iteration 2823/3560 Training loss: 1.9020 0.7017 sec/batch\n",
      "Epoch 16/20  Iteration 2824/3560 Training loss: 1.9020 0.6629 sec/batch\n",
      "Epoch 16/20  Iteration 2825/3560 Training loss: 1.9019 0.6524 sec/batch\n",
      "Epoch 16/20  Iteration 2826/3560 Training loss: 1.9019 0.5871 sec/batch\n",
      "Epoch 16/20  Iteration 2827/3560 Training loss: 1.9019 0.6925 sec/batch\n",
      "Epoch 16/20  Iteration 2828/3560 Training loss: 1.9019 0.7301 sec/batch\n",
      "Epoch 16/20  Iteration 2829/3560 Training loss: 1.9017 0.6079 sec/batch\n",
      "Epoch 16/20  Iteration 2830/3560 Training loss: 1.9018 0.6287 sec/batch\n",
      "Epoch 16/20  Iteration 2831/3560 Training loss: 1.9020 0.7397 sec/batch\n",
      "Epoch 16/20  Iteration 2832/3560 Training loss: 1.9020 0.6298 sec/batch\n",
      "Epoch 16/20  Iteration 2833/3560 Training loss: 1.9020 0.6658 sec/batch\n",
      "Epoch 16/20  Iteration 2834/3560 Training loss: 1.9020 0.6857 sec/batch\n",
      "Epoch 16/20  Iteration 2835/3560 Training loss: 1.9020 0.6371 sec/batch\n",
      "Epoch 16/20  Iteration 2836/3560 Training loss: 1.9019 0.7167 sec/batch\n",
      "Epoch 16/20  Iteration 2837/3560 Training loss: 1.9019 0.6151 sec/batch\n",
      "Epoch 16/20  Iteration 2838/3560 Training loss: 1.9022 0.6331 sec/batch\n",
      "Epoch 16/20  Iteration 2839/3560 Training loss: 1.9022 0.6297 sec/batch\n",
      "Epoch 16/20  Iteration 2840/3560 Training loss: 1.9022 0.6386 sec/batch\n",
      "Epoch 16/20  Iteration 2841/3560 Training loss: 1.9021 0.5899 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2842/3560 Training loss: 1.9020 0.6978 sec/batch\n",
      "Epoch 16/20  Iteration 2843/3560 Training loss: 1.9020 0.6914 sec/batch\n",
      "Epoch 16/20  Iteration 2844/3560 Training loss: 1.9019 0.6484 sec/batch\n",
      "Epoch 16/20  Iteration 2845/3560 Training loss: 1.9020 0.6320 sec/batch\n",
      "Epoch 16/20  Iteration 2846/3560 Training loss: 1.9020 0.6501 sec/batch\n",
      "Epoch 16/20  Iteration 2847/3560 Training loss: 1.9018 0.5587 sec/batch\n",
      "Epoch 16/20  Iteration 2848/3560 Training loss: 1.9018 0.7281 sec/batch\n",
      "Epoch 17/20  Iteration 2849/3560 Training loss: 1.9733 0.6603 sec/batch\n",
      "Epoch 17/20  Iteration 2850/3560 Training loss: 1.9311 0.6013 sec/batch\n",
      "Epoch 17/20  Iteration 2851/3560 Training loss: 1.9157 0.6353 sec/batch\n",
      "Epoch 17/20  Iteration 2852/3560 Training loss: 1.9094 0.5875 sec/batch\n",
      "Epoch 17/20  Iteration 2853/3560 Training loss: 1.9074 0.6628 sec/batch\n",
      "Epoch 17/20  Iteration 2854/3560 Training loss: 1.8999 0.6216 sec/batch\n",
      "Epoch 17/20  Iteration 2855/3560 Training loss: 1.8998 0.6583 sec/batch\n",
      "Epoch 17/20  Iteration 2856/3560 Training loss: 1.8992 0.6633 sec/batch\n",
      "Epoch 17/20  Iteration 2857/3560 Training loss: 1.9014 0.6528 sec/batch\n",
      "Epoch 17/20  Iteration 2858/3560 Training loss: 1.9018 0.6768 sec/batch\n",
      "Epoch 17/20  Iteration 2859/3560 Training loss: 1.8983 0.7545 sec/batch\n",
      "Epoch 17/20  Iteration 2860/3560 Training loss: 1.8974 0.6483 sec/batch\n",
      "Epoch 17/20  Iteration 2861/3560 Training loss: 1.8974 0.7056 sec/batch\n",
      "Epoch 17/20  Iteration 2862/3560 Training loss: 1.8995 0.5407 sec/batch\n",
      "Epoch 17/20  Iteration 2863/3560 Training loss: 1.8987 0.6657 sec/batch\n",
      "Epoch 17/20  Iteration 2864/3560 Training loss: 1.8965 0.6934 sec/batch\n",
      "Epoch 17/20  Iteration 2865/3560 Training loss: 1.8966 0.5638 sec/batch\n",
      "Epoch 17/20  Iteration 2866/3560 Training loss: 1.8980 0.6830 sec/batch\n",
      "Epoch 17/20  Iteration 2867/3560 Training loss: 1.8976 0.6265 sec/batch\n",
      "Epoch 17/20  Iteration 2868/3560 Training loss: 1.8985 0.7921 sec/batch\n",
      "Epoch 17/20  Iteration 2869/3560 Training loss: 1.8979 0.8034 sec/batch\n",
      "Epoch 17/20  Iteration 2870/3560 Training loss: 1.8989 0.6791 sec/batch\n",
      "Epoch 17/20  Iteration 2871/3560 Training loss: 1.8980 0.7005 sec/batch\n",
      "Epoch 17/20  Iteration 2872/3560 Training loss: 1.8969 0.7628 sec/batch\n",
      "Epoch 17/20  Iteration 2873/3560 Training loss: 1.8962 0.7523 sec/batch\n",
      "Epoch 17/20  Iteration 2874/3560 Training loss: 1.8952 0.7393 sec/batch\n",
      "Epoch 17/20  Iteration 2875/3560 Training loss: 1.8944 0.7282 sec/batch\n",
      "Epoch 17/20  Iteration 2876/3560 Training loss: 1.8948 0.7058 sec/batch\n",
      "Epoch 17/20  Iteration 2877/3560 Training loss: 1.8957 0.5829 sec/batch\n",
      "Epoch 17/20  Iteration 2878/3560 Training loss: 1.8961 0.6241 sec/batch\n",
      "Epoch 17/20  Iteration 2879/3560 Training loss: 1.8959 0.5307 sec/batch\n",
      "Epoch 17/20  Iteration 2880/3560 Training loss: 1.8946 0.6719 sec/batch\n",
      "Epoch 17/20  Iteration 2881/3560 Training loss: 1.8948 0.6004 sec/batch\n",
      "Epoch 17/20  Iteration 2882/3560 Training loss: 1.8956 0.6807 sec/batch\n",
      "Epoch 17/20  Iteration 2883/3560 Training loss: 1.8953 0.6875 sec/batch\n",
      "Epoch 17/20  Iteration 2884/3560 Training loss: 1.8951 0.6157 sec/batch\n",
      "Epoch 17/20  Iteration 2885/3560 Training loss: 1.8946 0.6689 sec/batch\n",
      "Epoch 17/20  Iteration 2886/3560 Training loss: 1.8933 0.7157 sec/batch\n",
      "Epoch 17/20  Iteration 2887/3560 Training loss: 1.8922 0.6149 sec/batch\n",
      "Epoch 17/20  Iteration 2888/3560 Training loss: 1.8917 0.6916 sec/batch\n",
      "Epoch 17/20  Iteration 2889/3560 Training loss: 1.8911 0.7374 sec/batch\n",
      "Epoch 17/20  Iteration 2890/3560 Training loss: 1.8912 0.6551 sec/batch\n",
      "Epoch 17/20  Iteration 2891/3560 Training loss: 1.8908 0.6511 sec/batch\n",
      "Epoch 17/20  Iteration 2892/3560 Training loss: 1.8898 0.6327 sec/batch\n",
      "Epoch 17/20  Iteration 2893/3560 Training loss: 1.8899 0.6296 sec/batch\n",
      "Epoch 17/20  Iteration 2894/3560 Training loss: 1.8888 0.6248 sec/batch\n",
      "Epoch 17/20  Iteration 2895/3560 Training loss: 1.8888 0.6434 sec/batch\n",
      "Epoch 17/20  Iteration 2896/3560 Training loss: 1.8886 0.5867 sec/batch\n",
      "Epoch 17/20  Iteration 2897/3560 Training loss: 1.8885 0.6736 sec/batch\n",
      "Epoch 17/20  Iteration 2898/3560 Training loss: 1.8893 0.6203 sec/batch\n",
      "Epoch 17/20  Iteration 2899/3560 Training loss: 1.8887 0.6857 sec/batch\n",
      "Epoch 17/20  Iteration 2900/3560 Training loss: 1.8893 0.6665 sec/batch\n",
      "Epoch 17/20  Iteration 2901/3560 Training loss: 1.8891 0.7015 sec/batch\n",
      "Epoch 17/20  Iteration 2902/3560 Training loss: 1.8890 0.5747 sec/batch\n",
      "Epoch 17/20  Iteration 2903/3560 Training loss: 1.8888 0.6949 sec/batch\n",
      "Epoch 17/20  Iteration 2904/3560 Training loss: 1.8890 0.6408 sec/batch\n",
      "Epoch 17/20  Iteration 2905/3560 Training loss: 1.8892 0.6226 sec/batch\n",
      "Epoch 17/20  Iteration 2906/3560 Training loss: 1.8889 0.6841 sec/batch\n",
      "Epoch 17/20  Iteration 2907/3560 Training loss: 1.8884 0.6973 sec/batch\n",
      "Epoch 17/20  Iteration 2908/3560 Training loss: 1.8888 0.6353 sec/batch\n",
      "Epoch 17/20  Iteration 2909/3560 Training loss: 1.8885 0.6884 sec/batch\n",
      "Epoch 17/20  Iteration 2910/3560 Training loss: 1.8892 0.6331 sec/batch\n",
      "Epoch 17/20  Iteration 2911/3560 Training loss: 1.8893 0.6495 sec/batch\n",
      "Epoch 17/20  Iteration 2912/3560 Training loss: 1.8895 0.6477 sec/batch\n",
      "Epoch 17/20  Iteration 2913/3560 Training loss: 1.8894 0.6168 sec/batch\n",
      "Epoch 17/20  Iteration 2914/3560 Training loss: 1.8896 0.6309 sec/batch\n",
      "Epoch 17/20  Iteration 2915/3560 Training loss: 1.8897 0.6903 sec/batch\n",
      "Epoch 17/20  Iteration 2916/3560 Training loss: 1.8894 0.7064 sec/batch\n",
      "Epoch 17/20  Iteration 2917/3560 Training loss: 1.8892 0.7070 sec/batch\n",
      "Epoch 17/20  Iteration 2918/3560 Training loss: 1.8893 0.6583 sec/batch\n",
      "Epoch 17/20  Iteration 2919/3560 Training loss: 1.8897 0.6357 sec/batch\n",
      "Epoch 17/20  Iteration 2920/3560 Training loss: 1.8898 0.7037 sec/batch\n",
      "Epoch 17/20  Iteration 2921/3560 Training loss: 1.8901 0.6754 sec/batch\n",
      "Epoch 17/20  Iteration 2922/3560 Training loss: 1.8898 0.6251 sec/batch\n",
      "Epoch 17/20  Iteration 2923/3560 Training loss: 1.8897 0.6527 sec/batch\n",
      "Epoch 17/20  Iteration 2924/3560 Training loss: 1.8899 0.6118 sec/batch\n",
      "Epoch 17/20  Iteration 2925/3560 Training loss: 1.8899 0.6331 sec/batch\n",
      "Epoch 17/20  Iteration 2926/3560 Training loss: 1.8900 0.6400 sec/batch\n",
      "Epoch 17/20  Iteration 2927/3560 Training loss: 1.8896 0.6043 sec/batch\n",
      "Epoch 17/20  Iteration 2928/3560 Training loss: 1.8894 0.6494 sec/batch\n",
      "Epoch 17/20  Iteration 2929/3560 Training loss: 1.8890 0.5675 sec/batch\n",
      "Epoch 17/20  Iteration 2930/3560 Training loss: 1.8891 0.6739 sec/batch\n",
      "Epoch 17/20  Iteration 2931/3560 Training loss: 1.8886 0.5890 sec/batch\n",
      "Epoch 17/20  Iteration 2932/3560 Training loss: 1.8887 0.7041 sec/batch\n",
      "Epoch 17/20  Iteration 2933/3560 Training loss: 1.8882 0.6862 sec/batch\n",
      "Epoch 17/20  Iteration 2934/3560 Training loss: 1.8879 0.6522 sec/batch\n",
      "Epoch 17/20  Iteration 2935/3560 Training loss: 1.8879 0.6253 sec/batch\n",
      "Epoch 17/20  Iteration 2936/3560 Training loss: 1.8877 0.6128 sec/batch\n",
      "Epoch 17/20  Iteration 2937/3560 Training loss: 1.8872 0.7015 sec/batch\n",
      "Epoch 17/20  Iteration 2938/3560 Training loss: 1.8872 0.6672 sec/batch\n",
      "Epoch 17/20  Iteration 2939/3560 Training loss: 1.8868 0.6291 sec/batch\n",
      "Epoch 17/20  Iteration 2940/3560 Training loss: 1.8868 0.5917 sec/batch\n",
      "Epoch 17/20  Iteration 2941/3560 Training loss: 1.8863 0.6860 sec/batch\n",
      "Epoch 17/20  Iteration 2942/3560 Training loss: 1.8859 0.6067 sec/batch\n",
      "Epoch 17/20  Iteration 2943/3560 Training loss: 1.8855 0.6198 sec/batch\n",
      "Epoch 17/20  Iteration 2944/3560 Training loss: 1.8856 0.6604 sec/batch\n",
      "Epoch 17/20  Iteration 2945/3560 Training loss: 1.8856 0.6779 sec/batch\n",
      "Epoch 17/20  Iteration 2946/3560 Training loss: 1.8852 0.6421 sec/batch\n",
      "Epoch 17/20  Iteration 2947/3560 Training loss: 1.8850 0.6468 sec/batch\n",
      "Epoch 17/20  Iteration 2948/3560 Training loss: 1.8847 0.6238 sec/batch\n",
      "Epoch 17/20  Iteration 2949/3560 Training loss: 1.8847 0.7399 sec/batch\n",
      "Epoch 17/20  Iteration 2950/3560 Training loss: 1.8847 0.6491 sec/batch\n",
      "Epoch 17/20  Iteration 2951/3560 Training loss: 1.8844 0.7153 sec/batch\n",
      "Epoch 17/20  Iteration 2952/3560 Training loss: 1.8843 0.8333 sec/batch\n",
      "Epoch 17/20  Iteration 2953/3560 Training loss: 1.8841 0.6362 sec/batch\n",
      "Epoch 17/20  Iteration 2954/3560 Training loss: 1.8840 0.6094 sec/batch\n",
      "Epoch 17/20  Iteration 2955/3560 Training loss: 1.8840 0.6253 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 2956/3560 Training loss: 1.8839 0.6177 sec/batch\n",
      "Epoch 17/20  Iteration 2957/3560 Training loss: 1.8839 0.6344 sec/batch\n",
      "Epoch 17/20  Iteration 2958/3560 Training loss: 1.8838 0.7171 sec/batch\n",
      "Epoch 17/20  Iteration 2959/3560 Training loss: 1.8838 0.6459 sec/batch\n",
      "Epoch 17/20  Iteration 2960/3560 Training loss: 1.8838 0.5894 sec/batch\n",
      "Epoch 17/20  Iteration 2961/3560 Training loss: 1.8836 0.6421 sec/batch\n",
      "Epoch 17/20  Iteration 2962/3560 Training loss: 1.8835 0.7078 sec/batch\n",
      "Epoch 17/20  Iteration 2963/3560 Training loss: 1.8832 0.6695 sec/batch\n",
      "Epoch 17/20  Iteration 2964/3560 Training loss: 1.8829 0.7028 sec/batch\n",
      "Epoch 17/20  Iteration 2965/3560 Training loss: 1.8829 0.6448 sec/batch\n",
      "Epoch 17/20  Iteration 2966/3560 Training loss: 1.8829 0.7059 sec/batch\n",
      "Epoch 17/20  Iteration 2967/3560 Training loss: 1.8828 0.6761 sec/batch\n",
      "Epoch 17/20  Iteration 2968/3560 Training loss: 1.8828 0.6545 sec/batch\n",
      "Epoch 17/20  Iteration 2969/3560 Training loss: 1.8828 0.7427 sec/batch\n",
      "Epoch 17/20  Iteration 2970/3560 Training loss: 1.8827 0.7880 sec/batch\n",
      "Epoch 17/20  Iteration 2971/3560 Training loss: 1.8825 0.6097 sec/batch\n",
      "Epoch 17/20  Iteration 2972/3560 Training loss: 1.8826 0.7088 sec/batch\n",
      "Epoch 17/20  Iteration 2973/3560 Training loss: 1.8826 0.7117 sec/batch\n",
      "Epoch 17/20  Iteration 2974/3560 Training loss: 1.8822 0.7304 sec/batch\n",
      "Epoch 17/20  Iteration 2975/3560 Training loss: 1.8824 0.7903 sec/batch\n",
      "Epoch 17/20  Iteration 2976/3560 Training loss: 1.8824 0.7983 sec/batch\n",
      "Epoch 17/20  Iteration 2977/3560 Training loss: 1.8824 0.7616 sec/batch\n",
      "Epoch 17/20  Iteration 2978/3560 Training loss: 1.8823 0.7682 sec/batch\n",
      "Epoch 17/20  Iteration 2979/3560 Training loss: 1.8820 0.6671 sec/batch\n",
      "Epoch 17/20  Iteration 2980/3560 Training loss: 1.8818 0.5570 sec/batch\n",
      "Epoch 17/20  Iteration 2981/3560 Training loss: 1.8819 0.6915 sec/batch\n",
      "Epoch 17/20  Iteration 2982/3560 Training loss: 1.8820 0.6402 sec/batch\n",
      "Epoch 17/20  Iteration 2983/3560 Training loss: 1.8820 0.7430 sec/batch\n",
      "Epoch 17/20  Iteration 2984/3560 Training loss: 1.8821 0.6773 sec/batch\n",
      "Epoch 17/20  Iteration 2985/3560 Training loss: 1.8822 0.7363 sec/batch\n",
      "Epoch 17/20  Iteration 2986/3560 Training loss: 1.8822 0.8006 sec/batch\n",
      "Epoch 17/20  Iteration 2987/3560 Training loss: 1.8824 0.6149 sec/batch\n",
      "Epoch 17/20  Iteration 2988/3560 Training loss: 1.8823 0.6982 sec/batch\n",
      "Epoch 17/20  Iteration 2989/3560 Training loss: 1.8825 0.8650 sec/batch\n",
      "Epoch 17/20  Iteration 2990/3560 Training loss: 1.8825 0.8052 sec/batch\n",
      "Epoch 17/20  Iteration 2991/3560 Training loss: 1.8824 0.7681 sec/batch\n",
      "Epoch 17/20  Iteration 2992/3560 Training loss: 1.8823 0.6762 sec/batch\n",
      "Epoch 17/20  Iteration 2993/3560 Training loss: 1.8823 0.7281 sec/batch\n",
      "Epoch 17/20  Iteration 2994/3560 Training loss: 1.8823 0.8739 sec/batch\n",
      "Epoch 17/20  Iteration 2995/3560 Training loss: 1.8824 0.7050 sec/batch\n",
      "Epoch 17/20  Iteration 2996/3560 Training loss: 1.8825 0.7039 sec/batch\n",
      "Epoch 17/20  Iteration 2997/3560 Training loss: 1.8825 0.6385 sec/batch\n",
      "Epoch 17/20  Iteration 2998/3560 Training loss: 1.8824 0.5599 sec/batch\n",
      "Epoch 17/20  Iteration 2999/3560 Training loss: 1.8823 0.7249 sec/batch\n",
      "Epoch 17/20  Iteration 3000/3560 Training loss: 1.8824 0.7623 sec/batch\n",
      "Epoch 17/20  Iteration 3001/3560 Training loss: 1.8825 0.7659 sec/batch\n",
      "Epoch 17/20  Iteration 3002/3560 Training loss: 1.8826 0.7055 sec/batch\n",
      "Epoch 17/20  Iteration 3003/3560 Training loss: 1.8825 0.6991 sec/batch\n",
      "Epoch 17/20  Iteration 3004/3560 Training loss: 1.8825 0.6824 sec/batch\n",
      "Epoch 17/20  Iteration 3005/3560 Training loss: 1.8825 0.7007 sec/batch\n",
      "Epoch 17/20  Iteration 3006/3560 Training loss: 1.8825 0.6462 sec/batch\n",
      "Epoch 17/20  Iteration 3007/3560 Training loss: 1.8822 0.6833 sec/batch\n",
      "Epoch 17/20  Iteration 3008/3560 Training loss: 1.8824 0.6129 sec/batch\n",
      "Epoch 17/20  Iteration 3009/3560 Training loss: 1.8826 0.7011 sec/batch\n",
      "Epoch 17/20  Iteration 3010/3560 Training loss: 1.8825 0.6191 sec/batch\n",
      "Epoch 17/20  Iteration 3011/3560 Training loss: 1.8825 0.6496 sec/batch\n",
      "Epoch 17/20  Iteration 3012/3560 Training loss: 1.8825 0.6408 sec/batch\n",
      "Epoch 17/20  Iteration 3013/3560 Training loss: 1.8825 0.8752 sec/batch\n",
      "Epoch 17/20  Iteration 3014/3560 Training loss: 1.8825 0.7198 sec/batch\n",
      "Epoch 17/20  Iteration 3015/3560 Training loss: 1.8825 0.6415 sec/batch\n",
      "Epoch 17/20  Iteration 3016/3560 Training loss: 1.8828 0.6609 sec/batch\n",
      "Epoch 17/20  Iteration 3017/3560 Training loss: 1.8828 0.7133 sec/batch\n",
      "Epoch 17/20  Iteration 3018/3560 Training loss: 1.8827 0.6316 sec/batch\n",
      "Epoch 17/20  Iteration 3019/3560 Training loss: 1.8826 0.6203 sec/batch\n",
      "Epoch 17/20  Iteration 3020/3560 Training loss: 1.8824 0.6313 sec/batch\n",
      "Epoch 17/20  Iteration 3021/3560 Training loss: 1.8825 0.6807 sec/batch\n",
      "Epoch 17/20  Iteration 3022/3560 Training loss: 1.8825 0.6339 sec/batch\n",
      "Epoch 17/20  Iteration 3023/3560 Training loss: 1.8826 0.6746 sec/batch\n",
      "Epoch 17/20  Iteration 3024/3560 Training loss: 1.8826 0.5836 sec/batch\n",
      "Epoch 17/20  Iteration 3025/3560 Training loss: 1.8824 0.6707 sec/batch\n",
      "Epoch 17/20  Iteration 3026/3560 Training loss: 1.8825 0.6957 sec/batch\n",
      "Epoch 18/20  Iteration 3027/3560 Training loss: 1.9334 0.6724 sec/batch\n",
      "Epoch 18/20  Iteration 3028/3560 Training loss: 1.9013 0.6901 sec/batch\n",
      "Epoch 18/20  Iteration 3029/3560 Training loss: 1.8932 0.6748 sec/batch\n",
      "Epoch 18/20  Iteration 3030/3560 Training loss: 1.8882 0.6397 sec/batch\n",
      "Epoch 18/20  Iteration 3031/3560 Training loss: 1.8854 0.6502 sec/batch\n",
      "Epoch 18/20  Iteration 3032/3560 Training loss: 1.8773 0.6728 sec/batch\n",
      "Epoch 18/20  Iteration 3033/3560 Training loss: 1.8778 0.5907 sec/batch\n",
      "Epoch 18/20  Iteration 3034/3560 Training loss: 1.8781 0.6173 sec/batch\n",
      "Epoch 18/20  Iteration 3035/3560 Training loss: 1.8802 0.6200 sec/batch\n",
      "Epoch 18/20  Iteration 3036/3560 Training loss: 1.8796 0.6725 sec/batch\n",
      "Epoch 18/20  Iteration 3037/3560 Training loss: 1.8755 0.6086 sec/batch\n",
      "Epoch 18/20  Iteration 3038/3560 Training loss: 1.8741 0.7185 sec/batch\n",
      "Epoch 18/20  Iteration 3039/3560 Training loss: 1.8747 0.7047 sec/batch\n",
      "Epoch 18/20  Iteration 3040/3560 Training loss: 1.8773 0.6080 sec/batch\n",
      "Epoch 18/20  Iteration 3041/3560 Training loss: 1.8772 0.7501 sec/batch\n",
      "Epoch 18/20  Iteration 3042/3560 Training loss: 1.8751 0.6135 sec/batch\n",
      "Epoch 18/20  Iteration 3043/3560 Training loss: 1.8749 0.6441 sec/batch\n",
      "Epoch 18/20  Iteration 3044/3560 Training loss: 1.8767 0.6012 sec/batch\n",
      "Epoch 18/20  Iteration 3045/3560 Training loss: 1.8762 0.6550 sec/batch\n",
      "Epoch 18/20  Iteration 3046/3560 Training loss: 1.8771 0.6016 sec/batch\n",
      "Epoch 18/20  Iteration 3047/3560 Training loss: 1.8767 0.5248 sec/batch\n",
      "Epoch 18/20  Iteration 3048/3560 Training loss: 1.8779 0.7167 sec/batch\n",
      "Epoch 18/20  Iteration 3049/3560 Training loss: 1.8776 0.6420 sec/batch\n",
      "Epoch 18/20  Iteration 3050/3560 Training loss: 1.8771 0.6956 sec/batch\n",
      "Epoch 18/20  Iteration 3051/3560 Training loss: 1.8768 0.6294 sec/batch\n",
      "Epoch 18/20  Iteration 3052/3560 Training loss: 1.8765 0.6841 sec/batch\n",
      "Epoch 18/20  Iteration 3053/3560 Training loss: 1.8758 0.6835 sec/batch\n",
      "Epoch 18/20  Iteration 3054/3560 Training loss: 1.8761 0.5825 sec/batch\n",
      "Epoch 18/20  Iteration 3055/3560 Training loss: 1.8768 0.7039 sec/batch\n",
      "Epoch 18/20  Iteration 3056/3560 Training loss: 1.8774 0.7074 sec/batch\n",
      "Epoch 18/20  Iteration 3057/3560 Training loss: 1.8769 0.6694 sec/batch\n",
      "Epoch 18/20  Iteration 3058/3560 Training loss: 1.8760 0.6114 sec/batch\n",
      "Epoch 18/20  Iteration 3059/3560 Training loss: 1.8763 0.6108 sec/batch\n",
      "Epoch 18/20  Iteration 3060/3560 Training loss: 1.8769 0.6332 sec/batch\n",
      "Epoch 18/20  Iteration 3061/3560 Training loss: 1.8765 0.6622 sec/batch\n",
      "Epoch 18/20  Iteration 3062/3560 Training loss: 1.8763 0.6266 sec/batch\n",
      "Epoch 18/20  Iteration 3063/3560 Training loss: 1.8761 0.6334 sec/batch\n",
      "Epoch 18/20  Iteration 3064/3560 Training loss: 1.8753 0.6719 sec/batch\n",
      "Epoch 18/20  Iteration 3065/3560 Training loss: 1.8741 0.6901 sec/batch\n",
      "Epoch 18/20  Iteration 3066/3560 Training loss: 1.8738 0.6219 sec/batch\n",
      "Epoch 18/20  Iteration 3067/3560 Training loss: 1.8731 0.7058 sec/batch\n",
      "Epoch 18/20  Iteration 3068/3560 Training loss: 1.8734 0.6366 sec/batch\n",
      "Epoch 18/20  Iteration 3069/3560 Training loss: 1.8727 0.6309 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3070/3560 Training loss: 1.8718 0.6406 sec/batch\n",
      "Epoch 18/20  Iteration 3071/3560 Training loss: 1.8717 0.6152 sec/batch\n",
      "Epoch 18/20  Iteration 3072/3560 Training loss: 1.8706 0.6259 sec/batch\n",
      "Epoch 18/20  Iteration 3073/3560 Training loss: 1.8704 0.6921 sec/batch\n",
      "Epoch 18/20  Iteration 3074/3560 Training loss: 1.8702 0.6310 sec/batch\n",
      "Epoch 18/20  Iteration 3075/3560 Training loss: 1.8701 0.6632 sec/batch\n",
      "Epoch 18/20  Iteration 3076/3560 Training loss: 1.8710 0.6284 sec/batch\n",
      "Epoch 18/20  Iteration 3077/3560 Training loss: 1.8707 0.5952 sec/batch\n",
      "Epoch 18/20  Iteration 3078/3560 Training loss: 1.8715 0.6789 sec/batch\n",
      "Epoch 18/20  Iteration 3079/3560 Training loss: 1.8713 0.6630 sec/batch\n",
      "Epoch 18/20  Iteration 3080/3560 Training loss: 1.8713 0.6312 sec/batch\n",
      "Epoch 18/20  Iteration 3081/3560 Training loss: 1.8709 0.6861 sec/batch\n",
      "Epoch 18/20  Iteration 3082/3560 Training loss: 1.8709 0.6223 sec/batch\n",
      "Epoch 18/20  Iteration 3083/3560 Training loss: 1.8711 0.5758 sec/batch\n",
      "Epoch 18/20  Iteration 3084/3560 Training loss: 1.8711 0.6658 sec/batch\n",
      "Epoch 18/20  Iteration 3085/3560 Training loss: 1.8708 0.6676 sec/batch\n",
      "Epoch 18/20  Iteration 3086/3560 Training loss: 1.8712 0.6387 sec/batch\n",
      "Epoch 18/20  Iteration 3087/3560 Training loss: 1.8711 0.6399 sec/batch\n",
      "Epoch 18/20  Iteration 3088/3560 Training loss: 1.8719 0.6337 sec/batch\n",
      "Epoch 18/20  Iteration 3089/3560 Training loss: 1.8720 0.5661 sec/batch\n",
      "Epoch 18/20  Iteration 3090/3560 Training loss: 1.8723 0.6874 sec/batch\n",
      "Epoch 18/20  Iteration 3091/3560 Training loss: 1.8720 0.6297 sec/batch\n",
      "Epoch 18/20  Iteration 3092/3560 Training loss: 1.8722 0.6268 sec/batch\n",
      "Epoch 18/20  Iteration 3093/3560 Training loss: 1.8723 0.6108 sec/batch\n",
      "Epoch 18/20  Iteration 3094/3560 Training loss: 1.8719 0.6292 sec/batch\n",
      "Epoch 18/20  Iteration 3095/3560 Training loss: 1.8718 0.6310 sec/batch\n",
      "Epoch 18/20  Iteration 3096/3560 Training loss: 1.8719 0.6171 sec/batch\n",
      "Epoch 18/20  Iteration 3097/3560 Training loss: 1.8723 0.5926 sec/batch\n",
      "Epoch 18/20  Iteration 3098/3560 Training loss: 1.8724 0.6290 sec/batch\n",
      "Epoch 18/20  Iteration 3099/3560 Training loss: 1.8728 0.6803 sec/batch\n",
      "Epoch 18/20  Iteration 3100/3560 Training loss: 1.8725 0.6512 sec/batch\n",
      "Epoch 18/20  Iteration 3101/3560 Training loss: 1.8725 0.6135 sec/batch\n",
      "Epoch 18/20  Iteration 3102/3560 Training loss: 1.8727 0.7156 sec/batch\n",
      "Epoch 18/20  Iteration 3103/3560 Training loss: 1.8726 0.6162 sec/batch\n",
      "Epoch 18/20  Iteration 3104/3560 Training loss: 1.8728 0.7240 sec/batch\n",
      "Epoch 18/20  Iteration 3105/3560 Training loss: 1.8724 0.6594 sec/batch\n",
      "Epoch 18/20  Iteration 3106/3560 Training loss: 1.8723 0.6596 sec/batch\n",
      "Epoch 18/20  Iteration 3107/3560 Training loss: 1.8719 0.5958 sec/batch\n",
      "Epoch 18/20  Iteration 3108/3560 Training loss: 1.8721 0.6120 sec/batch\n",
      "Epoch 18/20  Iteration 3109/3560 Training loss: 1.8717 0.6272 sec/batch\n",
      "Epoch 18/20  Iteration 3110/3560 Training loss: 1.8717 0.5432 sec/batch\n",
      "Epoch 18/20  Iteration 3111/3560 Training loss: 1.8712 0.7602 sec/batch\n",
      "Epoch 18/20  Iteration 3112/3560 Training loss: 1.8709 0.6306 sec/batch\n",
      "Epoch 18/20  Iteration 3113/3560 Training loss: 1.8710 0.6247 sec/batch\n",
      "Epoch 18/20  Iteration 3114/3560 Training loss: 1.8707 0.6539 sec/batch\n",
      "Epoch 18/20  Iteration 3115/3560 Training loss: 1.8700 0.6693 sec/batch\n",
      "Epoch 18/20  Iteration 3116/3560 Training loss: 1.8702 0.7416 sec/batch\n",
      "Epoch 18/20  Iteration 3117/3560 Training loss: 1.8698 0.6428 sec/batch\n",
      "Epoch 18/20  Iteration 3118/3560 Training loss: 1.8696 0.6412 sec/batch\n",
      "Epoch 18/20  Iteration 3119/3560 Training loss: 1.8692 0.6098 sec/batch\n",
      "Epoch 18/20  Iteration 3120/3560 Training loss: 1.8689 0.6153 sec/batch\n",
      "Epoch 18/20  Iteration 3121/3560 Training loss: 1.8686 0.6223 sec/batch\n",
      "Epoch 18/20  Iteration 3122/3560 Training loss: 1.8686 0.5506 sec/batch\n",
      "Epoch 18/20  Iteration 3123/3560 Training loss: 1.8686 0.6745 sec/batch\n",
      "Epoch 18/20  Iteration 3124/3560 Training loss: 1.8682 0.6331 sec/batch\n",
      "Epoch 18/20  Iteration 3125/3560 Training loss: 1.8678 0.6382 sec/batch\n",
      "Epoch 18/20  Iteration 3126/3560 Training loss: 1.8675 0.6935 sec/batch\n",
      "Epoch 18/20  Iteration 3127/3560 Training loss: 1.8675 0.6685 sec/batch\n",
      "Epoch 18/20  Iteration 3128/3560 Training loss: 1.8674 0.5750 sec/batch\n",
      "Epoch 18/20  Iteration 3129/3560 Training loss: 1.8672 0.7168 sec/batch\n",
      "Epoch 18/20  Iteration 3130/3560 Training loss: 1.8670 0.6933 sec/batch\n",
      "Epoch 18/20  Iteration 3131/3560 Training loss: 1.8668 0.5836 sec/batch\n",
      "Epoch 18/20  Iteration 3132/3560 Training loss: 1.8667 0.6331 sec/batch\n",
      "Epoch 18/20  Iteration 3133/3560 Training loss: 1.8668 0.6615 sec/batch\n",
      "Epoch 18/20  Iteration 3134/3560 Training loss: 1.8666 0.5855 sec/batch\n",
      "Epoch 18/20  Iteration 3135/3560 Training loss: 1.8666 0.6396 sec/batch\n",
      "Epoch 18/20  Iteration 3136/3560 Training loss: 1.8666 0.6599 sec/batch\n",
      "Epoch 18/20  Iteration 3137/3560 Training loss: 1.8666 0.6931 sec/batch\n",
      "Epoch 18/20  Iteration 3138/3560 Training loss: 1.8665 0.5958 sec/batch\n",
      "Epoch 18/20  Iteration 3139/3560 Training loss: 1.8664 0.6659 sec/batch\n",
      "Epoch 18/20  Iteration 3140/3560 Training loss: 1.8664 0.6088 sec/batch\n",
      "Epoch 18/20  Iteration 3141/3560 Training loss: 1.8662 0.6585 sec/batch\n",
      "Epoch 18/20  Iteration 3142/3560 Training loss: 1.8659 0.7024 sec/batch\n",
      "Epoch 18/20  Iteration 3143/3560 Training loss: 1.8659 0.8257 sec/batch\n",
      "Epoch 18/20  Iteration 3144/3560 Training loss: 1.8658 0.6560 sec/batch\n",
      "Epoch 18/20  Iteration 3145/3560 Training loss: 1.8658 0.6562 sec/batch\n",
      "Epoch 18/20  Iteration 3146/3560 Training loss: 1.8657 0.6489 sec/batch\n",
      "Epoch 18/20  Iteration 3147/3560 Training loss: 1.8657 0.6910 sec/batch\n",
      "Epoch 18/20  Iteration 3148/3560 Training loss: 1.8654 0.7123 sec/batch\n",
      "Epoch 18/20  Iteration 3149/3560 Training loss: 1.8652 0.8441 sec/batch\n",
      "Epoch 18/20  Iteration 3150/3560 Training loss: 1.8654 0.6410 sec/batch\n",
      "Epoch 18/20  Iteration 3151/3560 Training loss: 1.8654 0.7815 sec/batch\n",
      "Epoch 18/20  Iteration 3152/3560 Training loss: 1.8650 0.6956 sec/batch\n",
      "Epoch 18/20  Iteration 3153/3560 Training loss: 1.8651 0.8343 sec/batch\n",
      "Epoch 18/20  Iteration 3154/3560 Training loss: 1.8652 0.7315 sec/batch\n",
      "Epoch 18/20  Iteration 3155/3560 Training loss: 1.8651 0.7835 sec/batch\n",
      "Epoch 18/20  Iteration 3156/3560 Training loss: 1.8651 0.6551 sec/batch\n",
      "Epoch 18/20  Iteration 3157/3560 Training loss: 1.8649 0.7342 sec/batch\n",
      "Epoch 18/20  Iteration 3158/3560 Training loss: 1.8647 0.7253 sec/batch\n",
      "Epoch 18/20  Iteration 3159/3560 Training loss: 1.8647 0.8108 sec/batch\n",
      "Epoch 18/20  Iteration 3160/3560 Training loss: 1.8647 0.7151 sec/batch\n",
      "Epoch 18/20  Iteration 3161/3560 Training loss: 1.8647 0.8221 sec/batch\n",
      "Epoch 18/20  Iteration 3162/3560 Training loss: 1.8647 0.7316 sec/batch\n",
      "Epoch 18/20  Iteration 3163/3560 Training loss: 1.8647 0.7734 sec/batch\n",
      "Epoch 18/20  Iteration 3164/3560 Training loss: 1.8647 0.5978 sec/batch\n",
      "Epoch 18/20  Iteration 3165/3560 Training loss: 1.8648 0.6506 sec/batch\n",
      "Epoch 18/20  Iteration 3166/3560 Training loss: 1.8648 0.6719 sec/batch\n",
      "Epoch 18/20  Iteration 3167/3560 Training loss: 1.8649 0.7009 sec/batch\n",
      "Epoch 18/20  Iteration 3168/3560 Training loss: 1.8647 0.6264 sec/batch\n",
      "Epoch 18/20  Iteration 3169/3560 Training loss: 1.8648 0.7066 sec/batch\n",
      "Epoch 18/20  Iteration 3170/3560 Training loss: 1.8648 0.8755 sec/batch\n",
      "Epoch 18/20  Iteration 3171/3560 Training loss: 1.8647 0.7534 sec/batch\n",
      "Epoch 18/20  Iteration 3172/3560 Training loss: 1.8648 0.6534 sec/batch\n",
      "Epoch 18/20  Iteration 3173/3560 Training loss: 1.8648 0.7000 sec/batch\n",
      "Epoch 18/20  Iteration 3174/3560 Training loss: 1.8650 0.6805 sec/batch\n",
      "Epoch 18/20  Iteration 3175/3560 Training loss: 1.8649 0.8090 sec/batch\n",
      "Epoch 18/20  Iteration 3176/3560 Training loss: 1.8649 0.7167 sec/batch\n",
      "Epoch 18/20  Iteration 3177/3560 Training loss: 1.8647 0.6081 sec/batch\n",
      "Epoch 18/20  Iteration 3178/3560 Training loss: 1.8649 0.7101 sec/batch\n",
      "Epoch 18/20  Iteration 3179/3560 Training loss: 1.8649 0.6822 sec/batch\n",
      "Epoch 18/20  Iteration 3180/3560 Training loss: 1.8649 0.6334 sec/batch\n",
      "Epoch 18/20  Iteration 3181/3560 Training loss: 1.8649 0.6182 sec/batch\n",
      "Epoch 18/20  Iteration 3182/3560 Training loss: 1.8649 0.6031 sec/batch\n",
      "Epoch 18/20  Iteration 3183/3560 Training loss: 1.8650 0.7042 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3184/3560 Training loss: 1.8649 0.6554 sec/batch\n",
      "Epoch 18/20  Iteration 3185/3560 Training loss: 1.8647 0.6627 sec/batch\n",
      "Epoch 18/20  Iteration 3186/3560 Training loss: 1.8648 0.6198 sec/batch\n",
      "Epoch 18/20  Iteration 3187/3560 Training loss: 1.8650 0.7708 sec/batch\n",
      "Epoch 18/20  Iteration 3188/3560 Training loss: 1.8649 0.6204 sec/batch\n",
      "Epoch 18/20  Iteration 3189/3560 Training loss: 1.8649 0.6963 sec/batch\n",
      "Epoch 18/20  Iteration 3190/3560 Training loss: 1.8649 0.6981 sec/batch\n",
      "Epoch 18/20  Iteration 3191/3560 Training loss: 1.8649 0.7808 sec/batch\n",
      "Epoch 18/20  Iteration 3192/3560 Training loss: 1.8648 0.6486 sec/batch\n",
      "Epoch 18/20  Iteration 3193/3560 Training loss: 1.8648 0.5970 sec/batch\n",
      "Epoch 18/20  Iteration 3194/3560 Training loss: 1.8652 0.6566 sec/batch\n",
      "Epoch 18/20  Iteration 3195/3560 Training loss: 1.8651 0.6313 sec/batch\n",
      "Epoch 18/20  Iteration 3196/3560 Training loss: 1.8651 0.5782 sec/batch\n",
      "Epoch 18/20  Iteration 3197/3560 Training loss: 1.8649 0.6364 sec/batch\n",
      "Epoch 18/20  Iteration 3198/3560 Training loss: 1.8648 0.6215 sec/batch\n",
      "Epoch 18/20  Iteration 3199/3560 Training loss: 1.8648 0.6689 sec/batch\n",
      "Epoch 18/20  Iteration 3200/3560 Training loss: 1.8648 0.7149 sec/batch\n",
      "Epoch 18/20  Iteration 3201/3560 Training loss: 1.8649 0.6953 sec/batch\n",
      "Epoch 18/20  Iteration 3202/3560 Training loss: 1.8649 0.6379 sec/batch\n",
      "Epoch 18/20  Iteration 3203/3560 Training loss: 1.8648 0.6627 sec/batch\n",
      "Epoch 18/20  Iteration 3204/3560 Training loss: 1.8649 0.7049 sec/batch\n",
      "Epoch 19/20  Iteration 3205/3560 Training loss: 1.9252 0.5899 sec/batch\n",
      "Epoch 19/20  Iteration 3206/3560 Training loss: 1.8861 0.6193 sec/batch\n",
      "Epoch 19/20  Iteration 3207/3560 Training loss: 1.8757 0.6576 sec/batch\n",
      "Epoch 19/20  Iteration 3208/3560 Training loss: 1.8694 0.6422 sec/batch\n",
      "Epoch 19/20  Iteration 3209/3560 Training loss: 1.8666 0.7139 sec/batch\n",
      "Epoch 19/20  Iteration 3210/3560 Training loss: 1.8585 0.5742 sec/batch\n",
      "Epoch 19/20  Iteration 3211/3560 Training loss: 1.8596 0.6368 sec/batch\n",
      "Epoch 19/20  Iteration 3212/3560 Training loss: 1.8588 0.6272 sec/batch\n",
      "Epoch 19/20  Iteration 3213/3560 Training loss: 1.8615 0.6617 sec/batch\n",
      "Epoch 19/20  Iteration 3214/3560 Training loss: 1.8617 0.6793 sec/batch\n",
      "Epoch 19/20  Iteration 3215/3560 Training loss: 1.8593 0.6863 sec/batch\n",
      "Epoch 19/20  Iteration 3216/3560 Training loss: 1.8581 0.6285 sec/batch\n",
      "Epoch 19/20  Iteration 3217/3560 Training loss: 1.8578 0.6697 sec/batch\n",
      "Epoch 19/20  Iteration 3218/3560 Training loss: 1.8609 0.6820 sec/batch\n",
      "Epoch 19/20  Iteration 3219/3560 Training loss: 1.8604 0.7140 sec/batch\n",
      "Epoch 19/20  Iteration 3220/3560 Training loss: 1.8582 0.6015 sec/batch\n",
      "Epoch 19/20  Iteration 3221/3560 Training loss: 1.8586 0.6230 sec/batch\n",
      "Epoch 19/20  Iteration 3222/3560 Training loss: 1.8601 0.6481 sec/batch\n",
      "Epoch 19/20  Iteration 3223/3560 Training loss: 1.8599 0.6158 sec/batch\n",
      "Epoch 19/20  Iteration 3224/3560 Training loss: 1.8602 0.6787 sec/batch\n",
      "Epoch 19/20  Iteration 3225/3560 Training loss: 1.8599 0.7459 sec/batch\n",
      "Epoch 19/20  Iteration 3226/3560 Training loss: 1.8613 0.7209 sec/batch\n",
      "Epoch 19/20  Iteration 3227/3560 Training loss: 1.8608 0.8386 sec/batch\n",
      "Epoch 19/20  Iteration 3228/3560 Training loss: 1.8605 0.7335 sec/batch\n",
      "Epoch 19/20  Iteration 3229/3560 Training loss: 1.8599 0.6296 sec/batch\n",
      "Epoch 19/20  Iteration 3230/3560 Training loss: 1.8593 0.7417 sec/batch\n",
      "Epoch 19/20  Iteration 3231/3560 Training loss: 1.8588 0.6500 sec/batch\n",
      "Epoch 19/20  Iteration 3232/3560 Training loss: 1.8589 0.6464 sec/batch\n",
      "Epoch 19/20  Iteration 3233/3560 Training loss: 1.8601 0.6445 sec/batch\n",
      "Epoch 19/20  Iteration 3234/3560 Training loss: 1.8606 0.6476 sec/batch\n",
      "Epoch 19/20  Iteration 3235/3560 Training loss: 1.8606 0.6505 sec/batch\n",
      "Epoch 19/20  Iteration 3236/3560 Training loss: 1.8601 0.6673 sec/batch\n",
      "Epoch 19/20  Iteration 3237/3560 Training loss: 1.8602 0.6878 sec/batch\n",
      "Epoch 19/20  Iteration 3238/3560 Training loss: 1.8611 0.6271 sec/batch\n",
      "Epoch 19/20  Iteration 3239/3560 Training loss: 1.8607 0.6762 sec/batch\n",
      "Epoch 19/20  Iteration 3240/3560 Training loss: 1.8603 0.6558 sec/batch\n",
      "Epoch 19/20  Iteration 3241/3560 Training loss: 1.8599 0.6709 sec/batch\n",
      "Epoch 19/20  Iteration 3242/3560 Training loss: 1.8587 0.7248 sec/batch\n",
      "Epoch 19/20  Iteration 3243/3560 Training loss: 1.8576 0.6190 sec/batch\n",
      "Epoch 19/20  Iteration 3244/3560 Training loss: 1.8571 0.5934 sec/batch\n",
      "Epoch 19/20  Iteration 3245/3560 Training loss: 1.8567 0.6167 sec/batch\n",
      "Epoch 19/20  Iteration 3246/3560 Training loss: 1.8568 0.6318 sec/batch\n",
      "Epoch 19/20  Iteration 3247/3560 Training loss: 1.8564 0.6286 sec/batch\n",
      "Epoch 19/20  Iteration 3248/3560 Training loss: 1.8557 0.6568 sec/batch\n",
      "Epoch 19/20  Iteration 3249/3560 Training loss: 1.8558 0.6778 sec/batch\n",
      "Epoch 19/20  Iteration 3250/3560 Training loss: 1.8547 0.5649 sec/batch\n",
      "Epoch 19/20  Iteration 3251/3560 Training loss: 1.8546 0.7718 sec/batch\n",
      "Epoch 19/20  Iteration 3252/3560 Training loss: 1.8546 0.6517 sec/batch\n",
      "Epoch 19/20  Iteration 3253/3560 Training loss: 1.8542 0.6820 sec/batch\n",
      "Epoch 19/20  Iteration 3254/3560 Training loss: 1.8550 0.6776 sec/batch\n",
      "Epoch 19/20  Iteration 3255/3560 Training loss: 1.8546 0.6565 sec/batch\n",
      "Epoch 19/20  Iteration 3256/3560 Training loss: 1.8553 0.6553 sec/batch\n",
      "Epoch 19/20  Iteration 3257/3560 Training loss: 1.8551 0.7619 sec/batch\n",
      "Epoch 19/20  Iteration 3258/3560 Training loss: 1.8551 0.6292 sec/batch\n",
      "Epoch 19/20  Iteration 3259/3560 Training loss: 1.8548 0.6495 sec/batch\n",
      "Epoch 19/20  Iteration 3260/3560 Training loss: 1.8550 0.6332 sec/batch\n",
      "Epoch 19/20  Iteration 3261/3560 Training loss: 1.8552 0.5557 sec/batch\n",
      "Epoch 19/20  Iteration 3262/3560 Training loss: 1.8553 0.6447 sec/batch\n",
      "Epoch 19/20  Iteration 3263/3560 Training loss: 1.8547 0.6538 sec/batch\n",
      "Epoch 19/20  Iteration 3264/3560 Training loss: 1.8552 0.7166 sec/batch\n",
      "Epoch 19/20  Iteration 3265/3560 Training loss: 1.8552 0.6161 sec/batch\n",
      "Epoch 19/20  Iteration 3266/3560 Training loss: 1.8562 0.6797 sec/batch\n",
      "Epoch 19/20  Iteration 3267/3560 Training loss: 1.8564 0.7153 sec/batch\n",
      "Epoch 19/20  Iteration 3268/3560 Training loss: 1.8566 0.6850 sec/batch\n",
      "Epoch 19/20  Iteration 3269/3560 Training loss: 1.8566 0.6077 sec/batch\n",
      "Epoch 19/20  Iteration 3270/3560 Training loss: 1.8568 0.6964 sec/batch\n",
      "Epoch 19/20  Iteration 3271/3560 Training loss: 1.8568 0.6372 sec/batch\n",
      "Epoch 19/20  Iteration 3272/3560 Training loss: 1.8564 0.7513 sec/batch\n",
      "Epoch 19/20  Iteration 3273/3560 Training loss: 1.8562 0.7729 sec/batch\n",
      "Epoch 19/20  Iteration 3274/3560 Training loss: 1.8561 0.5950 sec/batch\n",
      "Epoch 19/20  Iteration 3275/3560 Training loss: 1.8566 0.8008 sec/batch\n",
      "Epoch 19/20  Iteration 3276/3560 Training loss: 1.8568 0.6484 sec/batch\n",
      "Epoch 19/20  Iteration 3277/3560 Training loss: 1.8571 0.6564 sec/batch\n",
      "Epoch 19/20  Iteration 3278/3560 Training loss: 1.8569 0.6048 sec/batch\n",
      "Epoch 19/20  Iteration 3279/3560 Training loss: 1.8567 0.7563 sec/batch\n",
      "Epoch 19/20  Iteration 3280/3560 Training loss: 1.8569 0.6710 sec/batch\n",
      "Epoch 19/20  Iteration 3281/3560 Training loss: 1.8569 0.6354 sec/batch\n",
      "Epoch 19/20  Iteration 3282/3560 Training loss: 1.8569 0.6613 sec/batch\n",
      "Epoch 19/20  Iteration 3283/3560 Training loss: 1.8565 0.6091 sec/batch\n",
      "Epoch 19/20  Iteration 3284/3560 Training loss: 1.8563 0.6227 sec/batch\n",
      "Epoch 19/20  Iteration 3285/3560 Training loss: 1.8558 0.6077 sec/batch\n",
      "Epoch 19/20  Iteration 3286/3560 Training loss: 1.8562 0.7132 sec/batch\n",
      "Epoch 19/20  Iteration 3287/3560 Training loss: 1.8557 0.6124 sec/batch\n",
      "Epoch 19/20  Iteration 3288/3560 Training loss: 1.8557 0.6015 sec/batch\n",
      "Epoch 19/20  Iteration 3289/3560 Training loss: 1.8552 0.6777 sec/batch\n",
      "Epoch 19/20  Iteration 3290/3560 Training loss: 1.8548 0.6149 sec/batch\n",
      "Epoch 19/20  Iteration 3291/3560 Training loss: 1.8547 0.6405 sec/batch\n",
      "Epoch 19/20  Iteration 3292/3560 Training loss: 1.8544 0.6586 sec/batch\n",
      "Epoch 19/20  Iteration 3293/3560 Training loss: 1.8539 0.6283 sec/batch\n",
      "Epoch 19/20  Iteration 3294/3560 Training loss: 1.8540 0.6939 sec/batch\n",
      "Epoch 19/20  Iteration 3295/3560 Training loss: 1.8537 0.6521 sec/batch\n",
      "Epoch 19/20  Iteration 3296/3560 Training loss: 1.8536 0.6036 sec/batch\n",
      "Epoch 19/20  Iteration 3297/3560 Training loss: 1.8531 0.6756 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3298/3560 Training loss: 1.8527 0.5900 sec/batch\n",
      "Epoch 19/20  Iteration 3299/3560 Training loss: 1.8525 0.6432 sec/batch\n",
      "Epoch 19/20  Iteration 3300/3560 Training loss: 1.8524 0.6416 sec/batch\n",
      "Epoch 19/20  Iteration 3301/3560 Training loss: 1.8524 0.7481 sec/batch\n",
      "Epoch 19/20  Iteration 3302/3560 Training loss: 1.8520 0.6993 sec/batch\n",
      "Epoch 19/20  Iteration 3303/3560 Training loss: 1.8518 0.6805 sec/batch\n",
      "Epoch 19/20  Iteration 3304/3560 Training loss: 1.8514 0.6576 sec/batch\n",
      "Epoch 19/20  Iteration 3305/3560 Training loss: 1.8514 0.6053 sec/batch\n",
      "Epoch 19/20  Iteration 3306/3560 Training loss: 1.8512 0.7024 sec/batch\n",
      "Epoch 19/20  Iteration 3307/3560 Training loss: 1.8510 0.6775 sec/batch\n",
      "Epoch 19/20  Iteration 3308/3560 Training loss: 1.8509 0.6448 sec/batch\n",
      "Epoch 19/20  Iteration 3309/3560 Training loss: 1.8507 0.6482 sec/batch\n",
      "Epoch 19/20  Iteration 3310/3560 Training loss: 1.8507 0.6337 sec/batch\n",
      "Epoch 19/20  Iteration 3311/3560 Training loss: 1.8506 0.6774 sec/batch\n",
      "Epoch 19/20  Iteration 3312/3560 Training loss: 1.8506 0.7565 sec/batch\n",
      "Epoch 19/20  Iteration 3313/3560 Training loss: 1.8506 0.6150 sec/batch\n",
      "Epoch 19/20  Iteration 3314/3560 Training loss: 1.8508 0.7024 sec/batch\n",
      "Epoch 19/20  Iteration 3315/3560 Training loss: 1.8507 0.6470 sec/batch\n",
      "Epoch 19/20  Iteration 3316/3560 Training loss: 1.8506 0.6483 sec/batch\n",
      "Epoch 19/20  Iteration 3317/3560 Training loss: 1.8505 0.7014 sec/batch\n",
      "Epoch 19/20  Iteration 3318/3560 Training loss: 1.8503 0.6708 sec/batch\n",
      "Epoch 19/20  Iteration 3319/3560 Training loss: 1.8502 0.5951 sec/batch\n",
      "Epoch 19/20  Iteration 3320/3560 Training loss: 1.8498 0.6856 sec/batch\n",
      "Epoch 19/20  Iteration 3321/3560 Training loss: 1.8498 0.5763 sec/batch\n",
      "Epoch 19/20  Iteration 3322/3560 Training loss: 1.8498 0.6319 sec/batch\n",
      "Epoch 19/20  Iteration 3323/3560 Training loss: 1.8498 0.5928 sec/batch\n",
      "Epoch 19/20  Iteration 3324/3560 Training loss: 1.8498 0.6778 sec/batch\n",
      "Epoch 19/20  Iteration 3325/3560 Training loss: 1.8496 0.5839 sec/batch\n",
      "Epoch 19/20  Iteration 3326/3560 Training loss: 1.8493 0.6527 sec/batch\n",
      "Epoch 19/20  Iteration 3327/3560 Training loss: 1.8491 0.6579 sec/batch\n",
      "Epoch 19/20  Iteration 3328/3560 Training loss: 1.8493 0.7003 sec/batch\n",
      "Epoch 19/20  Iteration 3329/3560 Training loss: 1.8493 0.7062 sec/batch\n",
      "Epoch 19/20  Iteration 3330/3560 Training loss: 1.8490 0.6041 sec/batch\n",
      "Epoch 19/20  Iteration 3331/3560 Training loss: 1.8490 0.6998 sec/batch\n",
      "Epoch 19/20  Iteration 3332/3560 Training loss: 1.8491 0.6611 sec/batch\n",
      "Epoch 19/20  Iteration 3333/3560 Training loss: 1.8490 0.6397 sec/batch\n",
      "Epoch 19/20  Iteration 3334/3560 Training loss: 1.8489 0.6561 sec/batch\n",
      "Epoch 19/20  Iteration 3335/3560 Training loss: 1.8487 0.5501 sec/batch\n",
      "Epoch 19/20  Iteration 3336/3560 Training loss: 1.8484 0.6258 sec/batch\n",
      "Epoch 19/20  Iteration 3337/3560 Training loss: 1.8484 0.5993 sec/batch\n",
      "Epoch 19/20  Iteration 3338/3560 Training loss: 1.8484 0.5960 sec/batch\n",
      "Epoch 19/20  Iteration 3339/3560 Training loss: 1.8484 0.6621 sec/batch\n",
      "Epoch 19/20  Iteration 3340/3560 Training loss: 1.8485 0.7053 sec/batch\n",
      "Epoch 19/20  Iteration 3341/3560 Training loss: 1.8485 0.6300 sec/batch\n",
      "Epoch 19/20  Iteration 3342/3560 Training loss: 1.8486 0.7121 sec/batch\n",
      "Epoch 19/20  Iteration 3343/3560 Training loss: 1.8487 0.6465 sec/batch\n",
      "Epoch 19/20  Iteration 3344/3560 Training loss: 1.8485 0.7427 sec/batch\n",
      "Epoch 19/20  Iteration 3345/3560 Training loss: 1.8488 0.5656 sec/batch\n",
      "Epoch 19/20  Iteration 3346/3560 Training loss: 1.8486 0.6727 sec/batch\n",
      "Epoch 19/20  Iteration 3347/3560 Training loss: 1.8487 0.6106 sec/batch\n",
      "Epoch 19/20  Iteration 3348/3560 Training loss: 1.8487 0.6042 sec/batch\n",
      "Epoch 19/20  Iteration 3349/3560 Training loss: 1.8486 0.5938 sec/batch\n",
      "Epoch 19/20  Iteration 3350/3560 Training loss: 1.8487 0.6207 sec/batch\n",
      "Epoch 19/20  Iteration 3351/3560 Training loss: 1.8487 0.6455 sec/batch\n",
      "Epoch 19/20  Iteration 3352/3560 Training loss: 1.8489 0.6181 sec/batch\n",
      "Epoch 19/20  Iteration 3353/3560 Training loss: 1.8489 0.6903 sec/batch\n",
      "Epoch 19/20  Iteration 3354/3560 Training loss: 1.8488 0.6948 sec/batch\n",
      "Epoch 19/20  Iteration 3355/3560 Training loss: 1.8487 0.6799 sec/batch\n",
      "Epoch 19/20  Iteration 3356/3560 Training loss: 1.8489 0.6320 sec/batch\n",
      "Epoch 19/20  Iteration 3357/3560 Training loss: 1.8489 0.5911 sec/batch\n",
      "Epoch 19/20  Iteration 3358/3560 Training loss: 1.8489 0.6447 sec/batch\n",
      "Epoch 19/20  Iteration 3359/3560 Training loss: 1.8489 0.5950 sec/batch\n",
      "Epoch 19/20  Iteration 3360/3560 Training loss: 1.8489 0.6630 sec/batch\n",
      "Epoch 19/20  Iteration 3361/3560 Training loss: 1.8489 0.6286 sec/batch\n",
      "Epoch 19/20  Iteration 3362/3560 Training loss: 1.8489 0.5828 sec/batch\n",
      "Epoch 19/20  Iteration 3363/3560 Training loss: 1.8487 0.6152 sec/batch\n",
      "Epoch 19/20  Iteration 3364/3560 Training loss: 1.8489 0.6517 sec/batch\n",
      "Epoch 19/20  Iteration 3365/3560 Training loss: 1.8490 0.6612 sec/batch\n",
      "Epoch 19/20  Iteration 3366/3560 Training loss: 1.8489 0.6366 sec/batch\n",
      "Epoch 19/20  Iteration 3367/3560 Training loss: 1.8490 0.7134 sec/batch\n",
      "Epoch 19/20  Iteration 3368/3560 Training loss: 1.8489 0.6366 sec/batch\n",
      "Epoch 19/20  Iteration 3369/3560 Training loss: 1.8489 0.6623 sec/batch\n",
      "Epoch 19/20  Iteration 3370/3560 Training loss: 1.8489 0.6365 sec/batch\n",
      "Epoch 19/20  Iteration 3371/3560 Training loss: 1.8489 0.6561 sec/batch\n",
      "Epoch 19/20  Iteration 3372/3560 Training loss: 1.8492 0.6275 sec/batch\n",
      "Epoch 19/20  Iteration 3373/3560 Training loss: 1.8491 0.5816 sec/batch\n",
      "Epoch 19/20  Iteration 3374/3560 Training loss: 1.8491 0.6236 sec/batch\n",
      "Epoch 19/20  Iteration 3375/3560 Training loss: 1.8489 0.6184 sec/batch\n",
      "Epoch 19/20  Iteration 3376/3560 Training loss: 1.8488 0.5611 sec/batch\n",
      "Epoch 19/20  Iteration 3377/3560 Training loss: 1.8490 0.6679 sec/batch\n",
      "Epoch 19/20  Iteration 3378/3560 Training loss: 1.8490 0.6245 sec/batch\n",
      "Epoch 19/20  Iteration 3379/3560 Training loss: 1.8491 0.6395 sec/batch\n",
      "Epoch 19/20  Iteration 3380/3560 Training loss: 1.8491 0.6541 sec/batch\n",
      "Epoch 19/20  Iteration 3381/3560 Training loss: 1.8489 0.7234 sec/batch\n",
      "Epoch 19/20  Iteration 3382/3560 Training loss: 1.8489 0.5769 sec/batch\n",
      "Epoch 20/20  Iteration 3383/3560 Training loss: 1.9095 0.6496 sec/batch\n",
      "Epoch 20/20  Iteration 3384/3560 Training loss: 1.8702 0.6362 sec/batch\n",
      "Epoch 20/20  Iteration 3385/3560 Training loss: 1.8594 0.6986 sec/batch\n",
      "Epoch 20/20  Iteration 3386/3560 Training loss: 1.8555 0.5924 sec/batch\n",
      "Epoch 20/20  Iteration 3387/3560 Training loss: 1.8514 0.6207 sec/batch\n",
      "Epoch 20/20  Iteration 3388/3560 Training loss: 1.8440 0.5473 sec/batch\n",
      "Epoch 20/20  Iteration 3389/3560 Training loss: 1.8454 0.6374 sec/batch\n",
      "Epoch 20/20  Iteration 3390/3560 Training loss: 1.8446 0.6937 sec/batch\n",
      "Epoch 20/20  Iteration 3391/3560 Training loss: 1.8464 0.6828 sec/batch\n",
      "Epoch 20/20  Iteration 3392/3560 Training loss: 1.8466 0.6665 sec/batch\n",
      "Epoch 20/20  Iteration 3393/3560 Training loss: 1.8432 0.7358 sec/batch\n",
      "Epoch 20/20  Iteration 3394/3560 Training loss: 1.8417 0.7378 sec/batch\n",
      "Epoch 20/20  Iteration 3395/3560 Training loss: 1.8414 0.5917 sec/batch\n",
      "Epoch 20/20  Iteration 3396/3560 Training loss: 1.8439 0.6120 sec/batch\n",
      "Epoch 20/20  Iteration 3397/3560 Training loss: 1.8437 0.6211 sec/batch\n",
      "Epoch 20/20  Iteration 3398/3560 Training loss: 1.8418 0.6095 sec/batch\n",
      "Epoch 20/20  Iteration 3399/3560 Training loss: 1.8416 0.6360 sec/batch\n",
      "Epoch 20/20  Iteration 3400/3560 Training loss: 1.8430 0.5930 sec/batch\n",
      "Epoch 20/20  Iteration 3401/3560 Training loss: 1.8431 0.5763 sec/batch\n",
      "Epoch 20/20  Iteration 3402/3560 Training loss: 1.8434 0.6519 sec/batch\n",
      "Epoch 20/20  Iteration 3403/3560 Training loss: 1.8425 0.6746 sec/batch\n",
      "Epoch 20/20  Iteration 3404/3560 Training loss: 1.8436 0.6124 sec/batch\n",
      "Epoch 20/20  Iteration 3405/3560 Training loss: 1.8433 0.6774 sec/batch\n",
      "Epoch 20/20  Iteration 3406/3560 Training loss: 1.8429 0.7538 sec/batch\n",
      "Epoch 20/20  Iteration 3407/3560 Training loss: 1.8427 0.6525 sec/batch\n",
      "Epoch 20/20  Iteration 3408/3560 Training loss: 1.8421 0.6708 sec/batch\n",
      "Epoch 20/20  Iteration 3409/3560 Training loss: 1.8413 0.6055 sec/batch\n",
      "Epoch 20/20  Iteration 3410/3560 Training loss: 1.8420 0.6502 sec/batch\n",
      "Epoch 20/20  Iteration 3411/3560 Training loss: 1.8426 0.5824 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3412/3560 Training loss: 1.8432 0.6220 sec/batch\n",
      "Epoch 20/20  Iteration 3413/3560 Training loss: 1.8430 0.6501 sec/batch\n",
      "Epoch 20/20  Iteration 3414/3560 Training loss: 1.8420 0.5800 sec/batch\n",
      "Epoch 20/20  Iteration 3415/3560 Training loss: 1.8422 0.6976 sec/batch\n",
      "Epoch 20/20  Iteration 3416/3560 Training loss: 1.8429 0.5931 sec/batch\n",
      "Epoch 20/20  Iteration 3417/3560 Training loss: 1.8423 0.7278 sec/batch\n",
      "Epoch 20/20  Iteration 3418/3560 Training loss: 1.8421 0.6551 sec/batch\n",
      "Epoch 20/20  Iteration 3419/3560 Training loss: 1.8416 0.7059 sec/batch\n",
      "Epoch 20/20  Iteration 3420/3560 Training loss: 1.8408 0.6081 sec/batch\n",
      "Epoch 20/20  Iteration 3421/3560 Training loss: 1.8397 0.6343 sec/batch\n",
      "Epoch 20/20  Iteration 3422/3560 Training loss: 1.8391 0.6587 sec/batch\n",
      "Epoch 20/20  Iteration 3423/3560 Training loss: 1.8387 0.6242 sec/batch\n",
      "Epoch 20/20  Iteration 3424/3560 Training loss: 1.8391 0.6092 sec/batch\n",
      "Epoch 20/20  Iteration 3425/3560 Training loss: 1.8385 0.6409 sec/batch\n",
      "Epoch 20/20  Iteration 3426/3560 Training loss: 1.8377 0.6910 sec/batch\n",
      "Epoch 20/20  Iteration 3427/3560 Training loss: 1.8379 0.5999 sec/batch\n",
      "Epoch 20/20  Iteration 3428/3560 Training loss: 1.8371 0.7134 sec/batch\n",
      "Epoch 20/20  Iteration 3429/3560 Training loss: 1.8369 0.6440 sec/batch\n",
      "Epoch 20/20  Iteration 3430/3560 Training loss: 1.8367 0.6513 sec/batch\n",
      "Epoch 20/20  Iteration 3431/3560 Training loss: 1.8366 0.6471 sec/batch\n",
      "Epoch 20/20  Iteration 3432/3560 Training loss: 1.8375 0.6671 sec/batch\n",
      "Epoch 20/20  Iteration 3433/3560 Training loss: 1.8369 0.6842 sec/batch\n",
      "Epoch 20/20  Iteration 3434/3560 Training loss: 1.8378 0.5956 sec/batch\n",
      "Epoch 20/20  Iteration 3435/3560 Training loss: 1.8375 0.6206 sec/batch\n",
      "Epoch 20/20  Iteration 3436/3560 Training loss: 1.8374 0.6691 sec/batch\n",
      "Epoch 20/20  Iteration 3437/3560 Training loss: 1.8371 0.6446 sec/batch\n",
      "Epoch 20/20  Iteration 3438/3560 Training loss: 1.8374 0.6815 sec/batch\n",
      "Epoch 20/20  Iteration 3439/3560 Training loss: 1.8378 0.6639 sec/batch\n",
      "Epoch 20/20  Iteration 3440/3560 Training loss: 1.8376 0.5941 sec/batch\n",
      "Epoch 20/20  Iteration 3441/3560 Training loss: 1.8371 0.6403 sec/batch\n",
      "Epoch 20/20  Iteration 3442/3560 Training loss: 1.8376 0.6673 sec/batch\n",
      "Epoch 20/20  Iteration 3443/3560 Training loss: 1.8375 0.6441 sec/batch\n",
      "Epoch 20/20  Iteration 3444/3560 Training loss: 1.8384 0.6696 sec/batch\n",
      "Epoch 20/20  Iteration 3445/3560 Training loss: 1.8385 0.6308 sec/batch\n",
      "Epoch 20/20  Iteration 3446/3560 Training loss: 1.8387 0.7431 sec/batch\n",
      "Epoch 20/20  Iteration 3447/3560 Training loss: 1.8387 0.6696 sec/batch\n",
      "Epoch 20/20  Iteration 3448/3560 Training loss: 1.8389 0.6855 sec/batch\n",
      "Epoch 20/20  Iteration 3449/3560 Training loss: 1.8390 0.6663 sec/batch\n",
      "Epoch 20/20  Iteration 3450/3560 Training loss: 1.8386 0.6078 sec/batch\n",
      "Epoch 20/20  Iteration 3451/3560 Training loss: 1.8383 0.5762 sec/batch\n",
      "Epoch 20/20  Iteration 3452/3560 Training loss: 1.8383 0.6249 sec/batch\n",
      "Epoch 20/20  Iteration 3453/3560 Training loss: 1.8387 0.6884 sec/batch\n",
      "Epoch 20/20  Iteration 3454/3560 Training loss: 1.8389 0.5826 sec/batch\n",
      "Epoch 20/20  Iteration 3455/3560 Training loss: 1.8393 0.6306 sec/batch\n",
      "Epoch 20/20  Iteration 3456/3560 Training loss: 1.8391 0.6563 sec/batch\n",
      "Epoch 20/20  Iteration 3457/3560 Training loss: 1.8392 0.6766 sec/batch\n",
      "Epoch 20/20  Iteration 3458/3560 Training loss: 1.8394 0.6733 sec/batch\n",
      "Epoch 20/20  Iteration 3459/3560 Training loss: 1.8394 0.6388 sec/batch\n",
      "Epoch 20/20  Iteration 3460/3560 Training loss: 1.8396 0.6263 sec/batch\n",
      "Epoch 20/20  Iteration 3461/3560 Training loss: 1.8392 0.6163 sec/batch\n",
      "Epoch 20/20  Iteration 3462/3560 Training loss: 1.8390 0.6590 sec/batch\n",
      "Epoch 20/20  Iteration 3463/3560 Training loss: 1.8386 0.5908 sec/batch\n",
      "Epoch 20/20  Iteration 3464/3560 Training loss: 1.8386 0.6491 sec/batch\n",
      "Epoch 20/20  Iteration 3465/3560 Training loss: 1.8382 0.6061 sec/batch\n",
      "Epoch 20/20  Iteration 3466/3560 Training loss: 1.8380 0.6530 sec/batch\n",
      "Epoch 20/20  Iteration 3467/3560 Training loss: 1.8376 0.6462 sec/batch\n",
      "Epoch 20/20  Iteration 3468/3560 Training loss: 1.8372 0.6541 sec/batch\n",
      "Epoch 20/20  Iteration 3469/3560 Training loss: 1.8372 0.7213 sec/batch\n",
      "Epoch 20/20  Iteration 3470/3560 Training loss: 1.8371 0.6731 sec/batch\n",
      "Epoch 20/20  Iteration 3471/3560 Training loss: 1.8366 0.6999 sec/batch\n",
      "Epoch 20/20  Iteration 3472/3560 Training loss: 1.8368 0.6294 sec/batch\n",
      "Epoch 20/20  Iteration 3473/3560 Training loss: 1.8366 0.5830 sec/batch\n",
      "Epoch 20/20  Iteration 3474/3560 Training loss: 1.8365 0.6700 sec/batch\n",
      "Epoch 20/20  Iteration 3475/3560 Training loss: 1.8360 0.5995 sec/batch\n",
      "Epoch 20/20  Iteration 3476/3560 Training loss: 1.8357 0.5570 sec/batch\n",
      "Epoch 20/20  Iteration 3477/3560 Training loss: 1.8355 0.6285 sec/batch\n",
      "Epoch 20/20  Iteration 3478/3560 Training loss: 1.8356 0.5829 sec/batch\n",
      "Epoch 20/20  Iteration 3479/3560 Training loss: 1.8356 0.6809 sec/batch\n",
      "Epoch 20/20  Iteration 3480/3560 Training loss: 1.8352 0.6434 sec/batch\n",
      "Epoch 20/20  Iteration 3481/3560 Training loss: 1.8349 0.6210 sec/batch\n",
      "Epoch 20/20  Iteration 3482/3560 Training loss: 1.8346 0.7776 sec/batch\n",
      "Epoch 20/20  Iteration 3483/3560 Training loss: 1.8346 0.6402 sec/batch\n",
      "Epoch 20/20  Iteration 3484/3560 Training loss: 1.8345 0.7289 sec/batch\n",
      "Epoch 20/20  Iteration 3485/3560 Training loss: 1.8343 0.5636 sec/batch\n",
      "Epoch 20/20  Iteration 3486/3560 Training loss: 1.8341 0.6836 sec/batch\n",
      "Epoch 20/20  Iteration 3487/3560 Training loss: 1.8338 0.5400 sec/batch\n",
      "Epoch 20/20  Iteration 3488/3560 Training loss: 1.8337 0.6398 sec/batch\n",
      "Epoch 20/20  Iteration 3489/3560 Training loss: 1.8337 0.6506 sec/batch\n",
      "Epoch 20/20  Iteration 3490/3560 Training loss: 1.8337 0.5650 sec/batch\n",
      "Epoch 20/20  Iteration 3491/3560 Training loss: 1.8337 0.6358 sec/batch\n",
      "Epoch 20/20  Iteration 3492/3560 Training loss: 1.8337 0.6600 sec/batch\n",
      "Epoch 20/20  Iteration 3493/3560 Training loss: 1.8337 0.6303 sec/batch\n",
      "Epoch 20/20  Iteration 3494/3560 Training loss: 1.8337 0.5898 sec/batch\n",
      "Epoch 20/20  Iteration 3495/3560 Training loss: 1.8336 0.7425 sec/batch\n",
      "Epoch 20/20  Iteration 3496/3560 Training loss: 1.8335 0.6588 sec/batch\n",
      "Epoch 20/20  Iteration 3497/3560 Training loss: 1.8333 0.6076 sec/batch\n",
      "Epoch 20/20  Iteration 3498/3560 Training loss: 1.8329 0.6116 sec/batch\n",
      "Epoch 20/20  Iteration 3499/3560 Training loss: 1.8329 0.6934 sec/batch\n",
      "Epoch 20/20  Iteration 3500/3560 Training loss: 1.8328 0.5876 sec/batch\n",
      "Epoch 20/20  Iteration 3501/3560 Training loss: 1.8329 0.5830 sec/batch\n",
      "Epoch 20/20  Iteration 3502/3560 Training loss: 1.8327 0.6410 sec/batch\n",
      "Epoch 20/20  Iteration 3503/3560 Training loss: 1.8328 0.6042 sec/batch\n",
      "Epoch 20/20  Iteration 3504/3560 Training loss: 1.8326 0.7131 sec/batch\n",
      "Epoch 20/20  Iteration 3505/3560 Training loss: 1.8324 0.5690 sec/batch\n",
      "Epoch 20/20  Iteration 3506/3560 Training loss: 1.8325 0.6768 sec/batch\n",
      "Epoch 20/20  Iteration 3507/3560 Training loss: 1.8325 0.6754 sec/batch\n",
      "Epoch 20/20  Iteration 3508/3560 Training loss: 1.8321 0.6991 sec/batch\n",
      "Epoch 20/20  Iteration 3509/3560 Training loss: 1.8322 0.6378 sec/batch\n",
      "Epoch 20/20  Iteration 3510/3560 Training loss: 1.8323 0.7437 sec/batch\n",
      "Epoch 20/20  Iteration 3511/3560 Training loss: 1.8322 0.6080 sec/batch\n",
      "Epoch 20/20  Iteration 3512/3560 Training loss: 1.8322 0.6899 sec/batch\n",
      "Epoch 20/20  Iteration 3513/3560 Training loss: 1.8320 0.5844 sec/batch\n",
      "Epoch 20/20  Iteration 3514/3560 Training loss: 1.8317 0.6603 sec/batch\n",
      "Epoch 20/20  Iteration 3515/3560 Training loss: 1.8318 0.6577 sec/batch\n",
      "Epoch 20/20  Iteration 3516/3560 Training loss: 1.8317 0.6548 sec/batch\n",
      "Epoch 20/20  Iteration 3517/3560 Training loss: 1.8317 0.6448 sec/batch\n",
      "Epoch 20/20  Iteration 3518/3560 Training loss: 1.8318 0.6811 sec/batch\n",
      "Epoch 20/20  Iteration 3519/3560 Training loss: 1.8319 0.6117 sec/batch\n",
      "Epoch 20/20  Iteration 3520/3560 Training loss: 1.8320 0.6461 sec/batch\n",
      "Epoch 20/20  Iteration 3521/3560 Training loss: 1.8322 0.7061 sec/batch\n",
      "Epoch 20/20  Iteration 3522/3560 Training loss: 1.8321 0.5969 sec/batch\n",
      "Epoch 20/20  Iteration 3523/3560 Training loss: 1.8324 0.5972 sec/batch\n",
      "Epoch 20/20  Iteration 3524/3560 Training loss: 1.8324 0.6176 sec/batch\n",
      "Epoch 20/20  Iteration 3525/3560 Training loss: 1.8323 0.6599 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3526/3560 Training loss: 1.8324 0.6481 sec/batch\n",
      "Epoch 20/20  Iteration 3527/3560 Training loss: 1.8322 0.6854 sec/batch\n",
      "Epoch 20/20  Iteration 3528/3560 Training loss: 1.8324 0.5825 sec/batch\n",
      "Epoch 20/20  Iteration 3529/3560 Training loss: 1.8323 0.6658 sec/batch\n",
      "Epoch 20/20  Iteration 3530/3560 Training loss: 1.8326 0.6479 sec/batch\n",
      "Epoch 20/20  Iteration 3531/3560 Training loss: 1.8325 0.7026 sec/batch\n",
      "Epoch 20/20  Iteration 3532/3560 Training loss: 1.8324 0.7052 sec/batch\n",
      "Epoch 20/20  Iteration 3533/3560 Training loss: 1.8322 0.6209 sec/batch\n",
      "Epoch 20/20  Iteration 3534/3560 Training loss: 1.8324 0.5761 sec/batch\n",
      "Epoch 20/20  Iteration 3535/3560 Training loss: 1.8325 0.6695 sec/batch\n",
      "Epoch 20/20  Iteration 3536/3560 Training loss: 1.8325 0.5991 sec/batch\n",
      "Epoch 20/20  Iteration 3537/3560 Training loss: 1.8325 0.6635 sec/batch\n",
      "Epoch 20/20  Iteration 3538/3560 Training loss: 1.8326 0.6007 sec/batch\n",
      "Epoch 20/20  Iteration 3539/3560 Training loss: 1.8326 0.6786 sec/batch\n",
      "Epoch 20/20  Iteration 3540/3560 Training loss: 1.8326 0.5674 sec/batch\n",
      "Epoch 20/20  Iteration 3541/3560 Training loss: 1.8324 0.6282 sec/batch\n",
      "Epoch 20/20  Iteration 3542/3560 Training loss: 1.8326 0.6377 sec/batch\n",
      "Epoch 20/20  Iteration 3543/3560 Training loss: 1.8327 0.7732 sec/batch\n",
      "Epoch 20/20  Iteration 3544/3560 Training loss: 1.8327 0.6878 sec/batch\n",
      "Epoch 20/20  Iteration 3545/3560 Training loss: 1.8327 0.6814 sec/batch\n",
      "Epoch 20/20  Iteration 3546/3560 Training loss: 1.8327 0.6284 sec/batch\n",
      "Epoch 20/20  Iteration 3547/3560 Training loss: 1.8327 0.6533 sec/batch\n",
      "Epoch 20/20  Iteration 3548/3560 Training loss: 1.8326 0.6723 sec/batch\n",
      "Epoch 20/20  Iteration 3549/3560 Training loss: 1.8326 0.6659 sec/batch\n",
      "Epoch 20/20  Iteration 3550/3560 Training loss: 1.8330 0.6318 sec/batch\n",
      "Epoch 20/20  Iteration 3551/3560 Training loss: 1.8330 0.6599 sec/batch\n",
      "Epoch 20/20  Iteration 3552/3560 Training loss: 1.8329 0.6333 sec/batch\n",
      "Epoch 20/20  Iteration 3553/3560 Training loss: 1.8328 0.6072 sec/batch\n",
      "Epoch 20/20  Iteration 3554/3560 Training loss: 1.8327 0.6537 sec/batch\n",
      "Epoch 20/20  Iteration 3555/3560 Training loss: 1.8328 0.6545 sec/batch\n",
      "Epoch 20/20  Iteration 3556/3560 Training loss: 1.8328 0.7206 sec/batch\n",
      "Epoch 20/20  Iteration 3557/3560 Training loss: 1.8329 0.6910 sec/batch\n",
      "Epoch 20/20  Iteration 3558/3560 Training loss: 1.8329 0.7161 sec/batch\n",
      "Epoch 20/20  Iteration 3559/3560 Training loss: 1.8327 0.6192 sec/batch\n",
      "Epoch 20/20  Iteration 3560/3560 Training loss: 1.8328 0.7394 sec/batch\n",
      "Epoch 1/20  Iteration 1/3560 Training loss: 4.4199 0.9133 sec/batch\n",
      "Epoch 1/20  Iteration 2/3560 Training loss: 4.3925 1.0074 sec/batch\n",
      "Epoch 1/20  Iteration 3/3560 Training loss: 4.3168 0.9931 sec/batch\n",
      "Epoch 1/20  Iteration 4/3560 Training loss: 4.1498 0.9345 sec/batch\n",
      "Epoch 1/20  Iteration 5/3560 Training loss: 4.0168 0.9497 sec/batch\n",
      "Epoch 1/20  Iteration 6/3560 Training loss: 3.9231 0.9021 sec/batch\n",
      "Epoch 1/20  Iteration 7/3560 Training loss: 3.8496 1.0030 sec/batch\n",
      "Epoch 1/20  Iteration 8/3560 Training loss: 3.7886 0.9462 sec/batch\n",
      "Epoch 1/20  Iteration 9/3560 Training loss: 3.7364 0.9540 sec/batch\n",
      "Epoch 1/20  Iteration 10/3560 Training loss: 3.6921 1.0757 sec/batch\n",
      "Epoch 1/20  Iteration 11/3560 Training loss: 3.6535 1.0075 sec/batch\n",
      "Epoch 1/20  Iteration 12/3560 Training loss: 3.6214 0.9627 sec/batch\n",
      "Epoch 1/20  Iteration 13/3560 Training loss: 3.5926 0.9478 sec/batch\n",
      "Epoch 1/20  Iteration 14/3560 Training loss: 3.5677 0.9865 sec/batch\n",
      "Epoch 1/20  Iteration 15/3560 Training loss: 3.5443 0.8834 sec/batch\n",
      "Epoch 1/20  Iteration 16/3560 Training loss: 3.5235 0.9052 sec/batch\n",
      "Epoch 1/20  Iteration 17/3560 Training loss: 3.5047 1.0981 sec/batch\n",
      "Epoch 1/20  Iteration 18/3560 Training loss: 3.4891 1.0517 sec/batch\n",
      "Epoch 1/20  Iteration 19/3560 Training loss: 3.4736 1.0656 sec/batch\n",
      "Epoch 1/20  Iteration 20/3560 Training loss: 3.4581 1.0345 sec/batch\n",
      "Epoch 1/20  Iteration 21/3560 Training loss: 3.4448 1.1168 sec/batch\n",
      "Epoch 1/20  Iteration 22/3560 Training loss: 3.4325 1.0164 sec/batch\n",
      "Epoch 1/20  Iteration 23/3560 Training loss: 3.4204 1.0356 sec/batch\n",
      "Epoch 1/20  Iteration 24/3560 Training loss: 3.4093 0.9198 sec/batch\n",
      "Epoch 1/20  Iteration 25/3560 Training loss: 3.3986 0.9914 sec/batch\n",
      "Epoch 1/20  Iteration 26/3560 Training loss: 3.3893 0.9843 sec/batch\n",
      "Epoch 1/20  Iteration 27/3560 Training loss: 3.3807 0.9224 sec/batch\n",
      "Epoch 1/20  Iteration 28/3560 Training loss: 3.3716 0.8974 sec/batch\n",
      "Epoch 1/20  Iteration 29/3560 Training loss: 3.3633 0.9395 sec/batch\n",
      "Epoch 1/20  Iteration 30/3560 Training loss: 3.3556 1.0520 sec/batch\n",
      "Epoch 1/20  Iteration 31/3560 Training loss: 3.3491 1.0032 sec/batch\n",
      "Epoch 1/20  Iteration 32/3560 Training loss: 3.3420 0.9644 sec/batch\n",
      "Epoch 1/20  Iteration 33/3560 Training loss: 3.3350 1.0074 sec/batch\n",
      "Epoch 1/20  Iteration 34/3560 Training loss: 3.3289 0.9683 sec/batch\n",
      "Epoch 1/20  Iteration 35/3560 Training loss: 3.3223 0.9533 sec/batch\n",
      "Epoch 1/20  Iteration 36/3560 Training loss: 3.3168 0.9279 sec/batch\n",
      "Epoch 1/20  Iteration 37/3560 Training loss: 3.3106 1.0171 sec/batch\n",
      "Epoch 1/20  Iteration 38/3560 Training loss: 3.3047 0.9895 sec/batch\n",
      "Epoch 1/20  Iteration 39/3560 Training loss: 3.2990 0.9475 sec/batch\n",
      "Epoch 1/20  Iteration 40/3560 Training loss: 3.2938 0.9631 sec/batch\n",
      "Epoch 1/20  Iteration 41/3560 Training loss: 3.2883 1.0163 sec/batch\n",
      "Epoch 1/20  Iteration 42/3560 Training loss: 3.2831 0.9962 sec/batch\n",
      "Epoch 1/20  Iteration 43/3560 Training loss: 3.2780 0.9665 sec/batch\n",
      "Epoch 1/20  Iteration 44/3560 Training loss: 3.2732 0.9323 sec/batch\n",
      "Epoch 1/20  Iteration 45/3560 Training loss: 3.2683 0.9907 sec/batch\n",
      "Epoch 1/20  Iteration 46/3560 Training loss: 3.2639 0.9932 sec/batch\n",
      "Epoch 1/20  Iteration 47/3560 Training loss: 3.2597 0.9068 sec/batch\n",
      "Epoch 1/20  Iteration 48/3560 Training loss: 3.2557 0.9164 sec/batch\n",
      "Epoch 1/20  Iteration 49/3560 Training loss: 3.2518 1.0464 sec/batch\n",
      "Epoch 1/20  Iteration 50/3560 Training loss: 3.2480 0.9614 sec/batch\n",
      "Epoch 1/20  Iteration 51/3560 Training loss: 3.2440 0.9416 sec/batch\n",
      "Epoch 1/20  Iteration 52/3560 Training loss: 3.2398 0.9715 sec/batch\n",
      "Epoch 1/20  Iteration 53/3560 Training loss: 3.2360 1.0147 sec/batch\n",
      "Epoch 1/20  Iteration 54/3560 Training loss: 3.2319 1.0244 sec/batch\n",
      "Epoch 1/20  Iteration 55/3560 Training loss: 3.2281 0.9585 sec/batch\n",
      "Epoch 1/20  Iteration 56/3560 Training loss: 3.2241 0.9111 sec/batch\n",
      "Epoch 1/20  Iteration 57/3560 Training loss: 3.2202 1.0253 sec/batch\n",
      "Epoch 1/20  Iteration 58/3560 Training loss: 3.2165 1.0056 sec/batch\n",
      "Epoch 1/20  Iteration 59/3560 Training loss: 3.2126 0.9145 sec/batch\n",
      "Epoch 1/20  Iteration 60/3560 Training loss: 3.2089 0.9923 sec/batch\n",
      "Epoch 1/20  Iteration 61/3560 Training loss: 3.2052 1.1973 sec/batch\n",
      "Epoch 1/20  Iteration 62/3560 Training loss: 3.2018 0.9822 sec/batch\n",
      "Epoch 1/20  Iteration 63/3560 Training loss: 3.1986 0.9140 sec/batch\n",
      "Epoch 1/20  Iteration 64/3560 Training loss: 3.1946 0.9309 sec/batch\n",
      "Epoch 1/20  Iteration 65/3560 Training loss: 3.1908 1.0789 sec/batch\n",
      "Epoch 1/20  Iteration 66/3560 Training loss: 3.1872 0.9502 sec/batch\n",
      "Epoch 1/20  Iteration 67/3560 Training loss: 3.1835 0.9646 sec/batch\n",
      "Epoch 1/20  Iteration 68/3560 Training loss: 3.1791 0.9942 sec/batch\n",
      "Epoch 1/20  Iteration 69/3560 Training loss: 3.1751 0.9320 sec/batch\n",
      "Epoch 1/20  Iteration 70/3560 Training loss: 3.1713 0.9586 sec/batch\n",
      "Epoch 1/20  Iteration 71/3560 Training loss: 3.1675 0.9293 sec/batch\n",
      "Epoch 1/20  Iteration 72/3560 Training loss: 3.1638 0.9709 sec/batch\n",
      "Epoch 1/20  Iteration 73/3560 Training loss: 3.1599 1.1234 sec/batch\n",
      "Epoch 1/20  Iteration 74/3560 Training loss: 3.1559 1.0812 sec/batch\n",
      "Epoch 1/20  Iteration 75/3560 Training loss: 3.1521 0.9657 sec/batch\n",
      "Epoch 1/20  Iteration 76/3560 Training loss: 3.1483 0.9236 sec/batch\n",
      "Epoch 1/20  Iteration 77/3560 Training loss: 3.1443 1.0011 sec/batch\n",
      "Epoch 1/20  Iteration 78/3560 Training loss: 3.1404 1.0024 sec/batch\n",
      "Epoch 1/20  Iteration 79/3560 Training loss: 3.1363 0.9833 sec/batch\n",
      "Epoch 1/20  Iteration 80/3560 Training loss: 3.1321 0.9467 sec/batch\n",
      "Epoch 1/20  Iteration 81/3560 Training loss: 3.1279 1.0588 sec/batch\n",
      "Epoch 1/20  Iteration 82/3560 Training loss: 3.1239 1.0158 sec/batch\n",
      "Epoch 1/20  Iteration 83/3560 Training loss: 3.1198 0.9683 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 84/3560 Training loss: 3.1155 1.0273 sec/batch\n",
      "Epoch 1/20  Iteration 85/3560 Training loss: 3.1110 0.9745 sec/batch\n",
      "Epoch 1/20  Iteration 86/3560 Training loss: 3.1067 1.0238 sec/batch\n",
      "Epoch 1/20  Iteration 87/3560 Training loss: 3.1024 0.8719 sec/batch\n",
      "Epoch 1/20  Iteration 88/3560 Training loss: 3.0980 0.9495 sec/batch\n",
      "Epoch 1/20  Iteration 89/3560 Training loss: 3.0937 1.0203 sec/batch\n",
      "Epoch 1/20  Iteration 90/3560 Training loss: 3.0896 1.0889 sec/batch\n",
      "Epoch 1/20  Iteration 91/3560 Training loss: 3.0853 0.8628 sec/batch\n",
      "Epoch 1/20  Iteration 92/3560 Training loss: 3.0811 0.9393 sec/batch\n",
      "Epoch 1/20  Iteration 93/3560 Training loss: 3.0768 1.0183 sec/batch\n",
      "Epoch 1/20  Iteration 94/3560 Training loss: 3.0723 1.0241 sec/batch\n",
      "Epoch 1/20  Iteration 95/3560 Training loss: 3.0679 0.9572 sec/batch\n",
      "Epoch 1/20  Iteration 96/3560 Training loss: 3.0635 1.0410 sec/batch\n",
      "Epoch 1/20  Iteration 97/3560 Training loss: 3.0595 1.0526 sec/batch\n",
      "Epoch 1/20  Iteration 98/3560 Training loss: 3.0559 1.0031 sec/batch\n",
      "Epoch 1/20  Iteration 99/3560 Training loss: 3.0517 0.9968 sec/batch\n",
      "Epoch 1/20  Iteration 100/3560 Training loss: 3.0478 0.9306 sec/batch\n",
      "Epoch 1/20  Iteration 101/3560 Training loss: 3.0440 1.0192 sec/batch\n",
      "Epoch 1/20  Iteration 102/3560 Training loss: 3.0400 1.0214 sec/batch\n",
      "Epoch 1/20  Iteration 103/3560 Training loss: 3.0359 1.0430 sec/batch\n",
      "Epoch 1/20  Iteration 104/3560 Training loss: 3.0318 0.9236 sec/batch\n",
      "Epoch 1/20  Iteration 105/3560 Training loss: 3.0278 1.0124 sec/batch\n",
      "Epoch 1/20  Iteration 106/3560 Training loss: 3.0238 1.0238 sec/batch\n",
      "Epoch 1/20  Iteration 107/3560 Training loss: 3.0198 0.9418 sec/batch\n",
      "Epoch 1/20  Iteration 108/3560 Training loss: 3.0159 0.9819 sec/batch\n",
      "Epoch 1/20  Iteration 109/3560 Training loss: 3.0122 1.0279 sec/batch\n",
      "Epoch 1/20  Iteration 110/3560 Training loss: 3.0080 0.9550 sec/batch\n",
      "Epoch 1/20  Iteration 111/3560 Training loss: 3.0041 0.9319 sec/batch\n",
      "Epoch 1/20  Iteration 112/3560 Training loss: 3.0003 1.0864 sec/batch\n",
      "Epoch 1/20  Iteration 113/3560 Training loss: 2.9963 0.9469 sec/batch\n",
      "Epoch 1/20  Iteration 114/3560 Training loss: 2.9923 1.0661 sec/batch\n",
      "Epoch 1/20  Iteration 115/3560 Training loss: 2.9884 0.9662 sec/batch\n",
      "Epoch 1/20  Iteration 116/3560 Training loss: 2.9843 1.0684 sec/batch\n",
      "Epoch 1/20  Iteration 117/3560 Training loss: 2.9805 0.9544 sec/batch\n",
      "Epoch 1/20  Iteration 118/3560 Training loss: 2.9767 0.9818 sec/batch\n",
      "Epoch 1/20  Iteration 119/3560 Training loss: 2.9730 0.9352 sec/batch\n",
      "Epoch 1/20  Iteration 120/3560 Training loss: 2.9693 0.9928 sec/batch\n",
      "Epoch 1/20  Iteration 121/3560 Training loss: 2.9657 1.0744 sec/batch\n",
      "Epoch 1/20  Iteration 122/3560 Training loss: 2.9620 1.1847 sec/batch\n",
      "Epoch 1/20  Iteration 123/3560 Training loss: 2.9583 0.8747 sec/batch\n",
      "Epoch 1/20  Iteration 124/3560 Training loss: 2.9547 0.9900 sec/batch\n",
      "Epoch 1/20  Iteration 125/3560 Training loss: 2.9511 0.9876 sec/batch\n",
      "Epoch 1/20  Iteration 126/3560 Training loss: 2.9473 0.9729 sec/batch\n",
      "Epoch 1/20  Iteration 127/3560 Training loss: 2.9438 0.9695 sec/batch\n",
      "Epoch 1/20  Iteration 128/3560 Training loss: 2.9404 0.9863 sec/batch\n",
      "Epoch 1/20  Iteration 129/3560 Training loss: 2.9369 0.9689 sec/batch\n",
      "Epoch 1/20  Iteration 130/3560 Training loss: 2.9335 0.9657 sec/batch\n",
      "Epoch 1/20  Iteration 131/3560 Training loss: 2.9300 0.9602 sec/batch\n",
      "Epoch 1/20  Iteration 132/3560 Training loss: 2.9264 1.0519 sec/batch\n",
      "Epoch 1/20  Iteration 133/3560 Training loss: 2.9230 1.0023 sec/batch\n",
      "Epoch 1/20  Iteration 134/3560 Training loss: 2.9197 0.9151 sec/batch\n",
      "Epoch 1/20  Iteration 135/3560 Training loss: 2.9161 1.1414 sec/batch\n",
      "Epoch 1/20  Iteration 136/3560 Training loss: 2.9127 1.0750 sec/batch\n",
      "Epoch 1/20  Iteration 137/3560 Training loss: 2.9094 1.0836 sec/batch\n",
      "Epoch 1/20  Iteration 138/3560 Training loss: 2.9061 0.9664 sec/batch\n",
      "Epoch 1/20  Iteration 139/3560 Training loss: 2.9029 0.9404 sec/batch\n",
      "Epoch 1/20  Iteration 140/3560 Training loss: 2.8996 1.0949 sec/batch\n",
      "Epoch 1/20  Iteration 141/3560 Training loss: 2.8966 0.9509 sec/batch\n",
      "Epoch 1/20  Iteration 142/3560 Training loss: 2.8934 0.9322 sec/batch\n",
      "Epoch 1/20  Iteration 143/3560 Training loss: 2.8902 1.0003 sec/batch\n",
      "Epoch 1/20  Iteration 144/3560 Training loss: 2.8870 0.9682 sec/batch\n",
      "Epoch 1/20  Iteration 145/3560 Training loss: 2.8838 0.9948 sec/batch\n",
      "Epoch 1/20  Iteration 146/3560 Training loss: 2.8809 0.9283 sec/batch\n",
      "Epoch 1/20  Iteration 147/3560 Training loss: 2.8778 0.9266 sec/batch\n",
      "Epoch 1/20  Iteration 148/3560 Training loss: 2.8750 1.0642 sec/batch\n",
      "Epoch 1/20  Iteration 149/3560 Training loss: 2.8719 0.9807 sec/batch\n",
      "Epoch 1/20  Iteration 150/3560 Training loss: 2.8688 0.9797 sec/batch\n",
      "Epoch 1/20  Iteration 151/3560 Training loss: 2.8661 0.9822 sec/batch\n",
      "Epoch 1/20  Iteration 152/3560 Training loss: 2.8634 0.9539 sec/batch\n",
      "Epoch 1/20  Iteration 153/3560 Training loss: 2.8605 0.9843 sec/batch\n",
      "Epoch 1/20  Iteration 154/3560 Training loss: 2.8577 0.9339 sec/batch\n",
      "Epoch 1/20  Iteration 155/3560 Training loss: 2.8548 0.8389 sec/batch\n",
      "Epoch 1/20  Iteration 156/3560 Training loss: 2.8519 0.9951 sec/batch\n",
      "Epoch 1/20  Iteration 157/3560 Training loss: 2.8490 1.0412 sec/batch\n",
      "Epoch 1/20  Iteration 158/3560 Training loss: 2.8462 0.9955 sec/batch\n",
      "Epoch 1/20  Iteration 159/3560 Training loss: 2.8432 0.9589 sec/batch\n",
      "Epoch 1/20  Iteration 160/3560 Training loss: 2.8406 0.9889 sec/batch\n",
      "Epoch 1/20  Iteration 161/3560 Training loss: 2.8378 1.0555 sec/batch\n",
      "Epoch 1/20  Iteration 162/3560 Training loss: 2.8349 0.9442 sec/batch\n",
      "Epoch 1/20  Iteration 163/3560 Training loss: 2.8321 1.1064 sec/batch\n",
      "Epoch 1/20  Iteration 164/3560 Training loss: 2.8293 0.9864 sec/batch\n",
      "Epoch 1/20  Iteration 165/3560 Training loss: 2.8266 0.9193 sec/batch\n",
      "Epoch 1/20  Iteration 166/3560 Training loss: 2.8239 0.8955 sec/batch\n",
      "Epoch 1/20  Iteration 167/3560 Training loss: 2.8213 0.8828 sec/batch\n",
      "Epoch 1/20  Iteration 168/3560 Training loss: 2.8187 1.0213 sec/batch\n",
      "Epoch 1/20  Iteration 169/3560 Training loss: 2.8161 0.9958 sec/batch\n",
      "Epoch 1/20  Iteration 170/3560 Training loss: 2.8134 0.9321 sec/batch\n",
      "Epoch 1/20  Iteration 171/3560 Training loss: 2.8109 0.9668 sec/batch\n",
      "Epoch 1/20  Iteration 172/3560 Training loss: 2.8084 1.0223 sec/batch\n",
      "Epoch 1/20  Iteration 173/3560 Training loss: 2.8061 0.9492 sec/batch\n",
      "Epoch 1/20  Iteration 174/3560 Training loss: 2.8038 0.9369 sec/batch\n",
      "Epoch 1/20  Iteration 175/3560 Training loss: 2.8015 0.9991 sec/batch\n",
      "Epoch 1/20  Iteration 176/3560 Training loss: 2.7990 1.0991 sec/batch\n",
      "Epoch 1/20  Iteration 177/3560 Training loss: 2.7964 0.8904 sec/batch\n",
      "Epoch 1/20  Iteration 178/3560 Training loss: 2.7938 0.8619 sec/batch\n",
      "Epoch 2/20  Iteration 179/3560 Training loss: 2.4030 1.0046 sec/batch\n",
      "Epoch 2/20  Iteration 180/3560 Training loss: 2.3611 0.9428 sec/batch\n",
      "Epoch 2/20  Iteration 181/3560 Training loss: 2.3488 0.9794 sec/batch\n",
      "Epoch 2/20  Iteration 182/3560 Training loss: 2.3437 0.9696 sec/batch\n",
      "Epoch 2/20  Iteration 183/3560 Training loss: 2.3417 0.9823 sec/batch\n",
      "Epoch 2/20  Iteration 184/3560 Training loss: 2.3391 1.0156 sec/batch\n",
      "Epoch 2/20  Iteration 185/3560 Training loss: 2.3386 1.0537 sec/batch\n",
      "Epoch 2/20  Iteration 186/3560 Training loss: 2.3397 0.9607 sec/batch\n",
      "Epoch 2/20  Iteration 187/3560 Training loss: 2.3411 0.9639 sec/batch\n",
      "Epoch 2/20  Iteration 188/3560 Training loss: 2.3408 1.0117 sec/batch\n",
      "Epoch 2/20  Iteration 189/3560 Training loss: 2.3386 0.9754 sec/batch\n",
      "Epoch 2/20  Iteration 190/3560 Training loss: 2.3375 0.9047 sec/batch\n",
      "Epoch 2/20  Iteration 191/3560 Training loss: 2.3370 0.9994 sec/batch\n",
      "Epoch 2/20  Iteration 192/3560 Training loss: 2.3388 0.9940 sec/batch\n",
      "Epoch 2/20  Iteration 193/3560 Training loss: 2.3382 0.9406 sec/batch\n",
      "Epoch 2/20  Iteration 194/3560 Training loss: 2.3372 0.9579 sec/batch\n",
      "Epoch 2/20  Iteration 195/3560 Training loss: 2.3367 0.9185 sec/batch\n",
      "Epoch 2/20  Iteration 196/3560 Training loss: 2.3375 1.0159 sec/batch\n",
      "Epoch 2/20  Iteration 197/3560 Training loss: 2.3368 0.9257 sec/batch\n",
      "Epoch 2/20  Iteration 198/3560 Training loss: 2.3347 0.9813 sec/batch\n",
      "Epoch 2/20  Iteration 199/3560 Training loss: 2.3335 0.9605 sec/batch\n",
      "Epoch 2/20  Iteration 200/3560 Training loss: 2.3341 0.9992 sec/batch\n",
      "Epoch 2/20  Iteration 201/3560 Training loss: 2.3325 0.9055 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 202/3560 Training loss: 2.3313 1.0027 sec/batch\n",
      "Epoch 2/20  Iteration 203/3560 Training loss: 2.3302 0.8791 sec/batch\n",
      "Epoch 2/20  Iteration 204/3560 Training loss: 2.3289 1.1876 sec/batch\n",
      "Epoch 2/20  Iteration 205/3560 Training loss: 2.3275 0.9970 sec/batch\n",
      "Epoch 2/20  Iteration 206/3560 Training loss: 2.3267 0.9031 sec/batch\n",
      "Epoch 2/20  Iteration 207/3560 Training loss: 2.3263 1.0276 sec/batch\n",
      "Epoch 2/20  Iteration 208/3560 Training loss: 2.3257 1.0048 sec/batch\n",
      "Epoch 2/20  Iteration 209/3560 Training loss: 2.3253 0.8781 sec/batch\n",
      "Epoch 2/20  Iteration 210/3560 Training loss: 2.3240 0.9502 sec/batch\n",
      "Epoch 2/20  Iteration 211/3560 Training loss: 2.3229 1.0382 sec/batch\n",
      "Epoch 2/20  Iteration 212/3560 Training loss: 2.3223 0.9825 sec/batch\n",
      "Epoch 2/20  Iteration 213/3560 Training loss: 2.3212 0.9037 sec/batch\n",
      "Epoch 2/20  Iteration 214/3560 Training loss: 2.3204 0.9903 sec/batch\n",
      "Epoch 2/20  Iteration 215/3560 Training loss: 2.3194 0.9934 sec/batch\n",
      "Epoch 2/20  Iteration 216/3560 Training loss: 2.3176 0.9764 sec/batch\n",
      "Epoch 2/20  Iteration 217/3560 Training loss: 2.3163 0.9751 sec/batch\n",
      "Epoch 2/20  Iteration 218/3560 Training loss: 2.3154 0.9453 sec/batch\n",
      "Epoch 2/20  Iteration 219/3560 Training loss: 2.3142 1.0541 sec/batch\n",
      "Epoch 2/20  Iteration 220/3560 Training loss: 2.3132 0.9678 sec/batch\n",
      "Epoch 2/20  Iteration 221/3560 Training loss: 2.3118 0.9188 sec/batch\n",
      "Epoch 2/20  Iteration 222/3560 Training loss: 2.3104 1.0235 sec/batch\n",
      "Epoch 2/20  Iteration 223/3560 Training loss: 2.3095 0.9627 sec/batch\n",
      "Epoch 2/20  Iteration 224/3560 Training loss: 2.3076 1.0374 sec/batch\n",
      "Epoch 2/20  Iteration 225/3560 Training loss: 2.3070 1.0003 sec/batch\n",
      "Epoch 2/20  Iteration 226/3560 Training loss: 2.3058 0.9349 sec/batch\n",
      "Epoch 2/20  Iteration 227/3560 Training loss: 2.3047 0.9751 sec/batch\n",
      "Epoch 2/20  Iteration 228/3560 Training loss: 2.3044 0.9931 sec/batch\n",
      "Epoch 2/20  Iteration 229/3560 Training loss: 2.3033 0.9321 sec/batch\n",
      "Epoch 2/20  Iteration 230/3560 Training loss: 2.3030 0.9887 sec/batch\n",
      "Epoch 2/20  Iteration 231/3560 Training loss: 2.3017 1.0812 sec/batch\n",
      "Epoch 2/20  Iteration 232/3560 Training loss: 2.3006 0.9174 sec/batch\n",
      "Epoch 2/20  Iteration 233/3560 Training loss: 2.2996 0.9222 sec/batch\n",
      "Epoch 2/20  Iteration 234/3560 Training loss: 2.2989 0.9524 sec/batch\n",
      "Epoch 2/20  Iteration 235/3560 Training loss: 2.2981 0.9595 sec/batch\n",
      "Epoch 2/20  Iteration 236/3560 Training loss: 2.2968 1.0081 sec/batch\n",
      "Epoch 2/20  Iteration 237/3560 Training loss: 2.2957 0.8812 sec/batch\n",
      "Epoch 2/20  Iteration 238/3560 Training loss: 2.2952 1.0074 sec/batch\n",
      "Epoch 2/20  Iteration 239/3560 Training loss: 2.2942 1.0140 sec/batch\n",
      "Epoch 2/20  Iteration 240/3560 Training loss: 2.2936 0.9716 sec/batch\n",
      "Epoch 2/20  Iteration 241/3560 Training loss: 2.2931 0.9823 sec/batch\n",
      "Epoch 2/20  Iteration 242/3560 Training loss: 2.2924 1.0212 sec/batch\n",
      "Epoch 2/20  Iteration 243/3560 Training loss: 2.2916 1.0959 sec/batch\n",
      "Epoch 2/20  Iteration 244/3560 Training loss: 2.2911 0.9256 sec/batch\n",
      "Epoch 2/20  Iteration 245/3560 Training loss: 2.2903 0.9338 sec/batch\n",
      "Epoch 2/20  Iteration 246/3560 Training loss: 2.2891 1.0144 sec/batch\n",
      "Epoch 2/20  Iteration 247/3560 Training loss: 2.2881 1.0759 sec/batch\n",
      "Epoch 2/20  Iteration 248/3560 Training loss: 2.2873 0.9285 sec/batch\n",
      "Epoch 2/20  Iteration 249/3560 Training loss: 2.2866 0.9505 sec/batch\n",
      "Epoch 2/20  Iteration 250/3560 Training loss: 2.2859 1.0001 sec/batch\n",
      "Epoch 2/20  Iteration 251/3560 Training loss: 2.2853 1.0455 sec/batch\n",
      "Epoch 2/20  Iteration 252/3560 Training loss: 2.2843 0.9556 sec/batch\n",
      "Epoch 2/20  Iteration 253/3560 Training loss: 2.2835 0.9795 sec/batch\n",
      "Epoch 2/20  Iteration 254/3560 Training loss: 2.2831 0.9468 sec/batch\n",
      "Epoch 2/20  Iteration 255/3560 Training loss: 2.2822 1.1167 sec/batch\n",
      "Epoch 2/20  Iteration 256/3560 Training loss: 2.2816 0.9238 sec/batch\n",
      "Epoch 2/20  Iteration 257/3560 Training loss: 2.2804 0.9961 sec/batch\n",
      "Epoch 2/20  Iteration 258/3560 Training loss: 2.2796 0.9231 sec/batch\n",
      "Epoch 2/20  Iteration 259/3560 Training loss: 2.2784 1.0016 sec/batch\n",
      "Epoch 2/20  Iteration 260/3560 Training loss: 2.2778 0.9351 sec/batch\n",
      "Epoch 2/20  Iteration 261/3560 Training loss: 2.2767 0.8769 sec/batch\n",
      "Epoch 2/20  Iteration 262/3560 Training loss: 2.2756 1.0585 sec/batch\n",
      "Epoch 2/20  Iteration 263/3560 Training loss: 2.2742 0.9283 sec/batch\n",
      "Epoch 2/20  Iteration 264/3560 Training loss: 2.2732 1.0272 sec/batch\n",
      "Epoch 2/20  Iteration 265/3560 Training loss: 2.2724 0.9516 sec/batch\n",
      "Epoch 2/20  Iteration 266/3560 Training loss: 2.2715 1.0638 sec/batch\n",
      "Epoch 2/20  Iteration 267/3560 Training loss: 2.2704 0.9302 sec/batch\n",
      "Epoch 2/20  Iteration 268/3560 Training loss: 2.2697 0.9347 sec/batch\n",
      "Epoch 2/20  Iteration 269/3560 Training loss: 2.2687 0.9747 sec/batch\n",
      "Epoch 2/20  Iteration 270/3560 Training loss: 2.2678 1.1013 sec/batch\n",
      "Epoch 2/20  Iteration 271/3560 Training loss: 2.2667 0.9052 sec/batch\n",
      "Epoch 2/20  Iteration 272/3560 Training loss: 2.2657 0.9666 sec/batch\n",
      "Epoch 2/20  Iteration 273/3560 Training loss: 2.2646 0.9474 sec/batch\n",
      "Epoch 2/20  Iteration 274/3560 Training loss: 2.2637 0.9828 sec/batch\n",
      "Epoch 2/20  Iteration 275/3560 Training loss: 2.2629 0.9744 sec/batch\n",
      "Epoch 2/20  Iteration 276/3560 Training loss: 2.2620 1.0052 sec/batch\n",
      "Epoch 2/20  Iteration 277/3560 Training loss: 2.2610 1.0228 sec/batch\n",
      "Epoch 2/20  Iteration 278/3560 Training loss: 2.2600 0.9986 sec/batch\n",
      "Epoch 2/20  Iteration 279/3560 Training loss: 2.2594 0.8866 sec/batch\n",
      "Epoch 2/20  Iteration 280/3560 Training loss: 2.2587 0.9155 sec/batch\n",
      "Epoch 2/20  Iteration 281/3560 Training loss: 2.2576 0.9286 sec/batch\n",
      "Epoch 2/20  Iteration 282/3560 Training loss: 2.2568 0.9743 sec/batch\n",
      "Epoch 2/20  Iteration 283/3560 Training loss: 2.2558 0.9528 sec/batch\n",
      "Epoch 2/20  Iteration 284/3560 Training loss: 2.2551 0.9118 sec/batch\n",
      "Epoch 2/20  Iteration 285/3560 Training loss: 2.2543 1.0309 sec/batch\n",
      "Epoch 2/20  Iteration 286/3560 Training loss: 2.2537 0.9489 sec/batch\n",
      "Epoch 2/20  Iteration 287/3560 Training loss: 2.2531 1.0176 sec/batch\n",
      "Epoch 2/20  Iteration 288/3560 Training loss: 2.2522 0.8898 sec/batch\n",
      "Epoch 2/20  Iteration 289/3560 Training loss: 2.2514 1.0291 sec/batch\n",
      "Epoch 2/20  Iteration 290/3560 Training loss: 2.2508 1.0704 sec/batch\n",
      "Epoch 2/20  Iteration 291/3560 Training loss: 2.2500 0.9688 sec/batch\n",
      "Epoch 2/20  Iteration 292/3560 Training loss: 2.2492 0.9454 sec/batch\n",
      "Epoch 2/20  Iteration 293/3560 Training loss: 2.2485 1.0214 sec/batch\n",
      "Epoch 2/20  Iteration 294/3560 Training loss: 2.2473 1.0078 sec/batch\n",
      "Epoch 2/20  Iteration 295/3560 Training loss: 2.2466 0.9203 sec/batch\n",
      "Epoch 2/20  Iteration 296/3560 Training loss: 2.2458 0.9898 sec/batch\n",
      "Epoch 2/20  Iteration 297/3560 Training loss: 2.2452 0.9921 sec/batch\n",
      "Epoch 2/20  Iteration 298/3560 Training loss: 2.2445 0.9790 sec/batch\n",
      "Epoch 2/20  Iteration 299/3560 Training loss: 2.2440 0.8998 sec/batch\n",
      "Epoch 2/20  Iteration 300/3560 Training loss: 2.2432 0.9971 sec/batch\n",
      "Epoch 2/20  Iteration 301/3560 Training loss: 2.2424 1.0787 sec/batch\n",
      "Epoch 2/20  Iteration 302/3560 Training loss: 2.2418 0.9558 sec/batch\n",
      "Epoch 2/20  Iteration 303/3560 Training loss: 2.2412 0.9855 sec/batch\n",
      "Epoch 2/20  Iteration 304/3560 Training loss: 2.2404 1.0082 sec/batch\n",
      "Epoch 2/20  Iteration 305/3560 Training loss: 2.2397 0.8734 sec/batch\n",
      "Epoch 2/20  Iteration 306/3560 Training loss: 2.2391 1.0573 sec/batch\n",
      "Epoch 2/20  Iteration 307/3560 Training loss: 2.2384 0.8669 sec/batch\n",
      "Epoch 2/20  Iteration 308/3560 Training loss: 2.2377 1.0095 sec/batch\n",
      "Epoch 2/20  Iteration 309/3560 Training loss: 2.2370 0.9771 sec/batch\n",
      "Epoch 2/20  Iteration 310/3560 Training loss: 2.2361 0.9484 sec/batch\n",
      "Epoch 2/20  Iteration 311/3560 Training loss: 2.2355 1.0253 sec/batch\n",
      "Epoch 2/20  Iteration 312/3560 Training loss: 2.2349 0.9294 sec/batch\n",
      "Epoch 2/20  Iteration 313/3560 Training loss: 2.2342 0.9631 sec/batch\n",
      "Epoch 2/20  Iteration 314/3560 Training loss: 2.2337 1.0069 sec/batch\n",
      "Epoch 2/20  Iteration 315/3560 Training loss: 2.2330 0.9790 sec/batch\n",
      "Epoch 2/20  Iteration 316/3560 Training loss: 2.2324 0.9570 sec/batch\n",
      "Epoch 2/20  Iteration 317/3560 Training loss: 2.2320 1.1038 sec/batch\n",
      "Epoch 2/20  Iteration 318/3560 Training loss: 2.2312 0.8711 sec/batch\n",
      "Epoch 2/20  Iteration 319/3560 Training loss: 2.2308 0.9716 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 320/3560 Training loss: 2.2302 0.8798 sec/batch\n",
      "Epoch 2/20  Iteration 321/3560 Training loss: 2.2295 0.9789 sec/batch\n",
      "Epoch 2/20  Iteration 322/3560 Training loss: 2.2289 1.0199 sec/batch\n",
      "Epoch 2/20  Iteration 323/3560 Training loss: 2.2282 0.9262 sec/batch\n",
      "Epoch 2/20  Iteration 324/3560 Training loss: 2.2277 0.9517 sec/batch\n",
      "Epoch 2/20  Iteration 325/3560 Training loss: 2.2271 1.0147 sec/batch\n",
      "Epoch 2/20  Iteration 326/3560 Training loss: 2.2266 0.9307 sec/batch\n",
      "Epoch 2/20  Iteration 327/3560 Training loss: 2.2260 0.9158 sec/batch\n",
      "Epoch 2/20  Iteration 328/3560 Training loss: 2.2252 0.9933 sec/batch\n",
      "Epoch 2/20  Iteration 329/3560 Training loss: 2.2245 0.9734 sec/batch\n",
      "Epoch 2/20  Iteration 330/3560 Training loss: 2.2241 1.0383 sec/batch\n",
      "Epoch 2/20  Iteration 331/3560 Training loss: 2.2235 0.9536 sec/batch\n",
      "Epoch 2/20  Iteration 332/3560 Training loss: 2.2229 1.0155 sec/batch\n",
      "Epoch 2/20  Iteration 333/3560 Training loss: 2.2222 0.9851 sec/batch\n",
      "Epoch 2/20  Iteration 334/3560 Training loss: 2.2215 0.9213 sec/batch\n",
      "Epoch 2/20  Iteration 335/3560 Training loss: 2.2209 0.9284 sec/batch\n",
      "Epoch 2/20  Iteration 336/3560 Training loss: 2.2202 0.9665 sec/batch\n",
      "Epoch 2/20  Iteration 337/3560 Training loss: 2.2194 1.0622 sec/batch\n",
      "Epoch 2/20  Iteration 338/3560 Training loss: 2.2190 0.9457 sec/batch\n",
      "Epoch 2/20  Iteration 339/3560 Training loss: 2.2185 0.9569 sec/batch\n",
      "Epoch 2/20  Iteration 340/3560 Training loss: 2.2178 0.9487 sec/batch\n",
      "Epoch 2/20  Iteration 341/3560 Training loss: 2.2172 0.9814 sec/batch\n",
      "Epoch 2/20  Iteration 342/3560 Training loss: 2.2165 0.9327 sec/batch\n",
      "Epoch 2/20  Iteration 343/3560 Training loss: 2.2160 0.9485 sec/batch\n",
      "Epoch 2/20  Iteration 344/3560 Training loss: 2.2153 1.0107 sec/batch\n",
      "Epoch 2/20  Iteration 345/3560 Training loss: 2.2148 0.9907 sec/batch\n",
      "Epoch 2/20  Iteration 346/3560 Training loss: 2.2143 0.9697 sec/batch\n",
      "Epoch 2/20  Iteration 347/3560 Training loss: 2.2137 0.9509 sec/batch\n",
      "Epoch 2/20  Iteration 348/3560 Training loss: 2.2131 0.9930 sec/batch\n",
      "Epoch 2/20  Iteration 349/3560 Training loss: 2.2124 1.0247 sec/batch\n",
      "Epoch 2/20  Iteration 350/3560 Training loss: 2.2118 0.8415 sec/batch\n",
      "Epoch 2/20  Iteration 351/3560 Training loss: 2.2113 0.9523 sec/batch\n",
      "Epoch 2/20  Iteration 352/3560 Training loss: 2.2109 1.0028 sec/batch\n",
      "Epoch 2/20  Iteration 353/3560 Training loss: 2.2106 1.1418 sec/batch\n",
      "Epoch 2/20  Iteration 354/3560 Training loss: 2.2106 0.9121 sec/batch\n",
      "Epoch 2/20  Iteration 355/3560 Training loss: 2.2104 1.1005 sec/batch\n",
      "Epoch 2/20  Iteration 356/3560 Training loss: 2.2103 0.9678 sec/batch\n",
      "Epoch 3/20  Iteration 357/3560 Training loss: 2.1648 0.9527 sec/batch\n",
      "Epoch 3/20  Iteration 358/3560 Training loss: 2.1152 1.1051 sec/batch\n",
      "Epoch 3/20  Iteration 359/3560 Training loss: 2.1037 0.9110 sec/batch\n",
      "Epoch 3/20  Iteration 360/3560 Training loss: 2.0989 0.9974 sec/batch\n",
      "Epoch 3/20  Iteration 361/3560 Training loss: 2.0954 0.9851 sec/batch\n",
      "Epoch 3/20  Iteration 362/3560 Training loss: 2.0902 0.9401 sec/batch\n",
      "Epoch 3/20  Iteration 363/3560 Training loss: 2.0915 0.9583 sec/batch\n",
      "Epoch 3/20  Iteration 364/3560 Training loss: 2.0926 0.9876 sec/batch\n",
      "Epoch 3/20  Iteration 365/3560 Training loss: 2.0940 0.9165 sec/batch\n",
      "Epoch 3/20  Iteration 366/3560 Training loss: 2.0941 0.9640 sec/batch\n",
      "Epoch 3/20  Iteration 367/3560 Training loss: 2.0916 0.9045 sec/batch\n",
      "Epoch 3/20  Iteration 368/3560 Training loss: 2.0887 1.0363 sec/batch\n",
      "Epoch 3/20  Iteration 369/3560 Training loss: 2.0889 0.8805 sec/batch\n",
      "Epoch 3/20  Iteration 370/3560 Training loss: 2.0905 1.0311 sec/batch\n",
      "Epoch 3/20  Iteration 371/3560 Training loss: 2.0893 0.9888 sec/batch\n",
      "Epoch 3/20  Iteration 372/3560 Training loss: 2.0872 1.0229 sec/batch\n",
      "Epoch 3/20  Iteration 373/3560 Training loss: 2.0860 0.9291 sec/batch\n",
      "Epoch 3/20  Iteration 374/3560 Training loss: 2.0877 0.9002 sec/batch\n",
      "Epoch 3/20  Iteration 375/3560 Training loss: 2.0873 0.9464 sec/batch\n",
      "Epoch 3/20  Iteration 376/3560 Training loss: 2.0867 1.0725 sec/batch\n",
      "Epoch 3/20  Iteration 377/3560 Training loss: 2.0853 0.9933 sec/batch\n",
      "Epoch 3/20  Iteration 378/3560 Training loss: 2.0860 0.9982 sec/batch\n",
      "Epoch 3/20  Iteration 379/3560 Training loss: 2.0852 1.0177 sec/batch\n",
      "Epoch 3/20  Iteration 380/3560 Training loss: 2.0837 1.0388 sec/batch\n",
      "Epoch 3/20  Iteration 381/3560 Training loss: 2.0828 0.9459 sec/batch\n",
      "Epoch 3/20  Iteration 382/3560 Training loss: 2.0815 0.9446 sec/batch\n",
      "Epoch 3/20  Iteration 383/3560 Training loss: 2.0804 0.9844 sec/batch\n",
      "Epoch 3/20  Iteration 384/3560 Training loss: 2.0800 0.9789 sec/batch\n",
      "Epoch 3/20  Iteration 385/3560 Training loss: 2.0805 0.9858 sec/batch\n",
      "Epoch 3/20  Iteration 386/3560 Training loss: 2.0800 0.9025 sec/batch\n",
      "Epoch 3/20  Iteration 387/3560 Training loss: 2.0796 1.0262 sec/batch\n",
      "Epoch 3/20  Iteration 388/3560 Training loss: 2.0785 0.9911 sec/batch\n",
      "Epoch 3/20  Iteration 389/3560 Training loss: 2.0777 1.0490 sec/batch\n",
      "Epoch 3/20  Iteration 390/3560 Training loss: 2.0780 0.9994 sec/batch\n",
      "Epoch 3/20  Iteration 391/3560 Training loss: 2.0772 0.9616 sec/batch\n",
      "Epoch 3/20  Iteration 392/3560 Training loss: 2.0766 1.0573 sec/batch\n",
      "Epoch 3/20  Iteration 393/3560 Training loss: 2.0758 0.9940 sec/batch\n",
      "Epoch 3/20  Iteration 394/3560 Training loss: 2.0743 1.0233 sec/batch\n",
      "Epoch 3/20  Iteration 395/3560 Training loss: 2.0727 1.0103 sec/batch\n",
      "Epoch 3/20  Iteration 396/3560 Training loss: 2.0714 1.0320 sec/batch\n",
      "Epoch 3/20  Iteration 397/3560 Training loss: 2.0706 1.1302 sec/batch\n",
      "Epoch 3/20  Iteration 398/3560 Training loss: 2.0704 0.9436 sec/batch\n",
      "Epoch 3/20  Iteration 399/3560 Training loss: 2.0694 0.9864 sec/batch\n",
      "Epoch 3/20  Iteration 400/3560 Training loss: 2.0681 1.0107 sec/batch\n",
      "Epoch 3/20  Iteration 401/3560 Training loss: 2.0676 1.0007 sec/batch\n",
      "Epoch 3/20  Iteration 402/3560 Training loss: 2.0659 0.9340 sec/batch\n",
      "Epoch 3/20  Iteration 403/3560 Training loss: 2.0655 0.8777 sec/batch\n",
      "Epoch 3/20  Iteration 404/3560 Training loss: 2.0644 0.9550 sec/batch\n",
      "Epoch 3/20  Iteration 405/3560 Training loss: 2.0638 0.9763 sec/batch\n",
      "Epoch 3/20  Iteration 406/3560 Training loss: 2.0640 0.9921 sec/batch\n",
      "Epoch 3/20  Iteration 407/3560 Training loss: 2.0630 0.9408 sec/batch\n",
      "Epoch 3/20  Iteration 408/3560 Training loss: 2.0631 1.0874 sec/batch\n",
      "Epoch 3/20  Iteration 409/3560 Training loss: 2.0622 1.0264 sec/batch\n",
      "Epoch 3/20  Iteration 410/3560 Training loss: 2.0615 1.0680 sec/batch\n",
      "Epoch 3/20  Iteration 411/3560 Training loss: 2.0607 1.0001 sec/batch\n",
      "Epoch 3/20  Iteration 412/3560 Training loss: 2.0604 0.9891 sec/batch\n",
      "Epoch 3/20  Iteration 413/3560 Training loss: 2.0602 0.9092 sec/batch\n",
      "Epoch 3/20  Iteration 414/3560 Training loss: 2.0597 0.8888 sec/batch\n",
      "Epoch 3/20  Iteration 415/3560 Training loss: 2.0588 0.9937 sec/batch\n",
      "Epoch 3/20  Iteration 416/3560 Training loss: 2.0590 1.0237 sec/batch\n",
      "Epoch 3/20  Iteration 417/3560 Training loss: 2.0582 0.9419 sec/batch\n",
      "Epoch 3/20  Iteration 418/3560 Training loss: 2.0583 0.9970 sec/batch\n",
      "Epoch 3/20  Iteration 419/3560 Training loss: 2.0583 1.0912 sec/batch\n",
      "Epoch 3/20  Iteration 420/3560 Training loss: 2.0580 0.9448 sec/batch\n",
      "Epoch 3/20  Iteration 421/3560 Training loss: 2.0574 0.9818 sec/batch\n",
      "Epoch 3/20  Iteration 422/3560 Training loss: 2.0573 0.9497 sec/batch\n",
      "Epoch 3/20  Iteration 423/3560 Training loss: 2.0570 1.0322 sec/batch\n",
      "Epoch 3/20  Iteration 424/3560 Training loss: 2.0560 1.0221 sec/batch\n",
      "Epoch 3/20  Iteration 425/3560 Training loss: 2.0553 0.9451 sec/batch\n",
      "Epoch 3/20  Iteration 426/3560 Training loss: 2.0549 0.9921 sec/batch\n",
      "Epoch 3/20  Iteration 427/3560 Training loss: 2.0548 1.1014 sec/batch\n",
      "Epoch 3/20  Iteration 428/3560 Training loss: 2.0544 0.9598 sec/batch\n",
      "Epoch 3/20  Iteration 429/3560 Training loss: 2.0542 0.9912 sec/batch\n",
      "Epoch 3/20  Iteration 430/3560 Training loss: 2.0534 0.9466 sec/batch\n",
      "Epoch 3/20  Iteration 431/3560 Training loss: 2.0529 1.0630 sec/batch\n",
      "Epoch 3/20  Iteration 432/3560 Training loss: 2.0529 0.9533 sec/batch\n",
      "Epoch 3/20  Iteration 433/3560 Training loss: 2.0524 0.9928 sec/batch\n",
      "Epoch 3/20  Iteration 434/3560 Training loss: 2.0521 0.9230 sec/batch\n",
      "Epoch 3/20  Iteration 435/3560 Training loss: 2.0512 0.9410 sec/batch\n",
      "Epoch 3/20  Iteration 436/3560 Training loss: 2.0507 0.8968 sec/batch\n",
      "Epoch 3/20  Iteration 437/3560 Training loss: 2.0498 0.9144 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 438/3560 Training loss: 2.0496 0.9067 sec/batch\n",
      "Epoch 3/20  Iteration 439/3560 Training loss: 2.0488 1.0789 sec/batch\n",
      "Epoch 3/20  Iteration 440/3560 Training loss: 2.0483 0.9794 sec/batch\n",
      "Epoch 3/20  Iteration 441/3560 Training loss: 2.0473 1.0174 sec/batch\n",
      "Epoch 3/20  Iteration 442/3560 Training loss: 2.0465 0.9006 sec/batch\n",
      "Epoch 3/20  Iteration 443/3560 Training loss: 2.0461 1.0606 sec/batch\n",
      "Epoch 3/20  Iteration 444/3560 Training loss: 2.0454 0.9298 sec/batch\n",
      "Epoch 3/20  Iteration 445/3560 Training loss: 2.0446 0.8580 sec/batch\n",
      "Epoch 3/20  Iteration 446/3560 Training loss: 2.0442 0.9496 sec/batch\n",
      "Epoch 3/20  Iteration 447/3560 Training loss: 2.0436 0.9932 sec/batch\n",
      "Epoch 3/20  Iteration 448/3560 Training loss: 2.0431 0.9938 sec/batch\n",
      "Epoch 3/20  Iteration 449/3560 Training loss: 2.0422 0.9331 sec/batch\n",
      "Epoch 3/20  Iteration 450/3560 Training loss: 2.0414 1.0428 sec/batch\n",
      "Epoch 3/20  Iteration 451/3560 Training loss: 2.0407 1.0147 sec/batch\n",
      "Epoch 3/20  Iteration 452/3560 Training loss: 2.0402 1.0054 sec/batch\n",
      "Epoch 3/20  Iteration 453/3560 Training loss: 2.0397 0.8549 sec/batch\n",
      "Epoch 3/20  Iteration 454/3560 Training loss: 2.0390 0.9491 sec/batch\n",
      "Epoch 3/20  Iteration 455/3560 Training loss: 2.0383 0.9988 sec/batch\n",
      "Epoch 3/20  Iteration 456/3560 Training loss: 2.0373 0.9251 sec/batch\n",
      "Epoch 3/20  Iteration 457/3560 Training loss: 2.0369 1.0094 sec/batch\n",
      "Epoch 3/20  Iteration 458/3560 Training loss: 2.0364 1.0679 sec/batch\n",
      "Epoch 3/20  Iteration 459/3560 Training loss: 2.0357 0.9659 sec/batch\n",
      "Epoch 3/20  Iteration 460/3560 Training loss: 2.0352 0.9266 sec/batch\n",
      "Epoch 3/20  Iteration 461/3560 Training loss: 2.0345 1.0286 sec/batch\n",
      "Epoch 3/20  Iteration 462/3560 Training loss: 2.0340 1.0371 sec/batch\n",
      "Epoch 3/20  Iteration 463/3560 Training loss: 2.0335 0.9890 sec/batch\n",
      "Epoch 3/20  Iteration 464/3560 Training loss: 2.0330 0.9457 sec/batch\n",
      "Epoch 3/20  Iteration 465/3560 Training loss: 2.0327 1.0375 sec/batch\n",
      "Epoch 3/20  Iteration 466/3560 Training loss: 2.0323 1.0222 sec/batch\n",
      "Epoch 3/20  Iteration 467/3560 Training loss: 2.0318 0.8963 sec/batch\n",
      "Epoch 3/20  Iteration 468/3560 Training loss: 2.0312 0.9590 sec/batch\n",
      "Epoch 3/20  Iteration 469/3560 Training loss: 2.0306 0.9360 sec/batch\n",
      "Epoch 3/20  Iteration 470/3560 Training loss: 2.0301 1.0784 sec/batch\n",
      "Epoch 3/20  Iteration 471/3560 Training loss: 2.0296 0.9415 sec/batch\n",
      "Epoch 3/20  Iteration 472/3560 Training loss: 2.0288 0.9655 sec/batch\n",
      "Epoch 3/20  Iteration 473/3560 Training loss: 2.0283 1.0024 sec/batch\n",
      "Epoch 3/20  Iteration 474/3560 Training loss: 2.0278 0.9789 sec/batch\n",
      "Epoch 3/20  Iteration 475/3560 Training loss: 2.0275 0.9548 sec/batch\n",
      "Epoch 3/20  Iteration 476/3560 Training loss: 2.0271 0.9274 sec/batch\n",
      "Epoch 3/20  Iteration 477/3560 Training loss: 2.0268 0.9261 sec/batch\n",
      "Epoch 3/20  Iteration 478/3560 Training loss: 2.0261 0.9654 sec/batch\n",
      "Epoch 3/20  Iteration 479/3560 Training loss: 2.0255 0.9629 sec/batch\n",
      "Epoch 3/20  Iteration 480/3560 Training loss: 2.0251 0.9625 sec/batch\n",
      "Epoch 3/20  Iteration 481/3560 Training loss: 2.0247 1.0883 sec/batch\n",
      "Epoch 3/20  Iteration 482/3560 Training loss: 2.0240 0.9566 sec/batch\n",
      "Epoch 3/20  Iteration 483/3560 Training loss: 2.0237 0.9567 sec/batch\n",
      "Epoch 3/20  Iteration 484/3560 Training loss: 2.0234 0.9739 sec/batch\n",
      "Epoch 3/20  Iteration 485/3560 Training loss: 2.0230 0.9907 sec/batch\n",
      "Epoch 3/20  Iteration 486/3560 Training loss: 2.0227 1.0211 sec/batch\n",
      "Epoch 3/20  Iteration 487/3560 Training loss: 2.0221 0.9964 sec/batch\n",
      "Epoch 3/20  Iteration 488/3560 Training loss: 2.0214 0.9467 sec/batch\n",
      "Epoch 3/20  Iteration 489/3560 Training loss: 2.0211 1.0675 sec/batch\n",
      "Epoch 3/20  Iteration 490/3560 Training loss: 2.0208 0.9610 sec/batch\n",
      "Epoch 3/20  Iteration 491/3560 Training loss: 2.0204 0.8987 sec/batch\n",
      "Epoch 3/20  Iteration 492/3560 Training loss: 2.0199 0.9886 sec/batch\n",
      "Epoch 3/20  Iteration 493/3560 Training loss: 2.0196 1.0274 sec/batch\n",
      "Epoch 3/20  Iteration 494/3560 Training loss: 2.0192 0.9382 sec/batch\n",
      "Epoch 3/20  Iteration 495/3560 Training loss: 2.0191 1.0483 sec/batch\n",
      "Epoch 3/20  Iteration 496/3560 Training loss: 2.0185 0.8905 sec/batch\n",
      "Epoch 3/20  Iteration 497/3560 Training loss: 2.0183 1.0423 sec/batch\n",
      "Epoch 3/20  Iteration 498/3560 Training loss: 2.0178 0.9641 sec/batch\n",
      "Epoch 3/20  Iteration 499/3560 Training loss: 2.0175 0.9856 sec/batch\n",
      "Epoch 3/20  Iteration 500/3560 Training loss: 2.0171 0.9322 sec/batch\n",
      "Epoch 3/20  Iteration 501/3560 Training loss: 2.0166 0.9896 sec/batch\n",
      "Epoch 3/20  Iteration 502/3560 Training loss: 2.0162 1.0909 sec/batch\n",
      "Epoch 3/20  Iteration 503/3560 Training loss: 2.0159 0.9827 sec/batch\n",
      "Epoch 3/20  Iteration 504/3560 Training loss: 2.0157 0.9274 sec/batch\n",
      "Epoch 3/20  Iteration 505/3560 Training loss: 2.0154 1.0128 sec/batch\n",
      "Epoch 3/20  Iteration 506/3560 Training loss: 2.0149 1.0004 sec/batch\n",
      "Epoch 3/20  Iteration 507/3560 Training loss: 2.0144 0.9155 sec/batch\n",
      "Epoch 3/20  Iteration 508/3560 Training loss: 2.0142 0.8723 sec/batch\n",
      "Epoch 3/20  Iteration 509/3560 Training loss: 2.0139 1.0253 sec/batch\n",
      "Epoch 3/20  Iteration 510/3560 Training loss: 2.0135 0.9961 sec/batch\n",
      "Epoch 3/20  Iteration 511/3560 Training loss: 2.0131 1.0886 sec/batch\n",
      "Epoch 3/20  Iteration 512/3560 Training loss: 2.0127 1.1702 sec/batch\n",
      "Epoch 3/20  Iteration 513/3560 Training loss: 2.0124 0.9769 sec/batch\n",
      "Epoch 3/20  Iteration 514/3560 Training loss: 2.0120 0.9382 sec/batch\n",
      "Epoch 3/20  Iteration 515/3560 Training loss: 2.0114 1.0016 sec/batch\n",
      "Epoch 3/20  Iteration 516/3560 Training loss: 2.0113 0.9374 sec/batch\n",
      "Epoch 3/20  Iteration 517/3560 Training loss: 2.0110 1.0225 sec/batch\n",
      "Epoch 3/20  Iteration 518/3560 Training loss: 2.0106 0.9705 sec/batch\n",
      "Epoch 3/20  Iteration 519/3560 Training loss: 2.0103 0.9648 sec/batch\n",
      "Epoch 3/20  Iteration 520/3560 Training loss: 2.0100 1.0031 sec/batch\n",
      "Epoch 3/20  Iteration 521/3560 Training loss: 2.0097 1.0471 sec/batch\n",
      "Epoch 3/20  Iteration 522/3560 Training loss: 2.0092 0.9841 sec/batch\n",
      "Epoch 3/20  Iteration 523/3560 Training loss: 2.0089 0.9169 sec/batch\n",
      "Epoch 3/20  Iteration 524/3560 Training loss: 2.0088 0.9887 sec/batch\n",
      "Epoch 3/20  Iteration 525/3560 Training loss: 2.0084 1.0720 sec/batch\n",
      "Epoch 3/20  Iteration 526/3560 Training loss: 2.0080 0.9338 sec/batch\n",
      "Epoch 3/20  Iteration 527/3560 Training loss: 2.0076 0.9907 sec/batch\n",
      "Epoch 3/20  Iteration 528/3560 Training loss: 2.0071 1.0098 sec/batch\n",
      "Epoch 3/20  Iteration 529/3560 Training loss: 2.0069 1.0210 sec/batch\n",
      "Epoch 3/20  Iteration 530/3560 Training loss: 2.0065 1.0260 sec/batch\n",
      "Epoch 3/20  Iteration 531/3560 Training loss: 2.0062 0.8979 sec/batch\n",
      "Epoch 3/20  Iteration 532/3560 Training loss: 2.0058 1.0223 sec/batch\n",
      "Epoch 3/20  Iteration 533/3560 Training loss: 2.0053 0.9999 sec/batch\n",
      "Epoch 3/20  Iteration 534/3560 Training loss: 2.0050 0.9736 sec/batch\n",
      "Epoch 4/20  Iteration 535/3560 Training loss: 2.0093 0.9468 sec/batch\n",
      "Epoch 4/20  Iteration 536/3560 Training loss: 1.9611 0.9972 sec/batch\n",
      "Epoch 4/20  Iteration 537/3560 Training loss: 1.9508 1.0254 sec/batch\n",
      "Epoch 4/20  Iteration 538/3560 Training loss: 1.9425 0.9290 sec/batch\n",
      "Epoch 4/20  Iteration 539/3560 Training loss: 1.9373 1.0019 sec/batch\n",
      "Epoch 4/20  Iteration 540/3560 Training loss: 1.9297 0.8835 sec/batch\n",
      "Epoch 4/20  Iteration 541/3560 Training loss: 1.9299 1.0309 sec/batch\n",
      "Epoch 4/20  Iteration 542/3560 Training loss: 1.9297 1.0983 sec/batch\n",
      "Epoch 4/20  Iteration 543/3560 Training loss: 1.9327 1.1517 sec/batch\n",
      "Epoch 4/20  Iteration 544/3560 Training loss: 1.9316 1.0813 sec/batch\n",
      "Epoch 4/20  Iteration 545/3560 Training loss: 1.9288 1.0445 sec/batch\n",
      "Epoch 4/20  Iteration 546/3560 Training loss: 1.9263 1.1063 sec/batch\n",
      "Epoch 4/20  Iteration 547/3560 Training loss: 1.9263 0.9876 sec/batch\n",
      "Epoch 4/20  Iteration 548/3560 Training loss: 1.9284 1.0934 sec/batch\n",
      "Epoch 4/20  Iteration 549/3560 Training loss: 1.9270 0.9985 sec/batch\n",
      "Epoch 4/20  Iteration 550/3560 Training loss: 1.9250 1.0944 sec/batch\n",
      "Epoch 4/20  Iteration 551/3560 Training loss: 1.9248 0.9166 sec/batch\n",
      "Epoch 4/20  Iteration 552/3560 Training loss: 1.9271 1.2986 sec/batch\n",
      "Epoch 4/20  Iteration 553/3560 Training loss: 1.9271 0.9545 sec/batch\n",
      "Epoch 4/20  Iteration 554/3560 Training loss: 1.9270 1.0557 sec/batch\n",
      "Epoch 4/20  Iteration 555/3560 Training loss: 1.9260 1.0386 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 556/3560 Training loss: 1.9267 0.9797 sec/batch\n",
      "Epoch 4/20  Iteration 557/3560 Training loss: 1.9263 0.9990 sec/batch\n",
      "Epoch 4/20  Iteration 558/3560 Training loss: 1.9253 1.0311 sec/batch\n",
      "Epoch 4/20  Iteration 559/3560 Training loss: 1.9243 1.0368 sec/batch\n",
      "Epoch 4/20  Iteration 560/3560 Training loss: 1.9233 0.9761 sec/batch\n",
      "Epoch 4/20  Iteration 561/3560 Training loss: 1.9222 1.0082 sec/batch\n",
      "Epoch 4/20  Iteration 562/3560 Training loss: 1.9225 0.9070 sec/batch\n",
      "Epoch 4/20  Iteration 563/3560 Training loss: 1.9235 1.0401 sec/batch\n",
      "Epoch 4/20  Iteration 564/3560 Training loss: 1.9235 1.0706 sec/batch\n",
      "Epoch 4/20  Iteration 565/3560 Training loss: 1.9231 0.9401 sec/batch\n",
      "Epoch 4/20  Iteration 566/3560 Training loss: 1.9220 0.9244 sec/batch\n",
      "Epoch 4/20  Iteration 567/3560 Training loss: 1.9218 0.9633 sec/batch\n",
      "Epoch 4/20  Iteration 568/3560 Training loss: 1.9222 1.0453 sec/batch\n",
      "Epoch 4/20  Iteration 569/3560 Training loss: 1.9216 0.9822 sec/batch\n",
      "Epoch 4/20  Iteration 570/3560 Training loss: 1.9210 0.9398 sec/batch\n",
      "Epoch 4/20  Iteration 571/3560 Training loss: 1.9202 0.9740 sec/batch\n",
      "Epoch 4/20  Iteration 572/3560 Training loss: 1.9187 1.0092 sec/batch\n",
      "Epoch 4/20  Iteration 573/3560 Training loss: 1.9176 1.0466 sec/batch\n",
      "Epoch 4/20  Iteration 574/3560 Training loss: 1.9165 0.9141 sec/batch\n",
      "Epoch 4/20  Iteration 575/3560 Training loss: 1.9156 0.9663 sec/batch\n",
      "Epoch 4/20  Iteration 576/3560 Training loss: 1.9156 1.0751 sec/batch\n",
      "Epoch 4/20  Iteration 577/3560 Training loss: 1.9147 0.8656 sec/batch\n",
      "Epoch 4/20  Iteration 578/3560 Training loss: 1.9136 0.9369 sec/batch\n",
      "Epoch 4/20  Iteration 579/3560 Training loss: 1.9135 0.9430 sec/batch\n",
      "Epoch 4/20  Iteration 580/3560 Training loss: 1.9119 1.0029 sec/batch\n",
      "Epoch 4/20  Iteration 581/3560 Training loss: 1.9113 0.9494 sec/batch\n",
      "Epoch 4/20  Iteration 582/3560 Training loss: 1.9104 0.9992 sec/batch\n",
      "Epoch 4/20  Iteration 583/3560 Training loss: 1.9101 1.0562 sec/batch\n",
      "Epoch 4/20  Iteration 584/3560 Training loss: 1.9105 0.9655 sec/batch\n",
      "Epoch 4/20  Iteration 585/3560 Training loss: 1.9096 0.9547 sec/batch\n",
      "Epoch 4/20  Iteration 586/3560 Training loss: 1.9101 1.0182 sec/batch\n",
      "Epoch 4/20  Iteration 587/3560 Training loss: 1.9096 0.9245 sec/batch\n",
      "Epoch 4/20  Iteration 588/3560 Training loss: 1.9091 1.0484 sec/batch\n",
      "Epoch 4/20  Iteration 589/3560 Training loss: 1.9085 0.9991 sec/batch\n",
      "Epoch 4/20  Iteration 590/3560 Training loss: 1.9084 0.9832 sec/batch\n",
      "Epoch 4/20  Iteration 591/3560 Training loss: 1.9083 1.1295 sec/batch\n",
      "Epoch 4/20  Iteration 592/3560 Training loss: 1.9078 0.9873 sec/batch\n",
      "Epoch 4/20  Iteration 593/3560 Training loss: 1.9070 0.9418 sec/batch\n",
      "Epoch 4/20  Iteration 594/3560 Training loss: 1.9074 1.0140 sec/batch\n",
      "Epoch 4/20  Iteration 595/3560 Training loss: 1.9070 0.9687 sec/batch\n",
      "Epoch 4/20  Iteration 596/3560 Training loss: 1.9073 1.0338 sec/batch\n",
      "Epoch 4/20  Iteration 597/3560 Training loss: 1.9073 0.9521 sec/batch\n",
      "Epoch 4/20  Iteration 598/3560 Training loss: 1.9073 1.0356 sec/batch\n",
      "Epoch 4/20  Iteration 599/3560 Training loss: 1.9068 1.0132 sec/batch\n",
      "Epoch 4/20  Iteration 600/3560 Training loss: 1.9069 1.0786 sec/batch\n",
      "Epoch 4/20  Iteration 601/3560 Training loss: 1.9067 0.9952 sec/batch\n",
      "Epoch 4/20  Iteration 602/3560 Training loss: 1.9060 0.9011 sec/batch\n",
      "Epoch 4/20  Iteration 603/3560 Training loss: 1.9056 1.0891 sec/batch\n",
      "Epoch 4/20  Iteration 604/3560 Training loss: 1.9052 1.0130 sec/batch\n",
      "Epoch 4/20  Iteration 605/3560 Training loss: 1.9053 0.9714 sec/batch\n",
      "Epoch 4/20  Iteration 606/3560 Training loss: 1.9051 0.9480 sec/batch\n",
      "Epoch 4/20  Iteration 607/3560 Training loss: 1.9052 0.9031 sec/batch\n",
      "Epoch 4/20  Iteration 608/3560 Training loss: 1.9044 0.9505 sec/batch\n",
      "Epoch 4/20  Iteration 609/3560 Training loss: 1.9040 0.9976 sec/batch\n",
      "Epoch 4/20  Iteration 610/3560 Training loss: 1.9039 0.8735 sec/batch\n",
      "Epoch 4/20  Iteration 611/3560 Training loss: 1.9036 1.0441 sec/batch\n",
      "Epoch 4/20  Iteration 612/3560 Training loss: 1.9034 0.9539 sec/batch\n",
      "Epoch 4/20  Iteration 613/3560 Training loss: 1.9025 1.0187 sec/batch\n",
      "Epoch 4/20  Iteration 614/3560 Training loss: 1.9022 0.9997 sec/batch\n",
      "Epoch 4/20  Iteration 615/3560 Training loss: 1.9014 0.9871 sec/batch\n",
      "Epoch 4/20  Iteration 616/3560 Training loss: 1.9012 0.9099 sec/batch\n",
      "Epoch 4/20  Iteration 617/3560 Training loss: 1.9004 0.9481 sec/batch\n",
      "Epoch 4/20  Iteration 618/3560 Training loss: 1.9001 1.0074 sec/batch\n",
      "Epoch 4/20  Iteration 619/3560 Training loss: 1.8993 0.9946 sec/batch\n",
      "Epoch 4/20  Iteration 620/3560 Training loss: 1.8986 1.0541 sec/batch\n",
      "Epoch 4/20  Iteration 621/3560 Training loss: 1.8982 0.9831 sec/batch\n",
      "Epoch 4/20  Iteration 622/3560 Training loss: 1.8977 0.9572 sec/batch\n",
      "Epoch 4/20  Iteration 623/3560 Training loss: 1.8969 1.0057 sec/batch\n",
      "Epoch 4/20  Iteration 624/3560 Training loss: 1.8967 0.9240 sec/batch\n",
      "Epoch 4/20  Iteration 625/3560 Training loss: 1.8961 0.9666 sec/batch\n",
      "Epoch 4/20  Iteration 626/3560 Training loss: 1.8957 0.9326 sec/batch\n",
      "Epoch 4/20  Iteration 627/3560 Training loss: 1.8950 0.9093 sec/batch\n",
      "Epoch 4/20  Iteration 628/3560 Training loss: 1.8945 1.0120 sec/batch\n",
      "Epoch 4/20  Iteration 629/3560 Training loss: 1.8939 0.9453 sec/batch\n",
      "Epoch 4/20  Iteration 630/3560 Training loss: 1.8935 0.9765 sec/batch\n",
      "Epoch 4/20  Iteration 631/3560 Training loss: 1.8931 1.0441 sec/batch\n",
      "Epoch 4/20  Iteration 632/3560 Training loss: 1.8925 0.9445 sec/batch\n",
      "Epoch 4/20  Iteration 633/3560 Training loss: 1.8919 0.9640 sec/batch\n",
      "Epoch 4/20  Iteration 634/3560 Training loss: 1.8911 1.0156 sec/batch\n",
      "Epoch 4/20  Iteration 635/3560 Training loss: 1.8908 1.0864 sec/batch\n",
      "Epoch 4/20  Iteration 636/3560 Training loss: 1.8905 0.9406 sec/batch\n",
      "Epoch 4/20  Iteration 637/3560 Training loss: 1.8900 0.9676 sec/batch\n",
      "Epoch 4/20  Iteration 638/3560 Training loss: 1.8896 0.9753 sec/batch\n",
      "Epoch 4/20  Iteration 639/3560 Training loss: 1.8891 0.9796 sec/batch\n",
      "Epoch 4/20  Iteration 640/3560 Training loss: 1.8886 0.9997 sec/batch\n",
      "Epoch 4/20  Iteration 641/3560 Training loss: 1.8883 0.9325 sec/batch\n",
      "Epoch 4/20  Iteration 642/3560 Training loss: 1.8880 0.9861 sec/batch\n",
      "Epoch 4/20  Iteration 643/3560 Training loss: 1.8878 1.0790 sec/batch\n",
      "Epoch 4/20  Iteration 644/3560 Training loss: 1.8876 1.0304 sec/batch\n",
      "Epoch 4/20  Iteration 645/3560 Training loss: 1.8873 1.0396 sec/batch\n",
      "Epoch 4/20  Iteration 646/3560 Training loss: 1.8869 0.9492 sec/batch\n",
      "Epoch 4/20  Iteration 647/3560 Training loss: 1.8865 0.9634 sec/batch\n",
      "Epoch 4/20  Iteration 648/3560 Training loss: 1.8861 0.9336 sec/batch\n",
      "Epoch 4/20  Iteration 649/3560 Training loss: 1.8856 0.9001 sec/batch\n",
      "Epoch 4/20  Iteration 650/3560 Training loss: 1.8850 1.0121 sec/batch\n",
      "Epoch 4/20  Iteration 651/3560 Training loss: 1.8847 0.9491 sec/batch\n",
      "Epoch 4/20  Iteration 652/3560 Training loss: 1.8843 1.0082 sec/batch\n",
      "Epoch 4/20  Iteration 653/3560 Training loss: 1.8841 0.9749 sec/batch\n",
      "Epoch 4/20  Iteration 654/3560 Training loss: 1.8837 0.9554 sec/batch\n",
      "Epoch 4/20  Iteration 655/3560 Training loss: 1.8834 0.9902 sec/batch\n",
      "Epoch 4/20  Iteration 656/3560 Training loss: 1.8828 0.8818 sec/batch\n",
      "Epoch 4/20  Iteration 657/3560 Training loss: 1.8823 0.9144 sec/batch\n",
      "Epoch 4/20  Iteration 658/3560 Training loss: 1.8820 1.0301 sec/batch\n",
      "Epoch 4/20  Iteration 659/3560 Training loss: 1.8817 0.9931 sec/batch\n",
      "Epoch 4/20  Iteration 660/3560 Training loss: 1.8811 0.9426 sec/batch\n",
      "Epoch 4/20  Iteration 661/3560 Training loss: 1.8810 0.9769 sec/batch\n",
      "Epoch 4/20  Iteration 662/3560 Training loss: 1.8808 0.9478 sec/batch\n",
      "Epoch 4/20  Iteration 663/3560 Training loss: 1.8805 0.9637 sec/batch\n",
      "Epoch 4/20  Iteration 664/3560 Training loss: 1.8802 0.8987 sec/batch\n",
      "Epoch 4/20  Iteration 665/3560 Training loss: 1.8797 1.0555 sec/batch\n",
      "Epoch 4/20  Iteration 666/3560 Training loss: 1.8792 1.0481 sec/batch\n",
      "Epoch 4/20  Iteration 667/3560 Training loss: 1.8790 1.0008 sec/batch\n",
      "Epoch 4/20  Iteration 668/3560 Training loss: 1.8788 0.8828 sec/batch\n",
      "Epoch 4/20  Iteration 669/3560 Training loss: 1.8785 0.9808 sec/batch\n",
      "Epoch 4/20  Iteration 670/3560 Training loss: 1.8782 1.0695 sec/batch\n",
      "Epoch 4/20  Iteration 671/3560 Training loss: 1.8780 0.9616 sec/batch\n",
      "Epoch 4/20  Iteration 672/3560 Training loss: 1.8778 0.9430 sec/batch\n",
      "Epoch 4/20  Iteration 673/3560 Training loss: 1.8777 0.9893 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 674/3560 Training loss: 1.8773 1.0119 sec/batch\n",
      "Epoch 4/20  Iteration 675/3560 Training loss: 1.8773 0.9903 sec/batch\n",
      "Epoch 4/20  Iteration 676/3560 Training loss: 1.8769 0.9641 sec/batch\n",
      "Epoch 4/20  Iteration 677/3560 Training loss: 1.8766 1.0371 sec/batch\n",
      "Epoch 4/20  Iteration 678/3560 Training loss: 1.8764 0.9209 sec/batch\n",
      "Epoch 4/20  Iteration 679/3560 Training loss: 1.8761 0.9400 sec/batch\n",
      "Epoch 4/20  Iteration 680/3560 Training loss: 1.8759 0.9335 sec/batch\n",
      "Epoch 4/20  Iteration 681/3560 Training loss: 1.8756 0.9560 sec/batch\n",
      "Epoch 4/20  Iteration 682/3560 Training loss: 1.8756 1.0242 sec/batch\n",
      "Epoch 4/20  Iteration 683/3560 Training loss: 1.8753 0.9735 sec/batch\n",
      "Epoch 4/20  Iteration 684/3560 Training loss: 1.8749 0.9741 sec/batch\n",
      "Epoch 4/20  Iteration 685/3560 Training loss: 1.8745 1.0162 sec/batch\n",
      "Epoch 4/20  Iteration 686/3560 Training loss: 1.8744 1.0161 sec/batch\n",
      "Epoch 4/20  Iteration 687/3560 Training loss: 1.8742 0.8601 sec/batch\n",
      "Epoch 4/20  Iteration 688/3560 Training loss: 1.8740 0.8799 sec/batch\n",
      "Epoch 4/20  Iteration 689/3560 Training loss: 1.8737 0.9756 sec/batch\n",
      "Epoch 4/20  Iteration 690/3560 Training loss: 1.8734 1.0104 sec/batch\n",
      "Epoch 4/20  Iteration 691/3560 Training loss: 1.8733 1.0364 sec/batch\n",
      "Epoch 4/20  Iteration 692/3560 Training loss: 1.8730 0.9056 sec/batch\n",
      "Epoch 4/20  Iteration 693/3560 Training loss: 1.8726 0.9917 sec/batch\n",
      "Epoch 4/20  Iteration 694/3560 Training loss: 1.8725 1.0681 sec/batch\n",
      "Epoch 4/20  Iteration 695/3560 Training loss: 1.8724 0.8812 sec/batch\n",
      "Epoch 4/20  Iteration 696/3560 Training loss: 1.8721 0.9579 sec/batch\n",
      "Epoch 4/20  Iteration 697/3560 Training loss: 1.8718 0.9654 sec/batch\n",
      "Epoch 4/20  Iteration 698/3560 Training loss: 1.8716 0.9412 sec/batch\n",
      "Epoch 4/20  Iteration 699/3560 Training loss: 1.8714 0.9731 sec/batch\n",
      "Epoch 4/20  Iteration 700/3560 Training loss: 1.8710 0.8938 sec/batch\n",
      "Epoch 4/20  Iteration 701/3560 Training loss: 1.8709 0.9898 sec/batch\n",
      "Epoch 4/20  Iteration 702/3560 Training loss: 1.8710 1.0176 sec/batch\n",
      "Epoch 4/20  Iteration 703/3560 Training loss: 1.8707 0.9628 sec/batch\n",
      "Epoch 4/20  Iteration 704/3560 Training loss: 1.8704 0.9385 sec/batch\n",
      "Epoch 4/20  Iteration 705/3560 Training loss: 1.8700 0.9769 sec/batch\n",
      "Epoch 4/20  Iteration 706/3560 Training loss: 1.8697 0.9932 sec/batch\n",
      "Epoch 4/20  Iteration 707/3560 Training loss: 1.8696 1.0139 sec/batch\n",
      "Epoch 4/20  Iteration 708/3560 Training loss: 1.8694 0.9321 sec/batch\n",
      "Epoch 4/20  Iteration 709/3560 Training loss: 1.8692 1.0797 sec/batch\n",
      "Epoch 4/20  Iteration 710/3560 Training loss: 1.8689 0.8859 sec/batch\n",
      "Epoch 4/20  Iteration 711/3560 Training loss: 1.8685 0.9866 sec/batch\n",
      "Epoch 4/20  Iteration 712/3560 Training loss: 1.8683 0.9253 sec/batch\n",
      "Epoch 5/20  Iteration 713/3560 Training loss: 1.8989 0.9810 sec/batch\n",
      "Epoch 5/20  Iteration 714/3560 Training loss: 1.8541 1.0150 sec/batch\n",
      "Epoch 5/20  Iteration 715/3560 Training loss: 1.8400 0.9030 sec/batch\n",
      "Epoch 5/20  Iteration 716/3560 Training loss: 1.8339 0.9952 sec/batch\n",
      "Epoch 5/20  Iteration 717/3560 Training loss: 1.8306 0.9750 sec/batch\n",
      "Epoch 5/20  Iteration 718/3560 Training loss: 1.8215 0.9795 sec/batch\n",
      "Epoch 5/20  Iteration 719/3560 Training loss: 1.8212 0.9616 sec/batch\n",
      "Epoch 5/20  Iteration 720/3560 Training loss: 1.8191 0.8730 sec/batch\n",
      "Epoch 5/20  Iteration 721/3560 Training loss: 1.8200 1.0405 sec/batch\n",
      "Epoch 5/20  Iteration 722/3560 Training loss: 1.8186 0.9804 sec/batch\n",
      "Epoch 5/20  Iteration 723/3560 Training loss: 1.8159 0.9810 sec/batch\n",
      "Epoch 5/20  Iteration 724/3560 Training loss: 1.8138 0.9607 sec/batch\n",
      "Epoch 5/20  Iteration 725/3560 Training loss: 1.8135 1.0448 sec/batch\n",
      "Epoch 5/20  Iteration 726/3560 Training loss: 1.8157 0.9162 sec/batch\n",
      "Epoch 5/20  Iteration 727/3560 Training loss: 1.8155 0.9775 sec/batch\n",
      "Epoch 5/20  Iteration 728/3560 Training loss: 1.8132 0.8868 sec/batch\n",
      "Epoch 5/20  Iteration 729/3560 Training loss: 1.8128 0.9655 sec/batch\n",
      "Epoch 5/20  Iteration 730/3560 Training loss: 1.8146 1.1108 sec/batch\n",
      "Epoch 5/20  Iteration 731/3560 Training loss: 1.8146 0.9445 sec/batch\n",
      "Epoch 5/20  Iteration 732/3560 Training loss: 1.8155 0.9488 sec/batch\n",
      "Epoch 5/20  Iteration 733/3560 Training loss: 1.8148 0.9912 sec/batch\n",
      "Epoch 5/20  Iteration 734/3560 Training loss: 1.8152 0.9622 sec/batch\n",
      "Epoch 5/20  Iteration 735/3560 Training loss: 1.8145 0.9870 sec/batch\n",
      "Epoch 5/20  Iteration 736/3560 Training loss: 1.8140 0.9994 sec/batch\n",
      "Epoch 5/20  Iteration 737/3560 Training loss: 1.8137 1.0361 sec/batch\n",
      "Epoch 5/20  Iteration 738/3560 Training loss: 1.8124 0.9994 sec/batch\n",
      "Epoch 5/20  Iteration 739/3560 Training loss: 1.8115 0.9829 sec/batch\n",
      "Epoch 5/20  Iteration 740/3560 Training loss: 1.8121 0.9824 sec/batch\n",
      "Epoch 5/20  Iteration 741/3560 Training loss: 1.8127 1.0528 sec/batch\n",
      "Epoch 5/20  Iteration 742/3560 Training loss: 1.8128 0.9040 sec/batch\n",
      "Epoch 5/20  Iteration 743/3560 Training loss: 1.8123 0.9325 sec/batch\n",
      "Epoch 5/20  Iteration 744/3560 Training loss: 1.8114 1.0238 sec/batch\n",
      "Epoch 5/20  Iteration 745/3560 Training loss: 1.8112 0.9881 sec/batch\n",
      "Epoch 5/20  Iteration 746/3560 Training loss: 1.8118 0.8919 sec/batch\n",
      "Epoch 5/20  Iteration 747/3560 Training loss: 1.8113 0.9845 sec/batch\n",
      "Epoch 5/20  Iteration 748/3560 Training loss: 1.8108 0.9896 sec/batch\n",
      "Epoch 5/20  Iteration 749/3560 Training loss: 1.8102 1.0090 sec/batch\n",
      "Epoch 5/20  Iteration 750/3560 Training loss: 1.8088 0.9561 sec/batch\n",
      "Epoch 5/20  Iteration 751/3560 Training loss: 1.8075 0.9652 sec/batch\n",
      "Epoch 5/20  Iteration 752/3560 Training loss: 1.8066 1.0186 sec/batch\n",
      "Epoch 5/20  Iteration 753/3560 Training loss: 1.8060 0.9846 sec/batch\n",
      "Epoch 5/20  Iteration 754/3560 Training loss: 1.8060 0.9465 sec/batch\n",
      "Epoch 5/20  Iteration 755/3560 Training loss: 1.8054 0.9355 sec/batch\n",
      "Epoch 5/20  Iteration 756/3560 Training loss: 1.8044 1.0132 sec/batch\n",
      "Epoch 5/20  Iteration 757/3560 Training loss: 1.8047 1.0658 sec/batch\n",
      "Epoch 5/20  Iteration 758/3560 Training loss: 1.8036 0.9248 sec/batch\n",
      "Epoch 5/20  Iteration 759/3560 Training loss: 1.8031 1.0926 sec/batch\n",
      "Epoch 5/20  Iteration 760/3560 Training loss: 1.8023 1.0226 sec/batch\n",
      "Epoch 5/20  Iteration 761/3560 Training loss: 1.8019 0.9730 sec/batch\n",
      "Epoch 5/20  Iteration 762/3560 Training loss: 1.8026 0.9737 sec/batch\n",
      "Epoch 5/20  Iteration 763/3560 Training loss: 1.8018 1.0299 sec/batch\n",
      "Epoch 5/20  Iteration 764/3560 Training loss: 1.8024 0.9776 sec/batch\n",
      "Epoch 5/20  Iteration 765/3560 Training loss: 1.8019 0.9377 sec/batch\n",
      "Epoch 5/20  Iteration 766/3560 Training loss: 1.8018 0.9376 sec/batch\n",
      "Epoch 5/20  Iteration 767/3560 Training loss: 1.8012 1.0794 sec/batch\n",
      "Epoch 5/20  Iteration 768/3560 Training loss: 1.8011 0.9751 sec/batch\n",
      "Epoch 5/20  Iteration 769/3560 Training loss: 1.8011 0.9242 sec/batch\n",
      "Epoch 5/20  Iteration 770/3560 Training loss: 1.8006 0.8794 sec/batch\n",
      "Epoch 5/20  Iteration 771/3560 Training loss: 1.8001 1.0121 sec/batch\n",
      "Epoch 5/20  Iteration 772/3560 Training loss: 1.8006 0.9925 sec/batch\n",
      "Epoch 5/20  Iteration 773/3560 Training loss: 1.8004 0.8602 sec/batch\n",
      "Epoch 5/20  Iteration 774/3560 Training loss: 1.8010 0.9762 sec/batch\n",
      "Epoch 5/20  Iteration 775/3560 Training loss: 1.8011 0.9791 sec/batch\n",
      "Epoch 5/20  Iteration 776/3560 Training loss: 1.8012 0.9555 sec/batch\n",
      "Epoch 5/20  Iteration 777/3560 Training loss: 1.8009 0.9329 sec/batch\n",
      "Epoch 5/20  Iteration 778/3560 Training loss: 1.8010 0.9719 sec/batch\n",
      "Epoch 5/20  Iteration 779/3560 Training loss: 1.8011 0.9597 sec/batch\n",
      "Epoch 5/20  Iteration 780/3560 Training loss: 1.8004 1.0728 sec/batch\n",
      "Epoch 5/20  Iteration 781/3560 Training loss: 1.8002 0.9353 sec/batch\n",
      "Epoch 5/20  Iteration 782/3560 Training loss: 1.8001 0.9700 sec/batch\n",
      "Epoch 5/20  Iteration 783/3560 Training loss: 1.8005 0.9927 sec/batch\n",
      "Epoch 5/20  Iteration 784/3560 Training loss: 1.8007 0.9902 sec/batch\n",
      "Epoch 5/20  Iteration 785/3560 Training loss: 1.8009 0.9111 sec/batch\n",
      "Epoch 5/20  Iteration 786/3560 Training loss: 1.8006 0.9530 sec/batch\n",
      "Epoch 5/20  Iteration 787/3560 Training loss: 1.8003 0.9705 sec/batch\n",
      "Epoch 5/20  Iteration 788/3560 Training loss: 1.8004 0.9666 sec/batch\n",
      "Epoch 5/20  Iteration 789/3560 Training loss: 1.8001 0.8876 sec/batch\n",
      "Epoch 5/20  Iteration 790/3560 Training loss: 1.7999 0.9714 sec/batch\n",
      "Epoch 5/20  Iteration 791/3560 Training loss: 1.7992 0.9567 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 792/3560 Training loss: 1.7988 1.0408 sec/batch\n",
      "Epoch 5/20  Iteration 793/3560 Training loss: 1.7983 0.9042 sec/batch\n",
      "Epoch 5/20  Iteration 794/3560 Training loss: 1.7983 0.9718 sec/batch\n",
      "Epoch 5/20  Iteration 795/3560 Training loss: 1.7975 1.0250 sec/batch\n",
      "Epoch 5/20  Iteration 796/3560 Training loss: 1.7973 1.0412 sec/batch\n",
      "Epoch 5/20  Iteration 797/3560 Training loss: 1.7965 0.9123 sec/batch\n",
      "Epoch 5/20  Iteration 798/3560 Training loss: 1.7960 0.9500 sec/batch\n",
      "Epoch 5/20  Iteration 799/3560 Training loss: 1.7957 1.0006 sec/batch\n",
      "Epoch 5/20  Iteration 800/3560 Training loss: 1.7953 0.9406 sec/batch\n",
      "Epoch 5/20  Iteration 801/3560 Training loss: 1.7947 0.9927 sec/batch\n",
      "Epoch 5/20  Iteration 802/3560 Training loss: 1.7946 0.9461 sec/batch\n",
      "Epoch 5/20  Iteration 803/3560 Training loss: 1.7942 1.0105 sec/batch\n",
      "Epoch 5/20  Iteration 804/3560 Training loss: 1.7939 1.0168 sec/batch\n",
      "Epoch 5/20  Iteration 805/3560 Training loss: 1.7934 0.9258 sec/batch\n",
      "Epoch 5/20  Iteration 806/3560 Training loss: 1.7929 0.9394 sec/batch\n",
      "Epoch 5/20  Iteration 807/3560 Training loss: 1.7925 0.9873 sec/batch\n",
      "Epoch 5/20  Iteration 808/3560 Training loss: 1.7923 0.9786 sec/batch\n",
      "Epoch 5/20  Iteration 809/3560 Training loss: 1.7920 0.9855 sec/batch\n",
      "Epoch 5/20  Iteration 810/3560 Training loss: 1.7915 1.0289 sec/batch\n",
      "Epoch 5/20  Iteration 811/3560 Training loss: 1.7909 1.0681 sec/batch\n",
      "Epoch 5/20  Iteration 812/3560 Training loss: 1.7903 1.0267 sec/batch\n",
      "Epoch 5/20  Iteration 813/3560 Training loss: 1.7901 0.9077 sec/batch\n",
      "Epoch 5/20  Iteration 814/3560 Training loss: 1.7898 0.9222 sec/batch\n",
      "Epoch 5/20  Iteration 815/3560 Training loss: 1.7894 1.0148 sec/batch\n",
      "Epoch 5/20  Iteration 816/3560 Training loss: 1.7890 0.9737 sec/batch\n",
      "Epoch 5/20  Iteration 817/3560 Training loss: 1.7886 1.0103 sec/batch\n",
      "Epoch 5/20  Iteration 818/3560 Training loss: 1.7882 1.0590 sec/batch\n",
      "Epoch 5/20  Iteration 819/3560 Training loss: 1.7880 1.0372 sec/batch\n",
      "Epoch 5/20  Iteration 820/3560 Training loss: 1.7878 0.9986 sec/batch\n",
      "Epoch 5/20  Iteration 821/3560 Training loss: 1.7877 0.9584 sec/batch\n",
      "Epoch 5/20  Iteration 822/3560 Training loss: 1.7876 0.9864 sec/batch\n",
      "Epoch 5/20  Iteration 823/3560 Training loss: 1.7873 0.9421 sec/batch\n",
      "Epoch 5/20  Iteration 824/3560 Training loss: 1.7870 0.9944 sec/batch\n",
      "Epoch 5/20  Iteration 825/3560 Training loss: 1.7867 1.0026 sec/batch\n",
      "Epoch 5/20  Iteration 826/3560 Training loss: 1.7863 0.9884 sec/batch\n",
      "Epoch 5/20  Iteration 827/3560 Training loss: 1.7860 1.0962 sec/batch\n",
      "Epoch 5/20  Iteration 828/3560 Training loss: 1.7854 0.9695 sec/batch\n",
      "Epoch 5/20  Iteration 829/3560 Training loss: 1.7852 0.9560 sec/batch\n",
      "Epoch 5/20  Iteration 830/3560 Training loss: 1.7849 0.9728 sec/batch\n",
      "Epoch 5/20  Iteration 831/3560 Training loss: 1.7846 0.9419 sec/batch\n",
      "Epoch 5/20  Iteration 832/3560 Training loss: 1.7844 1.0572 sec/batch\n",
      "Epoch 5/20  Iteration 833/3560 Training loss: 1.7842 1.0210 sec/batch\n",
      "Epoch 5/20  Iteration 834/3560 Training loss: 1.7838 1.1313 sec/batch\n",
      "Epoch 5/20  Iteration 835/3560 Training loss: 1.7833 1.0274 sec/batch\n",
      "Epoch 5/20  Iteration 836/3560 Training loss: 1.7833 0.9903 sec/batch\n",
      "Epoch 5/20  Iteration 837/3560 Training loss: 1.7830 1.0697 sec/batch\n",
      "Epoch 5/20  Iteration 838/3560 Training loss: 1.7824 1.0527 sec/batch\n",
      "Epoch 5/20  Iteration 839/3560 Training loss: 1.7824 1.1452 sec/batch\n",
      "Epoch 5/20  Iteration 840/3560 Training loss: 1.7822 1.0481 sec/batch\n",
      "Epoch 5/20  Iteration 841/3560 Training loss: 1.7820 1.0068 sec/batch\n",
      "Epoch 5/20  Iteration 842/3560 Training loss: 1.7817 1.1269 sec/batch\n",
      "Epoch 5/20  Iteration 843/3560 Training loss: 1.7812 1.0785 sec/batch\n",
      "Epoch 5/20  Iteration 844/3560 Training loss: 1.7808 1.0920 sec/batch\n",
      "Epoch 5/20  Iteration 845/3560 Training loss: 1.7807 0.9930 sec/batch\n",
      "Epoch 5/20  Iteration 846/3560 Training loss: 1.7806 0.9648 sec/batch\n",
      "Epoch 5/20  Iteration 847/3560 Training loss: 1.7804 1.1142 sec/batch\n",
      "Epoch 5/20  Iteration 848/3560 Training loss: 1.7803 0.9919 sec/batch\n",
      "Epoch 5/20  Iteration 849/3560 Training loss: 1.7803 1.0354 sec/batch\n",
      "Epoch 5/20  Iteration 850/3560 Training loss: 1.7801 1.1248 sec/batch\n",
      "Epoch 5/20  Iteration 851/3560 Training loss: 1.7800 1.0697 sec/batch\n",
      "Epoch 5/20  Iteration 852/3560 Training loss: 1.7797 0.9734 sec/batch\n",
      "Epoch 5/20  Iteration 853/3560 Training loss: 1.7798 1.1666 sec/batch\n",
      "Epoch 5/20  Iteration 854/3560 Training loss: 1.7796 1.0346 sec/batch\n",
      "Epoch 5/20  Iteration 855/3560 Training loss: 1.7793 1.2569 sec/batch\n",
      "Epoch 5/20  Iteration 856/3560 Training loss: 1.7792 1.1203 sec/batch\n",
      "Epoch 5/20  Iteration 857/3560 Training loss: 1.7789 1.1970 sec/batch\n",
      "Epoch 5/20  Iteration 858/3560 Training loss: 1.7788 1.1558 sec/batch\n",
      "Epoch 5/20  Iteration 859/3560 Training loss: 1.7787 1.3291 sec/batch\n",
      "Epoch 5/20  Iteration 860/3560 Training loss: 1.7788 1.3077 sec/batch\n",
      "Epoch 5/20  Iteration 861/3560 Training loss: 1.7786 1.2034 sec/batch\n",
      "Epoch 5/20  Iteration 862/3560 Training loss: 1.7784 1.0445 sec/batch\n",
      "Epoch 5/20  Iteration 863/3560 Training loss: 1.7780 1.0190 sec/batch\n",
      "Epoch 5/20  Iteration 864/3560 Training loss: 1.7779 1.1277 sec/batch\n",
      "Epoch 5/20  Iteration 865/3560 Training loss: 1.7778 1.1722 sec/batch\n",
      "Epoch 5/20  Iteration 866/3560 Training loss: 1.7777 1.0460 sec/batch\n",
      "Epoch 5/20  Iteration 867/3560 Training loss: 1.7776 1.1698 sec/batch\n",
      "Epoch 5/20  Iteration 868/3560 Training loss: 1.7774 1.1290 sec/batch\n",
      "Epoch 5/20  Iteration 869/3560 Training loss: 1.7773 1.1146 sec/batch\n",
      "Epoch 5/20  Iteration 870/3560 Training loss: 1.7771 1.1777 sec/batch\n",
      "Epoch 5/20  Iteration 871/3560 Training loss: 1.7767 0.9228 sec/batch\n",
      "Epoch 5/20  Iteration 872/3560 Training loss: 1.7767 1.0570 sec/batch\n",
      "Epoch 5/20  Iteration 873/3560 Training loss: 1.7767 1.0612 sec/batch\n",
      "Epoch 5/20  Iteration 874/3560 Training loss: 1.7765 1.0028 sec/batch\n",
      "Epoch 5/20  Iteration 875/3560 Training loss: 1.7763 0.9114 sec/batch\n",
      "Epoch 5/20  Iteration 876/3560 Training loss: 1.7762 1.0921 sec/batch\n",
      "Epoch 5/20  Iteration 877/3560 Training loss: 1.7761 0.9668 sec/batch\n",
      "Epoch 5/20  Iteration 878/3560 Training loss: 1.7759 0.9738 sec/batch\n",
      "Epoch 5/20  Iteration 879/3560 Training loss: 1.7759 0.9670 sec/batch\n",
      "Epoch 5/20  Iteration 880/3560 Training loss: 1.7761 0.9724 sec/batch\n",
      "Epoch 5/20  Iteration 881/3560 Training loss: 1.7758 1.0966 sec/batch\n",
      "Epoch 5/20  Iteration 882/3560 Training loss: 1.7757 1.0160 sec/batch\n",
      "Epoch 5/20  Iteration 883/3560 Training loss: 1.7755 1.0439 sec/batch\n",
      "Epoch 5/20  Iteration 884/3560 Training loss: 1.7753 0.9224 sec/batch\n",
      "Epoch 5/20  Iteration 885/3560 Training loss: 1.7753 0.9285 sec/batch\n",
      "Epoch 5/20  Iteration 886/3560 Training loss: 1.7751 0.9510 sec/batch\n",
      "Epoch 5/20  Iteration 887/3560 Training loss: 1.7751 0.9883 sec/batch\n",
      "Epoch 5/20  Iteration 888/3560 Training loss: 1.7749 0.9831 sec/batch\n",
      "Epoch 5/20  Iteration 889/3560 Training loss: 1.7746 1.1174 sec/batch\n",
      "Epoch 5/20  Iteration 890/3560 Training loss: 1.7745 0.9628 sec/batch\n",
      "Epoch 6/20  Iteration 891/3560 Training loss: 1.8197 0.9490 sec/batch\n",
      "Epoch 6/20  Iteration 892/3560 Training loss: 1.7802 0.9752 sec/batch\n",
      "Epoch 6/20  Iteration 893/3560 Training loss: 1.7674 0.9441 sec/batch\n",
      "Epoch 6/20  Iteration 894/3560 Training loss: 1.7605 0.9860 sec/batch\n",
      "Epoch 6/20  Iteration 895/3560 Training loss: 1.7560 1.0166 sec/batch\n",
      "Epoch 6/20  Iteration 896/3560 Training loss: 1.7457 0.9797 sec/batch\n",
      "Epoch 6/20  Iteration 897/3560 Training loss: 1.7456 1.1561 sec/batch\n",
      "Epoch 6/20  Iteration 898/3560 Training loss: 1.7432 1.0546 sec/batch\n",
      "Epoch 6/20  Iteration 899/3560 Training loss: 1.7439 0.9258 sec/batch\n",
      "Epoch 6/20  Iteration 900/3560 Training loss: 1.7420 0.9541 sec/batch\n",
      "Epoch 6/20  Iteration 901/3560 Training loss: 1.7399 0.9908 sec/batch\n",
      "Epoch 6/20  Iteration 902/3560 Training loss: 1.7376 0.8438 sec/batch\n",
      "Epoch 6/20  Iteration 903/3560 Training loss: 1.7384 1.0565 sec/batch\n",
      "Epoch 6/20  Iteration 904/3560 Training loss: 1.7410 1.0429 sec/batch\n",
      "Epoch 6/20  Iteration 905/3560 Training loss: 1.7398 1.0457 sec/batch\n",
      "Epoch 6/20  Iteration 906/3560 Training loss: 1.7381 0.9482 sec/batch\n",
      "Epoch 6/20  Iteration 907/3560 Training loss: 1.7378 1.0174 sec/batch\n",
      "Epoch 6/20  Iteration 908/3560 Training loss: 1.7399 0.9888 sec/batch\n",
      "Epoch 6/20  Iteration 909/3560 Training loss: 1.7400 0.9828 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 910/3560 Training loss: 1.7403 0.9784 sec/batch\n",
      "Epoch 6/20  Iteration 911/3560 Training loss: 1.7399 0.9941 sec/batch\n",
      "Epoch 6/20  Iteration 912/3560 Training loss: 1.7404 1.0151 sec/batch\n",
      "Epoch 6/20  Iteration 913/3560 Training loss: 1.7391 1.0440 sec/batch\n",
      "Epoch 6/20  Iteration 914/3560 Training loss: 1.7389 0.9543 sec/batch\n",
      "Epoch 6/20  Iteration 915/3560 Training loss: 1.7384 0.9718 sec/batch\n",
      "Epoch 6/20  Iteration 916/3560 Training loss: 1.7370 0.9567 sec/batch\n",
      "Epoch 6/20  Iteration 917/3560 Training loss: 1.7358 0.9650 sec/batch\n",
      "Epoch 6/20  Iteration 918/3560 Training loss: 1.7359 1.0170 sec/batch\n",
      "Epoch 6/20  Iteration 919/3560 Training loss: 1.7361 1.0166 sec/batch\n",
      "Epoch 6/20  Iteration 920/3560 Training loss: 1.7360 1.0221 sec/batch\n",
      "Epoch 6/20  Iteration 921/3560 Training loss: 1.7354 1.0273 sec/batch\n",
      "Epoch 6/20  Iteration 922/3560 Training loss: 1.7343 0.8897 sec/batch\n",
      "Epoch 6/20  Iteration 923/3560 Training loss: 1.7344 0.9443 sec/batch\n",
      "Epoch 6/20  Iteration 924/3560 Training loss: 1.7348 0.9234 sec/batch\n",
      "Epoch 6/20  Iteration 925/3560 Training loss: 1.7343 1.0695 sec/batch\n",
      "Epoch 6/20  Iteration 926/3560 Training loss: 1.7338 0.9711 sec/batch\n",
      "Epoch 6/20  Iteration 927/3560 Training loss: 1.7332 1.1458 sec/batch\n",
      "Epoch 6/20  Iteration 928/3560 Training loss: 1.7319 1.1323 sec/batch\n",
      "Epoch 6/20  Iteration 929/3560 Training loss: 1.7305 1.0361 sec/batch\n",
      "Epoch 6/20  Iteration 930/3560 Training loss: 1.7298 0.9616 sec/batch\n",
      "Epoch 6/20  Iteration 931/3560 Training loss: 1.7291 1.0943 sec/batch\n",
      "Epoch 6/20  Iteration 932/3560 Training loss: 1.7293 1.0812 sec/batch\n",
      "Epoch 6/20  Iteration 933/3560 Training loss: 1.7287 0.9417 sec/batch\n",
      "Epoch 6/20  Iteration 934/3560 Training loss: 1.7277 1.0748 sec/batch\n",
      "Epoch 6/20  Iteration 935/3560 Training loss: 1.7279 1.1460 sec/batch\n",
      "Epoch 6/20  Iteration 936/3560 Training loss: 1.7265 1.2783 sec/batch\n",
      "Epoch 6/20  Iteration 937/3560 Training loss: 1.7261 1.0065 sec/batch\n",
      "Epoch 6/20  Iteration 938/3560 Training loss: 1.7255 1.0606 sec/batch\n",
      "Epoch 6/20  Iteration 939/3560 Training loss: 1.7252 0.8943 sec/batch\n",
      "Epoch 6/20  Iteration 940/3560 Training loss: 1.7257 0.9191 sec/batch\n",
      "Epoch 6/20  Iteration 941/3560 Training loss: 1.7251 0.9486 sec/batch\n",
      "Epoch 6/20  Iteration 942/3560 Training loss: 1.7258 0.9611 sec/batch\n",
      "Epoch 6/20  Iteration 943/3560 Training loss: 1.7253 1.0811 sec/batch\n",
      "Epoch 6/20  Iteration 944/3560 Training loss: 1.7251 1.0080 sec/batch\n",
      "Epoch 6/20  Iteration 945/3560 Training loss: 1.7246 0.9238 sec/batch\n",
      "Epoch 6/20  Iteration 946/3560 Training loss: 1.7247 1.0150 sec/batch\n",
      "Epoch 6/20  Iteration 947/3560 Training loss: 1.7248 1.0099 sec/batch\n",
      "Epoch 6/20  Iteration 948/3560 Training loss: 1.7244 1.0401 sec/batch\n",
      "Epoch 6/20  Iteration 949/3560 Training loss: 1.7237 1.0619 sec/batch\n",
      "Epoch 6/20  Iteration 950/3560 Training loss: 1.7244 0.9858 sec/batch\n",
      "Epoch 6/20  Iteration 951/3560 Training loss: 1.7243 1.1115 sec/batch\n",
      "Epoch 6/20  Iteration 952/3560 Training loss: 1.7249 1.1119 sec/batch\n",
      "Epoch 6/20  Iteration 953/3560 Training loss: 1.7251 0.9435 sec/batch\n",
      "Epoch 6/20  Iteration 954/3560 Training loss: 1.7252 0.9770 sec/batch\n",
      "Epoch 6/20  Iteration 955/3560 Training loss: 1.7250 0.9796 sec/batch\n",
      "Epoch 6/20  Iteration 956/3560 Training loss: 1.7252 1.0044 sec/batch\n",
      "Epoch 6/20  Iteration 957/3560 Training loss: 1.7252 1.1155 sec/batch\n",
      "Epoch 6/20  Iteration 958/3560 Training loss: 1.7246 1.1189 sec/batch\n",
      "Epoch 6/20  Iteration 959/3560 Training loss: 1.7243 1.0890 sec/batch\n",
      "Epoch 6/20  Iteration 960/3560 Training loss: 1.7241 0.9679 sec/batch\n",
      "Epoch 6/20  Iteration 961/3560 Training loss: 1.7245 1.0325 sec/batch\n",
      "Epoch 6/20  Iteration 962/3560 Training loss: 1.7245 1.0043 sec/batch\n",
      "Epoch 6/20  Iteration 963/3560 Training loss: 1.7249 0.9513 sec/batch\n",
      "Epoch 6/20  Iteration 964/3560 Training loss: 1.7245 0.9437 sec/batch\n",
      "Epoch 6/20  Iteration 965/3560 Training loss: 1.7243 0.9155 sec/batch\n",
      "Epoch 6/20  Iteration 966/3560 Training loss: 1.7245 1.0458 sec/batch\n",
      "Epoch 6/20  Iteration 967/3560 Training loss: 1.7242 1.0015 sec/batch\n",
      "Epoch 6/20  Iteration 968/3560 Training loss: 1.7242 1.0720 sec/batch\n",
      "Epoch 6/20  Iteration 969/3560 Training loss: 1.7235 0.9406 sec/batch\n",
      "Epoch 6/20  Iteration 970/3560 Training loss: 1.7233 0.9412 sec/batch\n",
      "Epoch 6/20  Iteration 971/3560 Training loss: 1.7227 0.9628 sec/batch\n",
      "Epoch 6/20  Iteration 972/3560 Training loss: 1.7226 0.8352 sec/batch\n",
      "Epoch 6/20  Iteration 973/3560 Training loss: 1.7220 0.9882 sec/batch\n",
      "Epoch 6/20  Iteration 974/3560 Training loss: 1.7219 0.9877 sec/batch\n",
      "Epoch 6/20  Iteration 975/3560 Training loss: 1.7214 1.0995 sec/batch\n",
      "Epoch 6/20  Iteration 976/3560 Training loss: 1.7210 0.9881 sec/batch\n",
      "Epoch 6/20  Iteration 977/3560 Training loss: 1.7207 1.0436 sec/batch\n",
      "Epoch 6/20  Iteration 978/3560 Training loss: 1.7203 0.8517 sec/batch\n",
      "Epoch 6/20  Iteration 979/3560 Training loss: 1.7198 0.9884 sec/batch\n",
      "Epoch 6/20  Iteration 980/3560 Training loss: 1.7198 0.9231 sec/batch\n",
      "Epoch 6/20  Iteration 981/3560 Training loss: 1.7194 0.9154 sec/batch\n",
      "Epoch 6/20  Iteration 982/3560 Training loss: 1.7191 0.9601 sec/batch\n",
      "Epoch 6/20  Iteration 983/3560 Training loss: 1.7186 1.1481 sec/batch\n",
      "Epoch 6/20  Iteration 984/3560 Training loss: 1.7182 0.8700 sec/batch\n",
      "Epoch 6/20  Iteration 985/3560 Training loss: 1.7178 0.9780 sec/batch\n",
      "Epoch 6/20  Iteration 986/3560 Training loss: 1.7177 0.9544 sec/batch\n",
      "Epoch 6/20  Iteration 987/3560 Training loss: 1.7174 0.9800 sec/batch\n",
      "Epoch 6/20  Iteration 988/3560 Training loss: 1.7169 0.9735 sec/batch\n",
      "Epoch 6/20  Iteration 989/3560 Training loss: 1.7164 0.9855 sec/batch\n",
      "Epoch 6/20  Iteration 990/3560 Training loss: 1.7158 1.0143 sec/batch\n",
      "Epoch 6/20  Iteration 991/3560 Training loss: 1.7156 1.0420 sec/batch\n",
      "Epoch 6/20  Iteration 992/3560 Training loss: 1.7154 0.9683 sec/batch\n",
      "Epoch 6/20  Iteration 993/3560 Training loss: 1.7151 0.9566 sec/batch\n",
      "Epoch 6/20  Iteration 994/3560 Training loss: 1.7148 0.9252 sec/batch\n",
      "Epoch 6/20  Iteration 995/3560 Training loss: 1.7144 0.9894 sec/batch\n",
      "Epoch 6/20  Iteration 996/3560 Training loss: 1.7141 0.9483 sec/batch\n",
      "Epoch 6/20  Iteration 997/3560 Training loss: 1.7138 1.0024 sec/batch\n",
      "Epoch 6/20  Iteration 998/3560 Training loss: 1.7137 0.9795 sec/batch\n",
      "Epoch 6/20  Iteration 999/3560 Training loss: 1.7136 1.0811 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3560 Training loss: 1.7136 0.9885 sec/batch\n",
      "Epoch 6/20  Iteration 1001/3560 Training loss: 1.7133 0.9104 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3560 Training loss: 1.7131 0.9558 sec/batch\n",
      "Epoch 6/20  Iteration 1003/3560 Training loss: 1.7129 0.9710 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3560 Training loss: 1.7127 0.9607 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3560 Training loss: 1.7123 1.0406 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3560 Training loss: 1.7118 1.0252 sec/batch\n",
      "Epoch 6/20  Iteration 1007/3560 Training loss: 1.7117 1.0357 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3560 Training loss: 1.7115 0.9275 sec/batch\n",
      "Epoch 6/20  Iteration 1009/3560 Training loss: 1.7113 1.0423 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3560 Training loss: 1.7112 0.9949 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3560 Training loss: 1.7111 0.9408 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3560 Training loss: 1.7106 0.9258 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3560 Training loss: 1.7102 0.9663 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3560 Training loss: 1.7101 1.1363 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3560 Training loss: 1.7099 0.9488 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3560 Training loss: 1.7094 1.0323 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3560 Training loss: 1.7094 0.9528 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3560 Training loss: 1.7094 0.9798 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3560 Training loss: 1.7091 0.9366 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3560 Training loss: 1.7088 0.8731 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3560 Training loss: 1.7084 1.0439 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3560 Training loss: 1.7081 1.1179 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3560 Training loss: 1.7080 0.9425 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3560 Training loss: 1.7080 0.9068 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3560 Training loss: 1.7079 0.9314 sec/batch\n",
      "Epoch 6/20  Iteration 1026/3560 Training loss: 1.7078 0.9378 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 1027/3560 Training loss: 1.7078 0.9529 sec/batch\n",
      "Epoch 6/20  Iteration 1028/3560 Training loss: 1.7077 0.9478 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3560 Training loss: 1.7076 1.0795 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3560 Training loss: 1.7074 1.0284 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3560 Training loss: 1.7075 0.9579 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3560 Training loss: 1.7074 0.9425 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3560 Training loss: 1.7073 0.9200 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3560 Training loss: 1.7073 0.9871 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3560 Training loss: 1.7070 1.0612 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3560 Training loss: 1.7070 0.9738 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3560 Training loss: 1.7068 1.0231 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3560 Training loss: 1.7069 1.0207 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3560 Training loss: 1.7068 0.9992 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3560 Training loss: 1.7066 0.9423 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3560 Training loss: 1.7062 0.9576 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3560 Training loss: 1.7062 0.9363 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3560 Training loss: 1.7061 0.9639 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3560 Training loss: 1.7060 0.9680 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3560 Training loss: 1.7059 1.1158 sec/batch\n",
      "Epoch 6/20  Iteration 1046/3560 Training loss: 1.7057 1.0145 sec/batch\n",
      "Epoch 6/20  Iteration 1047/3560 Training loss: 1.7057 0.9382 sec/batch\n",
      "Epoch 6/20  Iteration 1048/3560 Training loss: 1.7056 1.0376 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3560 Training loss: 1.7052 0.9606 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3560 Training loss: 1.7052 0.9721 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3560 Training loss: 1.7053 0.9808 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3560 Training loss: 1.7051 1.0034 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3560 Training loss: 1.7050 1.0361 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3560 Training loss: 1.7049 0.9566 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3560 Training loss: 1.7049 0.9675 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3560 Training loss: 1.7047 0.9370 sec/batch\n",
      "Epoch 6/20  Iteration 1057/3560 Training loss: 1.7047 0.9632 sec/batch\n",
      "Epoch 6/20  Iteration 1058/3560 Training loss: 1.7049 1.0123 sec/batch\n",
      "Epoch 6/20  Iteration 1059/3560 Training loss: 1.7048 1.0071 sec/batch\n",
      "Epoch 6/20  Iteration 1060/3560 Training loss: 1.7046 1.0632 sec/batch\n",
      "Epoch 6/20  Iteration 1061/3560 Training loss: 1.7044 0.9813 sec/batch\n",
      "Epoch 6/20  Iteration 1062/3560 Training loss: 1.7042 0.9315 sec/batch\n",
      "Epoch 6/20  Iteration 1063/3560 Training loss: 1.7042 0.9090 sec/batch\n",
      "Epoch 6/20  Iteration 1064/3560 Training loss: 1.7041 0.8496 sec/batch\n",
      "Epoch 6/20  Iteration 1065/3560 Training loss: 1.7040 0.9521 sec/batch\n",
      "Epoch 6/20  Iteration 1066/3560 Training loss: 1.7038 0.8896 sec/batch\n",
      "Epoch 6/20  Iteration 1067/3560 Training loss: 1.7036 1.0396 sec/batch\n",
      "Epoch 6/20  Iteration 1068/3560 Training loss: 1.7035 1.0385 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3560 Training loss: 1.7562 1.0223 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3560 Training loss: 1.7139 1.0483 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3560 Training loss: 1.6990 0.8943 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3560 Training loss: 1.6913 0.9583 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3560 Training loss: 1.6879 0.9479 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3560 Training loss: 1.6790 0.9537 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3560 Training loss: 1.6796 0.9701 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3560 Training loss: 1.6771 0.9408 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3560 Training loss: 1.6783 1.0911 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3560 Training loss: 1.6769 1.0177 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3560 Training loss: 1.6736 0.9486 sec/batch\n",
      "Epoch 7/20  Iteration 1080/3560 Training loss: 1.6718 1.0360 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3560 Training loss: 1.6715 0.8909 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3560 Training loss: 1.6735 0.9875 sec/batch\n",
      "Epoch 7/20  Iteration 1083/3560 Training loss: 1.6729 0.9552 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3560 Training loss: 1.6706 1.0471 sec/batch\n",
      "Epoch 7/20  Iteration 1085/3560 Training loss: 1.6711 1.0385 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3560 Training loss: 1.6733 0.9246 sec/batch\n",
      "Epoch 7/20  Iteration 1087/3560 Training loss: 1.6732 0.9135 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3560 Training loss: 1.6736 0.9475 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3560 Training loss: 1.6733 0.9570 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3560 Training loss: 1.6736 0.9918 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3560 Training loss: 1.6725 0.9780 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3560 Training loss: 1.6720 1.1845 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3560 Training loss: 1.6720 0.9978 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3560 Training loss: 1.6709 0.9212 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3560 Training loss: 1.6698 0.9606 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3560 Training loss: 1.6704 0.9196 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3560 Training loss: 1.6709 0.9761 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3560 Training loss: 1.6709 0.9258 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3560 Training loss: 1.6706 1.1565 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3560 Training loss: 1.6699 1.2149 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3560 Training loss: 1.6702 1.0023 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3560 Training loss: 1.6705 1.0583 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3560 Training loss: 1.6699 0.9252 sec/batch\n",
      "Epoch 7/20  Iteration 1104/3560 Training loss: 1.6694 1.0864 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3560 Training loss: 1.6689 0.9738 sec/batch\n",
      "Epoch 7/20  Iteration 1106/3560 Training loss: 1.6677 0.8827 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3560 Training loss: 1.6664 1.0434 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3560 Training loss: 1.6656 1.1521 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3560 Training loss: 1.6650 0.9652 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3560 Training loss: 1.6653 1.0267 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3560 Training loss: 1.6648 1.0263 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3560 Training loss: 1.6641 0.8806 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3560 Training loss: 1.6644 1.0302 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3560 Training loss: 1.6634 0.9958 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3560 Training loss: 1.6629 1.0663 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3560 Training loss: 1.6625 1.0506 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3560 Training loss: 1.6623 0.9788 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3560 Training loss: 1.6627 0.9638 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3560 Training loss: 1.6621 0.9705 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3560 Training loss: 1.6628 0.9533 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3560 Training loss: 1.6625 0.9571 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3560 Training loss: 1.6627 1.0179 sec/batch\n",
      "Epoch 7/20  Iteration 1123/3560 Training loss: 1.6623 1.0962 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3560 Training loss: 1.6624 0.9246 sec/batch\n",
      "Epoch 7/20  Iteration 1125/3560 Training loss: 1.6627 0.9852 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3560 Training loss: 1.6622 0.9413 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3560 Training loss: 1.6616 0.9640 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3560 Training loss: 1.6623 0.9234 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3560 Training loss: 1.6622 1.0322 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3560 Training loss: 1.6630 0.9001 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3560 Training loss: 1.6633 1.1026 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3560 Training loss: 1.6637 0.9613 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3560 Training loss: 1.6635 0.9529 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3560 Training loss: 1.6638 0.9953 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3560 Training loss: 1.6639 0.9171 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3560 Training loss: 1.6636 0.9457 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3560 Training loss: 1.6635 0.8985 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3560 Training loss: 1.6634 1.0046 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3560 Training loss: 1.6640 1.0103 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3560 Training loss: 1.6642 1.0518 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3560 Training loss: 1.6646 1.0133 sec/batch\n",
      "Epoch 7/20  Iteration 1142/3560 Training loss: 1.6643 1.0501 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1143/3560 Training loss: 1.6641 0.9928 sec/batch\n",
      "Epoch 7/20  Iteration 1144/3560 Training loss: 1.6642 0.9768 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3560 Training loss: 1.6640 0.9858 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3560 Training loss: 1.6640 1.0229 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3560 Training loss: 1.6634 1.0900 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3560 Training loss: 1.6632 0.9993 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3560 Training loss: 1.6627 0.9885 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3560 Training loss: 1.6628 0.8871 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3560 Training loss: 1.6621 0.9576 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3560 Training loss: 1.6620 1.0711 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3560 Training loss: 1.6615 1.0374 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3560 Training loss: 1.6612 1.1470 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3560 Training loss: 1.6609 1.1682 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3560 Training loss: 1.6606 0.9440 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3560 Training loss: 1.6601 0.9561 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3560 Training loss: 1.6601 0.9379 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3560 Training loss: 1.6597 0.9860 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3560 Training loss: 1.6594 0.8946 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3560 Training loss: 1.6590 0.9659 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3560 Training loss: 1.6586 1.1826 sec/batch\n",
      "Epoch 7/20  Iteration 1163/3560 Training loss: 1.6582 0.9938 sec/batch\n",
      "Epoch 7/20  Iteration 1164/3560 Training loss: 1.6581 0.9708 sec/batch\n",
      "Epoch 7/20  Iteration 1165/3560 Training loss: 1.6579 0.9022 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3560 Training loss: 1.6574 0.9389 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3560 Training loss: 1.6570 0.9448 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3560 Training loss: 1.6565 0.9851 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3560 Training loss: 1.6564 1.0503 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3560 Training loss: 1.6562 1.0665 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3560 Training loss: 1.6561 1.0909 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3560 Training loss: 1.6558 1.2744 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3560 Training loss: 1.6555 0.9490 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3560 Training loss: 1.6552 1.0726 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3560 Training loss: 1.6551 0.9089 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3560 Training loss: 1.6550 0.9156 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3560 Training loss: 1.6549 1.0620 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3560 Training loss: 1.6548 1.1712 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3560 Training loss: 1.6545 0.9474 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3560 Training loss: 1.6544 0.9559 sec/batch\n",
      "Epoch 7/20  Iteration 1181/3560 Training loss: 1.6542 1.0165 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3560 Training loss: 1.6539 0.9924 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3560 Training loss: 1.6535 0.9898 sec/batch\n",
      "Epoch 7/20  Iteration 1184/3560 Training loss: 1.6531 0.9513 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3560 Training loss: 1.6530 1.0787 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3560 Training loss: 1.6529 1.0356 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3560 Training loss: 1.6527 0.9909 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3560 Training loss: 1.6527 0.8986 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3560 Training loss: 1.6526 0.9504 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3560 Training loss: 1.6522 1.0237 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3560 Training loss: 1.6518 0.9573 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3560 Training loss: 1.6518 0.9862 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3560 Training loss: 1.6517 1.0689 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3560 Training loss: 1.6512 1.0548 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3560 Training loss: 1.6513 1.0264 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3560 Training loss: 1.6513 1.0212 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3560 Training loss: 1.6511 0.9806 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3560 Training loss: 1.6508 0.9406 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3560 Training loss: 1.6505 0.9777 sec/batch\n",
      "Epoch 7/20  Iteration 1200/3560 Training loss: 1.6501 1.0977 sec/batch\n",
      "Epoch 7/20  Iteration 1201/3560 Training loss: 1.6501 1.1669 sec/batch\n",
      "Epoch 7/20  Iteration 1202/3560 Training loss: 1.6500 1.0755 sec/batch\n",
      "Epoch 7/20  Iteration 1203/3560 Training loss: 1.6500 0.9536 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3560 Training loss: 1.6499 0.9152 sec/batch\n",
      "Epoch 7/20  Iteration 1205/3560 Training loss: 1.6499 0.8855 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3560 Training loss: 1.6499 0.9613 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3560 Training loss: 1.6499 0.8863 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3560 Training loss: 1.6498 1.0724 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3560 Training loss: 1.6500 1.0684 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3560 Training loss: 1.6499 0.9855 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3560 Training loss: 1.6498 0.9393 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3560 Training loss: 1.6498 0.9956 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3560 Training loss: 1.6496 0.9419 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3560 Training loss: 1.6496 0.9291 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3560 Training loss: 1.6495 0.9001 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3560 Training loss: 1.6496 0.9997 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3560 Training loss: 1.6496 1.0637 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3560 Training loss: 1.6494 1.0073 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3560 Training loss: 1.6490 1.0224 sec/batch\n",
      "Epoch 7/20  Iteration 1220/3560 Training loss: 1.6490 0.9468 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3560 Training loss: 1.6489 0.8507 sec/batch\n",
      "Epoch 7/20  Iteration 1222/3560 Training loss: 1.6489 1.0150 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3560 Training loss: 1.6488 0.9551 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3560 Training loss: 1.6488 1.1154 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3560 Training loss: 1.6488 1.0246 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3560 Training loss: 1.6487 0.8823 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3560 Training loss: 1.6483 0.9565 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3560 Training loss: 1.6484 0.9625 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3560 Training loss: 1.6485 0.9328 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3560 Training loss: 1.6484 0.9570 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3560 Training loss: 1.6484 1.0091 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3560 Training loss: 1.6483 1.1395 sec/batch\n",
      "Epoch 7/20  Iteration 1233/3560 Training loss: 1.6482 0.9791 sec/batch\n",
      "Epoch 7/20  Iteration 1234/3560 Training loss: 1.6481 0.9846 sec/batch\n",
      "Epoch 7/20  Iteration 1235/3560 Training loss: 1.6482 0.9154 sec/batch\n",
      "Epoch 7/20  Iteration 1236/3560 Training loss: 1.6485 0.9582 sec/batch\n",
      "Epoch 7/20  Iteration 1237/3560 Training loss: 1.6484 0.8880 sec/batch\n",
      "Epoch 7/20  Iteration 1238/3560 Training loss: 1.6484 0.9305 sec/batch\n",
      "Epoch 7/20  Iteration 1239/3560 Training loss: 1.6482 1.0727 sec/batch\n",
      "Epoch 7/20  Iteration 1240/3560 Training loss: 1.6479 1.0093 sec/batch\n",
      "Epoch 7/20  Iteration 1241/3560 Training loss: 1.6480 1.1051 sec/batch\n",
      "Epoch 7/20  Iteration 1242/3560 Training loss: 1.6479 0.9978 sec/batch\n",
      "Epoch 7/20  Iteration 1243/3560 Training loss: 1.6479 0.9427 sec/batch\n",
      "Epoch 7/20  Iteration 1244/3560 Training loss: 1.6477 0.9557 sec/batch\n",
      "Epoch 7/20  Iteration 1245/3560 Training loss: 1.6475 0.9771 sec/batch\n",
      "Epoch 7/20  Iteration 1246/3560 Training loss: 1.6475 0.9030 sec/batch\n",
      "Epoch 8/20  Iteration 1247/3560 Training loss: 1.7021 0.9829 sec/batch\n",
      "Epoch 8/20  Iteration 1248/3560 Training loss: 1.6660 1.0843 sec/batch\n",
      "Epoch 8/20  Iteration 1249/3560 Training loss: 1.6512 0.9665 sec/batch\n",
      "Epoch 8/20  Iteration 1250/3560 Training loss: 1.6460 1.0112 sec/batch\n",
      "Epoch 8/20  Iteration 1251/3560 Training loss: 1.6425 1.0009 sec/batch\n",
      "Epoch 8/20  Iteration 1252/3560 Training loss: 1.6323 0.9177 sec/batch\n",
      "Epoch 8/20  Iteration 1253/3560 Training loss: 1.6333 0.9840 sec/batch\n",
      "Epoch 8/20  Iteration 1254/3560 Training loss: 1.6317 0.9236 sec/batch\n",
      "Epoch 8/20  Iteration 1255/3560 Training loss: 1.6326 1.0080 sec/batch\n",
      "Epoch 8/20  Iteration 1256/3560 Training loss: 1.6315 1.0368 sec/batch\n",
      "Epoch 8/20  Iteration 1257/3560 Training loss: 1.6281 0.9806 sec/batch\n",
      "Epoch 8/20  Iteration 1258/3560 Training loss: 1.6267 0.9698 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1259/3560 Training loss: 1.6266 0.9465 sec/batch\n",
      "Epoch 8/20  Iteration 1260/3560 Training loss: 1.6282 0.9970 sec/batch\n",
      "Epoch 8/20  Iteration 1261/3560 Training loss: 1.6272 0.9919 sec/batch\n",
      "Epoch 8/20  Iteration 1262/3560 Training loss: 1.6251 0.9862 sec/batch\n",
      "Epoch 8/20  Iteration 1263/3560 Training loss: 1.6251 1.0462 sec/batch\n",
      "Epoch 8/20  Iteration 1264/3560 Training loss: 1.6271 1.0923 sec/batch\n",
      "Epoch 8/20  Iteration 1265/3560 Training loss: 1.6272 0.9659 sec/batch\n",
      "Epoch 8/20  Iteration 1266/3560 Training loss: 1.6282 0.9606 sec/batch\n",
      "Epoch 8/20  Iteration 1267/3560 Training loss: 1.6273 0.9758 sec/batch\n",
      "Epoch 8/20  Iteration 1268/3560 Training loss: 1.6275 0.9670 sec/batch\n",
      "Epoch 8/20  Iteration 1269/3560 Training loss: 1.6265 0.8998 sec/batch\n",
      "Epoch 8/20  Iteration 1270/3560 Training loss: 1.6257 0.9880 sec/batch\n",
      "Epoch 8/20  Iteration 1271/3560 Training loss: 1.6253 0.9357 sec/batch\n",
      "Epoch 8/20  Iteration 1272/3560 Training loss: 1.6241 1.1136 sec/batch\n",
      "Epoch 8/20  Iteration 1273/3560 Training loss: 1.6228 1.0634 sec/batch\n",
      "Epoch 8/20  Iteration 1274/3560 Training loss: 1.6232 0.9478 sec/batch\n",
      "Epoch 8/20  Iteration 1275/3560 Training loss: 1.6236 1.0156 sec/batch\n",
      "Epoch 8/20  Iteration 1276/3560 Training loss: 1.6240 0.9035 sec/batch\n",
      "Epoch 8/20  Iteration 1277/3560 Training loss: 1.6236 0.9435 sec/batch\n",
      "Epoch 8/20  Iteration 1278/3560 Training loss: 1.6224 0.8787 sec/batch\n",
      "Epoch 8/20  Iteration 1279/3560 Training loss: 1.6225 1.1138 sec/batch\n",
      "Epoch 8/20  Iteration 1280/3560 Training loss: 1.6225 1.0388 sec/batch\n",
      "Epoch 8/20  Iteration 1281/3560 Training loss: 1.6222 1.0782 sec/batch\n",
      "Epoch 8/20  Iteration 1282/3560 Training loss: 1.6220 1.0014 sec/batch\n",
      "Epoch 8/20  Iteration 1283/3560 Training loss: 1.6214 0.8855 sec/batch\n",
      "Epoch 8/20  Iteration 1284/3560 Training loss: 1.6204 0.9692 sec/batch\n",
      "Epoch 8/20  Iteration 1285/3560 Training loss: 1.6190 1.0649 sec/batch\n",
      "Epoch 8/20  Iteration 1286/3560 Training loss: 1.6184 1.0177 sec/batch\n",
      "Epoch 8/20  Iteration 1287/3560 Training loss: 1.6178 1.1253 sec/batch\n",
      "Epoch 8/20  Iteration 1288/3560 Training loss: 1.6182 1.0790 sec/batch\n",
      "Epoch 8/20  Iteration 1289/3560 Training loss: 1.6178 1.0634 sec/batch\n",
      "Epoch 8/20  Iteration 1290/3560 Training loss: 1.6171 1.0020 sec/batch\n",
      "Epoch 8/20  Iteration 1291/3560 Training loss: 1.6173 0.9284 sec/batch\n",
      "Epoch 8/20  Iteration 1292/3560 Training loss: 1.6163 1.0703 sec/batch\n",
      "Epoch 8/20  Iteration 1293/3560 Training loss: 1.6159 0.9565 sec/batch\n",
      "Epoch 8/20  Iteration 1294/3560 Training loss: 1.6152 0.9851 sec/batch\n",
      "Epoch 8/20  Iteration 1295/3560 Training loss: 1.6149 0.9314 sec/batch\n",
      "Epoch 8/20  Iteration 1296/3560 Training loss: 1.6153 1.1178 sec/batch\n",
      "Epoch 8/20  Iteration 1297/3560 Training loss: 1.6149 0.9714 sec/batch\n",
      "Epoch 8/20  Iteration 1298/3560 Training loss: 1.6155 0.9640 sec/batch\n",
      "Epoch 8/20  Iteration 1299/3560 Training loss: 1.6153 0.9538 sec/batch\n",
      "Epoch 8/20  Iteration 1300/3560 Training loss: 1.6156 0.9408 sec/batch\n",
      "Epoch 8/20  Iteration 1301/3560 Training loss: 1.6153 0.8921 sec/batch\n",
      "Epoch 8/20  Iteration 1302/3560 Training loss: 1.6154 0.9677 sec/batch\n",
      "Epoch 8/20  Iteration 1303/3560 Training loss: 1.6155 0.9736 sec/batch\n",
      "Epoch 8/20  Iteration 1304/3560 Training loss: 1.6151 1.1317 sec/batch\n",
      "Epoch 8/20  Iteration 1305/3560 Training loss: 1.6146 0.9850 sec/batch\n",
      "Epoch 8/20  Iteration 1306/3560 Training loss: 1.6150 0.9889 sec/batch\n",
      "Epoch 8/20  Iteration 1307/3560 Training loss: 1.6150 0.9493 sec/batch\n",
      "Epoch 8/20  Iteration 1308/3560 Training loss: 1.6159 0.9198 sec/batch\n",
      "Epoch 8/20  Iteration 1309/3560 Training loss: 1.6162 0.9395 sec/batch\n",
      "Epoch 8/20  Iteration 1310/3560 Training loss: 1.6164 1.0011 sec/batch\n",
      "Epoch 8/20  Iteration 1311/3560 Training loss: 1.6160 0.9695 sec/batch\n",
      "Epoch 8/20  Iteration 1312/3560 Training loss: 1.6163 1.0647 sec/batch\n",
      "Epoch 8/20  Iteration 1313/3560 Training loss: 1.6163 0.9988 sec/batch\n",
      "Epoch 8/20  Iteration 1314/3560 Training loss: 1.6160 0.9277 sec/batch\n",
      "Epoch 8/20  Iteration 1315/3560 Training loss: 1.6159 0.9941 sec/batch\n",
      "Epoch 8/20  Iteration 1316/3560 Training loss: 1.6158 0.9275 sec/batch\n",
      "Epoch 8/20  Iteration 1317/3560 Training loss: 1.6163 1.0413 sec/batch\n",
      "Epoch 8/20  Iteration 1318/3560 Training loss: 1.6166 0.8972 sec/batch\n",
      "Epoch 8/20  Iteration 1319/3560 Training loss: 1.6170 1.0655 sec/batch\n",
      "Epoch 8/20  Iteration 1320/3560 Training loss: 1.6166 1.0727 sec/batch\n",
      "Epoch 8/20  Iteration 1321/3560 Training loss: 1.6165 0.9702 sec/batch\n",
      "Epoch 8/20  Iteration 1322/3560 Training loss: 1.6167 0.9988 sec/batch\n",
      "Epoch 8/20  Iteration 1323/3560 Training loss: 1.6165 0.9416 sec/batch\n",
      "Epoch 8/20  Iteration 1324/3560 Training loss: 1.6165 0.9466 sec/batch\n",
      "Epoch 8/20  Iteration 1325/3560 Training loss: 1.6160 0.9952 sec/batch\n",
      "Epoch 8/20  Iteration 1326/3560 Training loss: 1.6159 0.8979 sec/batch\n",
      "Epoch 8/20  Iteration 1327/3560 Training loss: 1.6152 1.0661 sec/batch\n",
      "Epoch 8/20  Iteration 1328/3560 Training loss: 1.6153 1.0809 sec/batch\n",
      "Epoch 8/20  Iteration 1329/3560 Training loss: 1.6147 0.9624 sec/batch\n",
      "Epoch 8/20  Iteration 1330/3560 Training loss: 1.6147 0.9573 sec/batch\n",
      "Epoch 8/20  Iteration 1331/3560 Training loss: 1.6143 0.9327 sec/batch\n",
      "Epoch 8/20  Iteration 1332/3560 Training loss: 1.6139 0.8989 sec/batch\n",
      "Epoch 8/20  Iteration 1333/3560 Training loss: 1.6136 1.0582 sec/batch\n",
      "Epoch 8/20  Iteration 1334/3560 Training loss: 1.6133 0.9614 sec/batch\n",
      "Epoch 8/20  Iteration 1335/3560 Training loss: 1.6128 1.0710 sec/batch\n",
      "Epoch 8/20  Iteration 1336/3560 Training loss: 1.6128 1.0427 sec/batch\n",
      "Epoch 8/20  Iteration 1337/3560 Training loss: 1.6124 0.9129 sec/batch\n",
      "Epoch 8/20  Iteration 1338/3560 Training loss: 1.6122 0.9153 sec/batch\n",
      "Epoch 8/20  Iteration 1339/3560 Training loss: 1.6118 0.9036 sec/batch\n",
      "Epoch 8/20  Iteration 1340/3560 Training loss: 1.6115 0.9193 sec/batch\n",
      "Epoch 8/20  Iteration 1341/3560 Training loss: 1.6112 0.9616 sec/batch\n",
      "Epoch 8/20  Iteration 1342/3560 Training loss: 1.6112 1.0369 sec/batch\n",
      "Epoch 8/20  Iteration 1343/3560 Training loss: 1.6111 1.0505 sec/batch\n",
      "Epoch 8/20  Iteration 1344/3560 Training loss: 1.6106 1.0566 sec/batch\n",
      "Epoch 8/20  Iteration 1345/3560 Training loss: 1.6101 1.0155 sec/batch\n",
      "Epoch 8/20  Iteration 1346/3560 Training loss: 1.6096 0.9138 sec/batch\n",
      "Epoch 8/20  Iteration 1347/3560 Training loss: 1.6095 0.9264 sec/batch\n",
      "Epoch 8/20  Iteration 1348/3560 Training loss: 1.6093 0.9219 sec/batch\n",
      "Epoch 8/20  Iteration 1349/3560 Training loss: 1.6091 0.9110 sec/batch\n",
      "Epoch 8/20  Iteration 1350/3560 Training loss: 1.6088 1.0037 sec/batch\n",
      "Epoch 8/20  Iteration 1351/3560 Training loss: 1.6085 1.0901 sec/batch\n",
      "Epoch 8/20  Iteration 1352/3560 Training loss: 1.6083 0.9884 sec/batch\n",
      "Epoch 8/20  Iteration 1353/3560 Training loss: 1.6082 0.9673 sec/batch\n",
      "Epoch 8/20  Iteration 1354/3560 Training loss: 1.6082 0.9036 sec/batch\n",
      "Epoch 8/20  Iteration 1355/3560 Training loss: 1.6080 1.0135 sec/batch\n",
      "Epoch 8/20  Iteration 1356/3560 Training loss: 1.6078 0.9511 sec/batch\n",
      "Epoch 8/20  Iteration 1357/3560 Training loss: 1.6076 0.9948 sec/batch\n",
      "Epoch 8/20  Iteration 1358/3560 Training loss: 1.6075 0.9959 sec/batch\n",
      "Epoch 8/20  Iteration 1359/3560 Training loss: 1.6073 0.9587 sec/batch\n",
      "Epoch 8/20  Iteration 1360/3560 Training loss: 1.6071 0.9895 sec/batch\n",
      "Epoch 8/20  Iteration 1361/3560 Training loss: 1.6067 0.9110 sec/batch\n",
      "Epoch 8/20  Iteration 1362/3560 Training loss: 1.6063 0.9473 sec/batch\n",
      "Epoch 8/20  Iteration 1363/3560 Training loss: 1.6063 1.0056 sec/batch\n",
      "Epoch 8/20  Iteration 1364/3560 Training loss: 1.6062 0.9845 sec/batch\n",
      "Epoch 8/20  Iteration 1365/3560 Training loss: 1.6060 0.9320 sec/batch\n",
      "Epoch 8/20  Iteration 1366/3560 Training loss: 1.6058 1.1010 sec/batch\n",
      "Epoch 8/20  Iteration 1367/3560 Training loss: 1.6057 1.0199 sec/batch\n",
      "Epoch 8/20  Iteration 1368/3560 Training loss: 1.6053 1.0518 sec/batch\n",
      "Epoch 8/20  Iteration 1369/3560 Training loss: 1.6049 0.8947 sec/batch\n",
      "Epoch 8/20  Iteration 1370/3560 Training loss: 1.6049 0.9254 sec/batch\n",
      "Epoch 8/20  Iteration 1371/3560 Training loss: 1.6048 0.9186 sec/batch\n",
      "Epoch 8/20  Iteration 1372/3560 Training loss: 1.6044 1.0982 sec/batch\n",
      "Epoch 8/20  Iteration 1373/3560 Training loss: 1.6045 0.9554 sec/batch\n",
      "Epoch 8/20  Iteration 1374/3560 Training loss: 1.6045 0.9313 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1375/3560 Training loss: 1.6043 1.1564 sec/batch\n",
      "Epoch 8/20  Iteration 1376/3560 Training loss: 1.6041 1.0467 sec/batch\n",
      "Epoch 8/20  Iteration 1377/3560 Training loss: 1.6036 0.9130 sec/batch\n",
      "Epoch 8/20  Iteration 1378/3560 Training loss: 1.6033 0.9777 sec/batch\n",
      "Epoch 8/20  Iteration 1379/3560 Training loss: 1.6033 0.8412 sec/batch\n",
      "Epoch 8/20  Iteration 1380/3560 Training loss: 1.6033 0.9722 sec/batch\n",
      "Epoch 8/20  Iteration 1381/3560 Training loss: 1.6032 0.8742 sec/batch\n",
      "Epoch 8/20  Iteration 1382/3560 Training loss: 1.6032 1.0372 sec/batch\n",
      "Epoch 8/20  Iteration 1383/3560 Training loss: 1.6033 1.0447 sec/batch\n",
      "Epoch 8/20  Iteration 1384/3560 Training loss: 1.6032 1.0452 sec/batch\n",
      "Epoch 8/20  Iteration 1385/3560 Training loss: 1.6032 0.9921 sec/batch\n",
      "Epoch 8/20  Iteration 1386/3560 Training loss: 1.6030 0.9226 sec/batch\n",
      "Epoch 8/20  Iteration 1387/3560 Training loss: 1.6033 0.9292 sec/batch\n",
      "Epoch 8/20  Iteration 1388/3560 Training loss: 1.6032 0.9876 sec/batch\n",
      "Epoch 8/20  Iteration 1389/3560 Training loss: 1.6031 0.8807 sec/batch\n",
      "Epoch 8/20  Iteration 1390/3560 Training loss: 1.6032 0.9921 sec/batch\n",
      "Epoch 8/20  Iteration 1391/3560 Training loss: 1.6030 0.9774 sec/batch\n",
      "Epoch 8/20  Iteration 1392/3560 Training loss: 1.6030 1.0556 sec/batch\n",
      "Epoch 8/20  Iteration 1393/3560 Training loss: 1.6030 0.8821 sec/batch\n",
      "Epoch 8/20  Iteration 1394/3560 Training loss: 1.6031 0.9559 sec/batch\n",
      "Epoch 8/20  Iteration 1395/3560 Training loss: 1.6031 0.9731 sec/batch\n",
      "Epoch 8/20  Iteration 1396/3560 Training loss: 1.6029 0.9350 sec/batch\n",
      "Epoch 8/20  Iteration 1397/3560 Training loss: 1.6026 0.9052 sec/batch\n",
      "Epoch 8/20  Iteration 1398/3560 Training loss: 1.6025 1.0175 sec/batch\n",
      "Epoch 8/20  Iteration 1399/3560 Training loss: 1.6026 1.0922 sec/batch\n",
      "Epoch 8/20  Iteration 1400/3560 Training loss: 1.6026 1.0250 sec/batch\n",
      "Epoch 8/20  Iteration 1401/3560 Training loss: 1.6025 0.9893 sec/batch\n",
      "Epoch 8/20  Iteration 1402/3560 Training loss: 1.6025 0.9592 sec/batch\n",
      "Epoch 8/20  Iteration 1403/3560 Training loss: 1.6025 0.8879 sec/batch\n",
      "Epoch 8/20  Iteration 1404/3560 Training loss: 1.6024 0.8917 sec/batch\n",
      "Epoch 8/20  Iteration 1405/3560 Training loss: 1.6021 1.0517 sec/batch\n",
      "Epoch 8/20  Iteration 1406/3560 Training loss: 1.6021 0.9884 sec/batch\n",
      "Epoch 8/20  Iteration 1407/3560 Training loss: 1.6022 1.1561 sec/batch\n",
      "Epoch 8/20  Iteration 1408/3560 Training loss: 1.6021 0.9819 sec/batch\n",
      "Epoch 8/20  Iteration 1409/3560 Training loss: 1.6021 0.9934 sec/batch\n",
      "Epoch 8/20  Iteration 1410/3560 Training loss: 1.6021 0.9733 sec/batch\n",
      "Epoch 8/20  Iteration 1411/3560 Training loss: 1.6020 0.9195 sec/batch\n",
      "Epoch 8/20  Iteration 1412/3560 Training loss: 1.6018 0.9803 sec/batch\n",
      "Epoch 8/20  Iteration 1413/3560 Training loss: 1.6020 0.9653 sec/batch\n",
      "Epoch 8/20  Iteration 1414/3560 Training loss: 1.6023 1.1122 sec/batch\n",
      "Epoch 8/20  Iteration 1415/3560 Training loss: 1.6022 0.9791 sec/batch\n",
      "Epoch 8/20  Iteration 1416/3560 Training loss: 1.6022 1.0035 sec/batch\n",
      "Epoch 8/20  Iteration 1417/3560 Training loss: 1.6020 1.0006 sec/batch\n",
      "Epoch 8/20  Iteration 1418/3560 Training loss: 1.6018 0.9716 sec/batch\n",
      "Epoch 8/20  Iteration 1419/3560 Training loss: 1.6019 0.8383 sec/batch\n",
      "Epoch 8/20  Iteration 1420/3560 Training loss: 1.6019 0.9370 sec/batch\n",
      "Epoch 8/20  Iteration 1421/3560 Training loss: 1.6019 0.9404 sec/batch\n",
      "Epoch 8/20  Iteration 1422/3560 Training loss: 1.6017 1.1205 sec/batch\n",
      "Epoch 8/20  Iteration 1423/3560 Training loss: 1.6015 0.9862 sec/batch\n",
      "Epoch 8/20  Iteration 1424/3560 Training loss: 1.6016 0.9200 sec/batch\n",
      "Epoch 9/20  Iteration 1425/3560 Training loss: 1.6621 0.9910 sec/batch\n",
      "Epoch 9/20  Iteration 1426/3560 Training loss: 1.6226 0.9188 sec/batch\n",
      "Epoch 9/20  Iteration 1427/3560 Training loss: 1.6070 0.9893 sec/batch\n",
      "Epoch 9/20  Iteration 1428/3560 Training loss: 1.6047 0.9709 sec/batch\n",
      "Epoch 9/20  Iteration 1429/3560 Training loss: 1.5981 0.9861 sec/batch\n",
      "Epoch 9/20  Iteration 1430/3560 Training loss: 1.5880 1.0865 sec/batch\n",
      "Epoch 9/20  Iteration 1431/3560 Training loss: 1.5895 1.0741 sec/batch\n",
      "Epoch 9/20  Iteration 1432/3560 Training loss: 1.5877 0.9214 sec/batch\n",
      "Epoch 9/20  Iteration 1433/3560 Training loss: 1.5890 0.9456 sec/batch\n",
      "Epoch 9/20  Iteration 1434/3560 Training loss: 1.5878 0.9603 sec/batch\n",
      "Epoch 9/20  Iteration 1435/3560 Training loss: 1.5843 0.9074 sec/batch\n",
      "Epoch 9/20  Iteration 1436/3560 Training loss: 1.5825 0.9866 sec/batch\n",
      "Epoch 9/20  Iteration 1437/3560 Training loss: 1.5824 1.1178 sec/batch\n",
      "Epoch 9/20  Iteration 1438/3560 Training loss: 1.5844 1.0802 sec/batch\n",
      "Epoch 9/20  Iteration 1439/3560 Training loss: 1.5837 1.0123 sec/batch\n",
      "Epoch 9/20  Iteration 1440/3560 Training loss: 1.5821 0.9274 sec/batch\n",
      "Epoch 9/20  Iteration 1441/3560 Training loss: 1.5824 0.9069 sec/batch\n",
      "Epoch 9/20  Iteration 1442/3560 Training loss: 1.5839 0.9024 sec/batch\n",
      "Epoch 9/20  Iteration 1443/3560 Training loss: 1.5843 0.8845 sec/batch\n",
      "Epoch 9/20  Iteration 1444/3560 Training loss: 1.5853 1.0341 sec/batch\n",
      "Epoch 9/20  Iteration 1445/3560 Training loss: 1.5847 1.0413 sec/batch\n",
      "Epoch 9/20  Iteration 1446/3560 Training loss: 1.5849 1.0245 sec/batch\n",
      "Epoch 9/20  Iteration 1447/3560 Training loss: 1.5841 1.1244 sec/batch\n",
      "Epoch 9/20  Iteration 1448/3560 Training loss: 1.5837 0.9125 sec/batch\n",
      "Epoch 9/20  Iteration 1449/3560 Training loss: 1.5835 1.0111 sec/batch\n",
      "Epoch 9/20  Iteration 1450/3560 Training loss: 1.5823 0.9511 sec/batch\n",
      "Epoch 9/20  Iteration 1451/3560 Training loss: 1.5812 0.9390 sec/batch\n",
      "Epoch 9/20  Iteration 1452/3560 Training loss: 1.5818 1.0074 sec/batch\n",
      "Epoch 9/20  Iteration 1453/3560 Training loss: 1.5825 1.0748 sec/batch\n",
      "Epoch 9/20  Iteration 1454/3560 Training loss: 1.5827 1.0274 sec/batch\n",
      "Epoch 9/20  Iteration 1455/3560 Training loss: 1.5826 0.9767 sec/batch\n",
      "Epoch 9/20  Iteration 1456/3560 Training loss: 1.5815 1.0309 sec/batch\n",
      "Epoch 9/20  Iteration 1457/3560 Training loss: 1.5818 1.0319 sec/batch\n",
      "Epoch 9/20  Iteration 1458/3560 Training loss: 1.5819 0.8950 sec/batch\n",
      "Epoch 9/20  Iteration 1459/3560 Training loss: 1.5815 0.9750 sec/batch\n",
      "Epoch 9/20  Iteration 1460/3560 Training loss: 1.5814 0.9512 sec/batch\n",
      "Epoch 9/20  Iteration 1461/3560 Training loss: 1.5807 1.0147 sec/batch\n",
      "Epoch 9/20  Iteration 1462/3560 Training loss: 1.5796 1.0669 sec/batch\n",
      "Epoch 9/20  Iteration 1463/3560 Training loss: 1.5779 0.9833 sec/batch\n",
      "Epoch 9/20  Iteration 1464/3560 Training loss: 1.5774 0.9678 sec/batch\n",
      "Epoch 9/20  Iteration 1465/3560 Training loss: 1.5767 0.9353 sec/batch\n",
      "Epoch 9/20  Iteration 1466/3560 Training loss: 1.5773 1.0091 sec/batch\n",
      "Epoch 9/20  Iteration 1467/3560 Training loss: 1.5771 0.9003 sec/batch\n",
      "Epoch 9/20  Iteration 1468/3560 Training loss: 1.5764 0.9814 sec/batch\n",
      "Epoch 9/20  Iteration 1469/3560 Training loss: 1.5765 1.0628 sec/batch\n",
      "Epoch 9/20  Iteration 1470/3560 Training loss: 1.5754 1.0001 sec/batch\n",
      "Epoch 9/20  Iteration 1471/3560 Training loss: 1.5750 0.9786 sec/batch\n",
      "Epoch 9/20  Iteration 1472/3560 Training loss: 1.5746 0.9728 sec/batch\n",
      "Epoch 9/20  Iteration 1473/3560 Training loss: 1.5744 0.9898 sec/batch\n",
      "Epoch 9/20  Iteration 1474/3560 Training loss: 1.5749 0.8904 sec/batch\n",
      "Epoch 9/20  Iteration 1475/3560 Training loss: 1.5744 0.9497 sec/batch\n",
      "Epoch 9/20  Iteration 1476/3560 Training loss: 1.5750 0.9905 sec/batch\n",
      "Epoch 9/20  Iteration 1477/3560 Training loss: 1.5747 1.1856 sec/batch\n",
      "Epoch 9/20  Iteration 1478/3560 Training loss: 1.5749 1.0150 sec/batch\n",
      "Epoch 9/20  Iteration 1479/3560 Training loss: 1.5746 0.9923 sec/batch\n",
      "Epoch 9/20  Iteration 1480/3560 Training loss: 1.5746 0.9159 sec/batch\n",
      "Epoch 9/20  Iteration 1481/3560 Training loss: 1.5748 0.9984 sec/batch\n",
      "Epoch 9/20  Iteration 1482/3560 Training loss: 1.5743 0.9337 sec/batch\n",
      "Epoch 9/20  Iteration 1483/3560 Training loss: 1.5737 0.9165 sec/batch\n",
      "Epoch 9/20  Iteration 1484/3560 Training loss: 1.5743 1.0854 sec/batch\n",
      "Epoch 9/20  Iteration 1485/3560 Training loss: 1.5742 1.1043 sec/batch\n",
      "Epoch 9/20  Iteration 1486/3560 Training loss: 1.5750 0.9372 sec/batch\n",
      "Epoch 9/20  Iteration 1487/3560 Training loss: 1.5753 0.9658 sec/batch\n",
      "Epoch 9/20  Iteration 1488/3560 Training loss: 1.5753 0.9476 sec/batch\n",
      "Epoch 9/20  Iteration 1489/3560 Training loss: 1.5751 0.9036 sec/batch\n",
      "Epoch 9/20  Iteration 1490/3560 Training loss: 1.5755 0.9689 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1491/3560 Training loss: 1.5756 0.9304 sec/batch\n",
      "Epoch 9/20  Iteration 1492/3560 Training loss: 1.5753 1.0653 sec/batch\n",
      "Epoch 9/20  Iteration 1493/3560 Training loss: 1.5752 1.0660 sec/batch\n",
      "Epoch 9/20  Iteration 1494/3560 Training loss: 1.5749 0.9955 sec/batch\n",
      "Epoch 9/20  Iteration 1495/3560 Training loss: 1.5756 1.0613 sec/batch\n",
      "Epoch 9/20  Iteration 1496/3560 Training loss: 1.5758 1.0134 sec/batch\n",
      "Epoch 9/20  Iteration 1497/3560 Training loss: 1.5763 0.9805 sec/batch\n",
      "Epoch 9/20  Iteration 1498/3560 Training loss: 1.5761 0.9968 sec/batch\n",
      "Epoch 9/20  Iteration 1499/3560 Training loss: 1.5759 0.9168 sec/batch\n",
      "Epoch 9/20  Iteration 1500/3560 Training loss: 1.5761 1.0594 sec/batch\n",
      "Epoch 9/20  Iteration 1501/3560 Training loss: 1.5759 0.9628 sec/batch\n",
      "Epoch 9/20  Iteration 1502/3560 Training loss: 1.5760 0.8842 sec/batch\n",
      "Epoch 9/20  Iteration 1503/3560 Training loss: 1.5754 0.9768 sec/batch\n",
      "Epoch 9/20  Iteration 1504/3560 Training loss: 1.5752 0.9357 sec/batch\n",
      "Epoch 9/20  Iteration 1505/3560 Training loss: 1.5746 0.9541 sec/batch\n",
      "Epoch 9/20  Iteration 1506/3560 Training loss: 1.5745 1.0144 sec/batch\n",
      "Epoch 9/20  Iteration 1507/3560 Training loss: 1.5740 0.9221 sec/batch\n",
      "Epoch 9/20  Iteration 1508/3560 Training loss: 1.5740 1.1322 sec/batch\n",
      "Epoch 9/20  Iteration 1509/3560 Training loss: 1.5736 0.9374 sec/batch\n",
      "Epoch 9/20  Iteration 1510/3560 Training loss: 1.5733 0.9718 sec/batch\n",
      "Epoch 9/20  Iteration 1511/3560 Training loss: 1.5731 0.8513 sec/batch\n",
      "Epoch 9/20  Iteration 1512/3560 Training loss: 1.5728 0.9267 sec/batch\n",
      "Epoch 9/20  Iteration 1513/3560 Training loss: 1.5723 0.9826 sec/batch\n",
      "Epoch 9/20  Iteration 1514/3560 Training loss: 1.5723 0.8858 sec/batch\n",
      "Epoch 9/20  Iteration 1515/3560 Training loss: 1.5721 0.9083 sec/batch\n",
      "Epoch 9/20  Iteration 1516/3560 Training loss: 1.5718 1.1119 sec/batch\n",
      "Epoch 9/20  Iteration 1517/3560 Training loss: 1.5714 1.0490 sec/batch\n",
      "Epoch 9/20  Iteration 1518/3560 Training loss: 1.5712 0.9680 sec/batch\n",
      "Epoch 9/20  Iteration 1519/3560 Training loss: 1.5709 0.9527 sec/batch\n",
      "Epoch 9/20  Iteration 1520/3560 Training loss: 1.5710 0.9231 sec/batch\n",
      "Epoch 9/20  Iteration 1521/3560 Training loss: 1.5709 0.9425 sec/batch\n",
      "Epoch 9/20  Iteration 1522/3560 Training loss: 1.5705 0.9395 sec/batch\n",
      "Epoch 9/20  Iteration 1523/3560 Training loss: 1.5700 1.0567 sec/batch\n",
      "Epoch 9/20  Iteration 1524/3560 Training loss: 1.5695 1.0179 sec/batch\n",
      "Epoch 9/20  Iteration 1525/3560 Training loss: 1.5695 1.0402 sec/batch\n",
      "Epoch 9/20  Iteration 1526/3560 Training loss: 1.5693 0.9859 sec/batch\n",
      "Epoch 9/20  Iteration 1527/3560 Training loss: 1.5691 1.0368 sec/batch\n",
      "Epoch 9/20  Iteration 1528/3560 Training loss: 1.5689 1.0594 sec/batch\n",
      "Epoch 9/20  Iteration 1529/3560 Training loss: 1.5687 0.9037 sec/batch\n",
      "Epoch 9/20  Iteration 1530/3560 Training loss: 1.5685 0.9424 sec/batch\n",
      "Epoch 9/20  Iteration 1531/3560 Training loss: 1.5684 1.1097 sec/batch\n",
      "Epoch 9/20  Iteration 1532/3560 Training loss: 1.5684 1.1237 sec/batch\n",
      "Epoch 9/20  Iteration 1533/3560 Training loss: 1.5683 0.9660 sec/batch\n",
      "Epoch 9/20  Iteration 1534/3560 Training loss: 1.5684 0.9263 sec/batch\n",
      "Epoch 9/20  Iteration 1535/3560 Training loss: 1.5681 0.9548 sec/batch\n",
      "Epoch 9/20  Iteration 1536/3560 Training loss: 1.5679 0.9518 sec/batch\n",
      "Epoch 9/20  Iteration 1537/3560 Training loss: 1.5678 0.9432 sec/batch\n",
      "Epoch 9/20  Iteration 1538/3560 Training loss: 1.5676 1.0541 sec/batch\n",
      "Epoch 9/20  Iteration 1539/3560 Training loss: 1.5673 0.9828 sec/batch\n",
      "Epoch 9/20  Iteration 1540/3560 Training loss: 1.5670 1.0008 sec/batch\n",
      "Epoch 9/20  Iteration 1541/3560 Training loss: 1.5669 1.0086 sec/batch\n",
      "Epoch 9/20  Iteration 1542/3560 Training loss: 1.5669 0.9724 sec/batch\n",
      "Epoch 9/20  Iteration 1543/3560 Training loss: 1.5668 0.9831 sec/batch\n",
      "Epoch 9/20  Iteration 1544/3560 Training loss: 1.5666 0.9661 sec/batch\n",
      "Epoch 9/20  Iteration 1545/3560 Training loss: 1.5666 0.9263 sec/batch\n",
      "Epoch 9/20  Iteration 1546/3560 Training loss: 1.5662 0.9976 sec/batch\n",
      "Epoch 9/20  Iteration 1547/3560 Training loss: 1.5658 1.0845 sec/batch\n",
      "Epoch 9/20  Iteration 1548/3560 Training loss: 1.5658 0.9706 sec/batch\n",
      "Epoch 9/20  Iteration 1549/3560 Training loss: 1.5657 1.0602 sec/batch\n",
      "Epoch 9/20  Iteration 1550/3560 Training loss: 1.5653 0.9476 sec/batch\n",
      "Epoch 9/20  Iteration 1551/3560 Training loss: 1.5654 1.0300 sec/batch\n",
      "Epoch 9/20  Iteration 1552/3560 Training loss: 1.5654 0.8609 sec/batch\n",
      "Epoch 9/20  Iteration 1553/3560 Training loss: 1.5652 0.9455 sec/batch\n",
      "Epoch 9/20  Iteration 1554/3560 Training loss: 1.5649 1.0301 sec/batch\n",
      "Epoch 9/20  Iteration 1555/3560 Training loss: 1.5644 1.1064 sec/batch\n",
      "Epoch 9/20  Iteration 1556/3560 Training loss: 1.5640 0.8941 sec/batch\n",
      "Epoch 9/20  Iteration 1557/3560 Training loss: 1.5640 0.9499 sec/batch\n",
      "Epoch 9/20  Iteration 1558/3560 Training loss: 1.5640 0.9923 sec/batch\n",
      "Epoch 9/20  Iteration 1559/3560 Training loss: 1.5640 0.9093 sec/batch\n",
      "Epoch 9/20  Iteration 1560/3560 Training loss: 1.5639 0.9364 sec/batch\n",
      "Epoch 9/20  Iteration 1561/3560 Training loss: 1.5640 0.9635 sec/batch\n",
      "Epoch 9/20  Iteration 1562/3560 Training loss: 1.5640 1.0996 sec/batch\n",
      "Epoch 9/20  Iteration 1563/3560 Training loss: 1.5640 1.0383 sec/batch\n",
      "Epoch 9/20  Iteration 1564/3560 Training loss: 1.5639 0.9134 sec/batch\n",
      "Epoch 9/20  Iteration 1565/3560 Training loss: 1.5643 0.9654 sec/batch\n",
      "Epoch 9/20  Iteration 1566/3560 Training loss: 1.5642 1.0150 sec/batch\n",
      "Epoch 9/20  Iteration 1567/3560 Training loss: 1.5642 0.9601 sec/batch\n",
      "Epoch 9/20  Iteration 1568/3560 Training loss: 1.5643 0.9075 sec/batch\n",
      "Epoch 9/20  Iteration 1569/3560 Training loss: 1.5641 1.0113 sec/batch\n",
      "Epoch 9/20  Iteration 1570/3560 Training loss: 1.5642 0.9859 sec/batch\n",
      "Epoch 9/20  Iteration 1571/3560 Training loss: 1.5642 1.0370 sec/batch\n",
      "Epoch 9/20  Iteration 1572/3560 Training loss: 1.5643 0.9463 sec/batch\n",
      "Epoch 9/20  Iteration 1573/3560 Training loss: 1.5643 0.9499 sec/batch\n",
      "Epoch 9/20  Iteration 1574/3560 Training loss: 1.5641 0.9342 sec/batch\n",
      "Epoch 9/20  Iteration 1575/3560 Training loss: 1.5638 0.9553 sec/batch\n",
      "Epoch 9/20  Iteration 1576/3560 Training loss: 1.5637 0.9141 sec/batch\n",
      "Epoch 9/20  Iteration 1577/3560 Training loss: 1.5637 0.9997 sec/batch\n",
      "Epoch 9/20  Iteration 1578/3560 Training loss: 1.5638 1.0845 sec/batch\n",
      "Epoch 9/20  Iteration 1579/3560 Training loss: 1.5637 1.0338 sec/batch\n",
      "Epoch 9/20  Iteration 1580/3560 Training loss: 1.5637 1.0475 sec/batch\n",
      "Epoch 9/20  Iteration 1581/3560 Training loss: 1.5637 0.8866 sec/batch\n",
      "Epoch 9/20  Iteration 1582/3560 Training loss: 1.5636 0.9039 sec/batch\n",
      "Epoch 9/20  Iteration 1583/3560 Training loss: 1.5634 0.9179 sec/batch\n",
      "Epoch 9/20  Iteration 1584/3560 Training loss: 1.5635 0.9003 sec/batch\n",
      "Epoch 9/20  Iteration 1585/3560 Training loss: 1.5637 1.0232 sec/batch\n",
      "Epoch 9/20  Iteration 1586/3560 Training loss: 1.5636 1.0904 sec/batch\n",
      "Epoch 9/20  Iteration 1587/3560 Training loss: 1.5636 1.0372 sec/batch\n",
      "Epoch 9/20  Iteration 1588/3560 Training loss: 1.5636 0.8992 sec/batch\n",
      "Epoch 9/20  Iteration 1589/3560 Training loss: 1.5636 0.9824 sec/batch\n",
      "Epoch 9/20  Iteration 1590/3560 Training loss: 1.5635 0.9565 sec/batch\n",
      "Epoch 9/20  Iteration 1591/3560 Training loss: 1.5636 0.9004 sec/batch\n",
      "Epoch 9/20  Iteration 1592/3560 Training loss: 1.5639 0.9508 sec/batch\n",
      "Epoch 9/20  Iteration 1593/3560 Training loss: 1.5639 1.0426 sec/batch\n",
      "Epoch 9/20  Iteration 1594/3560 Training loss: 1.5638 1.0445 sec/batch\n",
      "Epoch 9/20  Iteration 1595/3560 Training loss: 1.5637 0.9692 sec/batch\n",
      "Epoch 9/20  Iteration 1596/3560 Training loss: 1.5636 1.0002 sec/batch\n",
      "Epoch 9/20  Iteration 1597/3560 Training loss: 1.5636 0.9501 sec/batch\n",
      "Epoch 9/20  Iteration 1598/3560 Training loss: 1.5636 0.9469 sec/batch\n",
      "Epoch 9/20  Iteration 1599/3560 Training loss: 1.5636 0.9532 sec/batch\n",
      "Epoch 9/20  Iteration 1600/3560 Training loss: 1.5634 1.0143 sec/batch\n",
      "Epoch 9/20  Iteration 1601/3560 Training loss: 1.5632 1.1827 sec/batch\n",
      "Epoch 9/20  Iteration 1602/3560 Training loss: 1.5633 1.0888 sec/batch\n",
      "Epoch 10/20  Iteration 1603/3560 Training loss: 1.6391 0.9424 sec/batch\n",
      "Epoch 10/20  Iteration 1604/3560 Training loss: 1.5945 0.9806 sec/batch\n",
      "Epoch 10/20  Iteration 1605/3560 Training loss: 1.5796 0.9544 sec/batch\n",
      "Epoch 10/20  Iteration 1606/3560 Training loss: 1.5744 0.9467 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1607/3560 Training loss: 1.5673 0.9398 sec/batch\n",
      "Epoch 10/20  Iteration 1608/3560 Training loss: 1.5571 1.0504 sec/batch\n",
      "Epoch 10/20  Iteration 1609/3560 Training loss: 1.5573 1.0213 sec/batch\n",
      "Epoch 10/20  Iteration 1610/3560 Training loss: 1.5544 1.0055 sec/batch\n",
      "Epoch 10/20  Iteration 1611/3560 Training loss: 1.5545 1.0148 sec/batch\n",
      "Epoch 10/20  Iteration 1612/3560 Training loss: 1.5533 0.8920 sec/batch\n",
      "Epoch 10/20  Iteration 1613/3560 Training loss: 1.5500 0.9039 sec/batch\n",
      "Epoch 10/20  Iteration 1614/3560 Training loss: 1.5484 1.0090 sec/batch\n",
      "Epoch 10/20  Iteration 1615/3560 Training loss: 1.5481 0.8672 sec/batch\n",
      "Epoch 10/20  Iteration 1616/3560 Training loss: 1.5496 0.9611 sec/batch\n",
      "Epoch 10/20  Iteration 1617/3560 Training loss: 1.5493 1.1106 sec/batch\n",
      "Epoch 10/20  Iteration 1618/3560 Training loss: 1.5477 1.0220 sec/batch\n",
      "Epoch 10/20  Iteration 1619/3560 Training loss: 1.5479 0.9165 sec/batch\n",
      "Epoch 10/20  Iteration 1620/3560 Training loss: 1.5495 1.0558 sec/batch\n",
      "Epoch 10/20  Iteration 1621/3560 Training loss: 1.5497 0.9781 sec/batch\n",
      "Epoch 10/20  Iteration 1622/3560 Training loss: 1.5508 0.9308 sec/batch\n",
      "Epoch 10/20  Iteration 1623/3560 Training loss: 1.5496 0.9786 sec/batch\n",
      "Epoch 10/20  Iteration 1624/3560 Training loss: 1.5499 0.9360 sec/batch\n",
      "Epoch 10/20  Iteration 1625/3560 Training loss: 1.5487 1.0712 sec/batch\n",
      "Epoch 10/20  Iteration 1626/3560 Training loss: 1.5485 1.0408 sec/batch\n",
      "Epoch 10/20  Iteration 1627/3560 Training loss: 1.5484 0.9028 sec/batch\n",
      "Epoch 10/20  Iteration 1628/3560 Training loss: 1.5469 0.9567 sec/batch\n",
      "Epoch 10/20  Iteration 1629/3560 Training loss: 1.5459 0.9582 sec/batch\n",
      "Epoch 10/20  Iteration 1630/3560 Training loss: 1.5466 0.9982 sec/batch\n",
      "Epoch 10/20  Iteration 1631/3560 Training loss: 1.5471 0.9303 sec/batch\n",
      "Epoch 10/20  Iteration 1632/3560 Training loss: 1.5472 0.9761 sec/batch\n",
      "Epoch 10/20  Iteration 1633/3560 Training loss: 1.5469 1.1433 sec/batch\n",
      "Epoch 10/20  Iteration 1634/3560 Training loss: 1.5461 0.9354 sec/batch\n",
      "Epoch 10/20  Iteration 1635/3560 Training loss: 1.5464 0.9562 sec/batch\n",
      "Epoch 10/20  Iteration 1636/3560 Training loss: 1.5467 0.9949 sec/batch\n",
      "Epoch 10/20  Iteration 1637/3560 Training loss: 1.5467 0.8875 sec/batch\n",
      "Epoch 10/20  Iteration 1638/3560 Training loss: 1.5463 0.9633 sec/batch\n",
      "Epoch 10/20  Iteration 1639/3560 Training loss: 1.5455 1.0433 sec/batch\n",
      "Epoch 10/20  Iteration 1640/3560 Training loss: 1.5444 1.0278 sec/batch\n",
      "Epoch 10/20  Iteration 1641/3560 Training loss: 1.5430 1.0395 sec/batch\n",
      "Epoch 10/20  Iteration 1642/3560 Training loss: 1.5427 1.0317 sec/batch\n",
      "Epoch 10/20  Iteration 1643/3560 Training loss: 1.5421 0.9558 sec/batch\n",
      "Epoch 10/20  Iteration 1644/3560 Training loss: 1.5427 1.1247 sec/batch\n",
      "Epoch 10/20  Iteration 1645/3560 Training loss: 1.5425 0.9337 sec/batch\n",
      "Epoch 10/20  Iteration 1646/3560 Training loss: 1.5418 0.9721 sec/batch\n",
      "Epoch 10/20  Iteration 1647/3560 Training loss: 1.5421 0.9931 sec/batch\n",
      "Epoch 10/20  Iteration 1648/3560 Training loss: 1.5409 1.0736 sec/batch\n",
      "Epoch 10/20  Iteration 1649/3560 Training loss: 1.5405 0.9413 sec/batch\n",
      "Epoch 10/20  Iteration 1650/3560 Training loss: 1.5400 0.9610 sec/batch\n",
      "Epoch 10/20  Iteration 1651/3560 Training loss: 1.5399 0.9524 sec/batch\n",
      "Epoch 10/20  Iteration 1652/3560 Training loss: 1.5404 0.9455 sec/batch\n",
      "Epoch 10/20  Iteration 1653/3560 Training loss: 1.5399 1.0495 sec/batch\n",
      "Epoch 10/20  Iteration 1654/3560 Training loss: 1.5407 0.9330 sec/batch\n",
      "Epoch 10/20  Iteration 1655/3560 Training loss: 1.5405 1.0493 sec/batch\n",
      "Epoch 10/20  Iteration 1656/3560 Training loss: 1.5408 1.1122 sec/batch\n",
      "Epoch 10/20  Iteration 1657/3560 Training loss: 1.5406 1.0621 sec/batch\n",
      "Epoch 10/20  Iteration 1658/3560 Training loss: 1.5406 0.9786 sec/batch\n",
      "Epoch 10/20  Iteration 1659/3560 Training loss: 1.5411 0.9128 sec/batch\n",
      "Epoch 10/20  Iteration 1660/3560 Training loss: 1.5406 1.1229 sec/batch\n",
      "Epoch 10/20  Iteration 1661/3560 Training loss: 1.5400 0.9951 sec/batch\n",
      "Epoch 10/20  Iteration 1662/3560 Training loss: 1.5405 0.9437 sec/batch\n",
      "Epoch 10/20  Iteration 1663/3560 Training loss: 1.5404 1.2349 sec/batch\n",
      "Epoch 10/20  Iteration 1664/3560 Training loss: 1.5412 1.0412 sec/batch\n",
      "Epoch 10/20  Iteration 1665/3560 Training loss: 1.5416 0.9394 sec/batch\n",
      "Epoch 10/20  Iteration 1666/3560 Training loss: 1.5417 0.9639 sec/batch\n",
      "Epoch 10/20  Iteration 1667/3560 Training loss: 1.5416 0.9754 sec/batch\n",
      "Epoch 10/20  Iteration 1668/3560 Training loss: 1.5419 0.9689 sec/batch\n",
      "Epoch 10/20  Iteration 1669/3560 Training loss: 1.5420 0.9785 sec/batch\n",
      "Epoch 10/20  Iteration 1670/3560 Training loss: 1.5417 0.9949 sec/batch\n",
      "Epoch 10/20  Iteration 1671/3560 Training loss: 1.5416 0.9781 sec/batch\n",
      "Epoch 10/20  Iteration 1672/3560 Training loss: 1.5413 1.0904 sec/batch\n",
      "Epoch 10/20  Iteration 1673/3560 Training loss: 1.5420 0.9892 sec/batch\n",
      "Epoch 10/20  Iteration 1674/3560 Training loss: 1.5423 0.9377 sec/batch\n",
      "Epoch 10/20  Iteration 1675/3560 Training loss: 1.5427 1.0413 sec/batch\n",
      "Epoch 10/20  Iteration 1676/3560 Training loss: 1.5424 0.9111 sec/batch\n",
      "Epoch 10/20  Iteration 1677/3560 Training loss: 1.5422 1.0176 sec/batch\n",
      "Epoch 10/20  Iteration 1678/3560 Training loss: 1.5425 1.1112 sec/batch\n",
      "Epoch 10/20  Iteration 1679/3560 Training loss: 1.5422 1.0110 sec/batch\n",
      "Epoch 10/20  Iteration 1680/3560 Training loss: 1.5423 1.0422 sec/batch\n",
      "Epoch 10/20  Iteration 1681/3560 Training loss: 1.5418 0.9991 sec/batch\n",
      "Epoch 10/20  Iteration 1682/3560 Training loss: 1.5418 0.9398 sec/batch\n",
      "Epoch 10/20  Iteration 1683/3560 Training loss: 1.5411 1.0229 sec/batch\n",
      "Epoch 10/20  Iteration 1684/3560 Training loss: 1.5410 0.9556 sec/batch\n",
      "Epoch 10/20  Iteration 1685/3560 Training loss: 1.5406 0.9955 sec/batch\n",
      "Epoch 10/20  Iteration 1686/3560 Training loss: 1.5407 0.9550 sec/batch\n",
      "Epoch 10/20  Iteration 1687/3560 Training loss: 1.5403 1.0123 sec/batch\n",
      "Epoch 10/20  Iteration 1688/3560 Training loss: 1.5400 0.9957 sec/batch\n",
      "Epoch 10/20  Iteration 1689/3560 Training loss: 1.5397 0.9089 sec/batch\n",
      "Epoch 10/20  Iteration 1690/3560 Training loss: 1.5393 1.0645 sec/batch\n",
      "Epoch 10/20  Iteration 1691/3560 Training loss: 1.5390 0.9797 sec/batch\n",
      "Epoch 10/20  Iteration 1692/3560 Training loss: 1.5391 1.0018 sec/batch\n",
      "Epoch 10/20  Iteration 1693/3560 Training loss: 1.5388 0.9100 sec/batch\n",
      "Epoch 10/20  Iteration 1694/3560 Training loss: 1.5386 0.9220 sec/batch\n",
      "Epoch 10/20  Iteration 1695/3560 Training loss: 1.5381 0.9935 sec/batch\n",
      "Epoch 10/20  Iteration 1696/3560 Training loss: 1.5379 0.9639 sec/batch\n",
      "Epoch 10/20  Iteration 1697/3560 Training loss: 1.5378 1.0098 sec/batch\n",
      "Epoch 10/20  Iteration 1698/3560 Training loss: 1.5378 1.0256 sec/batch\n",
      "Epoch 10/20  Iteration 1699/3560 Training loss: 1.5377 0.9280 sec/batch\n",
      "Epoch 10/20  Iteration 1700/3560 Training loss: 1.5372 0.9937 sec/batch\n",
      "Epoch 10/20  Iteration 1701/3560 Training loss: 1.5369 1.0126 sec/batch\n",
      "Epoch 10/20  Iteration 1702/3560 Training loss: 1.5365 0.9088 sec/batch\n",
      "Epoch 10/20  Iteration 1703/3560 Training loss: 1.5365 1.0348 sec/batch\n",
      "Epoch 10/20  Iteration 1704/3560 Training loss: 1.5362 0.9402 sec/batch\n",
      "Epoch 10/20  Iteration 1705/3560 Training loss: 1.5361 1.0501 sec/batch\n",
      "Epoch 10/20  Iteration 1706/3560 Training loss: 1.5360 0.9575 sec/batch\n",
      "Epoch 10/20  Iteration 1707/3560 Training loss: 1.5358 0.9676 sec/batch\n",
      "Epoch 10/20  Iteration 1708/3560 Training loss: 1.5357 0.9666 sec/batch\n",
      "Epoch 10/20  Iteration 1709/3560 Training loss: 1.5356 0.9950 sec/batch\n",
      "Epoch 10/20  Iteration 1710/3560 Training loss: 1.5355 0.9709 sec/batch\n",
      "Epoch 10/20  Iteration 1711/3560 Training loss: 1.5354 1.0533 sec/batch\n",
      "Epoch 10/20  Iteration 1712/3560 Training loss: 1.5354 1.0121 sec/batch\n",
      "Epoch 10/20  Iteration 1713/3560 Training loss: 1.5352 0.9312 sec/batch\n",
      "Epoch 10/20  Iteration 1714/3560 Training loss: 1.5351 1.0107 sec/batch\n",
      "Epoch 10/20  Iteration 1715/3560 Training loss: 1.5350 0.9414 sec/batch\n",
      "Epoch 10/20  Iteration 1716/3560 Training loss: 1.5347 0.9168 sec/batch\n",
      "Epoch 10/20  Iteration 1717/3560 Training loss: 1.5345 0.8793 sec/batch\n",
      "Epoch 10/20  Iteration 1718/3560 Training loss: 1.5342 0.9182 sec/batch\n",
      "Epoch 10/20  Iteration 1719/3560 Training loss: 1.5342 0.9733 sec/batch\n",
      "Epoch 10/20  Iteration 1720/3560 Training loss: 1.5341 1.0270 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1721/3560 Training loss: 1.5340 1.0483 sec/batch\n",
      "Epoch 10/20  Iteration 1722/3560 Training loss: 1.5339 0.9650 sec/batch\n",
      "Epoch 10/20  Iteration 1723/3560 Training loss: 1.5339 0.9742 sec/batch\n",
      "Epoch 10/20  Iteration 1724/3560 Training loss: 1.5335 0.9162 sec/batch\n",
      "Epoch 10/20  Iteration 1725/3560 Training loss: 1.5331 0.9099 sec/batch\n",
      "Epoch 10/20  Iteration 1726/3560 Training loss: 1.5331 0.9320 sec/batch\n",
      "Epoch 10/20  Iteration 1727/3560 Training loss: 1.5330 1.0388 sec/batch\n",
      "Epoch 10/20  Iteration 1728/3560 Training loss: 1.5326 0.8964 sec/batch\n",
      "Epoch 10/20  Iteration 1729/3560 Training loss: 1.5326 0.9779 sec/batch\n",
      "Epoch 10/20  Iteration 1730/3560 Training loss: 1.5327 1.0080 sec/batch\n",
      "Epoch 10/20  Iteration 1731/3560 Training loss: 1.5325 0.9380 sec/batch\n",
      "Epoch 10/20  Iteration 1732/3560 Training loss: 1.5323 1.0120 sec/batch\n",
      "Epoch 10/20  Iteration 1733/3560 Training loss: 1.5318 0.8263 sec/batch\n",
      "Epoch 10/20  Iteration 1734/3560 Training loss: 1.5316 0.9765 sec/batch\n",
      "Epoch 10/20  Iteration 1735/3560 Training loss: 1.5317 0.9975 sec/batch\n",
      "Epoch 10/20  Iteration 1736/3560 Training loss: 1.5317 1.0162 sec/batch\n",
      "Epoch 10/20  Iteration 1737/3560 Training loss: 1.5316 0.9787 sec/batch\n",
      "Epoch 10/20  Iteration 1738/3560 Training loss: 1.5317 1.0269 sec/batch\n",
      "Epoch 10/20  Iteration 1739/3560 Training loss: 1.5318 0.9197 sec/batch\n",
      "Epoch 10/20  Iteration 1740/3560 Training loss: 1.5318 0.9661 sec/batch\n",
      "Epoch 10/20  Iteration 1741/3560 Training loss: 1.5318 0.9480 sec/batch\n",
      "Epoch 10/20  Iteration 1742/3560 Training loss: 1.5317 1.0250 sec/batch\n",
      "Epoch 10/20  Iteration 1743/3560 Training loss: 1.5320 0.9987 sec/batch\n",
      "Epoch 10/20  Iteration 1744/3560 Training loss: 1.5320 1.0259 sec/batch\n",
      "Epoch 10/20  Iteration 1745/3560 Training loss: 1.5319 1.0446 sec/batch\n",
      "Epoch 10/20  Iteration 1746/3560 Training loss: 1.5321 0.9578 sec/batch\n",
      "Epoch 10/20  Iteration 1747/3560 Training loss: 1.5320 0.9337 sec/batch\n",
      "Epoch 10/20  Iteration 1748/3560 Training loss: 1.5321 0.9080 sec/batch\n",
      "Epoch 10/20  Iteration 1749/3560 Training loss: 1.5321 1.0145 sec/batch\n",
      "Epoch 10/20  Iteration 1750/3560 Training loss: 1.5323 0.9837 sec/batch\n",
      "Epoch 10/20  Iteration 1751/3560 Training loss: 1.5324 1.1729 sec/batch\n",
      "Epoch 10/20  Iteration 1752/3560 Training loss: 1.5322 1.1222 sec/batch\n",
      "Epoch 10/20  Iteration 1753/3560 Training loss: 1.5319 1.0643 sec/batch\n",
      "Epoch 10/20  Iteration 1754/3560 Training loss: 1.5318 1.0016 sec/batch\n",
      "Epoch 10/20  Iteration 1755/3560 Training loss: 1.5318 1.0114 sec/batch\n",
      "Epoch 10/20  Iteration 1756/3560 Training loss: 1.5319 1.0221 sec/batch\n",
      "Epoch 10/20  Iteration 1757/3560 Training loss: 1.5318 0.8652 sec/batch\n",
      "Epoch 10/20  Iteration 1758/3560 Training loss: 1.5319 1.0395 sec/batch\n",
      "Epoch 10/20  Iteration 1759/3560 Training loss: 1.5319 0.9356 sec/batch\n",
      "Epoch 10/20  Iteration 1760/3560 Training loss: 1.5319 1.0063 sec/batch\n",
      "Epoch 10/20  Iteration 1761/3560 Training loss: 1.5317 0.9949 sec/batch\n",
      "Epoch 10/20  Iteration 1762/3560 Training loss: 1.5317 1.0128 sec/batch\n",
      "Epoch 10/20  Iteration 1763/3560 Training loss: 1.5319 0.9961 sec/batch\n",
      "Epoch 10/20  Iteration 1764/3560 Training loss: 1.5319 0.9256 sec/batch\n",
      "Epoch 10/20  Iteration 1765/3560 Training loss: 1.5319 0.9296 sec/batch\n",
      "Epoch 10/20  Iteration 1766/3560 Training loss: 1.5318 1.0729 sec/batch\n",
      "Epoch 10/20  Iteration 1767/3560 Training loss: 1.5319 0.9477 sec/batch\n",
      "Epoch 10/20  Iteration 1768/3560 Training loss: 1.5318 1.0313 sec/batch\n",
      "Epoch 10/20  Iteration 1769/3560 Training loss: 1.5319 1.0509 sec/batch\n",
      "Epoch 10/20  Iteration 1770/3560 Training loss: 1.5323 0.9579 sec/batch\n",
      "Epoch 10/20  Iteration 1771/3560 Training loss: 1.5323 1.1153 sec/batch\n",
      "Epoch 10/20  Iteration 1772/3560 Training loss: 1.5323 1.0056 sec/batch\n",
      "Epoch 10/20  Iteration 1773/3560 Training loss: 1.5323 0.8957 sec/batch\n",
      "Epoch 10/20  Iteration 1774/3560 Training loss: 1.5321 1.0604 sec/batch\n",
      "Epoch 10/20  Iteration 1775/3560 Training loss: 1.5322 0.9506 sec/batch\n",
      "Epoch 10/20  Iteration 1776/3560 Training loss: 1.5322 1.0572 sec/batch\n",
      "Epoch 10/20  Iteration 1777/3560 Training loss: 1.5322 1.0389 sec/batch\n",
      "Epoch 10/20  Iteration 1778/3560 Training loss: 1.5321 1.0258 sec/batch\n",
      "Epoch 10/20  Iteration 1779/3560 Training loss: 1.5320 1.0407 sec/batch\n",
      "Epoch 10/20  Iteration 1780/3560 Training loss: 1.5321 0.9559 sec/batch\n",
      "Epoch 11/20  Iteration 1781/3560 Training loss: 1.5984 0.9863 sec/batch\n",
      "Epoch 11/20  Iteration 1782/3560 Training loss: 1.5621 1.0243 sec/batch\n",
      "Epoch 11/20  Iteration 1783/3560 Training loss: 1.5487 1.1102 sec/batch\n",
      "Epoch 11/20  Iteration 1784/3560 Training loss: 1.5453 0.9419 sec/batch\n",
      "Epoch 11/20  Iteration 1785/3560 Training loss: 1.5390 1.0082 sec/batch\n",
      "Epoch 11/20  Iteration 1786/3560 Training loss: 1.5293 1.0571 sec/batch\n",
      "Epoch 11/20  Iteration 1787/3560 Training loss: 1.5300 0.9162 sec/batch\n",
      "Epoch 11/20  Iteration 1788/3560 Training loss: 1.5272 0.9580 sec/batch\n",
      "Epoch 11/20  Iteration 1789/3560 Training loss: 1.5275 0.9568 sec/batch\n",
      "Epoch 11/20  Iteration 1790/3560 Training loss: 1.5261 1.2041 sec/batch\n",
      "Epoch 11/20  Iteration 1791/3560 Training loss: 1.5224 0.9740 sec/batch\n",
      "Epoch 11/20  Iteration 1792/3560 Training loss: 1.5216 0.9652 sec/batch\n",
      "Epoch 11/20  Iteration 1793/3560 Training loss: 1.5218 0.9539 sec/batch\n",
      "Epoch 11/20  Iteration 1794/3560 Training loss: 1.5233 0.9769 sec/batch\n",
      "Epoch 11/20  Iteration 1795/3560 Training loss: 1.5222 0.9181 sec/batch\n",
      "Epoch 11/20  Iteration 1796/3560 Training loss: 1.5206 0.9384 sec/batch\n",
      "Epoch 11/20  Iteration 1797/3560 Training loss: 1.5206 0.9258 sec/batch\n",
      "Epoch 11/20  Iteration 1798/3560 Training loss: 1.5220 0.9702 sec/batch\n",
      "Epoch 11/20  Iteration 1799/3560 Training loss: 1.5222 1.0076 sec/batch\n",
      "Epoch 11/20  Iteration 1800/3560 Training loss: 1.5229 1.0122 sec/batch\n",
      "Epoch 11/20  Iteration 1801/3560 Training loss: 1.5219 0.9940 sec/batch\n",
      "Epoch 11/20  Iteration 1802/3560 Training loss: 1.5221 1.0118 sec/batch\n",
      "Epoch 11/20  Iteration 1803/3560 Training loss: 1.5213 0.9188 sec/batch\n",
      "Epoch 11/20  Iteration 1804/3560 Training loss: 1.5209 0.9756 sec/batch\n",
      "Epoch 11/20  Iteration 1805/3560 Training loss: 1.5209 0.8836 sec/batch\n",
      "Epoch 11/20  Iteration 1806/3560 Training loss: 1.5191 1.0279 sec/batch\n",
      "Epoch 11/20  Iteration 1807/3560 Training loss: 1.5183 1.0166 sec/batch\n",
      "Epoch 11/20  Iteration 1808/3560 Training loss: 1.5189 1.0142 sec/batch\n",
      "Epoch 11/20  Iteration 1809/3560 Training loss: 1.5192 0.9759 sec/batch\n",
      "Epoch 11/20  Iteration 1810/3560 Training loss: 1.5194 0.9223 sec/batch\n",
      "Epoch 11/20  Iteration 1811/3560 Training loss: 1.5191 0.8963 sec/batch\n",
      "Epoch 11/20  Iteration 1812/3560 Training loss: 1.5180 0.9996 sec/batch\n",
      "Epoch 11/20  Iteration 1813/3560 Training loss: 1.5183 0.9083 sec/batch\n",
      "Epoch 11/20  Iteration 1814/3560 Training loss: 1.5186 1.0512 sec/batch\n",
      "Epoch 11/20  Iteration 1815/3560 Training loss: 1.5181 0.9305 sec/batch\n",
      "Epoch 11/20  Iteration 1816/3560 Training loss: 1.5179 1.0353 sec/batch\n",
      "Epoch 11/20  Iteration 1817/3560 Training loss: 1.5176 0.9372 sec/batch\n",
      "Epoch 11/20  Iteration 1818/3560 Training loss: 1.5165 0.9464 sec/batch\n",
      "Epoch 11/20  Iteration 1819/3560 Training loss: 1.5150 0.9036 sec/batch\n",
      "Epoch 11/20  Iteration 1820/3560 Training loss: 1.5144 0.9955 sec/batch\n",
      "Epoch 11/20  Iteration 1821/3560 Training loss: 1.5138 0.9842 sec/batch\n",
      "Epoch 11/20  Iteration 1822/3560 Training loss: 1.5144 1.0074 sec/batch\n",
      "Epoch 11/20  Iteration 1823/3560 Training loss: 1.5141 0.9882 sec/batch\n",
      "Epoch 11/20  Iteration 1824/3560 Training loss: 1.5136 0.9868 sec/batch\n",
      "Epoch 11/20  Iteration 1825/3560 Training loss: 1.5139 1.0684 sec/batch\n",
      "Epoch 11/20  Iteration 1826/3560 Training loss: 1.5128 0.9294 sec/batch\n",
      "Epoch 11/20  Iteration 1827/3560 Training loss: 1.5126 0.8638 sec/batch\n",
      "Epoch 11/20  Iteration 1828/3560 Training loss: 1.5122 0.9538 sec/batch\n",
      "Epoch 11/20  Iteration 1829/3560 Training loss: 1.5121 0.9313 sec/batch\n",
      "Epoch 11/20  Iteration 1830/3560 Training loss: 1.5126 1.0585 sec/batch\n",
      "Epoch 11/20  Iteration 1831/3560 Training loss: 1.5121 1.0494 sec/batch\n",
      "Epoch 11/20  Iteration 1832/3560 Training loss: 1.5129 1.0541 sec/batch\n",
      "Epoch 11/20  Iteration 1833/3560 Training loss: 1.5126 0.9651 sec/batch\n",
      "Epoch 11/20  Iteration 1834/3560 Training loss: 1.5130 1.0145 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1835/3560 Training loss: 1.5126 0.9197 sec/batch\n",
      "Epoch 11/20  Iteration 1836/3560 Training loss: 1.5129 0.9304 sec/batch\n",
      "Epoch 11/20  Iteration 1837/3560 Training loss: 1.5133 1.0247 sec/batch\n",
      "Epoch 11/20  Iteration 1838/3560 Training loss: 1.5128 0.9650 sec/batch\n",
      "Epoch 11/20  Iteration 1839/3560 Training loss: 1.5122 0.9500 sec/batch\n",
      "Epoch 11/20  Iteration 1840/3560 Training loss: 1.5128 1.0452 sec/batch\n",
      "Epoch 11/20  Iteration 1841/3560 Training loss: 1.5129 0.9237 sec/batch\n",
      "Epoch 11/20  Iteration 1842/3560 Training loss: 1.5138 0.9753 sec/batch\n",
      "Epoch 11/20  Iteration 1843/3560 Training loss: 1.5141 0.9479 sec/batch\n",
      "Epoch 11/20  Iteration 1844/3560 Training loss: 1.5144 0.9620 sec/batch\n",
      "Epoch 11/20  Iteration 1845/3560 Training loss: 1.5142 1.0356 sec/batch\n",
      "Epoch 11/20  Iteration 1846/3560 Training loss: 1.5144 0.9895 sec/batch\n",
      "Epoch 11/20  Iteration 1847/3560 Training loss: 1.5146 0.9708 sec/batch\n",
      "Epoch 11/20  Iteration 1848/3560 Training loss: 1.5143 0.9845 sec/batch\n",
      "Epoch 11/20  Iteration 1849/3560 Training loss: 1.5143 1.0568 sec/batch\n",
      "Epoch 11/20  Iteration 1850/3560 Training loss: 1.5142 0.9780 sec/batch\n",
      "Epoch 11/20  Iteration 1851/3560 Training loss: 1.5148 0.9940 sec/batch\n",
      "Epoch 11/20  Iteration 1852/3560 Training loss: 1.5152 0.9069 sec/batch\n",
      "Epoch 11/20  Iteration 1853/3560 Training loss: 1.5157 1.0667 sec/batch\n",
      "Epoch 11/20  Iteration 1854/3560 Training loss: 1.5155 0.9670 sec/batch\n",
      "Epoch 11/20  Iteration 1855/3560 Training loss: 1.5153 1.0273 sec/batch\n",
      "Epoch 11/20  Iteration 1856/3560 Training loss: 1.5155 1.1062 sec/batch\n",
      "Epoch 11/20  Iteration 1857/3560 Training loss: 1.5154 1.0041 sec/batch\n",
      "Epoch 11/20  Iteration 1858/3560 Training loss: 1.5154 0.9929 sec/batch\n",
      "Epoch 11/20  Iteration 1859/3560 Training loss: 1.5149 0.9999 sec/batch\n",
      "Epoch 11/20  Iteration 1860/3560 Training loss: 1.5147 0.9541 sec/batch\n",
      "Epoch 11/20  Iteration 1861/3560 Training loss: 1.5142 1.0178 sec/batch\n",
      "Epoch 11/20  Iteration 1862/3560 Training loss: 1.5141 1.0136 sec/batch\n",
      "Epoch 11/20  Iteration 1863/3560 Training loss: 1.5135 0.9955 sec/batch\n",
      "Epoch 11/20  Iteration 1864/3560 Training loss: 1.5136 1.1386 sec/batch\n",
      "Epoch 11/20  Iteration 1865/3560 Training loss: 1.5132 1.0246 sec/batch\n",
      "Epoch 11/20  Iteration 1866/3560 Training loss: 1.5130 1.1085 sec/batch\n",
      "Epoch 11/20  Iteration 1867/3560 Training loss: 1.5126 0.9965 sec/batch\n",
      "Epoch 11/20  Iteration 1868/3560 Training loss: 1.5123 1.0008 sec/batch\n",
      "Epoch 11/20  Iteration 1869/3560 Training loss: 1.5119 1.0143 sec/batch\n",
      "Epoch 11/20  Iteration 1870/3560 Training loss: 1.5120 0.9427 sec/batch\n",
      "Epoch 11/20  Iteration 1871/3560 Training loss: 1.5118 1.0029 sec/batch\n",
      "Epoch 11/20  Iteration 1872/3560 Training loss: 1.5116 1.0328 sec/batch\n",
      "Epoch 11/20  Iteration 1873/3560 Training loss: 1.5113 0.9614 sec/batch\n",
      "Epoch 11/20  Iteration 1874/3560 Training loss: 1.5108 0.9958 sec/batch\n",
      "Epoch 11/20  Iteration 1875/3560 Training loss: 1.5106 0.9292 sec/batch\n",
      "Epoch 11/20  Iteration 1876/3560 Training loss: 1.5106 0.9368 sec/batch\n",
      "Epoch 11/20  Iteration 1877/3560 Training loss: 1.5106 1.0357 sec/batch\n",
      "Epoch 11/20  Iteration 1878/3560 Training loss: 1.5102 0.9730 sec/batch\n",
      "Epoch 11/20  Iteration 1879/3560 Training loss: 1.5099 0.9747 sec/batch\n",
      "Epoch 11/20  Iteration 1880/3560 Training loss: 1.5094 1.0086 sec/batch\n",
      "Epoch 11/20  Iteration 1881/3560 Training loss: 1.5093 0.9462 sec/batch\n",
      "Epoch 11/20  Iteration 1882/3560 Training loss: 1.5092 0.9921 sec/batch\n",
      "Epoch 11/20  Iteration 1883/3560 Training loss: 1.5092 0.8846 sec/batch\n",
      "Epoch 11/20  Iteration 1884/3560 Training loss: 1.5090 1.0814 sec/batch\n",
      "Epoch 11/20  Iteration 1885/3560 Training loss: 1.5088 0.9777 sec/batch\n",
      "Epoch 11/20  Iteration 1886/3560 Training loss: 1.5086 0.9537 sec/batch\n",
      "Epoch 11/20  Iteration 1887/3560 Training loss: 1.5086 1.0424 sec/batch\n",
      "Epoch 11/20  Iteration 1888/3560 Training loss: 1.5085 0.9821 sec/batch\n",
      "Epoch 11/20  Iteration 1889/3560 Training loss: 1.5084 0.9721 sec/batch\n",
      "Epoch 11/20  Iteration 1890/3560 Training loss: 1.5085 0.9660 sec/batch\n",
      "Epoch 11/20  Iteration 1891/3560 Training loss: 1.5082 1.0570 sec/batch\n",
      "Epoch 11/20  Iteration 1892/3560 Training loss: 1.5082 0.9767 sec/batch\n",
      "Epoch 11/20  Iteration 1893/3560 Training loss: 1.5080 0.9746 sec/batch\n",
      "Epoch 11/20  Iteration 1894/3560 Training loss: 1.5078 1.1119 sec/batch\n",
      "Epoch 11/20  Iteration 1895/3560 Training loss: 1.5075 0.9464 sec/batch\n",
      "Epoch 11/20  Iteration 1896/3560 Training loss: 1.5071 0.9540 sec/batch\n",
      "Epoch 11/20  Iteration 1897/3560 Training loss: 1.5070 0.9459 sec/batch\n",
      "Epoch 11/20  Iteration 1898/3560 Training loss: 1.5070 0.9596 sec/batch\n",
      "Epoch 11/20  Iteration 1899/3560 Training loss: 1.5069 0.9946 sec/batch\n",
      "Epoch 11/20  Iteration 1900/3560 Training loss: 1.5067 1.0013 sec/batch\n",
      "Epoch 11/20  Iteration 1901/3560 Training loss: 1.5067 0.9692 sec/batch\n",
      "Epoch 11/20  Iteration 1902/3560 Training loss: 1.5063 1.0509 sec/batch\n",
      "Epoch 11/20  Iteration 1903/3560 Training loss: 1.5059 1.0087 sec/batch\n",
      "Epoch 11/20  Iteration 1904/3560 Training loss: 1.5060 1.0337 sec/batch\n",
      "Epoch 11/20  Iteration 1905/3560 Training loss: 1.5059 1.0107 sec/batch\n",
      "Epoch 11/20  Iteration 1906/3560 Training loss: 1.5056 0.8789 sec/batch\n",
      "Epoch 11/20  Iteration 1907/3560 Training loss: 1.5057 0.9556 sec/batch\n",
      "Epoch 11/20  Iteration 1908/3560 Training loss: 1.5057 0.9724 sec/batch\n",
      "Epoch 11/20  Iteration 1909/3560 Training loss: 1.5055 0.9957 sec/batch\n",
      "Epoch 11/20  Iteration 1910/3560 Training loss: 1.5053 1.1306 sec/batch\n",
      "Epoch 11/20  Iteration 1911/3560 Training loss: 1.5048 1.0727 sec/batch\n",
      "Epoch 11/20  Iteration 1912/3560 Training loss: 1.5046 0.9517 sec/batch\n",
      "Epoch 11/20  Iteration 1913/3560 Training loss: 1.5046 0.9136 sec/batch\n",
      "Epoch 11/20  Iteration 1914/3560 Training loss: 1.5046 0.9657 sec/batch\n",
      "Epoch 11/20  Iteration 1915/3560 Training loss: 1.5046 0.9791 sec/batch\n",
      "Epoch 11/20  Iteration 1916/3560 Training loss: 1.5046 1.0309 sec/batch\n",
      "Epoch 11/20  Iteration 1917/3560 Training loss: 1.5046 0.9913 sec/batch\n",
      "Epoch 11/20  Iteration 1918/3560 Training loss: 1.5047 0.9239 sec/batch\n",
      "Epoch 11/20  Iteration 1919/3560 Training loss: 1.5047 1.0279 sec/batch\n",
      "Epoch 11/20  Iteration 1920/3560 Training loss: 1.5046 0.9690 sec/batch\n",
      "Epoch 11/20  Iteration 1921/3560 Training loss: 1.5049 0.8794 sec/batch\n",
      "Epoch 11/20  Iteration 1922/3560 Training loss: 1.5049 0.9371 sec/batch\n",
      "Epoch 11/20  Iteration 1923/3560 Training loss: 1.5049 1.2572 sec/batch\n",
      "Epoch 11/20  Iteration 1924/3560 Training loss: 1.5050 1.1013 sec/batch\n",
      "Epoch 11/20  Iteration 1925/3560 Training loss: 1.5048 1.0187 sec/batch\n",
      "Epoch 11/20  Iteration 1926/3560 Training loss: 1.5049 1.0032 sec/batch\n",
      "Epoch 11/20  Iteration 1927/3560 Training loss: 1.5049 0.9488 sec/batch\n",
      "Epoch 11/20  Iteration 1928/3560 Training loss: 1.5051 0.8952 sec/batch\n",
      "Epoch 11/20  Iteration 1929/3560 Training loss: 1.5052 0.9682 sec/batch\n",
      "Epoch 11/20  Iteration 1930/3560 Training loss: 1.5050 0.9121 sec/batch\n",
      "Epoch 11/20  Iteration 1931/3560 Training loss: 1.5046 1.0329 sec/batch\n",
      "Epoch 11/20  Iteration 1932/3560 Training loss: 1.5045 0.9662 sec/batch\n",
      "Epoch 11/20  Iteration 1933/3560 Training loss: 1.5046 0.9791 sec/batch\n",
      "Epoch 11/20  Iteration 1934/3560 Training loss: 1.5046 1.0712 sec/batch\n",
      "Epoch 11/20  Iteration 1935/3560 Training loss: 1.5046 0.9042 sec/batch\n",
      "Epoch 11/20  Iteration 1936/3560 Training loss: 1.5046 1.0711 sec/batch\n",
      "Epoch 11/20  Iteration 1937/3560 Training loss: 1.5046 0.9941 sec/batch\n",
      "Epoch 11/20  Iteration 1938/3560 Training loss: 1.5046 0.9732 sec/batch\n",
      "Epoch 11/20  Iteration 1939/3560 Training loss: 1.5043 0.9718 sec/batch\n",
      "Epoch 11/20  Iteration 1940/3560 Training loss: 1.5044 1.0589 sec/batch\n",
      "Epoch 11/20  Iteration 1941/3560 Training loss: 1.5046 0.9447 sec/batch\n",
      "Epoch 11/20  Iteration 1942/3560 Training loss: 1.5045 1.0721 sec/batch\n",
      "Epoch 11/20  Iteration 1943/3560 Training loss: 1.5045 1.1014 sec/batch\n",
      "Epoch 11/20  Iteration 1944/3560 Training loss: 1.5045 0.9959 sec/batch\n",
      "Epoch 11/20  Iteration 1945/3560 Training loss: 1.5045 1.0028 sec/batch\n",
      "Epoch 11/20  Iteration 1946/3560 Training loss: 1.5045 0.9566 sec/batch\n",
      "Epoch 11/20  Iteration 1947/3560 Training loss: 1.5046 0.9670 sec/batch\n",
      "Epoch 11/20  Iteration 1948/3560 Training loss: 1.5050 0.9683 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1949/3560 Training loss: 1.5050 0.9037 sec/batch\n",
      "Epoch 11/20  Iteration 1950/3560 Training loss: 1.5050 1.2350 sec/batch\n",
      "Epoch 11/20  Iteration 1951/3560 Training loss: 1.5050 0.8708 sec/batch\n",
      "Epoch 11/20  Iteration 1952/3560 Training loss: 1.5048 0.9697 sec/batch\n",
      "Epoch 11/20  Iteration 1953/3560 Training loss: 1.5049 0.9060 sec/batch\n",
      "Epoch 11/20  Iteration 1954/3560 Training loss: 1.5049 0.9647 sec/batch\n",
      "Epoch 11/20  Iteration 1955/3560 Training loss: 1.5049 1.0198 sec/batch\n",
      "Epoch 11/20  Iteration 1956/3560 Training loss: 1.5048 0.9897 sec/batch\n",
      "Epoch 11/20  Iteration 1957/3560 Training loss: 1.5046 1.0223 sec/batch\n",
      "Epoch 11/20  Iteration 1958/3560 Training loss: 1.5048 1.0383 sec/batch\n",
      "Epoch 12/20  Iteration 1959/3560 Training loss: 1.5785 0.9592 sec/batch\n",
      "Epoch 12/20  Iteration 1960/3560 Training loss: 1.5418 0.9840 sec/batch\n",
      "Epoch 12/20  Iteration 1961/3560 Training loss: 1.5282 0.8767 sec/batch\n",
      "Epoch 12/20  Iteration 1962/3560 Training loss: 1.5230 0.9060 sec/batch\n",
      "Epoch 12/20  Iteration 1963/3560 Training loss: 1.5149 1.0176 sec/batch\n",
      "Epoch 12/20  Iteration 1964/3560 Training loss: 1.5053 0.9732 sec/batch\n",
      "Epoch 12/20  Iteration 1965/3560 Training loss: 1.5051 0.9627 sec/batch\n",
      "Epoch 12/20  Iteration 1966/3560 Training loss: 1.5024 1.0308 sec/batch\n",
      "Epoch 12/20  Iteration 1967/3560 Training loss: 1.5018 0.9835 sec/batch\n",
      "Epoch 12/20  Iteration 1968/3560 Training loss: 1.5002 0.9929 sec/batch\n",
      "Epoch 12/20  Iteration 1969/3560 Training loss: 1.4960 0.9262 sec/batch\n",
      "Epoch 12/20  Iteration 1970/3560 Training loss: 1.4958 1.0215 sec/batch\n",
      "Epoch 12/20  Iteration 1971/3560 Training loss: 1.4953 0.9534 sec/batch\n",
      "Epoch 12/20  Iteration 1972/3560 Training loss: 1.4970 1.0350 sec/batch\n",
      "Epoch 12/20  Iteration 1973/3560 Training loss: 1.4970 1.0010 sec/batch\n",
      "Epoch 12/20  Iteration 1974/3560 Training loss: 1.4953 0.9983 sec/batch\n",
      "Epoch 12/20  Iteration 1975/3560 Training loss: 1.4955 0.9523 sec/batch\n",
      "Epoch 12/20  Iteration 1976/3560 Training loss: 1.4969 1.0116 sec/batch\n",
      "Epoch 12/20  Iteration 1977/3560 Training loss: 1.4971 0.9490 sec/batch\n",
      "Epoch 12/20  Iteration 1978/3560 Training loss: 1.4985 1.0693 sec/batch\n",
      "Epoch 12/20  Iteration 1979/3560 Training loss: 1.4974 0.9885 sec/batch\n",
      "Epoch 12/20  Iteration 1980/3560 Training loss: 1.4976 0.9104 sec/batch\n",
      "Epoch 12/20  Iteration 1981/3560 Training loss: 1.4970 1.0534 sec/batch\n",
      "Epoch 12/20  Iteration 1982/3560 Training loss: 1.4972 0.9868 sec/batch\n",
      "Epoch 12/20  Iteration 1983/3560 Training loss: 1.4975 0.8815 sec/batch\n",
      "Epoch 12/20  Iteration 1984/3560 Training loss: 1.4958 0.9171 sec/batch\n",
      "Epoch 12/20  Iteration 1985/3560 Training loss: 1.4946 0.9838 sec/batch\n",
      "Epoch 12/20  Iteration 1986/3560 Training loss: 1.4950 0.9084 sec/batch\n",
      "Epoch 12/20  Iteration 1987/3560 Training loss: 1.4952 1.0203 sec/batch\n",
      "Epoch 12/20  Iteration 1988/3560 Training loss: 1.4957 0.9284 sec/batch\n",
      "Epoch 12/20  Iteration 1989/3560 Training loss: 1.4954 1.0960 sec/batch\n",
      "Epoch 12/20  Iteration 1990/3560 Training loss: 1.4945 1.0153 sec/batch\n",
      "Epoch 12/20  Iteration 1991/3560 Training loss: 1.4948 0.9296 sec/batch\n",
      "Epoch 12/20  Iteration 1992/3560 Training loss: 1.4951 0.9397 sec/batch\n",
      "Epoch 12/20  Iteration 1993/3560 Training loss: 1.4949 0.9473 sec/batch\n",
      "Epoch 12/20  Iteration 1994/3560 Training loss: 1.4948 0.9950 sec/batch\n",
      "Epoch 12/20  Iteration 1995/3560 Training loss: 1.4944 1.0149 sec/batch\n",
      "Epoch 12/20  Iteration 1996/3560 Training loss: 1.4933 0.9514 sec/batch\n",
      "Epoch 12/20  Iteration 1997/3560 Training loss: 1.4918 1.0563 sec/batch\n",
      "Epoch 12/20  Iteration 1998/3560 Training loss: 1.4915 0.9508 sec/batch\n",
      "Epoch 12/20  Iteration 1999/3560 Training loss: 1.4909 0.9321 sec/batch\n",
      "Epoch 12/20  Iteration 2000/3560 Training loss: 1.4913 0.8998 sec/batch\n",
      "Epoch 12/20  Iteration 2001/3560 Training loss: 1.4909 0.9333 sec/batch\n",
      "Epoch 12/20  Iteration 2002/3560 Training loss: 1.4904 1.0151 sec/batch\n",
      "Epoch 12/20  Iteration 2003/3560 Training loss: 1.4907 1.0157 sec/batch\n",
      "Epoch 12/20  Iteration 2004/3560 Training loss: 1.4896 1.0125 sec/batch\n",
      "Epoch 12/20  Iteration 2005/3560 Training loss: 1.4892 1.0150 sec/batch\n",
      "Epoch 12/20  Iteration 2006/3560 Training loss: 1.4887 0.9421 sec/batch\n",
      "Epoch 12/20  Iteration 2007/3560 Training loss: 1.4887 1.0017 sec/batch\n",
      "Epoch 12/20  Iteration 2008/3560 Training loss: 1.4893 0.8920 sec/batch\n",
      "Epoch 12/20  Iteration 2009/3560 Training loss: 1.4890 0.8679 sec/batch\n",
      "Epoch 12/20  Iteration 2010/3560 Training loss: 1.4897 0.9285 sec/batch\n",
      "Epoch 12/20  Iteration 2011/3560 Training loss: 1.4895 0.9904 sec/batch\n",
      "Epoch 12/20  Iteration 2012/3560 Training loss: 1.4897 0.9817 sec/batch\n",
      "Epoch 12/20  Iteration 2013/3560 Training loss: 1.4893 1.0637 sec/batch\n",
      "Epoch 12/20  Iteration 2014/3560 Training loss: 1.4895 0.8934 sec/batch\n",
      "Epoch 12/20  Iteration 2015/3560 Training loss: 1.4900 0.9491 sec/batch\n",
      "Epoch 12/20  Iteration 2016/3560 Training loss: 1.4897 0.9648 sec/batch\n",
      "Epoch 12/20  Iteration 2017/3560 Training loss: 1.4892 0.9810 sec/batch\n",
      "Epoch 12/20  Iteration 2018/3560 Training loss: 1.4897 0.9991 sec/batch\n",
      "Epoch 12/20  Iteration 2019/3560 Training loss: 1.4896 0.9252 sec/batch\n",
      "Epoch 12/20  Iteration 2020/3560 Training loss: 1.4906 0.9121 sec/batch\n",
      "Epoch 12/20  Iteration 2021/3560 Training loss: 1.4909 0.9845 sec/batch\n",
      "Epoch 12/20  Iteration 2022/3560 Training loss: 1.4911 0.9963 sec/batch\n",
      "Epoch 12/20  Iteration 2023/3560 Training loss: 1.4911 0.9523 sec/batch\n",
      "Epoch 12/20  Iteration 2024/3560 Training loss: 1.4912 0.9388 sec/batch\n",
      "Epoch 12/20  Iteration 2025/3560 Training loss: 1.4915 0.9627 sec/batch\n",
      "Epoch 12/20  Iteration 2026/3560 Training loss: 1.4913 0.9509 sec/batch\n",
      "Epoch 12/20  Iteration 2027/3560 Training loss: 1.4912 0.9402 sec/batch\n",
      "Epoch 12/20  Iteration 2028/3560 Training loss: 1.4910 0.9195 sec/batch\n",
      "Epoch 12/20  Iteration 2029/3560 Training loss: 1.4916 0.9980 sec/batch\n",
      "Epoch 12/20  Iteration 2030/3560 Training loss: 1.4918 0.9629 sec/batch\n",
      "Epoch 12/20  Iteration 2031/3560 Training loss: 1.4924 0.9128 sec/batch\n",
      "Epoch 12/20  Iteration 2032/3560 Training loss: 1.4920 1.0849 sec/batch\n",
      "Epoch 12/20  Iteration 2033/3560 Training loss: 1.4918 1.0013 sec/batch\n",
      "Epoch 12/20  Iteration 2034/3560 Training loss: 1.4921 0.9914 sec/batch\n",
      "Epoch 12/20  Iteration 2035/3560 Training loss: 1.4919 0.9607 sec/batch\n",
      "Epoch 12/20  Iteration 2036/3560 Training loss: 1.4919 0.9499 sec/batch\n",
      "Epoch 12/20  Iteration 2037/3560 Training loss: 1.4913 1.0649 sec/batch\n",
      "Epoch 12/20  Iteration 2038/3560 Training loss: 1.4912 0.9447 sec/batch\n",
      "Epoch 12/20  Iteration 2039/3560 Training loss: 1.4906 1.0197 sec/batch\n",
      "Epoch 12/20  Iteration 2040/3560 Training loss: 1.4906 0.8591 sec/batch\n",
      "Epoch 12/20  Iteration 2041/3560 Training loss: 1.4901 0.9966 sec/batch\n",
      "Epoch 12/20  Iteration 2042/3560 Training loss: 1.4902 0.9914 sec/batch\n",
      "Epoch 12/20  Iteration 2043/3560 Training loss: 1.4897 0.9828 sec/batch\n",
      "Epoch 12/20  Iteration 2044/3560 Training loss: 1.4895 1.0104 sec/batch\n",
      "Epoch 12/20  Iteration 2045/3560 Training loss: 1.4892 0.9779 sec/batch\n",
      "Epoch 12/20  Iteration 2046/3560 Training loss: 1.4889 1.0393 sec/batch\n",
      "Epoch 12/20  Iteration 2047/3560 Training loss: 1.4885 1.0089 sec/batch\n",
      "Epoch 12/20  Iteration 2048/3560 Training loss: 1.4885 0.8391 sec/batch\n",
      "Epoch 12/20  Iteration 2049/3560 Training loss: 1.4882 0.9942 sec/batch\n",
      "Epoch 12/20  Iteration 2050/3560 Training loss: 1.4880 1.0165 sec/batch\n",
      "Epoch 12/20  Iteration 2051/3560 Training loss: 1.4876 0.9516 sec/batch\n",
      "Epoch 12/20  Iteration 2052/3560 Training loss: 1.4872 0.9154 sec/batch\n",
      "Epoch 12/20  Iteration 2053/3560 Training loss: 1.4871 0.9774 sec/batch\n",
      "Epoch 12/20  Iteration 2054/3560 Training loss: 1.4872 0.9520 sec/batch\n",
      "Epoch 12/20  Iteration 2055/3560 Training loss: 1.4872 0.9697 sec/batch\n",
      "Epoch 12/20  Iteration 2056/3560 Training loss: 1.4867 0.8992 sec/batch\n",
      "Epoch 12/20  Iteration 2057/3560 Training loss: 1.4863 0.9729 sec/batch\n",
      "Epoch 12/20  Iteration 2058/3560 Training loss: 1.4858 1.0726 sec/batch\n",
      "Epoch 12/20  Iteration 2059/3560 Training loss: 1.4858 1.0269 sec/batch\n",
      "Epoch 12/20  Iteration 2060/3560 Training loss: 1.4857 1.0192 sec/batch\n",
      "Epoch 12/20  Iteration 2061/3560 Training loss: 1.4856 0.9231 sec/batch\n",
      "Epoch 12/20  Iteration 2062/3560 Training loss: 1.4855 0.9972 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 2063/3560 Training loss: 1.4852 1.0195 sec/batch\n",
      "Epoch 12/20  Iteration 2064/3560 Training loss: 1.4850 0.9100 sec/batch\n",
      "Epoch 12/20  Iteration 2065/3560 Training loss: 1.4850 0.9910 sec/batch\n",
      "Epoch 12/20  Iteration 2066/3560 Training loss: 1.4850 0.9688 sec/batch\n",
      "Epoch 12/20  Iteration 2067/3560 Training loss: 1.4849 0.9407 sec/batch\n",
      "Epoch 12/20  Iteration 2068/3560 Training loss: 1.4849 0.9439 sec/batch\n",
      "Epoch 12/20  Iteration 2069/3560 Training loss: 1.4847 0.9885 sec/batch\n",
      "Epoch 12/20  Iteration 2070/3560 Training loss: 1.4846 1.0347 sec/batch\n",
      "Epoch 12/20  Iteration 2071/3560 Training loss: 1.4844 0.8758 sec/batch\n",
      "Epoch 12/20  Iteration 2072/3560 Training loss: 1.4843 0.9546 sec/batch\n",
      "Epoch 12/20  Iteration 2073/3560 Training loss: 1.4839 0.9417 sec/batch\n",
      "Epoch 12/20  Iteration 2074/3560 Training loss: 1.4836 1.0024 sec/batch\n",
      "Epoch 12/20  Iteration 2075/3560 Training loss: 1.4836 0.9621 sec/batch\n",
      "Epoch 12/20  Iteration 2076/3560 Training loss: 1.4837 1.0152 sec/batch\n",
      "Epoch 12/20  Iteration 2077/3560 Training loss: 1.4836 1.0288 sec/batch\n",
      "Epoch 12/20  Iteration 2078/3560 Training loss: 1.4836 0.9437 sec/batch\n",
      "Epoch 12/20  Iteration 2079/3560 Training loss: 1.4835 0.9524 sec/batch\n",
      "Epoch 12/20  Iteration 2080/3560 Training loss: 1.4831 1.0170 sec/batch\n",
      "Epoch 12/20  Iteration 2081/3560 Training loss: 1.4827 1.0756 sec/batch\n",
      "Epoch 12/20  Iteration 2082/3560 Training loss: 1.4827 0.9367 sec/batch\n",
      "Epoch 12/20  Iteration 2083/3560 Training loss: 1.4826 0.9318 sec/batch\n",
      "Epoch 12/20  Iteration 2084/3560 Training loss: 1.4822 0.9941 sec/batch\n",
      "Epoch 12/20  Iteration 2085/3560 Training loss: 1.4823 1.0344 sec/batch\n",
      "Epoch 12/20  Iteration 2086/3560 Training loss: 1.4822 0.9663 sec/batch\n",
      "Epoch 12/20  Iteration 2087/3560 Training loss: 1.4821 0.9675 sec/batch\n",
      "Epoch 12/20  Iteration 2088/3560 Training loss: 1.4819 1.0552 sec/batch\n",
      "Epoch 12/20  Iteration 2089/3560 Training loss: 1.4815 1.0312 sec/batch\n",
      "Epoch 12/20  Iteration 2090/3560 Training loss: 1.4813 1.0445 sec/batch\n",
      "Epoch 12/20  Iteration 2091/3560 Training loss: 1.4814 1.0132 sec/batch\n",
      "Epoch 12/20  Iteration 2092/3560 Training loss: 1.4814 1.0234 sec/batch\n",
      "Epoch 12/20  Iteration 2093/3560 Training loss: 1.4813 0.9535 sec/batch\n",
      "Epoch 12/20  Iteration 2094/3560 Training loss: 1.4813 0.9200 sec/batch\n",
      "Epoch 12/20  Iteration 2095/3560 Training loss: 1.4814 0.9234 sec/batch\n",
      "Epoch 12/20  Iteration 2096/3560 Training loss: 1.4815 0.9976 sec/batch\n",
      "Epoch 12/20  Iteration 2097/3560 Training loss: 1.4814 0.9919 sec/batch\n",
      "Epoch 12/20  Iteration 2098/3560 Training loss: 1.4813 0.9562 sec/batch\n",
      "Epoch 12/20  Iteration 2099/3560 Training loss: 1.4817 0.9535 sec/batch\n",
      "Epoch 12/20  Iteration 2100/3560 Training loss: 1.4816 1.0256 sec/batch\n",
      "Epoch 12/20  Iteration 2101/3560 Training loss: 1.4815 1.0653 sec/batch\n",
      "Epoch 12/20  Iteration 2102/3560 Training loss: 1.4818 0.9973 sec/batch\n",
      "Epoch 12/20  Iteration 2103/3560 Training loss: 1.4816 0.9382 sec/batch\n",
      "Epoch 12/20  Iteration 2104/3560 Training loss: 1.4817 0.9912 sec/batch\n",
      "Epoch 12/20  Iteration 2105/3560 Training loss: 1.4817 0.9884 sec/batch\n",
      "Epoch 12/20  Iteration 2106/3560 Training loss: 1.4819 0.8804 sec/batch\n",
      "Epoch 12/20  Iteration 2107/3560 Training loss: 1.4820 1.0133 sec/batch\n",
      "Epoch 12/20  Iteration 2108/3560 Training loss: 1.4818 0.9884 sec/batch\n",
      "Epoch 12/20  Iteration 2109/3560 Training loss: 1.4814 1.0140 sec/batch\n",
      "Epoch 12/20  Iteration 2110/3560 Training loss: 1.4813 0.9570 sec/batch\n",
      "Epoch 12/20  Iteration 2111/3560 Training loss: 1.4814 0.9763 sec/batch\n",
      "Epoch 12/20  Iteration 2112/3560 Training loss: 1.4814 1.0139 sec/batch\n",
      "Epoch 12/20  Iteration 2113/3560 Training loss: 1.4814 1.0545 sec/batch\n",
      "Epoch 12/20  Iteration 2114/3560 Training loss: 1.4814 0.8788 sec/batch\n",
      "Epoch 12/20  Iteration 2115/3560 Training loss: 1.4814 0.9750 sec/batch\n",
      "Epoch 12/20  Iteration 2116/3560 Training loss: 1.4814 0.9851 sec/batch\n",
      "Epoch 12/20  Iteration 2117/3560 Training loss: 1.4812 0.9432 sec/batch\n",
      "Epoch 12/20  Iteration 2118/3560 Training loss: 1.4813 0.9165 sec/batch\n",
      "Epoch 12/20  Iteration 2119/3560 Training loss: 1.4815 0.9491 sec/batch\n",
      "Epoch 12/20  Iteration 2120/3560 Training loss: 1.4815 1.0176 sec/batch\n",
      "Epoch 12/20  Iteration 2121/3560 Training loss: 1.4815 1.0829 sec/batch\n",
      "Epoch 12/20  Iteration 2122/3560 Training loss: 1.4815 0.9679 sec/batch\n",
      "Epoch 12/20  Iteration 2123/3560 Training loss: 1.4814 0.9827 sec/batch\n",
      "Epoch 12/20  Iteration 2124/3560 Training loss: 1.4814 0.9835 sec/batch\n",
      "Epoch 12/20  Iteration 2125/3560 Training loss: 1.4816 0.9797 sec/batch\n",
      "Epoch 12/20  Iteration 2126/3560 Training loss: 1.4821 0.9514 sec/batch\n",
      "Epoch 12/20  Iteration 2127/3560 Training loss: 1.4821 0.9364 sec/batch\n",
      "Epoch 12/20  Iteration 2128/3560 Training loss: 1.4822 1.0623 sec/batch\n",
      "Epoch 12/20  Iteration 2129/3560 Training loss: 1.4821 1.0189 sec/batch\n",
      "Epoch 12/20  Iteration 2130/3560 Training loss: 1.4819 0.9378 sec/batch\n",
      "Epoch 12/20  Iteration 2131/3560 Training loss: 1.4821 1.0545 sec/batch\n",
      "Epoch 12/20  Iteration 2132/3560 Training loss: 1.4821 1.0043 sec/batch\n",
      "Epoch 12/20  Iteration 2133/3560 Training loss: 1.4821 0.9481 sec/batch\n",
      "Epoch 12/20  Iteration 2134/3560 Training loss: 1.4820 0.9381 sec/batch\n",
      "Epoch 12/20  Iteration 2135/3560 Training loss: 1.4819 0.9440 sec/batch\n",
      "Epoch 12/20  Iteration 2136/3560 Training loss: 1.4821 0.9813 sec/batch\n",
      "Epoch 13/20  Iteration 2137/3560 Training loss: 1.5531 0.8751 sec/batch\n",
      "Epoch 13/20  Iteration 2138/3560 Training loss: 1.5210 0.9302 sec/batch\n",
      "Epoch 13/20  Iteration 2139/3560 Training loss: 1.5050 1.0224 sec/batch\n",
      "Epoch 13/20  Iteration 2140/3560 Training loss: 1.5010 1.0413 sec/batch\n",
      "Epoch 13/20  Iteration 2141/3560 Training loss: 1.4949 1.0346 sec/batch\n",
      "Epoch 13/20  Iteration 2142/3560 Training loss: 1.4855 0.9655 sec/batch\n",
      "Epoch 13/20  Iteration 2143/3560 Training loss: 1.4869 0.9850 sec/batch\n",
      "Epoch 13/20  Iteration 2144/3560 Training loss: 1.4845 0.9673 sec/batch\n",
      "Epoch 13/20  Iteration 2145/3560 Training loss: 1.4833 0.9149 sec/batch\n",
      "Epoch 13/20  Iteration 2146/3560 Training loss: 1.4816 0.9235 sec/batch\n",
      "Epoch 13/20  Iteration 2147/3560 Training loss: 1.4782 0.8986 sec/batch\n",
      "Epoch 13/20  Iteration 2148/3560 Training loss: 1.4773 0.9613 sec/batch\n",
      "Epoch 13/20  Iteration 2149/3560 Training loss: 1.4770 0.9587 sec/batch\n",
      "Epoch 13/20  Iteration 2150/3560 Training loss: 1.4781 0.9357 sec/batch\n",
      "Epoch 13/20  Iteration 2151/3560 Training loss: 1.4772 1.0394 sec/batch\n",
      "Epoch 13/20  Iteration 2152/3560 Training loss: 1.4752 1.0780 sec/batch\n",
      "Epoch 13/20  Iteration 2153/3560 Training loss: 1.4752 0.9286 sec/batch\n",
      "Epoch 13/20  Iteration 2154/3560 Training loss: 1.4766 0.9648 sec/batch\n",
      "Epoch 13/20  Iteration 2155/3560 Training loss: 1.4766 1.0038 sec/batch\n",
      "Epoch 13/20  Iteration 2156/3560 Training loss: 1.4772 0.9914 sec/batch\n",
      "Epoch 13/20  Iteration 2157/3560 Training loss: 1.4759 0.9845 sec/batch\n",
      "Epoch 13/20  Iteration 2158/3560 Training loss: 1.4760 0.8587 sec/batch\n",
      "Epoch 13/20  Iteration 2159/3560 Training loss: 1.4755 0.9937 sec/batch\n",
      "Epoch 13/20  Iteration 2160/3560 Training loss: 1.4754 1.1089 sec/batch\n",
      "Epoch 13/20  Iteration 2161/3560 Training loss: 1.4756 0.9856 sec/batch\n",
      "Epoch 13/20  Iteration 2162/3560 Training loss: 1.4735 0.9083 sec/batch\n",
      "Epoch 13/20  Iteration 2163/3560 Training loss: 1.4724 1.0405 sec/batch\n",
      "Epoch 13/20  Iteration 2164/3560 Training loss: 1.4733 1.0363 sec/batch\n",
      "Epoch 13/20  Iteration 2165/3560 Training loss: 1.4737 0.9910 sec/batch\n",
      "Epoch 13/20  Iteration 2166/3560 Training loss: 1.4742 0.9476 sec/batch\n",
      "Epoch 13/20  Iteration 2167/3560 Training loss: 1.4742 0.9179 sec/batch\n",
      "Epoch 13/20  Iteration 2168/3560 Training loss: 1.4734 1.0202 sec/batch\n",
      "Epoch 13/20  Iteration 2169/3560 Training loss: 1.4736 0.8522 sec/batch\n",
      "Epoch 13/20  Iteration 2170/3560 Training loss: 1.4741 0.9840 sec/batch\n",
      "Epoch 13/20  Iteration 2171/3560 Training loss: 1.4737 1.0684 sec/batch\n",
      "Epoch 13/20  Iteration 2172/3560 Training loss: 1.4736 1.0593 sec/batch\n",
      "Epoch 13/20  Iteration 2173/3560 Training loss: 1.4729 0.9337 sec/batch\n",
      "Epoch 13/20  Iteration 2174/3560 Training loss: 1.4717 0.9853 sec/batch\n",
      "Epoch 13/20  Iteration 2175/3560 Training loss: 1.4704 0.9878 sec/batch\n",
      "Epoch 13/20  Iteration 2176/3560 Training loss: 1.4700 1.0333 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2177/3560 Training loss: 1.4696 0.8986 sec/batch\n",
      "Epoch 13/20  Iteration 2178/3560 Training loss: 1.4704 1.0270 sec/batch\n",
      "Epoch 13/20  Iteration 2179/3560 Training loss: 1.4701 1.0071 sec/batch\n",
      "Epoch 13/20  Iteration 2180/3560 Training loss: 1.4694 0.9898 sec/batch\n",
      "Epoch 13/20  Iteration 2181/3560 Training loss: 1.4697 1.0513 sec/batch\n",
      "Epoch 13/20  Iteration 2182/3560 Training loss: 1.4689 0.9343 sec/batch\n",
      "Epoch 13/20  Iteration 2183/3560 Training loss: 1.4685 1.0418 sec/batch\n",
      "Epoch 13/20  Iteration 2184/3560 Training loss: 1.4681 1.0213 sec/batch\n",
      "Epoch 13/20  Iteration 2185/3560 Training loss: 1.4680 0.9268 sec/batch\n",
      "Epoch 13/20  Iteration 2186/3560 Training loss: 1.4685 0.9466 sec/batch\n",
      "Epoch 13/20  Iteration 2187/3560 Training loss: 1.4681 0.9900 sec/batch\n",
      "Epoch 13/20  Iteration 2188/3560 Training loss: 1.4689 0.9864 sec/batch\n",
      "Epoch 13/20  Iteration 2189/3560 Training loss: 1.4685 0.9388 sec/batch\n",
      "Epoch 13/20  Iteration 2190/3560 Training loss: 1.4687 0.9913 sec/batch\n",
      "Epoch 13/20  Iteration 2191/3560 Training loss: 1.4684 0.9697 sec/batch\n",
      "Epoch 13/20  Iteration 2192/3560 Training loss: 1.4686 1.0302 sec/batch\n",
      "Epoch 13/20  Iteration 2193/3560 Training loss: 1.4691 0.9149 sec/batch\n",
      "Epoch 13/20  Iteration 2194/3560 Training loss: 1.4685 0.9442 sec/batch\n",
      "Epoch 13/20  Iteration 2195/3560 Training loss: 1.4680 1.0501 sec/batch\n",
      "Epoch 13/20  Iteration 2196/3560 Training loss: 1.4685 0.9554 sec/batch\n",
      "Epoch 13/20  Iteration 2197/3560 Training loss: 1.4686 0.9512 sec/batch\n",
      "Epoch 13/20  Iteration 2198/3560 Training loss: 1.4695 0.9524 sec/batch\n",
      "Epoch 13/20  Iteration 2199/3560 Training loss: 1.4697 0.9874 sec/batch\n",
      "Epoch 13/20  Iteration 2200/3560 Training loss: 1.4699 0.9862 sec/batch\n",
      "Epoch 13/20  Iteration 2201/3560 Training loss: 1.4699 0.9591 sec/batch\n",
      "Epoch 13/20  Iteration 2202/3560 Training loss: 1.4702 1.0584 sec/batch\n",
      "Epoch 13/20  Iteration 2203/3560 Training loss: 1.4703 1.0354 sec/batch\n",
      "Epoch 13/20  Iteration 2204/3560 Training loss: 1.4700 0.9069 sec/batch\n",
      "Epoch 13/20  Iteration 2205/3560 Training loss: 1.4700 1.0662 sec/batch\n",
      "Epoch 13/20  Iteration 2206/3560 Training loss: 1.4700 0.9823 sec/batch\n",
      "Epoch 13/20  Iteration 2207/3560 Training loss: 1.4706 0.9487 sec/batch\n",
      "Epoch 13/20  Iteration 2208/3560 Training loss: 1.4709 0.9239 sec/batch\n",
      "Epoch 13/20  Iteration 2209/3560 Training loss: 1.4714 0.8775 sec/batch\n",
      "Epoch 13/20  Iteration 2210/3560 Training loss: 1.4711 0.9847 sec/batch\n",
      "Epoch 13/20  Iteration 2211/3560 Training loss: 1.4710 1.0499 sec/batch\n",
      "Epoch 13/20  Iteration 2212/3560 Training loss: 1.4712 0.9019 sec/batch\n",
      "Epoch 13/20  Iteration 2213/3560 Training loss: 1.4709 1.0276 sec/batch\n",
      "Epoch 13/20  Iteration 2214/3560 Training loss: 1.4709 1.0233 sec/batch\n",
      "Epoch 13/20  Iteration 2215/3560 Training loss: 1.4703 0.9838 sec/batch\n",
      "Epoch 13/20  Iteration 2216/3560 Training loss: 1.4703 0.9711 sec/batch\n",
      "Epoch 13/20  Iteration 2217/3560 Training loss: 1.4698 0.9800 sec/batch\n",
      "Epoch 13/20  Iteration 2218/3560 Training loss: 1.4699 0.9106 sec/batch\n",
      "Epoch 13/20  Iteration 2219/3560 Training loss: 1.4694 1.0273 sec/batch\n",
      "Epoch 13/20  Iteration 2220/3560 Training loss: 1.4694 0.9311 sec/batch\n",
      "Epoch 13/20  Iteration 2221/3560 Training loss: 1.4690 0.9928 sec/batch\n",
      "Epoch 13/20  Iteration 2222/3560 Training loss: 1.4687 1.0618 sec/batch\n",
      "Epoch 13/20  Iteration 2223/3560 Training loss: 1.4684 0.9996 sec/batch\n",
      "Epoch 13/20  Iteration 2224/3560 Training loss: 1.4681 0.9432 sec/batch\n",
      "Epoch 13/20  Iteration 2225/3560 Training loss: 1.4677 0.9829 sec/batch\n",
      "Epoch 13/20  Iteration 2226/3560 Training loss: 1.4680 0.9822 sec/batch\n",
      "Epoch 13/20  Iteration 2227/3560 Training loss: 1.4677 0.9591 sec/batch\n",
      "Epoch 13/20  Iteration 2228/3560 Training loss: 1.4674 0.9257 sec/batch\n",
      "Epoch 13/20  Iteration 2229/3560 Training loss: 1.4670 0.9770 sec/batch\n",
      "Epoch 13/20  Iteration 2230/3560 Training loss: 1.4666 1.0678 sec/batch\n",
      "Epoch 13/20  Iteration 2231/3560 Training loss: 1.4663 0.9419 sec/batch\n",
      "Epoch 13/20  Iteration 2232/3560 Training loss: 1.4664 0.9589 sec/batch\n",
      "Epoch 13/20  Iteration 2233/3560 Training loss: 1.4663 0.8084 sec/batch\n",
      "Epoch 13/20  Iteration 2234/3560 Training loss: 1.4658 1.0549 sec/batch\n",
      "Epoch 13/20  Iteration 2235/3560 Training loss: 1.4654 0.9307 sec/batch\n",
      "Epoch 13/20  Iteration 2236/3560 Training loss: 1.4650 0.9546 sec/batch\n",
      "Epoch 13/20  Iteration 2237/3560 Training loss: 1.4649 0.9442 sec/batch\n",
      "Epoch 13/20  Iteration 2238/3560 Training loss: 1.4647 0.9953 sec/batch\n",
      "Epoch 13/20  Iteration 2239/3560 Training loss: 1.4645 0.9945 sec/batch\n",
      "Epoch 13/20  Iteration 2240/3560 Training loss: 1.4644 0.9528 sec/batch\n",
      "Epoch 13/20  Iteration 2241/3560 Training loss: 1.4642 0.9427 sec/batch\n",
      "Epoch 13/20  Iteration 2242/3560 Training loss: 1.4641 0.9457 sec/batch\n",
      "Epoch 13/20  Iteration 2243/3560 Training loss: 1.4641 1.0151 sec/batch\n",
      "Epoch 13/20  Iteration 2244/3560 Training loss: 1.4640 0.9266 sec/batch\n",
      "Epoch 13/20  Iteration 2245/3560 Training loss: 1.4639 0.9900 sec/batch\n",
      "Epoch 13/20  Iteration 2246/3560 Training loss: 1.4640 1.0301 sec/batch\n",
      "Epoch 13/20  Iteration 2247/3560 Training loss: 1.4637 0.9919 sec/batch\n",
      "Epoch 13/20  Iteration 2248/3560 Training loss: 1.4637 0.9218 sec/batch\n",
      "Epoch 13/20  Iteration 2249/3560 Training loss: 1.4635 0.9406 sec/batch\n",
      "Epoch 13/20  Iteration 2250/3560 Training loss: 1.4633 0.9631 sec/batch\n",
      "Epoch 13/20  Iteration 2251/3560 Training loss: 1.4630 0.9640 sec/batch\n",
      "Epoch 13/20  Iteration 2252/3560 Training loss: 1.4627 0.9064 sec/batch\n",
      "Epoch 13/20  Iteration 2253/3560 Training loss: 1.4627 0.9963 sec/batch\n",
      "Epoch 13/20  Iteration 2254/3560 Training loss: 1.4627 1.0286 sec/batch\n",
      "Epoch 13/20  Iteration 2255/3560 Training loss: 1.4627 0.9811 sec/batch\n",
      "Epoch 13/20  Iteration 2256/3560 Training loss: 1.4626 0.9355 sec/batch\n",
      "Epoch 13/20  Iteration 2257/3560 Training loss: 1.4625 0.9707 sec/batch\n",
      "Epoch 13/20  Iteration 2258/3560 Training loss: 1.4622 1.0026 sec/batch\n",
      "Epoch 13/20  Iteration 2259/3560 Training loss: 1.4618 1.0499 sec/batch\n",
      "Epoch 13/20  Iteration 2260/3560 Training loss: 1.4619 0.8823 sec/batch\n",
      "Epoch 13/20  Iteration 2261/3560 Training loss: 1.4618 0.9959 sec/batch\n",
      "Epoch 13/20  Iteration 2262/3560 Training loss: 1.4613 1.0672 sec/batch\n",
      "Epoch 13/20  Iteration 2263/3560 Training loss: 1.4615 1.0458 sec/batch\n",
      "Epoch 13/20  Iteration 2264/3560 Training loss: 1.4615 1.0216 sec/batch\n",
      "Epoch 13/20  Iteration 2265/3560 Training loss: 1.4613 0.9595 sec/batch\n",
      "Epoch 13/20  Iteration 2266/3560 Training loss: 1.4610 1.1235 sec/batch\n",
      "Epoch 13/20  Iteration 2267/3560 Training loss: 1.4606 1.0313 sec/batch\n",
      "Epoch 13/20  Iteration 2268/3560 Training loss: 1.4603 0.9783 sec/batch\n",
      "Epoch 13/20  Iteration 2269/3560 Training loss: 1.4604 1.0168 sec/batch\n",
      "Epoch 13/20  Iteration 2270/3560 Training loss: 1.4604 1.0162 sec/batch\n",
      "Epoch 13/20  Iteration 2271/3560 Training loss: 1.4604 0.8982 sec/batch\n",
      "Epoch 13/20  Iteration 2272/3560 Training loss: 1.4605 1.0054 sec/batch\n",
      "Epoch 13/20  Iteration 2273/3560 Training loss: 1.4606 1.0157 sec/batch\n",
      "Epoch 13/20  Iteration 2274/3560 Training loss: 1.4606 1.0125 sec/batch\n",
      "Epoch 13/20  Iteration 2275/3560 Training loss: 1.4606 1.0292 sec/batch\n",
      "Epoch 13/20  Iteration 2276/3560 Training loss: 1.4605 0.9091 sec/batch\n",
      "Epoch 13/20  Iteration 2277/3560 Training loss: 1.4609 1.0369 sec/batch\n",
      "Epoch 13/20  Iteration 2278/3560 Training loss: 1.4608 0.9789 sec/batch\n",
      "Epoch 13/20  Iteration 2279/3560 Training loss: 1.4607 0.9607 sec/batch\n",
      "Epoch 13/20  Iteration 2280/3560 Training loss: 1.4609 0.9755 sec/batch\n",
      "Epoch 13/20  Iteration 2281/3560 Training loss: 1.4608 1.0555 sec/batch\n",
      "Epoch 13/20  Iteration 2282/3560 Training loss: 1.4610 0.9597 sec/batch\n",
      "Epoch 13/20  Iteration 2283/3560 Training loss: 1.4610 0.8786 sec/batch\n",
      "Epoch 13/20  Iteration 2284/3560 Training loss: 1.4612 0.9032 sec/batch\n",
      "Epoch 13/20  Iteration 2285/3560 Training loss: 1.4614 0.9928 sec/batch\n",
      "Epoch 13/20  Iteration 2286/3560 Training loss: 1.4613 0.9631 sec/batch\n",
      "Epoch 13/20  Iteration 2287/3560 Training loss: 1.4609 0.9002 sec/batch\n",
      "Epoch 13/20  Iteration 2288/3560 Training loss: 1.4608 1.0160 sec/batch\n",
      "Epoch 13/20  Iteration 2289/3560 Training loss: 1.4608 0.9938 sec/batch\n",
      "Epoch 13/20  Iteration 2290/3560 Training loss: 1.4608 1.0153 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2291/3560 Training loss: 1.4608 1.0192 sec/batch\n",
      "Epoch 13/20  Iteration 2292/3560 Training loss: 1.4608 0.9091 sec/batch\n",
      "Epoch 13/20  Iteration 2293/3560 Training loss: 1.4609 0.9970 sec/batch\n",
      "Epoch 13/20  Iteration 2294/3560 Training loss: 1.4609 1.0882 sec/batch\n",
      "Epoch 13/20  Iteration 2295/3560 Training loss: 1.4607 0.9576 sec/batch\n",
      "Epoch 13/20  Iteration 2296/3560 Training loss: 1.4608 0.9653 sec/batch\n",
      "Epoch 13/20  Iteration 2297/3560 Training loss: 1.4610 1.0265 sec/batch\n",
      "Epoch 13/20  Iteration 2298/3560 Training loss: 1.4611 0.9831 sec/batch\n",
      "Epoch 13/20  Iteration 2299/3560 Training loss: 1.4611 0.9200 sec/batch\n",
      "Epoch 13/20  Iteration 2300/3560 Training loss: 1.4610 0.9585 sec/batch\n",
      "Epoch 13/20  Iteration 2301/3560 Training loss: 1.4611 1.0027 sec/batch\n",
      "Epoch 13/20  Iteration 2302/3560 Training loss: 1.4611 0.9320 sec/batch\n",
      "Epoch 13/20  Iteration 2303/3560 Training loss: 1.4612 0.9326 sec/batch\n",
      "Epoch 13/20  Iteration 2304/3560 Training loss: 1.4617 0.9800 sec/batch\n",
      "Epoch 13/20  Iteration 2305/3560 Training loss: 1.4617 1.0784 sec/batch\n",
      "Epoch 13/20  Iteration 2306/3560 Training loss: 1.4617 1.0018 sec/batch\n",
      "Epoch 13/20  Iteration 2307/3560 Training loss: 1.4616 0.8824 sec/batch\n",
      "Epoch 13/20  Iteration 2308/3560 Training loss: 1.4615 1.0370 sec/batch\n",
      "Epoch 13/20  Iteration 2309/3560 Training loss: 1.4616 0.9881 sec/batch\n",
      "Epoch 13/20  Iteration 2310/3560 Training loss: 1.4616 0.9646 sec/batch\n",
      "Epoch 13/20  Iteration 2311/3560 Training loss: 1.4616 0.9966 sec/batch\n",
      "Epoch 13/20  Iteration 2312/3560 Training loss: 1.4615 0.9790 sec/batch\n",
      "Epoch 13/20  Iteration 2313/3560 Training loss: 1.4613 1.0239 sec/batch\n",
      "Epoch 13/20  Iteration 2314/3560 Training loss: 1.4615 0.9350 sec/batch\n",
      "Epoch 14/20  Iteration 2315/3560 Training loss: 1.5515 0.9501 sec/batch\n",
      "Epoch 14/20  Iteration 2316/3560 Training loss: 1.5064 1.0039 sec/batch\n",
      "Epoch 14/20  Iteration 2317/3560 Training loss: 1.4937 1.0682 sec/batch\n",
      "Epoch 14/20  Iteration 2318/3560 Training loss: 1.4872 0.8751 sec/batch\n",
      "Epoch 14/20  Iteration 2319/3560 Training loss: 1.4772 0.9640 sec/batch\n",
      "Epoch 14/20  Iteration 2320/3560 Training loss: 1.4668 0.9510 sec/batch\n",
      "Epoch 14/20  Iteration 2321/3560 Training loss: 1.4670 1.0213 sec/batch\n",
      "Epoch 14/20  Iteration 2322/3560 Training loss: 1.4641 0.9329 sec/batch\n",
      "Epoch 14/20  Iteration 2323/3560 Training loss: 1.4630 0.9246 sec/batch\n",
      "Epoch 14/20  Iteration 2324/3560 Training loss: 1.4622 0.9637 sec/batch\n",
      "Epoch 14/20  Iteration 2325/3560 Training loss: 1.4588 1.0309 sec/batch\n",
      "Epoch 14/20  Iteration 2326/3560 Training loss: 1.4586 0.9407 sec/batch\n",
      "Epoch 14/20  Iteration 2327/3560 Training loss: 1.4585 0.9152 sec/batch\n",
      "Epoch 14/20  Iteration 2328/3560 Training loss: 1.4595 0.9940 sec/batch\n",
      "Epoch 14/20  Iteration 2329/3560 Training loss: 1.4586 0.9856 sec/batch\n",
      "Epoch 14/20  Iteration 2330/3560 Training loss: 1.4565 0.9168 sec/batch\n",
      "Epoch 14/20  Iteration 2331/3560 Training loss: 1.4560 0.9732 sec/batch\n",
      "Epoch 14/20  Iteration 2332/3560 Training loss: 1.4574 1.0233 sec/batch\n",
      "Epoch 14/20  Iteration 2333/3560 Training loss: 1.4573 0.9843 sec/batch\n",
      "Epoch 14/20  Iteration 2334/3560 Training loss: 1.4582 0.9712 sec/batch\n",
      "Epoch 14/20  Iteration 2335/3560 Training loss: 1.4574 1.0250 sec/batch\n",
      "Epoch 14/20  Iteration 2336/3560 Training loss: 1.4572 1.0289 sec/batch\n",
      "Epoch 14/20  Iteration 2337/3560 Training loss: 1.4565 0.9752 sec/batch\n",
      "Epoch 14/20  Iteration 2338/3560 Training loss: 1.4567 0.9239 sec/batch\n",
      "Epoch 14/20  Iteration 2339/3560 Training loss: 1.4567 0.8917 sec/batch\n",
      "Epoch 14/20  Iteration 2340/3560 Training loss: 1.4547 1.0196 sec/batch\n",
      "Epoch 14/20  Iteration 2341/3560 Training loss: 1.4537 0.9433 sec/batch\n",
      "Epoch 14/20  Iteration 2342/3560 Training loss: 1.4545 0.8846 sec/batch\n",
      "Epoch 14/20  Iteration 2343/3560 Training loss: 1.4549 0.9733 sec/batch\n",
      "Epoch 14/20  Iteration 2344/3560 Training loss: 1.4546 1.0227 sec/batch\n",
      "Epoch 14/20  Iteration 2345/3560 Training loss: 1.4540 1.0647 sec/batch\n",
      "Epoch 14/20  Iteration 2346/3560 Training loss: 1.4532 0.9314 sec/batch\n",
      "Epoch 14/20  Iteration 2347/3560 Training loss: 1.4534 0.9362 sec/batch\n",
      "Epoch 14/20  Iteration 2348/3560 Training loss: 1.4534 1.0540 sec/batch\n",
      "Epoch 14/20  Iteration 2349/3560 Training loss: 1.4533 0.9999 sec/batch\n",
      "Epoch 14/20  Iteration 2350/3560 Training loss: 1.4530 0.9231 sec/batch\n",
      "Epoch 14/20  Iteration 2351/3560 Training loss: 1.4526 0.9392 sec/batch\n",
      "Epoch 14/20  Iteration 2352/3560 Training loss: 1.4516 0.9476 sec/batch\n",
      "Epoch 14/20  Iteration 2353/3560 Training loss: 1.4505 0.9586 sec/batch\n",
      "Epoch 14/20  Iteration 2354/3560 Training loss: 1.4502 0.9919 sec/batch\n",
      "Epoch 14/20  Iteration 2355/3560 Training loss: 1.4495 0.9156 sec/batch\n",
      "Epoch 14/20  Iteration 2356/3560 Training loss: 1.4502 1.0429 sec/batch\n",
      "Epoch 14/20  Iteration 2357/3560 Training loss: 1.4499 0.9919 sec/batch\n",
      "Epoch 14/20  Iteration 2358/3560 Training loss: 1.4493 1.0112 sec/batch\n",
      "Epoch 14/20  Iteration 2359/3560 Training loss: 1.4495 0.8916 sec/batch\n",
      "Epoch 14/20  Iteration 2360/3560 Training loss: 1.4486 1.0457 sec/batch\n",
      "Epoch 14/20  Iteration 2361/3560 Training loss: 1.4483 0.9658 sec/batch\n",
      "Epoch 14/20  Iteration 2362/3560 Training loss: 1.4478 0.9703 sec/batch\n",
      "Epoch 14/20  Iteration 2363/3560 Training loss: 1.4478 0.9514 sec/batch\n",
      "Epoch 14/20  Iteration 2364/3560 Training loss: 1.4482 0.9612 sec/batch\n",
      "Epoch 14/20  Iteration 2365/3560 Training loss: 1.4476 0.9900 sec/batch\n",
      "Epoch 14/20  Iteration 2366/3560 Training loss: 1.4484 0.9921 sec/batch\n",
      "Epoch 14/20  Iteration 2367/3560 Training loss: 1.4481 0.8849 sec/batch\n",
      "Epoch 14/20  Iteration 2368/3560 Training loss: 1.4483 1.0894 sec/batch\n",
      "Epoch 14/20  Iteration 2369/3560 Training loss: 1.4480 1.1137 sec/batch\n",
      "Epoch 14/20  Iteration 2370/3560 Training loss: 1.4480 0.9894 sec/batch\n",
      "Epoch 14/20  Iteration 2371/3560 Training loss: 1.4486 0.9345 sec/batch\n",
      "Epoch 14/20  Iteration 2372/3560 Training loss: 1.4483 0.9833 sec/batch\n",
      "Epoch 14/20  Iteration 2373/3560 Training loss: 1.4478 0.9950 sec/batch\n",
      "Epoch 14/20  Iteration 2374/3560 Training loss: 1.4482 0.9071 sec/batch\n",
      "Epoch 14/20  Iteration 2375/3560 Training loss: 1.4483 0.9151 sec/batch\n",
      "Epoch 14/20  Iteration 2376/3560 Training loss: 1.4491 1.0815 sec/batch\n",
      "Epoch 14/20  Iteration 2377/3560 Training loss: 1.4495 0.9815 sec/batch\n",
      "Epoch 14/20  Iteration 2378/3560 Training loss: 1.4497 1.0242 sec/batch\n",
      "Epoch 14/20  Iteration 2379/3560 Training loss: 1.4496 0.9519 sec/batch\n",
      "Epoch 14/20  Iteration 2380/3560 Training loss: 1.4498 0.9938 sec/batch\n",
      "Epoch 14/20  Iteration 2381/3560 Training loss: 1.4500 0.9828 sec/batch\n",
      "Epoch 14/20  Iteration 2382/3560 Training loss: 1.4498 0.9242 sec/batch\n",
      "Epoch 14/20  Iteration 2383/3560 Training loss: 1.4500 0.9293 sec/batch\n",
      "Epoch 14/20  Iteration 2384/3560 Training loss: 1.4499 1.0105 sec/batch\n",
      "Epoch 14/20  Iteration 2385/3560 Training loss: 1.4506 0.9962 sec/batch\n",
      "Epoch 14/20  Iteration 2386/3560 Training loss: 1.4508 1.0028 sec/batch\n",
      "Epoch 14/20  Iteration 2387/3560 Training loss: 1.4515 0.9836 sec/batch\n",
      "Epoch 14/20  Iteration 2388/3560 Training loss: 1.4514 1.0674 sec/batch\n",
      "Epoch 14/20  Iteration 2389/3560 Training loss: 1.4514 0.9381 sec/batch\n",
      "Epoch 14/20  Iteration 2390/3560 Training loss: 1.4516 0.8779 sec/batch\n",
      "Epoch 14/20  Iteration 2391/3560 Training loss: 1.4514 1.0153 sec/batch\n",
      "Epoch 14/20  Iteration 2392/3560 Training loss: 1.4514 0.9866 sec/batch\n",
      "Epoch 14/20  Iteration 2393/3560 Training loss: 1.4508 0.9665 sec/batch\n",
      "Epoch 14/20  Iteration 2394/3560 Training loss: 1.4507 0.9059 sec/batch\n",
      "Epoch 14/20  Iteration 2395/3560 Training loss: 1.4502 1.0931 sec/batch\n",
      "Epoch 14/20  Iteration 2396/3560 Training loss: 1.4502 1.0261 sec/batch\n",
      "Epoch 14/20  Iteration 2397/3560 Training loss: 1.4498 0.9905 sec/batch\n",
      "Epoch 14/20  Iteration 2398/3560 Training loss: 1.4499 0.8989 sec/batch\n",
      "Epoch 14/20  Iteration 2399/3560 Training loss: 1.4495 0.9602 sec/batch\n",
      "Epoch 14/20  Iteration 2400/3560 Training loss: 1.4493 0.9850 sec/batch\n",
      "Epoch 14/20  Iteration 2401/3560 Training loss: 1.4491 0.9834 sec/batch\n",
      "Epoch 14/20  Iteration 2402/3560 Training loss: 1.4487 0.9949 sec/batch\n",
      "Epoch 14/20  Iteration 2403/3560 Training loss: 1.4484 1.0205 sec/batch\n",
      "Epoch 14/20  Iteration 2404/3560 Training loss: 1.4485 0.9950 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2405/3560 Training loss: 1.4483 0.9581 sec/batch\n",
      "Epoch 14/20  Iteration 2406/3560 Training loss: 1.4482 0.9573 sec/batch\n",
      "Epoch 14/20  Iteration 2407/3560 Training loss: 1.4477 0.9712 sec/batch\n",
      "Epoch 14/20  Iteration 2408/3560 Training loss: 1.4474 1.0530 sec/batch\n",
      "Epoch 14/20  Iteration 2409/3560 Training loss: 1.4473 0.9499 sec/batch\n",
      "Epoch 14/20  Iteration 2410/3560 Training loss: 1.4474 0.9703 sec/batch\n",
      "Epoch 14/20  Iteration 2411/3560 Training loss: 1.4475 1.0644 sec/batch\n",
      "Epoch 14/20  Iteration 2412/3560 Training loss: 1.4471 0.9184 sec/batch\n",
      "Epoch 14/20  Iteration 2413/3560 Training loss: 1.4467 1.0153 sec/batch\n",
      "Epoch 14/20  Iteration 2414/3560 Training loss: 1.4464 0.9205 sec/batch\n",
      "Epoch 14/20  Iteration 2415/3560 Training loss: 1.4464 1.0504 sec/batch\n",
      "Epoch 14/20  Iteration 2416/3560 Training loss: 1.4463 1.0073 sec/batch\n",
      "Epoch 14/20  Iteration 2417/3560 Training loss: 1.4462 0.9494 sec/batch\n",
      "Epoch 14/20  Iteration 2418/3560 Training loss: 1.4460 1.0130 sec/batch\n",
      "Epoch 14/20  Iteration 2419/3560 Training loss: 1.4459 1.0595 sec/batch\n",
      "Epoch 14/20  Iteration 2420/3560 Training loss: 1.4457 0.9454 sec/batch\n",
      "Epoch 14/20  Iteration 2421/3560 Training loss: 1.4458 0.9277 sec/batch\n",
      "Epoch 14/20  Iteration 2422/3560 Training loss: 1.4458 1.0086 sec/batch\n",
      "Epoch 14/20  Iteration 2423/3560 Training loss: 1.4456 0.9807 sec/batch\n",
      "Epoch 14/20  Iteration 2424/3560 Training loss: 1.4456 0.9230 sec/batch\n",
      "Epoch 14/20  Iteration 2425/3560 Training loss: 1.4454 0.9688 sec/batch\n",
      "Epoch 14/20  Iteration 2426/3560 Training loss: 1.4453 0.9973 sec/batch\n",
      "Epoch 14/20  Iteration 2427/3560 Training loss: 1.4453 1.0173 sec/batch\n",
      "Epoch 14/20  Iteration 2428/3560 Training loss: 1.4451 1.0064 sec/batch\n",
      "Epoch 14/20  Iteration 2429/3560 Training loss: 1.4449 1.0803 sec/batch\n",
      "Epoch 14/20  Iteration 2430/3560 Training loss: 1.4445 0.8575 sec/batch\n",
      "Epoch 14/20  Iteration 2431/3560 Training loss: 1.4445 1.0526 sec/batch\n",
      "Epoch 14/20  Iteration 2432/3560 Training loss: 1.4446 0.9339 sec/batch\n",
      "Epoch 14/20  Iteration 2433/3560 Training loss: 1.4444 0.9642 sec/batch\n",
      "Epoch 14/20  Iteration 2434/3560 Training loss: 1.4444 0.9926 sec/batch\n",
      "Epoch 14/20  Iteration 2435/3560 Training loss: 1.4443 0.9820 sec/batch\n",
      "Epoch 14/20  Iteration 2436/3560 Training loss: 1.4439 0.9267 sec/batch\n",
      "Epoch 14/20  Iteration 2437/3560 Training loss: 1.4435 0.9688 sec/batch\n",
      "Epoch 14/20  Iteration 2438/3560 Training loss: 1.4435 0.8944 sec/batch\n",
      "Epoch 14/20  Iteration 2439/3560 Training loss: 1.4434 1.0661 sec/batch\n",
      "Epoch 14/20  Iteration 2440/3560 Training loss: 1.4430 0.9636 sec/batch\n",
      "Epoch 14/20  Iteration 2441/3560 Training loss: 1.4431 0.9639 sec/batch\n",
      "Epoch 14/20  Iteration 2442/3560 Training loss: 1.4431 0.9639 sec/batch\n",
      "Epoch 14/20  Iteration 2443/3560 Training loss: 1.4430 0.9335 sec/batch\n",
      "Epoch 14/20  Iteration 2444/3560 Training loss: 1.4428 0.9826 sec/batch\n",
      "Epoch 14/20  Iteration 2445/3560 Training loss: 1.4423 0.9592 sec/batch\n",
      "Epoch 14/20  Iteration 2446/3560 Training loss: 1.4421 1.0040 sec/batch\n",
      "Epoch 14/20  Iteration 2447/3560 Training loss: 1.4422 0.9945 sec/batch\n",
      "Epoch 14/20  Iteration 2448/3560 Training loss: 1.4422 0.9643 sec/batch\n",
      "Epoch 14/20  Iteration 2449/3560 Training loss: 1.4422 0.9405 sec/batch\n",
      "Epoch 14/20  Iteration 2450/3560 Training loss: 1.4421 1.0216 sec/batch\n",
      "Epoch 14/20  Iteration 2451/3560 Training loss: 1.4423 1.0192 sec/batch\n",
      "Epoch 14/20  Iteration 2452/3560 Training loss: 1.4423 0.7890 sec/batch\n",
      "Epoch 14/20  Iteration 2453/3560 Training loss: 1.4423 0.9221 sec/batch\n",
      "Epoch 14/20  Iteration 2454/3560 Training loss: 1.4422 1.0207 sec/batch\n",
      "Epoch 14/20  Iteration 2455/3560 Training loss: 1.4426 1.0281 sec/batch\n",
      "Epoch 14/20  Iteration 2456/3560 Training loss: 1.4427 1.0162 sec/batch\n",
      "Epoch 14/20  Iteration 2457/3560 Training loss: 1.4426 0.9946 sec/batch\n",
      "Epoch 14/20  Iteration 2458/3560 Training loss: 1.4428 0.9651 sec/batch\n",
      "Epoch 14/20  Iteration 2459/3560 Training loss: 1.4426 0.9818 sec/batch\n",
      "Epoch 14/20  Iteration 2460/3560 Training loss: 1.4427 0.9287 sec/batch\n",
      "Epoch 14/20  Iteration 2461/3560 Training loss: 1.4426 0.9602 sec/batch\n",
      "Epoch 14/20  Iteration 2462/3560 Training loss: 1.4428 1.0552 sec/batch\n",
      "Epoch 14/20  Iteration 2463/3560 Training loss: 1.4429 1.0191 sec/batch\n",
      "Epoch 14/20  Iteration 2464/3560 Training loss: 1.4427 0.9471 sec/batch\n",
      "Epoch 14/20  Iteration 2465/3560 Training loss: 1.4424 0.9551 sec/batch\n",
      "Epoch 14/20  Iteration 2466/3560 Training loss: 1.4424 0.9419 sec/batch\n",
      "Epoch 14/20  Iteration 2467/3560 Training loss: 1.4424 1.0116 sec/batch\n",
      "Epoch 14/20  Iteration 2468/3560 Training loss: 1.4425 0.9423 sec/batch\n",
      "Epoch 14/20  Iteration 2469/3560 Training loss: 1.4425 0.8970 sec/batch\n",
      "Epoch 14/20  Iteration 2470/3560 Training loss: 1.4425 0.9743 sec/batch\n",
      "Epoch 14/20  Iteration 2471/3560 Training loss: 1.4426 1.0500 sec/batch\n",
      "Epoch 14/20  Iteration 2472/3560 Training loss: 1.4425 0.9003 sec/batch\n",
      "Epoch 14/20  Iteration 2473/3560 Training loss: 1.4424 1.0019 sec/batch\n",
      "Epoch 14/20  Iteration 2474/3560 Training loss: 1.4425 1.0097 sec/batch\n",
      "Epoch 14/20  Iteration 2475/3560 Training loss: 1.4427 0.9136 sec/batch\n",
      "Epoch 14/20  Iteration 2476/3560 Training loss: 1.4427 0.9561 sec/batch\n",
      "Epoch 14/20  Iteration 2477/3560 Training loss: 1.4428 0.9018 sec/batch\n",
      "Epoch 14/20  Iteration 2478/3560 Training loss: 1.4427 1.0122 sec/batch\n",
      "Epoch 14/20  Iteration 2479/3560 Training loss: 1.4427 1.0101 sec/batch\n",
      "Epoch 14/20  Iteration 2480/3560 Training loss: 1.4428 0.9860 sec/batch\n",
      "Epoch 14/20  Iteration 2481/3560 Training loss: 1.4429 0.9659 sec/batch\n",
      "Epoch 14/20  Iteration 2482/3560 Training loss: 1.4433 0.9731 sec/batch\n",
      "Epoch 14/20  Iteration 2483/3560 Training loss: 1.4433 0.9150 sec/batch\n",
      "Epoch 14/20  Iteration 2484/3560 Training loss: 1.4433 1.0195 sec/batch\n",
      "Epoch 14/20  Iteration 2485/3560 Training loss: 1.4433 0.9489 sec/batch\n",
      "Epoch 14/20  Iteration 2486/3560 Training loss: 1.4432 0.9600 sec/batch\n",
      "Epoch 14/20  Iteration 2487/3560 Training loss: 1.4433 0.9471 sec/batch\n",
      "Epoch 14/20  Iteration 2488/3560 Training loss: 1.4433 0.9146 sec/batch\n",
      "Epoch 14/20  Iteration 2489/3560 Training loss: 1.4433 1.0492 sec/batch\n",
      "Epoch 14/20  Iteration 2490/3560 Training loss: 1.4432 1.0737 sec/batch\n",
      "Epoch 14/20  Iteration 2491/3560 Training loss: 1.4431 1.0046 sec/batch\n",
      "Epoch 14/20  Iteration 2492/3560 Training loss: 1.4433 0.9316 sec/batch\n",
      "Epoch 15/20  Iteration 2493/3560 Training loss: 1.5201 1.0436 sec/batch\n",
      "Epoch 15/20  Iteration 2494/3560 Training loss: 1.4819 0.9864 sec/batch\n",
      "Epoch 15/20  Iteration 2495/3560 Training loss: 1.4707 0.9630 sec/batch\n",
      "Epoch 15/20  Iteration 2496/3560 Training loss: 1.4660 0.9889 sec/batch\n",
      "Epoch 15/20  Iteration 2497/3560 Training loss: 1.4572 1.0162 sec/batch\n",
      "Epoch 15/20  Iteration 2498/3560 Training loss: 1.4475 1.0057 sec/batch\n",
      "Epoch 15/20  Iteration 2499/3560 Training loss: 1.4470 0.9940 sec/batch\n",
      "Epoch 15/20  Iteration 2500/3560 Training loss: 1.4442 1.0337 sec/batch\n",
      "Epoch 15/20  Iteration 2501/3560 Training loss: 1.4439 0.9917 sec/batch\n",
      "Epoch 15/20  Iteration 2502/3560 Training loss: 1.4424 0.9021 sec/batch\n",
      "Epoch 15/20  Iteration 2503/3560 Training loss: 1.4391 0.9891 sec/batch\n",
      "Epoch 15/20  Iteration 2504/3560 Training loss: 1.4385 0.9909 sec/batch\n",
      "Epoch 15/20  Iteration 2505/3560 Training loss: 1.4388 1.0295 sec/batch\n",
      "Epoch 15/20  Iteration 2506/3560 Training loss: 1.4400 1.0076 sec/batch\n",
      "Epoch 15/20  Iteration 2507/3560 Training loss: 1.4398 1.0221 sec/batch\n",
      "Epoch 15/20  Iteration 2508/3560 Training loss: 1.4381 1.0092 sec/batch\n",
      "Epoch 15/20  Iteration 2509/3560 Training loss: 1.4383 0.9099 sec/batch\n",
      "Epoch 15/20  Iteration 2510/3560 Training loss: 1.4398 0.9890 sec/batch\n",
      "Epoch 15/20  Iteration 2511/3560 Training loss: 1.4398 0.9494 sec/batch\n",
      "Epoch 15/20  Iteration 2512/3560 Training loss: 1.4407 0.9786 sec/batch\n",
      "Epoch 15/20  Iteration 2513/3560 Training loss: 1.4404 1.1028 sec/batch\n",
      "Epoch 15/20  Iteration 2514/3560 Training loss: 1.4408 0.9831 sec/batch\n",
      "Epoch 15/20  Iteration 2515/3560 Training loss: 1.4401 1.0055 sec/batch\n",
      "Epoch 15/20  Iteration 2516/3560 Training loss: 1.4404 0.8541 sec/batch\n",
      "Epoch 15/20  Iteration 2517/3560 Training loss: 1.4404 1.0289 sec/batch\n",
      "Epoch 15/20  Iteration 2518/3560 Training loss: 1.4383 0.9405 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2519/3560 Training loss: 1.4372 0.8871 sec/batch\n",
      "Epoch 15/20  Iteration 2520/3560 Training loss: 1.4379 1.0226 sec/batch\n",
      "Epoch 15/20  Iteration 2521/3560 Training loss: 1.4380 1.0383 sec/batch\n",
      "Epoch 15/20  Iteration 2522/3560 Training loss: 1.4383 1.0961 sec/batch\n",
      "Epoch 15/20  Iteration 2523/3560 Training loss: 1.4382 0.9710 sec/batch\n",
      "Epoch 15/20  Iteration 2524/3560 Training loss: 1.4372 0.9995 sec/batch\n",
      "Epoch 15/20  Iteration 2525/3560 Training loss: 1.4371 0.9687 sec/batch\n",
      "Epoch 15/20  Iteration 2526/3560 Training loss: 1.4373 0.9174 sec/batch\n",
      "Epoch 15/20  Iteration 2527/3560 Training loss: 1.4373 1.0495 sec/batch\n",
      "Epoch 15/20  Iteration 2528/3560 Training loss: 1.4373 1.1139 sec/batch\n",
      "Epoch 15/20  Iteration 2529/3560 Training loss: 1.4366 1.0310 sec/batch\n",
      "Epoch 15/20  Iteration 2530/3560 Training loss: 1.4356 0.9099 sec/batch\n",
      "Epoch 15/20  Iteration 2531/3560 Training loss: 1.4343 1.0157 sec/batch\n",
      "Epoch 15/20  Iteration 2532/3560 Training loss: 1.4341 0.9263 sec/batch\n",
      "Epoch 15/20  Iteration 2533/3560 Training loss: 1.4334 0.9846 sec/batch\n",
      "Epoch 15/20  Iteration 2534/3560 Training loss: 1.4340 1.0191 sec/batch\n",
      "Epoch 15/20  Iteration 2535/3560 Training loss: 1.4336 1.0360 sec/batch\n",
      "Epoch 15/20  Iteration 2536/3560 Training loss: 1.4329 1.0053 sec/batch\n",
      "Epoch 15/20  Iteration 2537/3560 Training loss: 1.4331 1.0311 sec/batch\n",
      "Epoch 15/20  Iteration 2538/3560 Training loss: 1.4321 0.9022 sec/batch\n",
      "Epoch 15/20  Iteration 2539/3560 Training loss: 1.4318 0.9601 sec/batch\n",
      "Epoch 15/20  Iteration 2540/3560 Training loss: 1.4312 0.9494 sec/batch\n",
      "Epoch 15/20  Iteration 2541/3560 Training loss: 1.4312 0.9295 sec/batch\n",
      "Epoch 15/20  Iteration 2542/3560 Training loss: 1.4316 1.0611 sec/batch\n",
      "Epoch 15/20  Iteration 2543/3560 Training loss: 1.4312 1.0507 sec/batch\n",
      "Epoch 15/20  Iteration 2544/3560 Training loss: 1.4319 0.9960 sec/batch\n",
      "Epoch 15/20  Iteration 2545/3560 Training loss: 1.4318 1.0141 sec/batch\n",
      "Epoch 15/20  Iteration 2546/3560 Training loss: 1.4321 0.9372 sec/batch\n",
      "Epoch 15/20  Iteration 2547/3560 Training loss: 1.4319 0.9265 sec/batch\n",
      "Epoch 15/20  Iteration 2548/3560 Training loss: 1.4319 0.9288 sec/batch\n",
      "Epoch 15/20  Iteration 2549/3560 Training loss: 1.4325 0.9483 sec/batch\n",
      "Epoch 15/20  Iteration 2550/3560 Training loss: 1.4322 0.9586 sec/batch\n",
      "Epoch 15/20  Iteration 2551/3560 Training loss: 1.4317 1.1611 sec/batch\n",
      "Epoch 15/20  Iteration 2552/3560 Training loss: 1.4323 1.0371 sec/batch\n",
      "Epoch 15/20  Iteration 2553/3560 Training loss: 1.4322 0.8745 sec/batch\n",
      "Epoch 15/20  Iteration 2554/3560 Training loss: 1.4331 0.9733 sec/batch\n",
      "Epoch 15/20  Iteration 2555/3560 Training loss: 1.4334 0.9961 sec/batch\n",
      "Epoch 15/20  Iteration 2556/3560 Training loss: 1.4336 0.9308 sec/batch\n",
      "Epoch 15/20  Iteration 2557/3560 Training loss: 1.4335 0.9695 sec/batch\n",
      "Epoch 15/20  Iteration 2558/3560 Training loss: 1.4337 1.0079 sec/batch\n",
      "Epoch 15/20  Iteration 2559/3560 Training loss: 1.4339 1.1602 sec/batch\n",
      "Epoch 15/20  Iteration 2560/3560 Training loss: 1.4338 0.9895 sec/batch\n",
      "Epoch 15/20  Iteration 2561/3560 Training loss: 1.4336 0.9375 sec/batch\n",
      "Epoch 15/20  Iteration 2562/3560 Training loss: 1.4335 1.0550 sec/batch\n",
      "Epoch 15/20  Iteration 2563/3560 Training loss: 1.4342 0.9199 sec/batch\n",
      "Epoch 15/20  Iteration 2564/3560 Training loss: 1.4344 0.9416 sec/batch\n",
      "Epoch 15/20  Iteration 2565/3560 Training loss: 1.4349 0.9951 sec/batch\n",
      "Epoch 15/20  Iteration 2566/3560 Training loss: 1.4347 1.0753 sec/batch\n",
      "Epoch 15/20  Iteration 2567/3560 Training loss: 1.4346 1.0497 sec/batch\n",
      "Epoch 15/20  Iteration 2568/3560 Training loss: 1.4349 0.9404 sec/batch\n",
      "Epoch 15/20  Iteration 2569/3560 Training loss: 1.4347 0.9585 sec/batch\n",
      "Epoch 15/20  Iteration 2570/3560 Training loss: 1.4346 0.8761 sec/batch\n",
      "Epoch 15/20  Iteration 2571/3560 Training loss: 1.4340 0.9355 sec/batch\n",
      "Epoch 15/20  Iteration 2572/3560 Training loss: 1.4340 0.9229 sec/batch\n",
      "Epoch 15/20  Iteration 2573/3560 Training loss: 1.4336 1.0020 sec/batch\n",
      "Epoch 15/20  Iteration 2574/3560 Training loss: 1.4336 0.9861 sec/batch\n",
      "Epoch 15/20  Iteration 2575/3560 Training loss: 1.4331 1.0168 sec/batch\n",
      "Epoch 15/20  Iteration 2576/3560 Training loss: 1.4332 1.0206 sec/batch\n",
      "Epoch 15/20  Iteration 2577/3560 Training loss: 1.4329 0.8928 sec/batch\n",
      "Epoch 15/20  Iteration 2578/3560 Training loss: 1.4329 0.9581 sec/batch\n",
      "Epoch 15/20  Iteration 2579/3560 Training loss: 1.4325 0.9007 sec/batch\n",
      "Epoch 15/20  Iteration 2580/3560 Training loss: 1.4324 1.0059 sec/batch\n",
      "Epoch 15/20  Iteration 2581/3560 Training loss: 1.4319 0.9426 sec/batch\n",
      "Epoch 15/20  Iteration 2582/3560 Training loss: 1.4320 1.1025 sec/batch\n",
      "Epoch 15/20  Iteration 2583/3560 Training loss: 1.4317 1.0578 sec/batch\n",
      "Epoch 15/20  Iteration 2584/3560 Training loss: 1.4315 0.9577 sec/batch\n",
      "Epoch 15/20  Iteration 2585/3560 Training loss: 1.4312 1.0017 sec/batch\n",
      "Epoch 15/20  Iteration 2586/3560 Training loss: 1.4310 0.9487 sec/batch\n",
      "Epoch 15/20  Iteration 2587/3560 Training loss: 1.4308 0.9517 sec/batch\n",
      "Epoch 15/20  Iteration 2588/3560 Training loss: 1.4310 1.0136 sec/batch\n",
      "Epoch 15/20  Iteration 2589/3560 Training loss: 1.4310 0.9534 sec/batch\n",
      "Epoch 15/20  Iteration 2590/3560 Training loss: 1.4306 1.1213 sec/batch\n",
      "Epoch 15/20  Iteration 2591/3560 Training loss: 1.4303 1.0481 sec/batch\n",
      "Epoch 15/20  Iteration 2592/3560 Training loss: 1.4298 1.0146 sec/batch\n",
      "Epoch 15/20  Iteration 2593/3560 Training loss: 1.4298 0.9277 sec/batch\n",
      "Epoch 15/20  Iteration 2594/3560 Training loss: 1.4297 1.0173 sec/batch\n",
      "Epoch 15/20  Iteration 2595/3560 Training loss: 1.4295 1.0062 sec/batch\n",
      "Epoch 15/20  Iteration 2596/3560 Training loss: 1.4294 0.9564 sec/batch\n",
      "Epoch 15/20  Iteration 2597/3560 Training loss: 1.4293 0.9566 sec/batch\n",
      "Epoch 15/20  Iteration 2598/3560 Training loss: 1.4292 1.0445 sec/batch\n",
      "Epoch 15/20  Iteration 2599/3560 Training loss: 1.4292 1.0250 sec/batch\n",
      "Epoch 15/20  Iteration 2600/3560 Training loss: 1.4292 0.9471 sec/batch\n",
      "Epoch 15/20  Iteration 2601/3560 Training loss: 1.4290 0.9423 sec/batch\n",
      "Epoch 15/20  Iteration 2602/3560 Training loss: 1.4291 1.0072 sec/batch\n",
      "Epoch 15/20  Iteration 2603/3560 Training loss: 1.4288 0.9171 sec/batch\n",
      "Epoch 15/20  Iteration 2604/3560 Training loss: 1.4288 1.0023 sec/batch\n",
      "Epoch 15/20  Iteration 2605/3560 Training loss: 1.4287 1.0598 sec/batch\n",
      "Epoch 15/20  Iteration 2606/3560 Training loss: 1.4286 0.9938 sec/batch\n",
      "Epoch 15/20  Iteration 2607/3560 Training loss: 1.4283 1.0001 sec/batch\n",
      "Epoch 15/20  Iteration 2608/3560 Training loss: 1.4280 0.9183 sec/batch\n",
      "Epoch 15/20  Iteration 2609/3560 Training loss: 1.4280 0.9706 sec/batch\n",
      "Epoch 15/20  Iteration 2610/3560 Training loss: 1.4281 0.9070 sec/batch\n",
      "Epoch 15/20  Iteration 2611/3560 Training loss: 1.4279 0.9677 sec/batch\n",
      "Epoch 15/20  Iteration 2612/3560 Training loss: 1.4279 1.0754 sec/batch\n",
      "Epoch 15/20  Iteration 2613/3560 Training loss: 1.4278 1.0075 sec/batch\n",
      "Epoch 15/20  Iteration 2614/3560 Training loss: 1.4275 1.0781 sec/batch\n",
      "Epoch 15/20  Iteration 2615/3560 Training loss: 1.4271 0.9717 sec/batch\n",
      "Epoch 15/20  Iteration 2616/3560 Training loss: 1.4271 0.9934 sec/batch\n",
      "Epoch 15/20  Iteration 2617/3560 Training loss: 1.4271 0.9476 sec/batch\n",
      "Epoch 15/20  Iteration 2618/3560 Training loss: 1.4267 0.9732 sec/batch\n",
      "Epoch 15/20  Iteration 2619/3560 Training loss: 1.4267 0.9642 sec/batch\n",
      "Epoch 15/20  Iteration 2620/3560 Training loss: 1.4267 1.0124 sec/batch\n",
      "Epoch 15/20  Iteration 2621/3560 Training loss: 1.4266 1.0667 sec/batch\n",
      "Epoch 15/20  Iteration 2622/3560 Training loss: 1.4264 0.9517 sec/batch\n",
      "Epoch 15/20  Iteration 2623/3560 Training loss: 1.4259 0.9119 sec/batch\n",
      "Epoch 15/20  Iteration 2624/3560 Training loss: 1.4257 1.0810 sec/batch\n",
      "Epoch 15/20  Iteration 2625/3560 Training loss: 1.4257 0.9102 sec/batch\n",
      "Epoch 15/20  Iteration 2626/3560 Training loss: 1.4258 1.0066 sec/batch\n",
      "Epoch 15/20  Iteration 2627/3560 Training loss: 1.4258 0.8984 sec/batch\n",
      "Epoch 15/20  Iteration 2628/3560 Training loss: 1.4258 0.9651 sec/batch\n",
      "Epoch 15/20  Iteration 2629/3560 Training loss: 1.4260 1.0585 sec/batch\n",
      "Epoch 15/20  Iteration 2630/3560 Training loss: 1.4260 1.0375 sec/batch\n",
      "Epoch 15/20  Iteration 2631/3560 Training loss: 1.4260 0.9442 sec/batch\n",
      "Epoch 15/20  Iteration 2632/3560 Training loss: 1.4259 0.8594 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2633/3560 Training loss: 1.4263 0.9749 sec/batch\n",
      "Epoch 15/20  Iteration 2634/3560 Training loss: 1.4263 0.9762 sec/batch\n",
      "Epoch 15/20  Iteration 2635/3560 Training loss: 1.4262 0.9943 sec/batch\n",
      "Epoch 15/20  Iteration 2636/3560 Training loss: 1.4264 1.0177 sec/batch\n",
      "Epoch 15/20  Iteration 2637/3560 Training loss: 1.4263 1.0413 sec/batch\n",
      "Epoch 15/20  Iteration 2638/3560 Training loss: 1.4265 0.9491 sec/batch\n",
      "Epoch 15/20  Iteration 2639/3560 Training loss: 1.4265 0.9871 sec/batch\n",
      "Epoch 15/20  Iteration 2640/3560 Training loss: 1.4267 0.9787 sec/batch\n",
      "Epoch 15/20  Iteration 2641/3560 Training loss: 1.4268 0.9713 sec/batch\n",
      "Epoch 15/20  Iteration 2642/3560 Training loss: 1.4266 0.9646 sec/batch\n",
      "Epoch 15/20  Iteration 2643/3560 Training loss: 1.4263 1.0156 sec/batch\n",
      "Epoch 15/20  Iteration 2644/3560 Training loss: 1.4263 1.1290 sec/batch\n",
      "Epoch 15/20  Iteration 2645/3560 Training loss: 1.4263 1.0053 sec/batch\n",
      "Epoch 15/20  Iteration 2646/3560 Training loss: 1.4264 0.9938 sec/batch\n",
      "Epoch 15/20  Iteration 2647/3560 Training loss: 1.4264 0.8831 sec/batch\n",
      "Epoch 15/20  Iteration 2648/3560 Training loss: 1.4265 0.9082 sec/batch\n",
      "Epoch 15/20  Iteration 2649/3560 Training loss: 1.4265 1.0427 sec/batch\n",
      "Epoch 15/20  Iteration 2650/3560 Training loss: 1.4265 0.9166 sec/batch\n",
      "Epoch 15/20  Iteration 2651/3560 Training loss: 1.4263 0.9860 sec/batch\n",
      "Epoch 15/20  Iteration 2652/3560 Training loss: 1.4265 1.1047 sec/batch\n",
      "Epoch 15/20  Iteration 2653/3560 Training loss: 1.4267 1.0948 sec/batch\n",
      "Epoch 15/20  Iteration 2654/3560 Training loss: 1.4267 0.9299 sec/batch\n",
      "Epoch 15/20  Iteration 2655/3560 Training loss: 1.4268 0.9551 sec/batch\n",
      "Epoch 15/20  Iteration 2656/3560 Training loss: 1.4268 0.9643 sec/batch\n",
      "Epoch 15/20  Iteration 2657/3560 Training loss: 1.4268 0.8692 sec/batch\n",
      "Epoch 15/20  Iteration 2658/3560 Training loss: 1.4267 0.8921 sec/batch\n",
      "Epoch 15/20  Iteration 2659/3560 Training loss: 1.4269 0.9959 sec/batch\n",
      "Epoch 15/20  Iteration 2660/3560 Training loss: 1.4273 1.0675 sec/batch\n",
      "Epoch 15/20  Iteration 2661/3560 Training loss: 1.4274 1.0658 sec/batch\n",
      "Epoch 15/20  Iteration 2662/3560 Training loss: 1.4274 0.8936 sec/batch\n",
      "Epoch 15/20  Iteration 2663/3560 Training loss: 1.4273 1.0011 sec/batch\n",
      "Epoch 15/20  Iteration 2664/3560 Training loss: 1.4272 0.9490 sec/batch\n",
      "Epoch 15/20  Iteration 2665/3560 Training loss: 1.4274 0.9736 sec/batch\n",
      "Epoch 15/20  Iteration 2666/3560 Training loss: 1.4274 0.9381 sec/batch\n",
      "Epoch 15/20  Iteration 2667/3560 Training loss: 1.4275 1.0075 sec/batch\n",
      "Epoch 15/20  Iteration 2668/3560 Training loss: 1.4273 1.0462 sec/batch\n",
      "Epoch 15/20  Iteration 2669/3560 Training loss: 1.4272 1.0422 sec/batch\n",
      "Epoch 15/20  Iteration 2670/3560 Training loss: 1.4273 0.9687 sec/batch\n",
      "Epoch 16/20  Iteration 2671/3560 Training loss: 1.5121 0.9418 sec/batch\n",
      "Epoch 16/20  Iteration 2672/3560 Training loss: 1.4706 0.9750 sec/batch\n",
      "Epoch 16/20  Iteration 2673/3560 Training loss: 1.4596 0.9408 sec/batch\n",
      "Epoch 16/20  Iteration 2674/3560 Training loss: 1.4547 0.9752 sec/batch\n",
      "Epoch 16/20  Iteration 2675/3560 Training loss: 1.4462 1.1207 sec/batch\n",
      "Epoch 16/20  Iteration 2676/3560 Training loss: 1.4362 1.0182 sec/batch\n",
      "Epoch 16/20  Iteration 2677/3560 Training loss: 1.4362 1.0616 sec/batch\n",
      "Epoch 16/20  Iteration 2678/3560 Training loss: 1.4330 0.9339 sec/batch\n",
      "Epoch 16/20  Iteration 2679/3560 Training loss: 1.4329 0.9130 sec/batch\n",
      "Epoch 16/20  Iteration 2680/3560 Training loss: 1.4327 0.9681 sec/batch\n",
      "Epoch 16/20  Iteration 2681/3560 Training loss: 1.4288 0.9201 sec/batch\n",
      "Epoch 16/20  Iteration 2682/3560 Training loss: 1.4275 1.0350 sec/batch\n",
      "Epoch 16/20  Iteration 2683/3560 Training loss: 1.4275 1.0920 sec/batch\n",
      "Epoch 16/20  Iteration 2684/3560 Training loss: 1.4289 1.0056 sec/batch\n",
      "Epoch 16/20  Iteration 2685/3560 Training loss: 1.4284 1.0126 sec/batch\n",
      "Epoch 16/20  Iteration 2686/3560 Training loss: 1.4265 0.9682 sec/batch\n",
      "Epoch 16/20  Iteration 2687/3560 Training loss: 1.4261 1.0266 sec/batch\n",
      "Epoch 16/20  Iteration 2688/3560 Training loss: 1.4273 0.8896 sec/batch\n",
      "Epoch 16/20  Iteration 2689/3560 Training loss: 1.4274 0.9916 sec/batch\n",
      "Epoch 16/20  Iteration 2690/3560 Training loss: 1.4278 0.9805 sec/batch\n",
      "Epoch 16/20  Iteration 2691/3560 Training loss: 1.4269 1.0942 sec/batch\n",
      "Epoch 16/20  Iteration 2692/3560 Training loss: 1.4271 1.0024 sec/batch\n",
      "Epoch 16/20  Iteration 2693/3560 Training loss: 1.4260 0.9768 sec/batch\n",
      "Epoch 16/20  Iteration 2694/3560 Training loss: 1.4261 1.0130 sec/batch\n",
      "Epoch 16/20  Iteration 2695/3560 Training loss: 1.4260 0.9740 sec/batch\n",
      "Epoch 16/20  Iteration 2696/3560 Training loss: 1.4241 0.9560 sec/batch\n",
      "Epoch 16/20  Iteration 2697/3560 Training loss: 1.4228 1.0072 sec/batch\n",
      "Epoch 16/20  Iteration 2698/3560 Training loss: 1.4236 1.0260 sec/batch\n",
      "Epoch 16/20  Iteration 2699/3560 Training loss: 1.4238 1.1060 sec/batch\n",
      "Epoch 16/20  Iteration 2700/3560 Training loss: 1.4241 0.9867 sec/batch\n",
      "Epoch 16/20  Iteration 2701/3560 Training loss: 1.4238 0.9617 sec/batch\n",
      "Epoch 16/20  Iteration 2702/3560 Training loss: 1.4226 0.8492 sec/batch\n",
      "Epoch 16/20  Iteration 2703/3560 Training loss: 1.4228 0.9891 sec/batch\n",
      "Epoch 16/20  Iteration 2704/3560 Training loss: 1.4228 0.8622 sec/batch\n",
      "Epoch 16/20  Iteration 2705/3560 Training loss: 1.4227 1.0087 sec/batch\n",
      "Epoch 16/20  Iteration 2706/3560 Training loss: 1.4224 1.0541 sec/batch\n",
      "Epoch 16/20  Iteration 2707/3560 Training loss: 1.4218 0.9896 sec/batch\n",
      "Epoch 16/20  Iteration 2708/3560 Training loss: 1.4209 1.0536 sec/batch\n",
      "Epoch 16/20  Iteration 2709/3560 Training loss: 1.4197 0.9772 sec/batch\n",
      "Epoch 16/20  Iteration 2710/3560 Training loss: 1.4193 0.9415 sec/batch\n",
      "Epoch 16/20  Iteration 2711/3560 Training loss: 1.4191 0.9705 sec/batch\n",
      "Epoch 16/20  Iteration 2712/3560 Training loss: 1.4197 0.9627 sec/batch\n",
      "Epoch 16/20  Iteration 2713/3560 Training loss: 1.4195 0.9868 sec/batch\n",
      "Epoch 16/20  Iteration 2714/3560 Training loss: 1.4190 1.1003 sec/batch\n",
      "Epoch 16/20  Iteration 2715/3560 Training loss: 1.4193 0.9813 sec/batch\n",
      "Epoch 16/20  Iteration 2716/3560 Training loss: 1.4183 0.9565 sec/batch\n",
      "Epoch 16/20  Iteration 2717/3560 Training loss: 1.4180 0.9170 sec/batch\n",
      "Epoch 16/20  Iteration 2718/3560 Training loss: 1.4175 0.9438 sec/batch\n",
      "Epoch 16/20  Iteration 2719/3560 Training loss: 1.4175 0.9226 sec/batch\n",
      "Epoch 16/20  Iteration 2720/3560 Training loss: 1.4180 0.9004 sec/batch\n",
      "Epoch 16/20  Iteration 2721/3560 Training loss: 1.4176 0.9716 sec/batch\n",
      "Epoch 16/20  Iteration 2722/3560 Training loss: 1.4182 1.0654 sec/batch\n",
      "Epoch 16/20  Iteration 2723/3560 Training loss: 1.4181 0.9384 sec/batch\n",
      "Epoch 16/20  Iteration 2724/3560 Training loss: 1.4183 0.9916 sec/batch\n",
      "Epoch 16/20  Iteration 2725/3560 Training loss: 1.4179 0.9569 sec/batch\n",
      "Epoch 16/20  Iteration 2726/3560 Training loss: 1.4181 1.0216 sec/batch\n",
      "Epoch 16/20  Iteration 2727/3560 Training loss: 1.4186 0.9416 sec/batch\n",
      "Epoch 16/20  Iteration 2728/3560 Training loss: 1.4180 0.9932 sec/batch\n",
      "Epoch 16/20  Iteration 2729/3560 Training loss: 1.4176 0.9942 sec/batch\n",
      "Epoch 16/20  Iteration 2730/3560 Training loss: 1.4181 1.1058 sec/batch\n",
      "Epoch 16/20  Iteration 2731/3560 Training loss: 1.4181 0.9243 sec/batch\n",
      "Epoch 16/20  Iteration 2732/3560 Training loss: 1.4188 1.0369 sec/batch\n",
      "Epoch 16/20  Iteration 2733/3560 Training loss: 1.4191 0.9735 sec/batch\n",
      "Epoch 16/20  Iteration 2734/3560 Training loss: 1.4192 0.9602 sec/batch\n",
      "Epoch 16/20  Iteration 2735/3560 Training loss: 1.4191 0.9014 sec/batch\n",
      "Epoch 16/20  Iteration 2736/3560 Training loss: 1.4194 1.0292 sec/batch\n",
      "Epoch 16/20  Iteration 2737/3560 Training loss: 1.4197 1.0383 sec/batch\n",
      "Epoch 16/20  Iteration 2738/3560 Training loss: 1.4194 1.0834 sec/batch\n",
      "Epoch 16/20  Iteration 2739/3560 Training loss: 1.4193 1.0732 sec/batch\n",
      "Epoch 16/20  Iteration 2740/3560 Training loss: 1.4190 1.0110 sec/batch\n",
      "Epoch 16/20  Iteration 2741/3560 Training loss: 1.4198 0.9319 sec/batch\n",
      "Epoch 16/20  Iteration 2742/3560 Training loss: 1.4201 1.0046 sec/batch\n",
      "Epoch 16/20  Iteration 2743/3560 Training loss: 1.4206 1.0707 sec/batch\n",
      "Epoch 16/20  Iteration 2744/3560 Training loss: 1.4205 1.0661 sec/batch\n",
      "Epoch 16/20  Iteration 2745/3560 Training loss: 1.4206 1.0948 sec/batch\n",
      "Epoch 16/20  Iteration 2746/3560 Training loss: 1.4208 1.2004 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2747/3560 Training loss: 1.4207 0.9651 sec/batch\n",
      "Epoch 16/20  Iteration 2748/3560 Training loss: 1.4206 0.9816 sec/batch\n",
      "Epoch 16/20  Iteration 2749/3560 Training loss: 1.4200 0.9985 sec/batch\n",
      "Epoch 16/20  Iteration 2750/3560 Training loss: 1.4200 0.9869 sec/batch\n",
      "Epoch 16/20  Iteration 2751/3560 Training loss: 1.4198 0.9324 sec/batch\n",
      "Epoch 16/20  Iteration 2752/3560 Training loss: 1.4197 1.0988 sec/batch\n",
      "Epoch 16/20  Iteration 2753/3560 Training loss: 1.4191 1.0472 sec/batch\n",
      "Epoch 16/20  Iteration 2754/3560 Training loss: 1.4192 1.0175 sec/batch\n",
      "Epoch 16/20  Iteration 2755/3560 Training loss: 1.4189 1.0195 sec/batch\n",
      "Epoch 16/20  Iteration 2756/3560 Training loss: 1.4188 0.9646 sec/batch\n",
      "Epoch 16/20  Iteration 2757/3560 Training loss: 1.4184 0.9872 sec/batch\n",
      "Epoch 16/20  Iteration 2758/3560 Training loss: 1.4182 0.9171 sec/batch\n",
      "Epoch 16/20  Iteration 2759/3560 Training loss: 1.4178 0.8969 sec/batch\n",
      "Epoch 16/20  Iteration 2760/3560 Training loss: 1.4179 0.9243 sec/batch\n",
      "Epoch 16/20  Iteration 2761/3560 Training loss: 1.4176 1.1183 sec/batch\n",
      "Epoch 16/20  Iteration 2762/3560 Training loss: 1.4175 1.0697 sec/batch\n",
      "Epoch 16/20  Iteration 2763/3560 Training loss: 1.4172 1.0003 sec/batch\n",
      "Epoch 16/20  Iteration 2764/3560 Training loss: 1.4169 0.9318 sec/batch\n",
      "Epoch 16/20  Iteration 2765/3560 Training loss: 1.4167 0.9212 sec/batch\n",
      "Epoch 16/20  Iteration 2766/3560 Training loss: 1.4169 0.9704 sec/batch\n",
      "Epoch 16/20  Iteration 2767/3560 Training loss: 1.4169 0.9818 sec/batch\n",
      "Epoch 16/20  Iteration 2768/3560 Training loss: 1.4166 0.9712 sec/batch\n",
      "Epoch 16/20  Iteration 2769/3560 Training loss: 1.4163 1.0877 sec/batch\n",
      "Epoch 16/20  Iteration 2770/3560 Training loss: 1.4159 1.0064 sec/batch\n",
      "Epoch 16/20  Iteration 2771/3560 Training loss: 1.4159 0.9396 sec/batch\n",
      "Epoch 16/20  Iteration 2772/3560 Training loss: 1.4158 0.9193 sec/batch\n",
      "Epoch 16/20  Iteration 2773/3560 Training loss: 1.4156 0.8846 sec/batch\n",
      "Epoch 16/20  Iteration 2774/3560 Training loss: 1.4155 1.0445 sec/batch\n",
      "Epoch 16/20  Iteration 2775/3560 Training loss: 1.4153 0.9439 sec/batch\n",
      "Epoch 16/20  Iteration 2776/3560 Training loss: 1.4153 0.9853 sec/batch\n",
      "Epoch 16/20  Iteration 2777/3560 Training loss: 1.4152 1.1748 sec/batch\n",
      "Epoch 16/20  Iteration 2778/3560 Training loss: 1.4152 1.0855 sec/batch\n",
      "Epoch 16/20  Iteration 2779/3560 Training loss: 1.4151 1.0837 sec/batch\n",
      "Epoch 16/20  Iteration 2780/3560 Training loss: 1.4151 1.0434 sec/batch\n",
      "Epoch 16/20  Iteration 2781/3560 Training loss: 1.4149 1.5903 sec/batch\n",
      "Epoch 16/20  Iteration 2782/3560 Training loss: 1.4148 1.3918 sec/batch\n",
      "Epoch 16/20  Iteration 2783/3560 Training loss: 1.4147 1.0449 sec/batch\n",
      "Epoch 16/20  Iteration 2784/3560 Training loss: 1.4145 1.0646 sec/batch\n",
      "Epoch 16/20  Iteration 2785/3560 Training loss: 1.4142 1.1457 sec/batch\n",
      "Epoch 16/20  Iteration 2786/3560 Training loss: 1.4138 1.1258 sec/batch\n",
      "Epoch 16/20  Iteration 2787/3560 Training loss: 1.4139 1.0949 sec/batch\n",
      "Epoch 16/20  Iteration 2788/3560 Training loss: 1.4139 0.9459 sec/batch\n",
      "Epoch 16/20  Iteration 2789/3560 Training loss: 1.4139 0.9460 sec/batch\n",
      "Epoch 16/20  Iteration 2790/3560 Training loss: 1.4138 1.0492 sec/batch\n",
      "Epoch 16/20  Iteration 2791/3560 Training loss: 1.4136 1.0192 sec/batch\n",
      "Epoch 16/20  Iteration 2792/3560 Training loss: 1.4133 1.2785 sec/batch\n",
      "Epoch 16/20  Iteration 2793/3560 Training loss: 1.4129 1.2807 sec/batch\n",
      "Epoch 16/20  Iteration 2794/3560 Training loss: 1.4129 1.0906 sec/batch\n",
      "Epoch 16/20  Iteration 2795/3560 Training loss: 1.4129 1.1333 sec/batch\n",
      "Epoch 16/20  Iteration 2796/3560 Training loss: 1.4125 1.1095 sec/batch\n",
      "Epoch 16/20  Iteration 2797/3560 Training loss: 1.4126 1.0513 sec/batch\n",
      "Epoch 16/20  Iteration 2798/3560 Training loss: 1.4126 1.1606 sec/batch\n",
      "Epoch 16/20  Iteration 2799/3560 Training loss: 1.4125 1.0993 sec/batch\n",
      "Epoch 16/20  Iteration 2800/3560 Training loss: 1.4122 1.1558 sec/batch\n",
      "Epoch 16/20  Iteration 2801/3560 Training loss: 1.4117 1.1809 sec/batch\n",
      "Epoch 16/20  Iteration 2802/3560 Training loss: 1.4115 1.2047 sec/batch\n",
      "Epoch 16/20  Iteration 2803/3560 Training loss: 1.4115 1.0750 sec/batch\n",
      "Epoch 16/20  Iteration 2804/3560 Training loss: 1.4115 1.1555 sec/batch\n",
      "Epoch 16/20  Iteration 2805/3560 Training loss: 1.4115 0.9968 sec/batch\n",
      "Epoch 16/20  Iteration 2806/3560 Training loss: 1.4115 1.0534 sec/batch\n",
      "Epoch 16/20  Iteration 2807/3560 Training loss: 1.4117 1.0745 sec/batch\n",
      "Epoch 16/20  Iteration 2808/3560 Training loss: 1.4117 1.0941 sec/batch\n",
      "Epoch 16/20  Iteration 2809/3560 Training loss: 1.4118 1.1916 sec/batch\n",
      "Epoch 16/20  Iteration 2810/3560 Training loss: 1.4117 1.0127 sec/batch\n",
      "Epoch 16/20  Iteration 2811/3560 Training loss: 1.4120 1.0137 sec/batch\n",
      "Epoch 16/20  Iteration 2812/3560 Training loss: 1.4120 0.9912 sec/batch\n",
      "Epoch 16/20  Iteration 2813/3560 Training loss: 1.4119 0.9364 sec/batch\n",
      "Epoch 16/20  Iteration 2814/3560 Training loss: 1.4121 0.9569 sec/batch\n",
      "Epoch 16/20  Iteration 2815/3560 Training loss: 1.4119 0.9620 sec/batch\n",
      "Epoch 16/20  Iteration 2816/3560 Training loss: 1.4120 1.0820 sec/batch\n",
      "Epoch 16/20  Iteration 2817/3560 Training loss: 1.4120 1.0952 sec/batch\n",
      "Epoch 16/20  Iteration 2818/3560 Training loss: 1.4123 0.9954 sec/batch\n",
      "Epoch 16/20  Iteration 2819/3560 Training loss: 1.4124 1.0031 sec/batch\n",
      "Epoch 16/20  Iteration 2820/3560 Training loss: 1.4122 0.9920 sec/batch\n",
      "Epoch 16/20  Iteration 2821/3560 Training loss: 1.4118 0.9598 sec/batch\n",
      "Epoch 16/20  Iteration 2822/3560 Training loss: 1.4117 1.0838 sec/batch\n",
      "Epoch 16/20  Iteration 2823/3560 Training loss: 1.4117 0.9677 sec/batch\n",
      "Epoch 16/20  Iteration 2824/3560 Training loss: 1.4118 0.9965 sec/batch\n",
      "Epoch 16/20  Iteration 2825/3560 Training loss: 1.4117 1.0149 sec/batch\n",
      "Epoch 16/20  Iteration 2826/3560 Training loss: 1.4117 0.9540 sec/batch\n",
      "Epoch 16/20  Iteration 2827/3560 Training loss: 1.4118 0.9485 sec/batch\n",
      "Epoch 16/20  Iteration 2828/3560 Training loss: 1.4118 0.9540 sec/batch\n",
      "Epoch 16/20  Iteration 2829/3560 Training loss: 1.4116 0.9040 sec/batch\n",
      "Epoch 16/20  Iteration 2830/3560 Training loss: 1.4117 1.0113 sec/batch\n",
      "Epoch 16/20  Iteration 2831/3560 Training loss: 1.4119 0.9410 sec/batch\n",
      "Epoch 16/20  Iteration 2832/3560 Training loss: 1.4119 1.1011 sec/batch\n",
      "Epoch 16/20  Iteration 2833/3560 Training loss: 1.4119 1.2281 sec/batch\n",
      "Epoch 16/20  Iteration 2834/3560 Training loss: 1.4119 0.8702 sec/batch\n",
      "Epoch 16/20  Iteration 2835/3560 Training loss: 1.4119 0.9682 sec/batch\n",
      "Epoch 16/20  Iteration 2836/3560 Training loss: 1.4120 0.9209 sec/batch\n",
      "Epoch 16/20  Iteration 2837/3560 Training loss: 1.4121 0.9144 sec/batch\n",
      "Epoch 16/20  Iteration 2838/3560 Training loss: 1.4126 0.9248 sec/batch\n",
      "Epoch 16/20  Iteration 2839/3560 Training loss: 1.4126 1.0490 sec/batch\n",
      "Epoch 16/20  Iteration 2840/3560 Training loss: 1.4127 1.1092 sec/batch\n",
      "Epoch 16/20  Iteration 2841/3560 Training loss: 1.4126 0.9823 sec/batch\n",
      "Epoch 16/20  Iteration 2842/3560 Training loss: 1.4125 0.8816 sec/batch\n",
      "Epoch 16/20  Iteration 2843/3560 Training loss: 1.4126 0.9782 sec/batch\n",
      "Epoch 16/20  Iteration 2844/3560 Training loss: 1.4126 0.9743 sec/batch\n",
      "Epoch 16/20  Iteration 2845/3560 Training loss: 1.4126 0.9555 sec/batch\n",
      "Epoch 16/20  Iteration 2846/3560 Training loss: 1.4125 0.9002 sec/batch\n",
      "Epoch 16/20  Iteration 2847/3560 Training loss: 1.4124 0.9882 sec/batch\n",
      "Epoch 16/20  Iteration 2848/3560 Training loss: 1.4126 1.0731 sec/batch\n",
      "Epoch 17/20  Iteration 2849/3560 Training loss: 1.4897 0.9879 sec/batch\n",
      "Epoch 17/20  Iteration 2850/3560 Training loss: 1.4534 0.9603 sec/batch\n",
      "Epoch 17/20  Iteration 2851/3560 Training loss: 1.4441 0.9535 sec/batch\n",
      "Epoch 17/20  Iteration 2852/3560 Training loss: 1.4392 1.0139 sec/batch\n",
      "Epoch 17/20  Iteration 2853/3560 Training loss: 1.4288 0.9485 sec/batch\n",
      "Epoch 17/20  Iteration 2854/3560 Training loss: 1.4182 0.9281 sec/batch\n",
      "Epoch 17/20  Iteration 2855/3560 Training loss: 1.4181 1.1232 sec/batch\n",
      "Epoch 17/20  Iteration 2856/3560 Training loss: 1.4147 0.9954 sec/batch\n",
      "Epoch 17/20  Iteration 2857/3560 Training loss: 1.4145 1.0528 sec/batch\n",
      "Epoch 17/20  Iteration 2858/3560 Training loss: 1.4135 0.9886 sec/batch\n",
      "Epoch 17/20  Iteration 2859/3560 Training loss: 1.4098 0.8998 sec/batch\n",
      "Epoch 17/20  Iteration 2860/3560 Training loss: 1.4095 1.0235 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 2861/3560 Training loss: 1.4096 0.9214 sec/batch\n",
      "Epoch 17/20  Iteration 2862/3560 Training loss: 1.4108 0.9985 sec/batch\n",
      "Epoch 17/20  Iteration 2863/3560 Training loss: 1.4096 1.0204 sec/batch\n",
      "Epoch 17/20  Iteration 2864/3560 Training loss: 1.4078 1.0634 sec/batch\n",
      "Epoch 17/20  Iteration 2865/3560 Training loss: 1.4072 0.9911 sec/batch\n",
      "Epoch 17/20  Iteration 2866/3560 Training loss: 1.4083 1.0124 sec/batch\n",
      "Epoch 17/20  Iteration 2867/3560 Training loss: 1.4082 0.9148 sec/batch\n",
      "Epoch 17/20  Iteration 2868/3560 Training loss: 1.4094 0.9501 sec/batch\n",
      "Epoch 17/20  Iteration 2869/3560 Training loss: 1.4087 1.0080 sec/batch\n",
      "Epoch 17/20  Iteration 2870/3560 Training loss: 1.4085 1.0468 sec/batch\n",
      "Epoch 17/20  Iteration 2871/3560 Training loss: 1.4081 1.0201 sec/batch\n",
      "Epoch 17/20  Iteration 2872/3560 Training loss: 1.4083 0.9403 sec/batch\n",
      "Epoch 17/20  Iteration 2873/3560 Training loss: 1.4087 0.9901 sec/batch\n",
      "Epoch 17/20  Iteration 2874/3560 Training loss: 1.4068 1.0975 sec/batch\n",
      "Epoch 17/20  Iteration 2875/3560 Training loss: 1.4060 0.9687 sec/batch\n",
      "Epoch 17/20  Iteration 2876/3560 Training loss: 1.4071 0.9926 sec/batch\n",
      "Epoch 17/20  Iteration 2877/3560 Training loss: 1.4076 0.9749 sec/batch\n",
      "Epoch 17/20  Iteration 2878/3560 Training loss: 1.4079 0.9844 sec/batch\n",
      "Epoch 17/20  Iteration 2879/3560 Training loss: 1.4075 1.0242 sec/batch\n",
      "Epoch 17/20  Iteration 2880/3560 Training loss: 1.4065 0.9988 sec/batch\n",
      "Epoch 17/20  Iteration 2881/3560 Training loss: 1.4071 0.9469 sec/batch\n",
      "Epoch 17/20  Iteration 2882/3560 Training loss: 1.4074 0.9358 sec/batch\n",
      "Epoch 17/20  Iteration 2883/3560 Training loss: 1.4072 0.9986 sec/batch\n",
      "Epoch 17/20  Iteration 2884/3560 Training loss: 1.4070 0.9702 sec/batch\n",
      "Epoch 17/20  Iteration 2885/3560 Training loss: 1.4066 1.0725 sec/batch\n",
      "Epoch 17/20  Iteration 2886/3560 Training loss: 1.4057 0.9526 sec/batch\n",
      "Epoch 17/20  Iteration 2887/3560 Training loss: 1.4045 1.0570 sec/batch\n",
      "Epoch 17/20  Iteration 2888/3560 Training loss: 1.4041 0.9259 sec/batch\n",
      "Epoch 17/20  Iteration 2889/3560 Training loss: 1.4035 0.9518 sec/batch\n",
      "Epoch 17/20  Iteration 2890/3560 Training loss: 1.4042 0.9524 sec/batch\n",
      "Epoch 17/20  Iteration 2891/3560 Training loss: 1.4041 0.9569 sec/batch\n",
      "Epoch 17/20  Iteration 2892/3560 Training loss: 1.4034 0.9405 sec/batch\n",
      "Epoch 17/20  Iteration 2893/3560 Training loss: 1.4037 0.9915 sec/batch\n",
      "Epoch 17/20  Iteration 2894/3560 Training loss: 1.4029 1.0154 sec/batch\n",
      "Epoch 17/20  Iteration 2895/3560 Training loss: 1.4027 1.0162 sec/batch\n",
      "Epoch 17/20  Iteration 2896/3560 Training loss: 1.4022 1.0141 sec/batch\n",
      "Epoch 17/20  Iteration 2897/3560 Training loss: 1.4022 0.9832 sec/batch\n",
      "Epoch 17/20  Iteration 2898/3560 Training loss: 1.4028 0.8945 sec/batch\n",
      "Epoch 17/20  Iteration 2899/3560 Training loss: 1.4022 0.9652 sec/batch\n",
      "Epoch 17/20  Iteration 2900/3560 Training loss: 1.4030 0.9998 sec/batch\n",
      "Epoch 17/20  Iteration 2901/3560 Training loss: 1.4029 0.9635 sec/batch\n",
      "Epoch 17/20  Iteration 2902/3560 Training loss: 1.4032 0.9751 sec/batch\n",
      "Epoch 17/20  Iteration 2903/3560 Training loss: 1.4029 1.0683 sec/batch\n",
      "Epoch 17/20  Iteration 2904/3560 Training loss: 1.4030 0.9648 sec/batch\n",
      "Epoch 17/20  Iteration 2905/3560 Training loss: 1.4035 0.9313 sec/batch\n",
      "Epoch 17/20  Iteration 2906/3560 Training loss: 1.4034 0.9875 sec/batch\n",
      "Epoch 17/20  Iteration 2907/3560 Training loss: 1.4030 0.9814 sec/batch\n",
      "Epoch 17/20  Iteration 2908/3560 Training loss: 1.4034 0.9687 sec/batch\n",
      "Epoch 17/20  Iteration 2909/3560 Training loss: 1.4035 0.9121 sec/batch\n",
      "Epoch 17/20  Iteration 2910/3560 Training loss: 1.4043 1.1243 sec/batch\n",
      "Epoch 17/20  Iteration 2911/3560 Training loss: 1.4045 0.9573 sec/batch\n",
      "Epoch 17/20  Iteration 2912/3560 Training loss: 1.4046 0.7606 sec/batch\n",
      "Epoch 17/20  Iteration 2913/3560 Training loss: 1.4047 1.0718 sec/batch\n",
      "Epoch 17/20  Iteration 2914/3560 Training loss: 1.4050 1.0385 sec/batch\n",
      "Epoch 17/20  Iteration 2915/3560 Training loss: 1.4053 1.0548 sec/batch\n",
      "Epoch 17/20  Iteration 2916/3560 Training loss: 1.4051 0.9229 sec/batch\n",
      "Epoch 17/20  Iteration 2917/3560 Training loss: 1.4050 0.9820 sec/batch\n",
      "Epoch 17/20  Iteration 2918/3560 Training loss: 1.4049 1.0545 sec/batch\n",
      "Epoch 17/20  Iteration 2919/3560 Training loss: 1.4056 0.9801 sec/batch\n",
      "Epoch 17/20  Iteration 2920/3560 Training loss: 1.4060 1.0632 sec/batch\n",
      "Epoch 17/20  Iteration 2921/3560 Training loss: 1.4064 0.9478 sec/batch\n",
      "Epoch 17/20  Iteration 2922/3560 Training loss: 1.4063 0.9602 sec/batch\n",
      "Epoch 17/20  Iteration 2923/3560 Training loss: 1.4063 0.9295 sec/batch\n",
      "Epoch 17/20  Iteration 2924/3560 Training loss: 1.4066 0.9950 sec/batch\n",
      "Epoch 17/20  Iteration 2925/3560 Training loss: 1.4065 1.0439 sec/batch\n",
      "Epoch 17/20  Iteration 2926/3560 Training loss: 1.4065 1.0165 sec/batch\n",
      "Epoch 17/20  Iteration 2927/3560 Training loss: 1.4061 0.9350 sec/batch\n",
      "Epoch 17/20  Iteration 2928/3560 Training loss: 1.4061 0.9602 sec/batch\n",
      "Epoch 17/20  Iteration 2929/3560 Training loss: 1.4058 1.0166 sec/batch\n",
      "Epoch 17/20  Iteration 2930/3560 Training loss: 1.4058 1.0504 sec/batch\n",
      "Epoch 17/20  Iteration 2931/3560 Training loss: 1.4054 0.9225 sec/batch\n",
      "Epoch 17/20  Iteration 2932/3560 Training loss: 1.4054 0.9877 sec/batch\n",
      "Epoch 17/20  Iteration 2933/3560 Training loss: 1.4050 0.9621 sec/batch\n",
      "Epoch 17/20  Iteration 2934/3560 Training loss: 1.4048 1.0362 sec/batch\n",
      "Epoch 17/20  Iteration 2935/3560 Training loss: 1.4044 0.9760 sec/batch\n",
      "Epoch 17/20  Iteration 2936/3560 Training loss: 1.4042 1.0475 sec/batch\n",
      "Epoch 17/20  Iteration 2937/3560 Training loss: 1.4037 0.9621 sec/batch\n",
      "Epoch 17/20  Iteration 2938/3560 Training loss: 1.4038 1.0813 sec/batch\n",
      "Epoch 17/20  Iteration 2939/3560 Training loss: 1.4034 1.0018 sec/batch\n",
      "Epoch 17/20  Iteration 2940/3560 Training loss: 1.4034 0.9758 sec/batch\n",
      "Epoch 17/20  Iteration 2941/3560 Training loss: 1.4030 1.0895 sec/batch\n",
      "Epoch 17/20  Iteration 2942/3560 Training loss: 1.4026 0.9307 sec/batch\n",
      "Epoch 17/20  Iteration 2943/3560 Training loss: 1.4024 0.9124 sec/batch\n",
      "Epoch 17/20  Iteration 2944/3560 Training loss: 1.4025 1.0572 sec/batch\n",
      "Epoch 17/20  Iteration 2945/3560 Training loss: 1.4026 1.0951 sec/batch\n",
      "Epoch 17/20  Iteration 2946/3560 Training loss: 1.4022 1.0213 sec/batch\n",
      "Epoch 17/20  Iteration 2947/3560 Training loss: 1.4019 0.9146 sec/batch\n",
      "Epoch 17/20  Iteration 2948/3560 Training loss: 1.4015 1.1040 sec/batch\n",
      "Epoch 17/20  Iteration 2949/3560 Training loss: 1.4015 0.9803 sec/batch\n",
      "Epoch 17/20  Iteration 2950/3560 Training loss: 1.4014 1.0087 sec/batch\n",
      "Epoch 17/20  Iteration 2951/3560 Training loss: 1.4013 1.0276 sec/batch\n",
      "Epoch 17/20  Iteration 2952/3560 Training loss: 1.4012 1.0246 sec/batch\n",
      "Epoch 17/20  Iteration 2953/3560 Training loss: 1.4010 1.0654 sec/batch\n",
      "Epoch 17/20  Iteration 2954/3560 Training loss: 1.4009 0.9472 sec/batch\n",
      "Epoch 17/20  Iteration 2955/3560 Training loss: 1.4009 0.9180 sec/batch\n",
      "Epoch 17/20  Iteration 2956/3560 Training loss: 1.4010 1.0547 sec/batch\n",
      "Epoch 17/20  Iteration 2957/3560 Training loss: 1.4009 1.0167 sec/batch\n",
      "Epoch 17/20  Iteration 2958/3560 Training loss: 1.4010 0.9622 sec/batch\n",
      "Epoch 17/20  Iteration 2959/3560 Training loss: 1.4008 1.0340 sec/batch\n",
      "Epoch 17/20  Iteration 2960/3560 Training loss: 1.4008 1.0206 sec/batch\n",
      "Epoch 17/20  Iteration 2961/3560 Training loss: 1.4008 1.0014 sec/batch\n",
      "Epoch 17/20  Iteration 2962/3560 Training loss: 1.4006 0.9682 sec/batch\n",
      "Epoch 17/20  Iteration 2963/3560 Training loss: 1.4004 0.9144 sec/batch\n",
      "Epoch 17/20  Iteration 2964/3560 Training loss: 1.4001 1.0288 sec/batch\n",
      "Epoch 17/20  Iteration 2965/3560 Training loss: 1.4001 1.0568 sec/batch\n",
      "Epoch 17/20  Iteration 2966/3560 Training loss: 1.4002 1.0502 sec/batch\n",
      "Epoch 17/20  Iteration 2967/3560 Training loss: 1.4001 0.9612 sec/batch\n",
      "Epoch 17/20  Iteration 2968/3560 Training loss: 1.4001 1.0545 sec/batch\n",
      "Epoch 17/20  Iteration 2969/3560 Training loss: 1.4000 0.9514 sec/batch\n",
      "Epoch 17/20  Iteration 2970/3560 Training loss: 1.3998 1.0832 sec/batch\n",
      "Epoch 17/20  Iteration 2971/3560 Training loss: 1.3994 0.9580 sec/batch\n",
      "Epoch 17/20  Iteration 2972/3560 Training loss: 1.3994 1.0511 sec/batch\n",
      "Epoch 17/20  Iteration 2973/3560 Training loss: 1.3994 0.9424 sec/batch\n",
      "Epoch 17/20  Iteration 2974/3560 Training loss: 1.3990 1.0316 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 2975/3560 Training loss: 1.3991 0.9553 sec/batch\n",
      "Epoch 17/20  Iteration 2976/3560 Training loss: 1.3991 1.0414 sec/batch\n",
      "Epoch 17/20  Iteration 2977/3560 Training loss: 1.3989 0.9175 sec/batch\n",
      "Epoch 17/20  Iteration 2978/3560 Training loss: 1.3986 0.9299 sec/batch\n",
      "Epoch 17/20  Iteration 2979/3560 Training loss: 1.3982 0.9463 sec/batch\n",
      "Epoch 17/20  Iteration 2980/3560 Training loss: 1.3979 0.9632 sec/batch\n",
      "Epoch 17/20  Iteration 2981/3560 Training loss: 1.3980 1.0873 sec/batch\n",
      "Epoch 17/20  Iteration 2982/3560 Training loss: 1.3980 0.9512 sec/batch\n",
      "Epoch 17/20  Iteration 2983/3560 Training loss: 1.3980 0.9315 sec/batch\n",
      "Epoch 17/20  Iteration 2984/3560 Training loss: 1.3980 1.0768 sec/batch\n",
      "Epoch 17/20  Iteration 2985/3560 Training loss: 1.3982 0.9842 sec/batch\n",
      "Epoch 17/20  Iteration 2986/3560 Training loss: 1.3982 0.9502 sec/batch\n",
      "Epoch 17/20  Iteration 2987/3560 Training loss: 1.3983 1.0544 sec/batch\n",
      "Epoch 17/20  Iteration 2988/3560 Training loss: 1.3983 1.0027 sec/batch\n",
      "Epoch 17/20  Iteration 2989/3560 Training loss: 1.3986 0.9618 sec/batch\n",
      "Epoch 17/20  Iteration 2990/3560 Training loss: 1.3986 0.9253 sec/batch\n",
      "Epoch 17/20  Iteration 2991/3560 Training loss: 1.3986 1.0201 sec/batch\n",
      "Epoch 17/20  Iteration 2992/3560 Training loss: 1.3988 1.0058 sec/batch\n",
      "Epoch 17/20  Iteration 2993/3560 Training loss: 1.3987 1.0190 sec/batch\n",
      "Epoch 17/20  Iteration 2994/3560 Training loss: 1.3988 0.9971 sec/batch\n",
      "Epoch 17/20  Iteration 2995/3560 Training loss: 1.3989 1.0567 sec/batch\n",
      "Epoch 17/20  Iteration 2996/3560 Training loss: 1.3991 0.9884 sec/batch\n",
      "Epoch 17/20  Iteration 2997/3560 Training loss: 1.3992 0.9401 sec/batch\n",
      "Epoch 17/20  Iteration 2998/3560 Training loss: 1.3990 0.9919 sec/batch\n",
      "Epoch 17/20  Iteration 2999/3560 Training loss: 1.3987 0.9692 sec/batch\n",
      "Epoch 17/20  Iteration 3000/3560 Training loss: 1.3985 1.0238 sec/batch\n",
      "Epoch 17/20  Iteration 3001/3560 Training loss: 1.3987 0.9339 sec/batch\n",
      "Epoch 17/20  Iteration 3002/3560 Training loss: 1.3987 0.9096 sec/batch\n",
      "Epoch 17/20  Iteration 3003/3560 Training loss: 1.3987 0.9855 sec/batch\n",
      "Epoch 17/20  Iteration 3004/3560 Training loss: 1.3988 0.9954 sec/batch\n",
      "Epoch 17/20  Iteration 3005/3560 Training loss: 1.3988 1.0204 sec/batch\n",
      "Epoch 17/20  Iteration 3006/3560 Training loss: 1.3988 0.9422 sec/batch\n",
      "Epoch 17/20  Iteration 3007/3560 Training loss: 1.3986 0.9977 sec/batch\n",
      "Epoch 17/20  Iteration 3008/3560 Training loss: 1.3987 0.9704 sec/batch\n",
      "Epoch 17/20  Iteration 3009/3560 Training loss: 1.3989 0.9315 sec/batch\n",
      "Epoch 17/20  Iteration 3010/3560 Training loss: 1.3990 0.9487 sec/batch\n",
      "Epoch 17/20  Iteration 3011/3560 Training loss: 1.3990 1.0021 sec/batch\n",
      "Epoch 17/20  Iteration 3012/3560 Training loss: 1.3991 1.0296 sec/batch\n",
      "Epoch 17/20  Iteration 3013/3560 Training loss: 1.3991 1.0456 sec/batch\n",
      "Epoch 17/20  Iteration 3014/3560 Training loss: 1.3991 0.9339 sec/batch\n",
      "Epoch 17/20  Iteration 3015/3560 Training loss: 1.3992 1.0689 sec/batch\n",
      "Epoch 17/20  Iteration 3016/3560 Training loss: 1.3997 0.9794 sec/batch\n",
      "Epoch 17/20  Iteration 3017/3560 Training loss: 1.3997 0.9178 sec/batch\n",
      "Epoch 17/20  Iteration 3018/3560 Training loss: 1.3996 1.0106 sec/batch\n",
      "Epoch 17/20  Iteration 3019/3560 Training loss: 1.3997 0.9398 sec/batch\n",
      "Epoch 17/20  Iteration 3020/3560 Training loss: 1.3996 1.1215 sec/batch\n",
      "Epoch 17/20  Iteration 3021/3560 Training loss: 1.3996 1.0867 sec/batch\n",
      "Epoch 17/20  Iteration 3022/3560 Training loss: 1.3996 0.9917 sec/batch\n",
      "Epoch 17/20  Iteration 3023/3560 Training loss: 1.3997 0.9605 sec/batch\n",
      "Epoch 17/20  Iteration 3024/3560 Training loss: 1.3996 1.0791 sec/batch\n",
      "Epoch 17/20  Iteration 3025/3560 Training loss: 1.3995 0.8232 sec/batch\n",
      "Epoch 17/20  Iteration 3026/3560 Training loss: 1.3997 0.9748 sec/batch\n",
      "Epoch 18/20  Iteration 3027/3560 Training loss: 1.4836 0.9827 sec/batch\n",
      "Epoch 18/20  Iteration 3028/3560 Training loss: 1.4498 1.0440 sec/batch\n",
      "Epoch 18/20  Iteration 3029/3560 Training loss: 1.4363 0.9805 sec/batch\n",
      "Epoch 18/20  Iteration 3030/3560 Training loss: 1.4315 0.8490 sec/batch\n",
      "Epoch 18/20  Iteration 3031/3560 Training loss: 1.4225 1.1134 sec/batch\n",
      "Epoch 18/20  Iteration 3032/3560 Training loss: 1.4115 0.8627 sec/batch\n",
      "Epoch 18/20  Iteration 3033/3560 Training loss: 1.4105 0.9709 sec/batch\n",
      "Epoch 18/20  Iteration 3034/3560 Training loss: 1.4072 0.9688 sec/batch\n",
      "Epoch 18/20  Iteration 3035/3560 Training loss: 1.4065 1.0516 sec/batch\n",
      "Epoch 18/20  Iteration 3036/3560 Training loss: 1.4049 0.9901 sec/batch\n",
      "Epoch 18/20  Iteration 3037/3560 Training loss: 1.4017 1.0563 sec/batch\n",
      "Epoch 18/20  Iteration 3038/3560 Training loss: 1.4012 0.9983 sec/batch\n",
      "Epoch 18/20  Iteration 3039/3560 Training loss: 1.4006 0.9874 sec/batch\n",
      "Epoch 18/20  Iteration 3040/3560 Training loss: 1.4011 0.9648 sec/batch\n",
      "Epoch 18/20  Iteration 3041/3560 Training loss: 1.4007 0.9038 sec/batch\n",
      "Epoch 18/20  Iteration 3042/3560 Training loss: 1.3987 0.9180 sec/batch\n",
      "Epoch 18/20  Iteration 3043/3560 Training loss: 1.3983 0.9769 sec/batch\n",
      "Epoch 18/20  Iteration 3044/3560 Training loss: 1.3991 1.0157 sec/batch\n",
      "Epoch 18/20  Iteration 3045/3560 Training loss: 1.3995 0.9395 sec/batch\n",
      "Epoch 18/20  Iteration 3046/3560 Training loss: 1.4009 0.9334 sec/batch\n",
      "Epoch 18/20  Iteration 3047/3560 Training loss: 1.4002 1.0430 sec/batch\n",
      "Epoch 18/20  Iteration 3048/3560 Training loss: 1.4005 0.9057 sec/batch\n",
      "Epoch 18/20  Iteration 3049/3560 Training loss: 1.3995 0.9994 sec/batch\n",
      "Epoch 18/20  Iteration 3050/3560 Training loss: 1.3996 0.8952 sec/batch\n",
      "Epoch 18/20  Iteration 3051/3560 Training loss: 1.3995 0.9975 sec/batch\n",
      "Epoch 18/20  Iteration 3052/3560 Training loss: 1.3978 1.0363 sec/batch\n",
      "Epoch 18/20  Iteration 3053/3560 Training loss: 1.3968 1.0144 sec/batch\n",
      "Epoch 18/20  Iteration 3054/3560 Training loss: 1.3975 0.9942 sec/batch\n",
      "Epoch 18/20  Iteration 3055/3560 Training loss: 1.3980 1.0350 sec/batch\n",
      "Epoch 18/20  Iteration 3056/3560 Training loss: 1.3982 0.9408 sec/batch\n",
      "Epoch 18/20  Iteration 3057/3560 Training loss: 1.3976 0.9902 sec/batch\n",
      "Epoch 18/20  Iteration 3058/3560 Training loss: 1.3966 0.8950 sec/batch\n",
      "Epoch 18/20  Iteration 3059/3560 Training loss: 1.3966 1.0173 sec/batch\n",
      "Epoch 18/20  Iteration 3060/3560 Training loss: 1.3968 0.9830 sec/batch\n",
      "Epoch 18/20  Iteration 3061/3560 Training loss: 1.3969 0.9868 sec/batch\n",
      "Epoch 18/20  Iteration 3062/3560 Training loss: 1.3969 1.0175 sec/batch\n",
      "Epoch 18/20  Iteration 3063/3560 Training loss: 1.3961 1.0673 sec/batch\n",
      "Epoch 18/20  Iteration 3064/3560 Training loss: 1.3950 0.9135 sec/batch\n",
      "Epoch 18/20  Iteration 3065/3560 Training loss: 1.3939 0.9555 sec/batch\n",
      "Epoch 18/20  Iteration 3066/3560 Training loss: 1.3936 0.9284 sec/batch\n",
      "Epoch 18/20  Iteration 3067/3560 Training loss: 1.3929 1.1118 sec/batch\n",
      "Epoch 18/20  Iteration 3068/3560 Training loss: 1.3937 0.9933 sec/batch\n",
      "Epoch 18/20  Iteration 3069/3560 Training loss: 1.3935 0.9713 sec/batch\n",
      "Epoch 18/20  Iteration 3070/3560 Training loss: 1.3928 0.9938 sec/batch\n",
      "Epoch 18/20  Iteration 3071/3560 Training loss: 1.3932 0.9720 sec/batch\n",
      "Epoch 18/20  Iteration 3072/3560 Training loss: 1.3923 0.9776 sec/batch\n",
      "Epoch 18/20  Iteration 3073/3560 Training loss: 1.3919 0.9402 sec/batch\n",
      "Epoch 18/20  Iteration 3074/3560 Training loss: 1.3914 0.9606 sec/batch\n",
      "Epoch 18/20  Iteration 3075/3560 Training loss: 1.3913 1.0704 sec/batch\n",
      "Epoch 18/20  Iteration 3076/3560 Training loss: 1.3916 0.9548 sec/batch\n",
      "Epoch 18/20  Iteration 3077/3560 Training loss: 1.3913 1.0330 sec/batch\n",
      "Epoch 18/20  Iteration 3078/3560 Training loss: 1.3919 0.9940 sec/batch\n",
      "Epoch 18/20  Iteration 3079/3560 Training loss: 1.3917 0.9344 sec/batch\n",
      "Epoch 18/20  Iteration 3080/3560 Training loss: 1.3921 0.9519 sec/batch\n",
      "Epoch 18/20  Iteration 3081/3560 Training loss: 1.3919 0.8887 sec/batch\n",
      "Epoch 18/20  Iteration 3082/3560 Training loss: 1.3920 1.0195 sec/batch\n",
      "Epoch 18/20  Iteration 3083/3560 Training loss: 1.3924 1.0231 sec/batch\n",
      "Epoch 18/20  Iteration 3084/3560 Training loss: 1.3919 0.9413 sec/batch\n",
      "Epoch 18/20  Iteration 3085/3560 Training loss: 1.3914 0.8696 sec/batch\n",
      "Epoch 18/20  Iteration 3086/3560 Training loss: 1.3921 1.0515 sec/batch\n",
      "Epoch 18/20  Iteration 3087/3560 Training loss: 1.3921 1.0184 sec/batch\n",
      "Epoch 18/20  Iteration 3088/3560 Training loss: 1.3929 0.9920 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3089/3560 Training loss: 1.3933 0.9694 sec/batch\n",
      "Epoch 18/20  Iteration 3090/3560 Training loss: 1.3935 0.9901 sec/batch\n",
      "Epoch 18/20  Iteration 3091/3560 Training loss: 1.3934 1.0210 sec/batch\n",
      "Epoch 18/20  Iteration 3092/3560 Training loss: 1.3937 0.8957 sec/batch\n",
      "Epoch 18/20  Iteration 3093/3560 Training loss: 1.3938 1.0035 sec/batch\n",
      "Epoch 18/20  Iteration 3094/3560 Training loss: 1.3935 0.9956 sec/batch\n",
      "Epoch 18/20  Iteration 3095/3560 Training loss: 1.3935 0.9520 sec/batch\n",
      "Epoch 18/20  Iteration 3096/3560 Training loss: 1.3933 0.9662 sec/batch\n",
      "Epoch 18/20  Iteration 3097/3560 Training loss: 1.3941 0.8831 sec/batch\n",
      "Epoch 18/20  Iteration 3098/3560 Training loss: 1.3944 1.0130 sec/batch\n",
      "Epoch 18/20  Iteration 3099/3560 Training loss: 1.3948 1.0057 sec/batch\n",
      "Epoch 18/20  Iteration 3100/3560 Training loss: 1.3945 1.0471 sec/batch\n",
      "Epoch 18/20  Iteration 3101/3560 Training loss: 1.3946 0.9439 sec/batch\n",
      "Epoch 18/20  Iteration 3102/3560 Training loss: 1.3948 0.9898 sec/batch\n",
      "Epoch 18/20  Iteration 3103/3560 Training loss: 1.3945 0.9345 sec/batch\n",
      "Epoch 18/20  Iteration 3104/3560 Training loss: 1.3945 0.9871 sec/batch\n",
      "Epoch 18/20  Iteration 3105/3560 Training loss: 1.3940 0.9064 sec/batch\n",
      "Epoch 18/20  Iteration 3106/3560 Training loss: 1.3940 1.0692 sec/batch\n",
      "Epoch 18/20  Iteration 3107/3560 Training loss: 1.3937 1.0381 sec/batch\n",
      "Epoch 18/20  Iteration 3108/3560 Training loss: 1.3937 0.9392 sec/batch\n",
      "Epoch 18/20  Iteration 3109/3560 Training loss: 1.3933 0.9856 sec/batch\n",
      "Epoch 18/20  Iteration 3110/3560 Training loss: 1.3934 1.0310 sec/batch\n",
      "Epoch 18/20  Iteration 3111/3560 Training loss: 1.3931 0.9437 sec/batch\n",
      "Epoch 18/20  Iteration 3112/3560 Training loss: 1.3931 0.9829 sec/batch\n",
      "Epoch 18/20  Iteration 3113/3560 Training loss: 1.3928 0.9415 sec/batch\n",
      "Epoch 18/20  Iteration 3114/3560 Training loss: 1.3925 1.0188 sec/batch\n",
      "Epoch 18/20  Iteration 3115/3560 Training loss: 1.3921 0.9620 sec/batch\n",
      "Epoch 18/20  Iteration 3116/3560 Training loss: 1.3922 0.8988 sec/batch\n",
      "Epoch 18/20  Iteration 3117/3560 Training loss: 1.3919 0.9943 sec/batch\n",
      "Epoch 18/20  Iteration 3118/3560 Training loss: 1.3918 1.0386 sec/batch\n",
      "Epoch 18/20  Iteration 3119/3560 Training loss: 1.3915 1.0065 sec/batch\n",
      "Epoch 18/20  Iteration 3120/3560 Training loss: 1.3912 0.9557 sec/batch\n",
      "Epoch 18/20  Iteration 3121/3560 Training loss: 1.3910 0.9117 sec/batch\n",
      "Epoch 18/20  Iteration 3122/3560 Training loss: 1.3910 1.0652 sec/batch\n",
      "Epoch 18/20  Iteration 3123/3560 Training loss: 1.3910 1.0031 sec/batch\n",
      "Epoch 18/20  Iteration 3124/3560 Training loss: 1.3906 0.9743 sec/batch\n",
      "Epoch 18/20  Iteration 3125/3560 Training loss: 1.3903 0.9737 sec/batch\n",
      "Epoch 18/20  Iteration 3126/3560 Training loss: 1.3900 0.9294 sec/batch\n",
      "Epoch 18/20  Iteration 3127/3560 Training loss: 1.3900 1.0117 sec/batch\n",
      "Epoch 18/20  Iteration 3128/3560 Training loss: 1.3899 1.0519 sec/batch\n",
      "Epoch 18/20  Iteration 3129/3560 Training loss: 1.3899 1.0662 sec/batch\n",
      "Epoch 18/20  Iteration 3130/3560 Training loss: 1.3899 0.9746 sec/batch\n",
      "Epoch 18/20  Iteration 3131/3560 Training loss: 1.3897 0.9682 sec/batch\n",
      "Epoch 18/20  Iteration 3132/3560 Training loss: 1.3896 0.9479 sec/batch\n",
      "Epoch 18/20  Iteration 3133/3560 Training loss: 1.3896 0.9689 sec/batch\n",
      "Epoch 18/20  Iteration 3134/3560 Training loss: 1.3896 1.0495 sec/batch\n",
      "Epoch 18/20  Iteration 3135/3560 Training loss: 1.3895 1.0354 sec/batch\n",
      "Epoch 18/20  Iteration 3136/3560 Training loss: 1.3896 0.8543 sec/batch\n",
      "Epoch 18/20  Iteration 3137/3560 Training loss: 1.3893 0.9508 sec/batch\n",
      "Epoch 18/20  Iteration 3138/3560 Training loss: 1.3893 1.0477 sec/batch\n",
      "Epoch 18/20  Iteration 3139/3560 Training loss: 1.3891 1.0650 sec/batch\n",
      "Epoch 18/20  Iteration 3140/3560 Training loss: 1.3890 0.9362 sec/batch\n",
      "Epoch 18/20  Iteration 3141/3560 Training loss: 1.3888 1.0391 sec/batch\n",
      "Epoch 18/20  Iteration 3142/3560 Training loss: 1.3885 0.9804 sec/batch\n",
      "Epoch 18/20  Iteration 3143/3560 Training loss: 1.3886 0.9798 sec/batch\n",
      "Epoch 18/20  Iteration 3144/3560 Training loss: 1.3886 0.8896 sec/batch\n",
      "Epoch 18/20  Iteration 3145/3560 Training loss: 1.3886 1.0866 sec/batch\n",
      "Epoch 18/20  Iteration 3146/3560 Training loss: 1.3885 1.0179 sec/batch\n",
      "Epoch 18/20  Iteration 3147/3560 Training loss: 1.3884 0.9673 sec/batch\n",
      "Epoch 18/20  Iteration 3148/3560 Training loss: 1.3881 1.0016 sec/batch\n",
      "Epoch 18/20  Iteration 3149/3560 Training loss: 1.3877 1.0408 sec/batch\n",
      "Epoch 18/20  Iteration 3150/3560 Training loss: 1.3878 0.9604 sec/batch\n",
      "Epoch 18/20  Iteration 3151/3560 Training loss: 1.3878 0.9934 sec/batch\n",
      "Epoch 18/20  Iteration 3152/3560 Training loss: 1.3873 0.9330 sec/batch\n",
      "Epoch 18/20  Iteration 3153/3560 Training loss: 1.3874 1.0040 sec/batch\n",
      "Epoch 18/20  Iteration 3154/3560 Training loss: 1.3874 0.9901 sec/batch\n",
      "Epoch 18/20  Iteration 3155/3560 Training loss: 1.3873 0.9576 sec/batch\n",
      "Epoch 18/20  Iteration 3156/3560 Training loss: 1.3870 0.9954 sec/batch\n",
      "Epoch 18/20  Iteration 3157/3560 Training loss: 1.3865 1.0287 sec/batch\n",
      "Epoch 18/20  Iteration 3158/3560 Training loss: 1.3863 0.9579 sec/batch\n",
      "Epoch 18/20  Iteration 3159/3560 Training loss: 1.3864 1.0261 sec/batch\n",
      "Epoch 18/20  Iteration 3160/3560 Training loss: 1.3864 0.9778 sec/batch\n",
      "Epoch 18/20  Iteration 3161/3560 Training loss: 1.3864 0.9369 sec/batch\n",
      "Epoch 18/20  Iteration 3162/3560 Training loss: 1.3865 0.9993 sec/batch\n",
      "Epoch 18/20  Iteration 3163/3560 Training loss: 1.3867 0.9502 sec/batch\n",
      "Epoch 18/20  Iteration 3164/3560 Training loss: 1.3868 0.9859 sec/batch\n",
      "Epoch 18/20  Iteration 3165/3560 Training loss: 1.3868 0.9975 sec/batch\n",
      "Epoch 18/20  Iteration 3166/3560 Training loss: 1.3868 0.9936 sec/batch\n",
      "Epoch 18/20  Iteration 3167/3560 Training loss: 1.3872 0.9820 sec/batch\n",
      "Epoch 18/20  Iteration 3168/3560 Training loss: 1.3872 1.0059 sec/batch\n",
      "Epoch 18/20  Iteration 3169/3560 Training loss: 1.3872 1.0019 sec/batch\n",
      "Epoch 18/20  Iteration 3170/3560 Training loss: 1.3875 0.9199 sec/batch\n",
      "Epoch 18/20  Iteration 3171/3560 Training loss: 1.3873 0.9525 sec/batch\n",
      "Epoch 18/20  Iteration 3172/3560 Training loss: 1.3875 0.9176 sec/batch\n",
      "Epoch 18/20  Iteration 3173/3560 Training loss: 1.3875 1.0246 sec/batch\n",
      "Epoch 18/20  Iteration 3174/3560 Training loss: 1.3878 1.0022 sec/batch\n",
      "Epoch 18/20  Iteration 3175/3560 Training loss: 1.3880 0.9897 sec/batch\n",
      "Epoch 18/20  Iteration 3176/3560 Training loss: 1.3878 0.9262 sec/batch\n",
      "Epoch 18/20  Iteration 3177/3560 Training loss: 1.3875 0.9932 sec/batch\n",
      "Epoch 18/20  Iteration 3178/3560 Training loss: 1.3874 1.1036 sec/batch\n",
      "Epoch 18/20  Iteration 3179/3560 Training loss: 1.3875 0.9512 sec/batch\n",
      "Epoch 18/20  Iteration 3180/3560 Training loss: 1.3875 1.0673 sec/batch\n",
      "Epoch 18/20  Iteration 3181/3560 Training loss: 1.3874 1.0011 sec/batch\n",
      "Epoch 18/20  Iteration 3182/3560 Training loss: 1.3875 0.9284 sec/batch\n",
      "Epoch 18/20  Iteration 3183/3560 Training loss: 1.3875 0.9567 sec/batch\n",
      "Epoch 18/20  Iteration 3184/3560 Training loss: 1.3875 0.9275 sec/batch\n",
      "Epoch 18/20  Iteration 3185/3560 Training loss: 1.3873 1.0745 sec/batch\n",
      "Epoch 18/20  Iteration 3186/3560 Training loss: 1.3874 1.0181 sec/batch\n",
      "Epoch 18/20  Iteration 3187/3560 Training loss: 1.3877 0.9427 sec/batch\n",
      "Epoch 18/20  Iteration 3188/3560 Training loss: 1.3877 1.0334 sec/batch\n",
      "Epoch 18/20  Iteration 3189/3560 Training loss: 1.3877 0.9939 sec/batch\n",
      "Epoch 18/20  Iteration 3190/3560 Training loss: 1.3877 1.0689 sec/batch\n",
      "Epoch 18/20  Iteration 3191/3560 Training loss: 1.3878 0.9607 sec/batch\n",
      "Epoch 18/20  Iteration 3192/3560 Training loss: 1.3879 0.9256 sec/batch\n",
      "Epoch 18/20  Iteration 3193/3560 Training loss: 1.3881 1.0354 sec/batch\n",
      "Epoch 18/20  Iteration 3194/3560 Training loss: 1.3885 0.9391 sec/batch\n",
      "Epoch 18/20  Iteration 3195/3560 Training loss: 1.3886 1.0262 sec/batch\n",
      "Epoch 18/20  Iteration 3196/3560 Training loss: 1.3886 0.9993 sec/batch\n",
      "Epoch 18/20  Iteration 3197/3560 Training loss: 1.3886 0.9916 sec/batch\n",
      "Epoch 18/20  Iteration 3198/3560 Training loss: 1.3885 0.9954 sec/batch\n",
      "Epoch 18/20  Iteration 3199/3560 Training loss: 1.3886 0.9932 sec/batch\n",
      "Epoch 18/20  Iteration 3200/3560 Training loss: 1.3886 0.9905 sec/batch\n",
      "Epoch 18/20  Iteration 3201/3560 Training loss: 1.3886 1.0232 sec/batch\n",
      "Epoch 18/20  Iteration 3202/3560 Training loss: 1.3885 0.8990 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3203/3560 Training loss: 1.3883 0.9110 sec/batch\n",
      "Epoch 18/20  Iteration 3204/3560 Training loss: 1.3885 1.0173 sec/batch\n",
      "Epoch 19/20  Iteration 3205/3560 Training loss: 1.4687 0.9572 sec/batch\n",
      "Epoch 19/20  Iteration 3206/3560 Training loss: 1.4356 0.9836 sec/batch\n",
      "Epoch 19/20  Iteration 3207/3560 Training loss: 1.4249 0.9545 sec/batch\n",
      "Epoch 19/20  Iteration 3208/3560 Training loss: 1.4191 1.0048 sec/batch\n",
      "Epoch 19/20  Iteration 3209/3560 Training loss: 1.4090 1.0029 sec/batch\n",
      "Epoch 19/20  Iteration 3210/3560 Training loss: 1.3977 1.0132 sec/batch\n",
      "Epoch 19/20  Iteration 3211/3560 Training loss: 1.3977 0.9953 sec/batch\n",
      "Epoch 19/20  Iteration 3212/3560 Training loss: 1.3957 1.0811 sec/batch\n",
      "Epoch 19/20  Iteration 3213/3560 Training loss: 1.3944 0.9416 sec/batch\n",
      "Epoch 19/20  Iteration 3214/3560 Training loss: 1.3929 0.9085 sec/batch\n",
      "Epoch 19/20  Iteration 3215/3560 Training loss: 1.3893 0.9595 sec/batch\n",
      "Epoch 19/20  Iteration 3216/3560 Training loss: 1.3894 1.0211 sec/batch\n",
      "Epoch 19/20  Iteration 3217/3560 Training loss: 1.3896 1.0442 sec/batch\n",
      "Epoch 19/20  Iteration 3218/3560 Training loss: 1.3905 1.0245 sec/batch\n",
      "Epoch 19/20  Iteration 3219/3560 Training loss: 1.3898 1.0428 sec/batch\n",
      "Epoch 19/20  Iteration 3220/3560 Training loss: 1.3881 1.0433 sec/batch\n",
      "Epoch 19/20  Iteration 3221/3560 Training loss: 1.3881 1.0147 sec/batch\n",
      "Epoch 19/20  Iteration 3222/3560 Training loss: 1.3893 0.9732 sec/batch\n",
      "Epoch 19/20  Iteration 3223/3560 Training loss: 1.3890 1.0147 sec/batch\n",
      "Epoch 19/20  Iteration 3224/3560 Training loss: 1.3898 1.0037 sec/batch\n",
      "Epoch 19/20  Iteration 3225/3560 Training loss: 1.3891 0.9641 sec/batch\n",
      "Epoch 19/20  Iteration 3226/3560 Training loss: 1.3891 0.9941 sec/batch\n",
      "Epoch 19/20  Iteration 3227/3560 Training loss: 1.3886 0.9997 sec/batch\n",
      "Epoch 19/20  Iteration 3228/3560 Training loss: 1.3888 1.0322 sec/batch\n",
      "Epoch 19/20  Iteration 3229/3560 Training loss: 1.3886 0.9569 sec/batch\n",
      "Epoch 19/20  Iteration 3230/3560 Training loss: 1.3868 1.0082 sec/batch\n",
      "Epoch 19/20  Iteration 3231/3560 Training loss: 1.3857 0.9220 sec/batch\n",
      "Epoch 19/20  Iteration 3232/3560 Training loss: 1.3864 1.0512 sec/batch\n",
      "Epoch 19/20  Iteration 3233/3560 Training loss: 1.3864 0.9528 sec/batch\n",
      "Epoch 19/20  Iteration 3234/3560 Training loss: 1.3863 0.9694 sec/batch\n",
      "Epoch 19/20  Iteration 3235/3560 Training loss: 1.3857 0.9950 sec/batch\n",
      "Epoch 19/20  Iteration 3236/3560 Training loss: 1.3848 0.9021 sec/batch\n",
      "Epoch 19/20  Iteration 3237/3560 Training loss: 1.3851 1.0143 sec/batch\n",
      "Epoch 19/20  Iteration 3238/3560 Training loss: 1.3853 0.9371 sec/batch\n",
      "Epoch 19/20  Iteration 3239/3560 Training loss: 1.3852 1.0350 sec/batch\n",
      "Epoch 19/20  Iteration 3240/3560 Training loss: 1.3849 1.0317 sec/batch\n",
      "Epoch 19/20  Iteration 3241/3560 Training loss: 1.3842 0.9910 sec/batch\n",
      "Epoch 19/20  Iteration 3242/3560 Training loss: 1.3832 0.9353 sec/batch\n",
      "Epoch 19/20  Iteration 3243/3560 Training loss: 1.3819 0.9699 sec/batch\n",
      "Epoch 19/20  Iteration 3244/3560 Training loss: 1.3816 0.9290 sec/batch\n",
      "Epoch 19/20  Iteration 3245/3560 Training loss: 1.3809 0.9960 sec/batch\n",
      "Epoch 19/20  Iteration 3246/3560 Training loss: 1.3817 0.9630 sec/batch\n",
      "Epoch 19/20  Iteration 3247/3560 Training loss: 1.3815 0.9690 sec/batch\n",
      "Epoch 19/20  Iteration 3248/3560 Training loss: 1.3807 1.0370 sec/batch\n",
      "Epoch 19/20  Iteration 3249/3560 Training loss: 1.3811 1.0049 sec/batch\n",
      "Epoch 19/20  Iteration 3250/3560 Training loss: 1.3802 1.1611 sec/batch\n",
      "Epoch 19/20  Iteration 3251/3560 Training loss: 1.3798 0.9381 sec/batch\n",
      "Epoch 19/20  Iteration 3252/3560 Training loss: 1.3793 0.9337 sec/batch\n",
      "Epoch 19/20  Iteration 3253/3560 Training loss: 1.3793 0.9159 sec/batch\n",
      "Epoch 19/20  Iteration 3254/3560 Training loss: 1.3797 0.9600 sec/batch\n",
      "Epoch 19/20  Iteration 3255/3560 Training loss: 1.3793 0.9904 sec/batch\n",
      "Epoch 19/20  Iteration 3256/3560 Training loss: 1.3800 1.0028 sec/batch\n",
      "Epoch 19/20  Iteration 3257/3560 Training loss: 1.3799 0.9915 sec/batch\n",
      "Epoch 19/20  Iteration 3258/3560 Training loss: 1.3802 0.9808 sec/batch\n",
      "Epoch 19/20  Iteration 3259/3560 Training loss: 1.3799 1.0685 sec/batch\n",
      "Epoch 19/20  Iteration 3260/3560 Training loss: 1.3801 0.8764 sec/batch\n",
      "Epoch 19/20  Iteration 3261/3560 Training loss: 1.3805 1.0572 sec/batch\n",
      "Epoch 19/20  Iteration 3262/3560 Training loss: 1.3801 0.9369 sec/batch\n",
      "Epoch 19/20  Iteration 3263/3560 Training loss: 1.3798 0.9796 sec/batch\n",
      "Epoch 19/20  Iteration 3264/3560 Training loss: 1.3804 1.0087 sec/batch\n",
      "Epoch 19/20  Iteration 3265/3560 Training loss: 1.3805 0.9044 sec/batch\n",
      "Epoch 19/20  Iteration 3266/3560 Training loss: 1.3814 0.9852 sec/batch\n",
      "Epoch 19/20  Iteration 3267/3560 Training loss: 1.3818 1.0180 sec/batch\n",
      "Epoch 19/20  Iteration 3268/3560 Training loss: 1.3820 0.9695 sec/batch\n",
      "Epoch 19/20  Iteration 3269/3560 Training loss: 1.3819 0.9725 sec/batch\n",
      "Epoch 19/20  Iteration 3270/3560 Training loss: 1.3821 1.0102 sec/batch\n",
      "Epoch 19/20  Iteration 3271/3560 Training loss: 1.3825 1.0303 sec/batch\n",
      "Epoch 19/20  Iteration 3272/3560 Training loss: 1.3823 1.0119 sec/batch\n",
      "Epoch 19/20  Iteration 3273/3560 Training loss: 1.3823 1.0080 sec/batch\n",
      "Epoch 19/20  Iteration 3274/3560 Training loss: 1.3820 1.0242 sec/batch\n",
      "Epoch 19/20  Iteration 3275/3560 Training loss: 1.3827 0.9399 sec/batch\n",
      "Epoch 19/20  Iteration 3276/3560 Training loss: 1.3830 0.9141 sec/batch\n",
      "Epoch 19/20  Iteration 3277/3560 Training loss: 1.3835 0.9238 sec/batch\n",
      "Epoch 19/20  Iteration 3278/3560 Training loss: 1.3833 0.9407 sec/batch\n",
      "Epoch 19/20  Iteration 3279/3560 Training loss: 1.3834 1.0418 sec/batch\n",
      "Epoch 19/20  Iteration 3280/3560 Training loss: 1.3835 0.9978 sec/batch\n",
      "Epoch 19/20  Iteration 3281/3560 Training loss: 1.3834 0.9338 sec/batch\n",
      "Epoch 19/20  Iteration 3282/3560 Training loss: 1.3834 0.9322 sec/batch\n",
      "Epoch 19/20  Iteration 3283/3560 Training loss: 1.3828 1.0791 sec/batch\n",
      "Epoch 19/20  Iteration 3284/3560 Training loss: 1.3828 0.9373 sec/batch\n",
      "Epoch 19/20  Iteration 3285/3560 Training loss: 1.3824 0.9650 sec/batch\n",
      "Epoch 19/20  Iteration 3286/3560 Training loss: 1.3825 1.0003 sec/batch\n",
      "Epoch 19/20  Iteration 3287/3560 Training loss: 1.3822 0.9715 sec/batch\n",
      "Epoch 19/20  Iteration 3288/3560 Training loss: 1.3821 0.9743 sec/batch\n",
      "Epoch 19/20  Iteration 3289/3560 Training loss: 1.3819 0.9535 sec/batch\n",
      "Epoch 19/20  Iteration 3290/3560 Training loss: 1.3818 1.0077 sec/batch\n",
      "Epoch 19/20  Iteration 3291/3560 Training loss: 1.3814 0.9837 sec/batch\n",
      "Epoch 19/20  Iteration 3292/3560 Training loss: 1.3812 1.0215 sec/batch\n",
      "Epoch 19/20  Iteration 3293/3560 Training loss: 1.3809 0.9711 sec/batch\n",
      "Epoch 19/20  Iteration 3294/3560 Training loss: 1.3810 0.9661 sec/batch\n",
      "Epoch 19/20  Iteration 3295/3560 Training loss: 1.3807 1.0715 sec/batch\n",
      "Epoch 19/20  Iteration 3296/3560 Training loss: 1.3806 0.9721 sec/batch\n",
      "Epoch 19/20  Iteration 3297/3560 Training loss: 1.3801 0.9913 sec/batch\n",
      "Epoch 19/20  Iteration 3298/3560 Training loss: 1.3798 0.9407 sec/batch\n",
      "Epoch 19/20  Iteration 3299/3560 Training loss: 1.3796 0.9983 sec/batch\n",
      "Epoch 19/20  Iteration 3300/3560 Training loss: 1.3797 0.9732 sec/batch\n",
      "Epoch 19/20  Iteration 3301/3560 Training loss: 1.3798 0.8952 sec/batch\n",
      "Epoch 19/20  Iteration 3302/3560 Training loss: 1.3794 0.9390 sec/batch\n",
      "Epoch 19/20  Iteration 3303/3560 Training loss: 1.3791 1.0157 sec/batch\n",
      "Epoch 19/20  Iteration 3304/3560 Training loss: 1.3789 1.0080 sec/batch\n",
      "Epoch 19/20  Iteration 3305/3560 Training loss: 1.3788 0.9512 sec/batch\n",
      "Epoch 19/20  Iteration 3306/3560 Training loss: 1.3787 1.1010 sec/batch\n",
      "Epoch 19/20  Iteration 3307/3560 Training loss: 1.3786 0.9653 sec/batch\n",
      "Epoch 19/20  Iteration 3308/3560 Training loss: 1.3786 0.9661 sec/batch\n",
      "Epoch 19/20  Iteration 3309/3560 Training loss: 1.3785 0.9597 sec/batch\n",
      "Epoch 19/20  Iteration 3310/3560 Training loss: 1.3785 0.9635 sec/batch\n",
      "Epoch 19/20  Iteration 3311/3560 Training loss: 1.3784 1.0058 sec/batch\n",
      "Epoch 19/20  Iteration 3312/3560 Training loss: 1.3784 0.9098 sec/batch\n",
      "Epoch 19/20  Iteration 3313/3560 Training loss: 1.3783 1.0047 sec/batch\n",
      "Epoch 19/20  Iteration 3314/3560 Training loss: 1.3784 1.0603 sec/batch\n",
      "Epoch 19/20  Iteration 3315/3560 Training loss: 1.3781 0.9925 sec/batch\n",
      "Epoch 19/20  Iteration 3316/3560 Training loss: 1.3781 1.0125 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3317/3560 Training loss: 1.3780 1.0124 sec/batch\n",
      "Epoch 19/20  Iteration 3318/3560 Training loss: 1.3778 0.9488 sec/batch\n",
      "Epoch 19/20  Iteration 3319/3560 Training loss: 1.3776 0.9654 sec/batch\n",
      "Epoch 19/20  Iteration 3320/3560 Training loss: 1.3773 1.0350 sec/batch\n",
      "Epoch 19/20  Iteration 3321/3560 Training loss: 1.3773 1.0016 sec/batch\n",
      "Epoch 19/20  Iteration 3322/3560 Training loss: 1.3774 1.0744 sec/batch\n",
      "Epoch 19/20  Iteration 3323/3560 Training loss: 1.3774 0.9726 sec/batch\n",
      "Epoch 19/20  Iteration 3324/3560 Training loss: 1.3774 1.0347 sec/batch\n",
      "Epoch 19/20  Iteration 3325/3560 Training loss: 1.3773 1.0017 sec/batch\n",
      "Epoch 19/20  Iteration 3326/3560 Training loss: 1.3770 0.9534 sec/batch\n",
      "Epoch 19/20  Iteration 3327/3560 Training loss: 1.3767 0.9053 sec/batch\n",
      "Epoch 19/20  Iteration 3328/3560 Training loss: 1.3767 1.0120 sec/batch\n",
      "Epoch 19/20  Iteration 3329/3560 Training loss: 1.3766 0.9793 sec/batch\n",
      "Epoch 19/20  Iteration 3330/3560 Training loss: 1.3762 1.0160 sec/batch\n",
      "Epoch 19/20  Iteration 3331/3560 Training loss: 1.3764 0.9266 sec/batch\n",
      "Epoch 19/20  Iteration 3332/3560 Training loss: 1.3764 0.9978 sec/batch\n",
      "Epoch 19/20  Iteration 3333/3560 Training loss: 1.3763 1.0273 sec/batch\n",
      "Epoch 19/20  Iteration 3334/3560 Training loss: 1.3761 0.9637 sec/batch\n",
      "Epoch 19/20  Iteration 3335/3560 Training loss: 1.3756 0.9947 sec/batch\n",
      "Epoch 19/20  Iteration 3336/3560 Training loss: 1.3755 1.0387 sec/batch\n",
      "Epoch 19/20  Iteration 3337/3560 Training loss: 1.3755 1.0078 sec/batch\n",
      "Epoch 19/20  Iteration 3338/3560 Training loss: 1.3755 1.0526 sec/batch\n",
      "Epoch 19/20  Iteration 3339/3560 Training loss: 1.3755 0.9257 sec/batch\n",
      "Epoch 19/20  Iteration 3340/3560 Training loss: 1.3756 1.1578 sec/batch\n",
      "Epoch 19/20  Iteration 3341/3560 Training loss: 1.3758 0.9037 sec/batch\n",
      "Epoch 19/20  Iteration 3342/3560 Training loss: 1.3759 1.0156 sec/batch\n",
      "Epoch 19/20  Iteration 3343/3560 Training loss: 1.3759 1.0349 sec/batch\n",
      "Epoch 19/20  Iteration 3344/3560 Training loss: 1.3758 1.0282 sec/batch\n",
      "Epoch 19/20  Iteration 3345/3560 Training loss: 1.3762 1.0783 sec/batch\n",
      "Epoch 19/20  Iteration 3346/3560 Training loss: 1.3762 1.0271 sec/batch\n",
      "Epoch 19/20  Iteration 3347/3560 Training loss: 1.3762 0.9238 sec/batch\n",
      "Epoch 19/20  Iteration 3348/3560 Training loss: 1.3764 0.8627 sec/batch\n",
      "Epoch 19/20  Iteration 3349/3560 Training loss: 1.3763 0.9850 sec/batch\n",
      "Epoch 19/20  Iteration 3350/3560 Training loss: 1.3765 1.0119 sec/batch\n",
      "Epoch 19/20  Iteration 3351/3560 Training loss: 1.3766 0.9473 sec/batch\n",
      "Epoch 19/20  Iteration 3352/3560 Training loss: 1.3768 1.0708 sec/batch\n",
      "Epoch 19/20  Iteration 3353/3560 Training loss: 1.3770 1.0472 sec/batch\n",
      "Epoch 19/20  Iteration 3354/3560 Training loss: 1.3769 1.0481 sec/batch\n",
      "Epoch 19/20  Iteration 3355/3560 Training loss: 1.3765 0.9757 sec/batch\n",
      "Epoch 19/20  Iteration 3356/3560 Training loss: 1.3765 1.0562 sec/batch\n",
      "Epoch 19/20  Iteration 3357/3560 Training loss: 1.3765 1.0210 sec/batch\n",
      "Epoch 19/20  Iteration 3358/3560 Training loss: 1.3766 0.9141 sec/batch\n",
      "Epoch 19/20  Iteration 3359/3560 Training loss: 1.3766 1.0546 sec/batch\n",
      "Epoch 19/20  Iteration 3360/3560 Training loss: 1.3765 1.1425 sec/batch\n",
      "Epoch 19/20  Iteration 3361/3560 Training loss: 1.3765 0.9889 sec/batch\n",
      "Epoch 19/20  Iteration 3362/3560 Training loss: 1.3765 0.9659 sec/batch\n",
      "Epoch 19/20  Iteration 3363/3560 Training loss: 1.3763 0.9676 sec/batch\n",
      "Epoch 19/20  Iteration 3364/3560 Training loss: 1.3765 0.9113 sec/batch\n",
      "Epoch 19/20  Iteration 3365/3560 Training loss: 1.3767 0.9796 sec/batch\n",
      "Epoch 19/20  Iteration 3366/3560 Training loss: 1.3767 1.0112 sec/batch\n",
      "Epoch 19/20  Iteration 3367/3560 Training loss: 1.3768 0.9878 sec/batch\n",
      "Epoch 19/20  Iteration 3368/3560 Training loss: 1.3768 1.0214 sec/batch\n",
      "Epoch 19/20  Iteration 3369/3560 Training loss: 1.3768 0.9685 sec/batch\n",
      "Epoch 19/20  Iteration 3370/3560 Training loss: 1.3768 0.9912 sec/batch\n",
      "Epoch 19/20  Iteration 3371/3560 Training loss: 1.3769 0.8813 sec/batch\n",
      "Epoch 19/20  Iteration 3372/3560 Training loss: 1.3774 1.0124 sec/batch\n",
      "Epoch 19/20  Iteration 3373/3560 Training loss: 1.3774 1.0052 sec/batch\n",
      "Epoch 19/20  Iteration 3374/3560 Training loss: 1.3774 0.9726 sec/batch\n",
      "Epoch 19/20  Iteration 3375/3560 Training loss: 1.3773 1.0416 sec/batch\n",
      "Epoch 19/20  Iteration 3376/3560 Training loss: 1.3772 1.0625 sec/batch\n",
      "Epoch 19/20  Iteration 3377/3560 Training loss: 1.3773 0.9280 sec/batch\n",
      "Epoch 19/20  Iteration 3378/3560 Training loss: 1.3773 0.9396 sec/batch\n",
      "Epoch 19/20  Iteration 3379/3560 Training loss: 1.3773 0.9546 sec/batch\n",
      "Epoch 19/20  Iteration 3380/3560 Training loss: 1.3772 0.9755 sec/batch\n",
      "Epoch 19/20  Iteration 3381/3560 Training loss: 1.3772 0.9158 sec/batch\n",
      "Epoch 19/20  Iteration 3382/3560 Training loss: 1.3774 0.9582 sec/batch\n",
      "Epoch 20/20  Iteration 3383/3560 Training loss: 1.4692 1.0825 sec/batch\n",
      "Epoch 20/20  Iteration 3384/3560 Training loss: 1.4292 1.0787 sec/batch\n",
      "Epoch 20/20  Iteration 3385/3560 Training loss: 1.4164 1.0077 sec/batch\n",
      "Epoch 20/20  Iteration 3386/3560 Training loss: 1.4113 0.9828 sec/batch\n",
      "Epoch 20/20  Iteration 3387/3560 Training loss: 1.3999 0.9749 sec/batch\n",
      "Epoch 20/20  Iteration 3388/3560 Training loss: 1.3876 0.9788 sec/batch\n",
      "Epoch 20/20  Iteration 3389/3560 Training loss: 1.3870 0.9203 sec/batch\n",
      "Epoch 20/20  Iteration 3390/3560 Training loss: 1.3834 1.0645 sec/batch\n",
      "Epoch 20/20  Iteration 3391/3560 Training loss: 1.3830 0.9493 sec/batch\n",
      "Epoch 20/20  Iteration 3392/3560 Training loss: 1.3819 1.1026 sec/batch\n",
      "Epoch 20/20  Iteration 3393/3560 Training loss: 1.3791 0.9469 sec/batch\n",
      "Epoch 20/20  Iteration 3394/3560 Training loss: 1.3784 0.9864 sec/batch\n",
      "Epoch 20/20  Iteration 3395/3560 Training loss: 1.3781 0.9543 sec/batch\n",
      "Epoch 20/20  Iteration 3396/3560 Training loss: 1.3788 0.9450 sec/batch\n",
      "Epoch 20/20  Iteration 3397/3560 Training loss: 1.3783 0.9365 sec/batch\n",
      "Epoch 20/20  Iteration 3398/3560 Training loss: 1.3764 1.0332 sec/batch\n",
      "Epoch 20/20  Iteration 3399/3560 Training loss: 1.3764 1.0986 sec/batch\n",
      "Epoch 20/20  Iteration 3400/3560 Training loss: 1.3779 1.1025 sec/batch\n",
      "Epoch 20/20  Iteration 3401/3560 Training loss: 1.3778 1.0151 sec/batch\n",
      "Epoch 20/20  Iteration 3402/3560 Training loss: 1.3787 0.8492 sec/batch\n",
      "Epoch 20/20  Iteration 3403/3560 Training loss: 1.3777 0.9873 sec/batch\n",
      "Epoch 20/20  Iteration 3404/3560 Training loss: 1.3776 0.9124 sec/batch\n",
      "Epoch 20/20  Iteration 3405/3560 Training loss: 1.3771 1.0390 sec/batch\n",
      "Epoch 20/20  Iteration 3406/3560 Training loss: 1.3774 0.9848 sec/batch\n",
      "Epoch 20/20  Iteration 3407/3560 Training loss: 1.3776 1.0713 sec/batch\n",
      "Epoch 20/20  Iteration 3408/3560 Training loss: 1.3761 0.9654 sec/batch\n",
      "Epoch 20/20  Iteration 3409/3560 Training loss: 1.3753 0.9747 sec/batch\n",
      "Epoch 20/20  Iteration 3410/3560 Training loss: 1.3760 0.9595 sec/batch\n",
      "Epoch 20/20  Iteration 3411/3560 Training loss: 1.3762 0.9100 sec/batch\n",
      "Epoch 20/20  Iteration 3412/3560 Training loss: 1.3764 0.9861 sec/batch\n",
      "Epoch 20/20  Iteration 3413/3560 Training loss: 1.3760 1.0481 sec/batch\n",
      "Epoch 20/20  Iteration 3414/3560 Training loss: 1.3751 1.0265 sec/batch\n",
      "Epoch 20/20  Iteration 3415/3560 Training loss: 1.3753 1.0581 sec/batch\n",
      "Epoch 20/20  Iteration 3416/3560 Training loss: 1.3754 1.0316 sec/batch\n",
      "Epoch 20/20  Iteration 3417/3560 Training loss: 1.3751 0.9557 sec/batch\n",
      "Epoch 20/20  Iteration 3418/3560 Training loss: 1.3749 0.9285 sec/batch\n",
      "Epoch 20/20  Iteration 3419/3560 Training loss: 1.3742 0.9072 sec/batch\n",
      "Epoch 20/20  Iteration 3420/3560 Training loss: 1.3733 0.9394 sec/batch\n",
      "Epoch 20/20  Iteration 3421/3560 Training loss: 1.3722 1.0428 sec/batch\n",
      "Epoch 20/20  Iteration 3422/3560 Training loss: 1.3718 1.0051 sec/batch\n",
      "Epoch 20/20  Iteration 3423/3560 Training loss: 1.3711 1.0707 sec/batch\n",
      "Epoch 20/20  Iteration 3424/3560 Training loss: 1.3721 1.0696 sec/batch\n",
      "Epoch 20/20  Iteration 3425/3560 Training loss: 1.3718 0.9652 sec/batch\n",
      "Epoch 20/20  Iteration 3426/3560 Training loss: 1.3713 0.8456 sec/batch\n",
      "Epoch 20/20  Iteration 3427/3560 Training loss: 1.3716 1.0949 sec/batch\n",
      "Epoch 20/20  Iteration 3428/3560 Training loss: 1.3709 0.9474 sec/batch\n",
      "Epoch 20/20  Iteration 3429/3560 Training loss: 1.3706 1.0093 sec/batch\n",
      "Epoch 20/20  Iteration 3430/3560 Training loss: 1.3701 1.0285 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3431/3560 Training loss: 1.3699 0.9862 sec/batch\n",
      "Epoch 20/20  Iteration 3432/3560 Training loss: 1.3705 1.0129 sec/batch\n",
      "Epoch 20/20  Iteration 3433/3560 Training loss: 1.3703 0.9266 sec/batch\n",
      "Epoch 20/20  Iteration 3434/3560 Training loss: 1.3710 1.0006 sec/batch\n",
      "Epoch 20/20  Iteration 3435/3560 Training loss: 1.3709 0.9751 sec/batch\n",
      "Epoch 20/20  Iteration 3436/3560 Training loss: 1.3714 0.9711 sec/batch\n",
      "Epoch 20/20  Iteration 3437/3560 Training loss: 1.3710 0.9535 sec/batch\n",
      "Epoch 20/20  Iteration 3438/3560 Training loss: 1.3712 0.9422 sec/batch\n",
      "Epoch 20/20  Iteration 3439/3560 Training loss: 1.3717 1.0480 sec/batch\n",
      "Epoch 20/20  Iteration 3440/3560 Training loss: 1.3714 0.8995 sec/batch\n",
      "Epoch 20/20  Iteration 3441/3560 Training loss: 1.3709 0.9818 sec/batch\n",
      "Epoch 20/20  Iteration 3442/3560 Training loss: 1.3715 0.9334 sec/batch\n",
      "Epoch 20/20  Iteration 3443/3560 Training loss: 1.3717 0.8415 sec/batch\n",
      "Epoch 20/20  Iteration 3444/3560 Training loss: 1.3724 1.0598 sec/batch\n",
      "Epoch 20/20  Iteration 3445/3560 Training loss: 1.3727 1.0467 sec/batch\n",
      "Epoch 20/20  Iteration 3446/3560 Training loss: 1.3728 0.9751 sec/batch\n",
      "Epoch 20/20  Iteration 3447/3560 Training loss: 1.3727 1.0539 sec/batch\n",
      "Epoch 20/20  Iteration 3448/3560 Training loss: 1.3730 0.9211 sec/batch\n",
      "Epoch 20/20  Iteration 3449/3560 Training loss: 1.3732 0.9669 sec/batch\n",
      "Epoch 20/20  Iteration 3450/3560 Training loss: 1.3730 0.9445 sec/batch\n",
      "Epoch 20/20  Iteration 3451/3560 Training loss: 1.3729 0.9465 sec/batch\n",
      "Epoch 20/20  Iteration 3452/3560 Training loss: 1.3728 0.9262 sec/batch\n",
      "Epoch 20/20  Iteration 3453/3560 Training loss: 1.3733 1.0409 sec/batch\n",
      "Epoch 20/20  Iteration 3454/3560 Training loss: 1.3737 1.0060 sec/batch\n",
      "Epoch 20/20  Iteration 3455/3560 Training loss: 1.3742 1.0040 sec/batch\n",
      "Epoch 20/20  Iteration 3456/3560 Training loss: 1.3740 0.9293 sec/batch\n",
      "Epoch 20/20  Iteration 3457/3560 Training loss: 1.3740 0.9289 sec/batch\n",
      "Epoch 20/20  Iteration 3458/3560 Training loss: 1.3741 0.9550 sec/batch\n",
      "Epoch 20/20  Iteration 3459/3560 Training loss: 1.3740 0.9289 sec/batch\n",
      "Epoch 20/20  Iteration 3460/3560 Training loss: 1.3740 1.0562 sec/batch\n",
      "Epoch 20/20  Iteration 3461/3560 Training loss: 1.3735 0.9977 sec/batch\n",
      "Epoch 20/20  Iteration 3462/3560 Training loss: 1.3735 1.0365 sec/batch\n",
      "Epoch 20/20  Iteration 3463/3560 Training loss: 1.3731 0.9561 sec/batch\n",
      "Epoch 20/20  Iteration 3464/3560 Training loss: 1.3730 0.9747 sec/batch\n",
      "Epoch 20/20  Iteration 3465/3560 Training loss: 1.3726 0.9895 sec/batch\n",
      "Epoch 20/20  Iteration 3466/3560 Training loss: 1.3726 0.9198 sec/batch\n",
      "Epoch 20/20  Iteration 3467/3560 Training loss: 1.3724 0.9152 sec/batch\n",
      "Epoch 20/20  Iteration 3468/3560 Training loss: 1.3723 1.0254 sec/batch\n",
      "Epoch 20/20  Iteration 3469/3560 Training loss: 1.3720 0.9948 sec/batch\n",
      "Epoch 20/20  Iteration 3470/3560 Training loss: 1.3718 1.0834 sec/batch\n",
      "Epoch 20/20  Iteration 3471/3560 Training loss: 1.3714 0.9051 sec/batch\n",
      "Epoch 20/20  Iteration 3472/3560 Training loss: 1.3716 0.9554 sec/batch\n",
      "Epoch 20/20  Iteration 3473/3560 Training loss: 1.3713 0.8973 sec/batch\n",
      "Epoch 20/20  Iteration 3474/3560 Training loss: 1.3713 0.9705 sec/batch\n",
      "Epoch 20/20  Iteration 3475/3560 Training loss: 1.3709 0.9961 sec/batch\n",
      "Epoch 20/20  Iteration 3476/3560 Training loss: 1.3705 1.0075 sec/batch\n",
      "Epoch 20/20  Iteration 3477/3560 Training loss: 1.3703 1.0744 sec/batch\n",
      "Epoch 20/20  Iteration 3478/3560 Training loss: 1.3704 0.9209 sec/batch\n",
      "Epoch 20/20  Iteration 3479/3560 Training loss: 1.3704 0.8945 sec/batch\n",
      "Epoch 20/20  Iteration 3480/3560 Training loss: 1.3701 0.9348 sec/batch\n",
      "Epoch 20/20  Iteration 3481/3560 Training loss: 1.3699 0.9257 sec/batch\n",
      "Epoch 20/20  Iteration 3482/3560 Training loss: 1.3696 0.9056 sec/batch\n",
      "Epoch 20/20  Iteration 3483/3560 Training loss: 1.3696 0.9750 sec/batch\n",
      "Epoch 20/20  Iteration 3484/3560 Training loss: 1.3695 1.0428 sec/batch\n",
      "Epoch 20/20  Iteration 3485/3560 Training loss: 1.3694 0.9802 sec/batch\n",
      "Epoch 20/20  Iteration 3486/3560 Training loss: 1.3693 1.0153 sec/batch\n",
      "Epoch 20/20  Iteration 3487/3560 Training loss: 1.3691 1.0389 sec/batch\n",
      "Epoch 20/20  Iteration 3488/3560 Training loss: 1.3690 0.9195 sec/batch\n",
      "Epoch 20/20  Iteration 3489/3560 Training loss: 1.3689 0.8984 sec/batch\n",
      "Epoch 20/20  Iteration 3490/3560 Training loss: 1.3689 0.9299 sec/batch\n",
      "Epoch 20/20  Iteration 3491/3560 Training loss: 1.3688 0.9858 sec/batch\n",
      "Epoch 20/20  Iteration 3492/3560 Training loss: 1.3688 0.9899 sec/batch\n",
      "Epoch 20/20  Iteration 3493/3560 Training loss: 1.3686 1.0586 sec/batch\n",
      "Epoch 20/20  Iteration 3494/3560 Training loss: 1.3685 0.9720 sec/batch\n",
      "Epoch 20/20  Iteration 3495/3560 Training loss: 1.3684 0.9464 sec/batch\n",
      "Epoch 20/20  Iteration 3496/3560 Training loss: 1.3683 0.9330 sec/batch\n",
      "Epoch 20/20  Iteration 3497/3560 Training loss: 1.3681 1.0503 sec/batch\n",
      "Epoch 20/20  Iteration 3498/3560 Training loss: 1.3677 0.9315 sec/batch\n",
      "Epoch 20/20  Iteration 3499/3560 Training loss: 1.3677 0.9894 sec/batch\n",
      "Epoch 20/20  Iteration 3500/3560 Training loss: 1.3678 1.0286 sec/batch\n",
      "Epoch 20/20  Iteration 3501/3560 Training loss: 1.3677 1.0994 sec/batch\n",
      "Epoch 20/20  Iteration 3502/3560 Training loss: 1.3676 0.9835 sec/batch\n",
      "Epoch 20/20  Iteration 3503/3560 Training loss: 1.3676 0.9535 sec/batch\n",
      "Epoch 20/20  Iteration 3504/3560 Training loss: 1.3673 0.9713 sec/batch\n",
      "Epoch 20/20  Iteration 3505/3560 Training loss: 1.3669 0.8830 sec/batch\n",
      "Epoch 20/20  Iteration 3506/3560 Training loss: 1.3669 0.9276 sec/batch\n",
      "Epoch 20/20  Iteration 3507/3560 Training loss: 1.3669 1.0190 sec/batch\n",
      "Epoch 20/20  Iteration 3508/3560 Training loss: 1.3665 1.0746 sec/batch\n",
      "Epoch 20/20  Iteration 3509/3560 Training loss: 1.3666 1.0752 sec/batch\n",
      "Epoch 20/20  Iteration 3510/3560 Training loss: 1.3666 0.8688 sec/batch\n",
      "Epoch 20/20  Iteration 3511/3560 Training loss: 1.3665 0.9098 sec/batch\n",
      "Epoch 20/20  Iteration 3512/3560 Training loss: 1.3662 0.9220 sec/batch\n",
      "Epoch 20/20  Iteration 3513/3560 Training loss: 1.3659 0.9248 sec/batch\n",
      "Epoch 20/20  Iteration 3514/3560 Training loss: 1.3657 0.9577 sec/batch\n",
      "Epoch 20/20  Iteration 3515/3560 Training loss: 1.3658 0.9653 sec/batch\n",
      "Epoch 20/20  Iteration 3516/3560 Training loss: 1.3658 1.0227 sec/batch\n",
      "Epoch 20/20  Iteration 3517/3560 Training loss: 1.3658 1.0188 sec/batch\n",
      "Epoch 20/20  Iteration 3518/3560 Training loss: 1.3659 1.0439 sec/batch\n",
      "Epoch 20/20  Iteration 3519/3560 Training loss: 1.3662 0.9196 sec/batch\n",
      "Epoch 20/20  Iteration 3520/3560 Training loss: 1.3662 0.9641 sec/batch\n",
      "Epoch 20/20  Iteration 3521/3560 Training loss: 1.3662 0.8844 sec/batch\n",
      "Epoch 20/20  Iteration 3522/3560 Training loss: 1.3662 0.9206 sec/batch\n",
      "Epoch 20/20  Iteration 3523/3560 Training loss: 1.3666 1.0362 sec/batch\n",
      "Epoch 20/20  Iteration 3524/3560 Training loss: 1.3666 1.0194 sec/batch\n",
      "Epoch 20/20  Iteration 3525/3560 Training loss: 1.3666 1.0488 sec/batch\n",
      "Epoch 20/20  Iteration 3526/3560 Training loss: 1.3669 0.9216 sec/batch\n",
      "Epoch 20/20  Iteration 3527/3560 Training loss: 1.3668 0.9709 sec/batch\n",
      "Epoch 20/20  Iteration 3528/3560 Training loss: 1.3669 0.9233 sec/batch\n",
      "Epoch 20/20  Iteration 3529/3560 Training loss: 1.3670 1.0127 sec/batch\n",
      "Epoch 20/20  Iteration 3530/3560 Training loss: 1.3673 0.9896 sec/batch\n",
      "Epoch 20/20  Iteration 3531/3560 Training loss: 1.3674 1.0130 sec/batch\n",
      "Epoch 20/20  Iteration 3532/3560 Training loss: 1.3673 0.9671 sec/batch\n",
      "Epoch 20/20  Iteration 3533/3560 Training loss: 1.3670 0.9927 sec/batch\n",
      "Epoch 20/20  Iteration 3534/3560 Training loss: 1.3670 0.9033 sec/batch\n",
      "Epoch 20/20  Iteration 3535/3560 Training loss: 1.3671 0.9934 sec/batch\n",
      "Epoch 20/20  Iteration 3536/3560 Training loss: 1.3671 0.9454 sec/batch\n",
      "Epoch 20/20  Iteration 3537/3560 Training loss: 1.3671 0.9503 sec/batch\n",
      "Epoch 20/20  Iteration 3538/3560 Training loss: 1.3671 1.0044 sec/batch\n",
      "Epoch 20/20  Iteration 3539/3560 Training loss: 1.3672 1.0560 sec/batch\n",
      "Epoch 20/20  Iteration 3540/3560 Training loss: 1.3672 1.0054 sec/batch\n",
      "Epoch 20/20  Iteration 3541/3560 Training loss: 1.3670 0.9449 sec/batch\n",
      "Epoch 20/20  Iteration 3542/3560 Training loss: 1.3672 0.9146 sec/batch\n",
      "Epoch 20/20  Iteration 3543/3560 Training loss: 1.3674 0.9610 sec/batch\n",
      "Epoch 20/20  Iteration 3544/3560 Training loss: 1.3675 0.9285 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3545/3560 Training loss: 1.3675 0.8822 sec/batch\n",
      "Epoch 20/20  Iteration 3546/3560 Training loss: 1.3675 1.0330 sec/batch\n",
      "Epoch 20/20  Iteration 3547/3560 Training loss: 1.3675 1.0728 sec/batch\n",
      "Epoch 20/20  Iteration 3548/3560 Training loss: 1.3675 0.9147 sec/batch\n",
      "Epoch 20/20  Iteration 3549/3560 Training loss: 1.3677 1.0541 sec/batch\n",
      "Epoch 20/20  Iteration 3550/3560 Training loss: 1.3681 0.8803 sec/batch\n",
      "Epoch 20/20  Iteration 3551/3560 Training loss: 1.3681 0.9249 sec/batch\n",
      "Epoch 20/20  Iteration 3552/3560 Training loss: 1.3682 1.0215 sec/batch\n",
      "Epoch 20/20  Iteration 3553/3560 Training loss: 1.3682 0.9155 sec/batch\n",
      "Epoch 20/20  Iteration 3554/3560 Training loss: 1.3681 1.0388 sec/batch\n",
      "Epoch 20/20  Iteration 3555/3560 Training loss: 1.3682 1.0844 sec/batch\n",
      "Epoch 20/20  Iteration 3556/3560 Training loss: 1.3682 1.0427 sec/batch\n",
      "Epoch 20/20  Iteration 3557/3560 Training loss: 1.3683 0.9115 sec/batch\n",
      "Epoch 20/20  Iteration 3558/3560 Training loss: 1.3681 1.0780 sec/batch\n",
      "Epoch 20/20  Iteration 3559/3560 Training loss: 1.3681 0.9222 sec/batch\n",
      "Epoch 20/20  Iteration 3560/3560 Training loss: 1.3682 0.9737 sec/batch\n",
      "Epoch 1/20  Iteration 1/3560 Training loss: 4.4204 0.9468 sec/batch\n",
      "Epoch 1/20  Iteration 2/3560 Training loss: 4.4086 0.9286 sec/batch\n",
      "Epoch 1/20  Iteration 3/3560 Training loss: 4.3953 0.9320 sec/batch\n",
      "Epoch 1/20  Iteration 4/3560 Training loss: 4.3776 0.9500 sec/batch\n",
      "Epoch 1/20  Iteration 5/3560 Training loss: 4.3459 0.9172 sec/batch\n",
      "Epoch 1/20  Iteration 6/3560 Training loss: 4.2651 0.9921 sec/batch\n",
      "Epoch 1/20  Iteration 7/3560 Training loss: 4.1809 1.0307 sec/batch\n",
      "Epoch 1/20  Iteration 8/3560 Training loss: 4.1064 1.1104 sec/batch\n",
      "Epoch 1/20  Iteration 9/3560 Training loss: 4.0399 1.0741 sec/batch\n",
      "Epoch 1/20  Iteration 10/3560 Training loss: 3.9823 0.9562 sec/batch\n",
      "Epoch 1/20  Iteration 11/3560 Training loss: 3.9304 0.9534 sec/batch\n",
      "Epoch 1/20  Iteration 12/3560 Training loss: 3.8866 0.9279 sec/batch\n",
      "Epoch 1/20  Iteration 13/3560 Training loss: 3.8484 0.9714 sec/batch\n",
      "Epoch 1/20  Iteration 14/3560 Training loss: 3.8140 0.9477 sec/batch\n",
      "Epoch 1/20  Iteration 15/3560 Training loss: 3.7823 1.0289 sec/batch\n",
      "Epoch 1/20  Iteration 16/3560 Training loss: 3.7540 1.0681 sec/batch\n",
      "Epoch 1/20  Iteration 17/3560 Training loss: 3.7270 0.9882 sec/batch\n",
      "Epoch 1/20  Iteration 18/3560 Training loss: 3.7045 0.9725 sec/batch\n",
      "Epoch 1/20  Iteration 19/3560 Training loss: 3.6821 0.9808 sec/batch\n",
      "Epoch 1/20  Iteration 20/3560 Training loss: 3.6597 0.9309 sec/batch\n",
      "Epoch 1/20  Iteration 21/3560 Training loss: 3.6407 0.9385 sec/batch\n",
      "Epoch 1/20  Iteration 22/3560 Training loss: 3.6232 1.0196 sec/batch\n",
      "Epoch 1/20  Iteration 23/3560 Training loss: 3.6069 0.9980 sec/batch\n",
      "Epoch 1/20  Iteration 24/3560 Training loss: 3.5917 1.1054 sec/batch\n",
      "Epoch 1/20  Iteration 25/3560 Training loss: 3.5770 0.9403 sec/batch\n",
      "Epoch 1/20  Iteration 26/3560 Training loss: 3.5639 0.9558 sec/batch\n",
      "Epoch 1/20  Iteration 27/3560 Training loss: 3.5519 0.9695 sec/batch\n",
      "Epoch 1/20  Iteration 28/3560 Training loss: 3.5391 1.0254 sec/batch\n",
      "Epoch 1/20  Iteration 29/3560 Training loss: 3.5277 0.9074 sec/batch\n",
      "Epoch 1/20  Iteration 30/3560 Training loss: 3.5173 0.9543 sec/batch\n",
      "Epoch 1/20  Iteration 31/3560 Training loss: 3.5082 1.0919 sec/batch\n",
      "Epoch 1/20  Iteration 32/3560 Training loss: 3.4982 1.0667 sec/batch\n",
      "Epoch 1/20  Iteration 33/3560 Training loss: 3.4888 0.8590 sec/batch\n",
      "Epoch 1/20  Iteration 34/3560 Training loss: 3.4803 0.9828 sec/batch\n",
      "Epoch 1/20  Iteration 35/3560 Training loss: 3.4718 0.9062 sec/batch\n",
      "Epoch 1/20  Iteration 36/3560 Training loss: 3.4640 0.8860 sec/batch\n",
      "Epoch 1/20  Iteration 37/3560 Training loss: 3.4558 0.9303 sec/batch\n",
      "Epoch 1/20  Iteration 38/3560 Training loss: 3.4481 1.0317 sec/batch\n",
      "Epoch 1/20  Iteration 39/3560 Training loss: 3.4406 1.0457 sec/batch\n",
      "Epoch 1/20  Iteration 40/3560 Training loss: 3.4337 0.9740 sec/batch\n",
      "Epoch 1/20  Iteration 41/3560 Training loss: 3.4268 0.9448 sec/batch\n",
      "Epoch 1/20  Iteration 42/3560 Training loss: 3.4203 0.9311 sec/batch\n",
      "Epoch 1/20  Iteration 43/3560 Training loss: 3.4139 0.9252 sec/batch\n",
      "Epoch 1/20  Iteration 44/3560 Training loss: 3.4077 0.9797 sec/batch\n",
      "Epoch 1/20  Iteration 45/3560 Training loss: 3.4018 0.9126 sec/batch\n",
      "Epoch 1/20  Iteration 46/3560 Training loss: 3.3965 1.0523 sec/batch\n",
      "Epoch 1/20  Iteration 47/3560 Training loss: 3.3911 1.0371 sec/batch\n",
      "Epoch 1/20  Iteration 48/3560 Training loss: 3.3861 0.9524 sec/batch\n",
      "Epoch 1/20  Iteration 49/3560 Training loss: 3.3813 0.9959 sec/batch\n",
      "Epoch 1/20  Iteration 50/3560 Training loss: 3.3766 0.9488 sec/batch\n",
      "Epoch 1/20  Iteration 51/3560 Training loss: 3.3719 0.8756 sec/batch\n",
      "Epoch 1/20  Iteration 52/3560 Training loss: 3.3671 1.0800 sec/batch\n",
      "Epoch 1/20  Iteration 53/3560 Training loss: 3.3628 0.8659 sec/batch\n",
      "Epoch 1/20  Iteration 54/3560 Training loss: 3.3582 1.0572 sec/batch\n",
      "Epoch 1/20  Iteration 55/3560 Training loss: 3.3541 1.0624 sec/batch\n",
      "Epoch 1/20  Iteration 56/3560 Training loss: 3.3498 0.9032 sec/batch\n",
      "Epoch 1/20  Iteration 57/3560 Training loss: 3.3458 0.9169 sec/batch\n",
      "Epoch 1/20  Iteration 58/3560 Training loss: 3.3420 0.9500 sec/batch\n",
      "Epoch 1/20  Iteration 59/3560 Training loss: 3.3381 0.8891 sec/batch\n",
      "Epoch 1/20  Iteration 60/3560 Training loss: 3.3345 0.9508 sec/batch\n",
      "Epoch 1/20  Iteration 61/3560 Training loss: 3.3309 0.9535 sec/batch\n",
      "Epoch 1/20  Iteration 62/3560 Training loss: 3.3277 1.0300 sec/batch\n",
      "Epoch 1/20  Iteration 63/3560 Training loss: 3.3248 1.1559 sec/batch\n",
      "Epoch 1/20  Iteration 64/3560 Training loss: 3.3212 1.0021 sec/batch\n",
      "Epoch 1/20  Iteration 65/3560 Training loss: 3.3178 0.9362 sec/batch\n",
      "Epoch 1/20  Iteration 66/3560 Training loss: 3.3148 0.8579 sec/batch\n",
      "Epoch 1/20  Iteration 67/3560 Training loss: 3.3119 0.9258 sec/batch\n",
      "Epoch 1/20  Iteration 68/3560 Training loss: 3.3084 0.9529 sec/batch\n",
      "Epoch 1/20  Iteration 69/3560 Training loss: 3.3052 0.9318 sec/batch\n",
      "Epoch 1/20  Iteration 70/3560 Training loss: 3.3024 1.1099 sec/batch\n",
      "Epoch 1/20  Iteration 71/3560 Training loss: 3.2995 1.0092 sec/batch\n",
      "Epoch 1/20  Iteration 72/3560 Training loss: 3.2969 0.8900 sec/batch\n",
      "Epoch 1/20  Iteration 73/3560 Training loss: 3.2941 0.9571 sec/batch\n",
      "Epoch 1/20  Iteration 74/3560 Training loss: 3.2914 0.8602 sec/batch\n",
      "Epoch 1/20  Iteration 75/3560 Training loss: 3.2889 0.9029 sec/batch\n",
      "Epoch 1/20  Iteration 76/3560 Training loss: 3.2865 0.9560 sec/batch\n",
      "Epoch 1/20  Iteration 77/3560 Training loss: 3.2840 1.0097 sec/batch\n",
      "Epoch 1/20  Iteration 78/3560 Training loss: 3.2815 1.0012 sec/batch\n",
      "Epoch 1/20  Iteration 79/3560 Training loss: 3.2790 1.0431 sec/batch\n",
      "Epoch 1/20  Iteration 80/3560 Training loss: 3.2764 0.9133 sec/batch\n",
      "Epoch 1/20  Iteration 81/3560 Training loss: 3.2739 1.0179 sec/batch\n",
      "Epoch 1/20  Iteration 82/3560 Training loss: 3.2716 0.9020 sec/batch\n",
      "Epoch 1/20  Iteration 83/3560 Training loss: 3.2693 0.9765 sec/batch\n",
      "Epoch 1/20  Iteration 84/3560 Training loss: 3.2669 0.9228 sec/batch\n",
      "Epoch 1/20  Iteration 85/3560 Training loss: 3.2644 0.9952 sec/batch\n",
      "Epoch 1/20  Iteration 86/3560 Training loss: 3.2620 1.2247 sec/batch\n",
      "Epoch 1/20  Iteration 87/3560 Training loss: 3.2596 0.9675 sec/batch\n",
      "Epoch 1/20  Iteration 88/3560 Training loss: 3.2573 0.9324 sec/batch\n",
      "Epoch 1/20  Iteration 89/3560 Training loss: 3.2552 1.0289 sec/batch\n",
      "Epoch 1/20  Iteration 90/3560 Training loss: 3.2531 0.9802 sec/batch\n",
      "Epoch 1/20  Iteration 91/3560 Training loss: 3.2510 0.8728 sec/batch\n",
      "Epoch 1/20  Iteration 92/3560 Training loss: 3.2488 0.9659 sec/batch\n",
      "Epoch 1/20  Iteration 93/3560 Training loss: 3.2467 1.1078 sec/batch\n",
      "Epoch 1/20  Iteration 94/3560 Training loss: 3.2446 0.9906 sec/batch\n",
      "Epoch 1/20  Iteration 95/3560 Training loss: 3.2424 0.9510 sec/batch\n",
      "Epoch 1/20  Iteration 96/3560 Training loss: 3.2402 0.9540 sec/batch\n",
      "Epoch 1/20  Iteration 97/3560 Training loss: 3.2382 0.9633 sec/batch\n",
      "Epoch 1/20  Iteration 98/3560 Training loss: 3.2360 0.9117 sec/batch\n",
      "Epoch 1/20  Iteration 99/3560 Training loss: 3.2339 0.9692 sec/batch\n",
      "Epoch 1/20  Iteration 100/3560 Training loss: 3.2318 0.9802 sec/batch\n",
      "Epoch 1/20  Iteration 101/3560 Training loss: 3.2297 1.0950 sec/batch\n",
      "Epoch 1/20  Iteration 102/3560 Training loss: 3.2277 1.0766 sec/batch\n",
      "Epoch 1/20  Iteration 103/3560 Training loss: 3.2256 0.9893 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 104/3560 Training loss: 3.2234 0.9498 sec/batch\n",
      "Epoch 1/20  Iteration 105/3560 Training loss: 3.2213 0.8743 sec/batch\n",
      "Epoch 1/20  Iteration 106/3560 Training loss: 3.2193 0.9715 sec/batch\n",
      "Epoch 1/20  Iteration 107/3560 Training loss: 3.2170 0.9519 sec/batch\n",
      "Epoch 1/20  Iteration 108/3560 Training loss: 3.2147 0.9543 sec/batch\n",
      "Epoch 1/20  Iteration 109/3560 Training loss: 3.2127 1.0853 sec/batch\n",
      "Epoch 1/20  Iteration 110/3560 Training loss: 3.2104 1.0181 sec/batch\n",
      "Epoch 1/20  Iteration 111/3560 Training loss: 3.2082 0.9406 sec/batch\n",
      "Epoch 1/20  Iteration 112/3560 Training loss: 3.2062 1.0039 sec/batch\n",
      "Epoch 1/20  Iteration 113/3560 Training loss: 3.2039 0.9694 sec/batch\n",
      "Epoch 1/20  Iteration 114/3560 Training loss: 3.2016 0.8753 sec/batch\n",
      "Epoch 1/20  Iteration 115/3560 Training loss: 3.1993 0.9182 sec/batch\n",
      "Epoch 1/20  Iteration 116/3560 Training loss: 3.1970 0.9463 sec/batch\n",
      "Epoch 1/20  Iteration 117/3560 Training loss: 3.1947 1.0229 sec/batch\n",
      "Epoch 1/20  Iteration 118/3560 Training loss: 3.1926 1.0994 sec/batch\n",
      "Epoch 1/20  Iteration 119/3560 Training loss: 3.1905 0.9982 sec/batch\n",
      "Epoch 1/20  Iteration 120/3560 Training loss: 3.1882 0.8905 sec/batch\n",
      "Epoch 1/20  Iteration 121/3560 Training loss: 3.1861 0.9939 sec/batch\n",
      "Epoch 1/20  Iteration 122/3560 Training loss: 3.1839 0.9057 sec/batch\n",
      "Epoch 1/20  Iteration 123/3560 Training loss: 3.1817 0.9094 sec/batch\n",
      "Epoch 1/20  Iteration 124/3560 Training loss: 3.1796 0.9500 sec/batch\n",
      "Epoch 1/20  Iteration 125/3560 Training loss: 3.1773 1.0494 sec/batch\n",
      "Epoch 1/20  Iteration 126/3560 Training loss: 3.1747 1.0640 sec/batch\n",
      "Epoch 1/20  Iteration 127/3560 Training loss: 3.1725 0.9404 sec/batch\n",
      "Epoch 1/20  Iteration 128/3560 Training loss: 3.1702 0.9789 sec/batch\n",
      "Epoch 1/20  Iteration 129/3560 Training loss: 3.1678 0.9293 sec/batch\n",
      "Epoch 1/20  Iteration 130/3560 Training loss: 3.1654 0.9101 sec/batch\n",
      "Epoch 1/20  Iteration 131/3560 Training loss: 3.1632 0.9994 sec/batch\n",
      "Epoch 1/20  Iteration 132/3560 Training loss: 3.1607 0.9339 sec/batch\n",
      "Epoch 1/20  Iteration 133/3560 Training loss: 3.1584 1.0571 sec/batch\n",
      "Epoch 1/20  Iteration 134/3560 Training loss: 3.1560 0.9932 sec/batch\n",
      "Epoch 1/20  Iteration 135/3560 Training loss: 3.1534 0.9546 sec/batch\n",
      "Epoch 1/20  Iteration 136/3560 Training loss: 3.1508 0.9881 sec/batch\n",
      "Epoch 1/20  Iteration 137/3560 Training loss: 3.1483 0.9149 sec/batch\n",
      "Epoch 1/20  Iteration 138/3560 Training loss: 3.1457 0.9863 sec/batch\n",
      "Epoch 1/20  Iteration 139/3560 Training loss: 3.1434 0.9404 sec/batch\n",
      "Epoch 1/20  Iteration 140/3560 Training loss: 3.1408 1.0294 sec/batch\n",
      "Epoch 1/20  Iteration 141/3560 Training loss: 3.1384 1.1819 sec/batch\n",
      "Epoch 1/20  Iteration 142/3560 Training loss: 3.1358 1.0497 sec/batch\n",
      "Epoch 1/20  Iteration 143/3560 Training loss: 3.1332 0.8631 sec/batch\n",
      "Epoch 1/20  Iteration 144/3560 Training loss: 3.1306 0.9372 sec/batch\n",
      "Epoch 1/20  Iteration 145/3560 Training loss: 3.1281 0.9695 sec/batch\n",
      "Epoch 1/20  Iteration 146/3560 Training loss: 3.1256 0.9443 sec/batch\n",
      "Epoch 1/20  Iteration 147/3560 Training loss: 3.1231 0.9203 sec/batch\n",
      "Epoch 1/20  Iteration 148/3560 Training loss: 3.1207 0.9990 sec/batch\n",
      "Epoch 1/20  Iteration 149/3560 Training loss: 3.1181 1.0496 sec/batch\n",
      "Epoch 1/20  Iteration 150/3560 Training loss: 3.1154 0.9325 sec/batch\n",
      "Epoch 1/20  Iteration 151/3560 Training loss: 3.1131 0.9755 sec/batch\n",
      "Epoch 1/20  Iteration 152/3560 Training loss: 3.1108 0.9843 sec/batch\n",
      "Epoch 1/20  Iteration 153/3560 Training loss: 3.1083 0.9616 sec/batch\n",
      "Epoch 1/20  Iteration 154/3560 Training loss: 3.1058 0.9299 sec/batch\n",
      "Epoch 1/20  Iteration 155/3560 Training loss: 3.1032 1.0010 sec/batch\n",
      "Epoch 1/20  Iteration 156/3560 Training loss: 3.1005 1.0435 sec/batch\n",
      "Epoch 1/20  Iteration 157/3560 Training loss: 3.0978 1.0568 sec/batch\n",
      "Epoch 1/20  Iteration 158/3560 Training loss: 3.0952 0.9429 sec/batch\n",
      "Epoch 1/20  Iteration 159/3560 Training loss: 3.0925 0.8790 sec/batch\n",
      "Epoch 1/20  Iteration 160/3560 Training loss: 3.0899 0.9433 sec/batch\n",
      "Epoch 1/20  Iteration 161/3560 Training loss: 3.0875 1.0998 sec/batch\n",
      "Epoch 1/20  Iteration 162/3560 Training loss: 3.0848 0.9582 sec/batch\n",
      "Epoch 1/20  Iteration 163/3560 Training loss: 3.0821 0.9541 sec/batch\n",
      "Epoch 1/20  Iteration 164/3560 Training loss: 3.0795 1.1159 sec/batch\n",
      "Epoch 1/20  Iteration 165/3560 Training loss: 3.0769 1.0491 sec/batch\n",
      "Epoch 1/20  Iteration 166/3560 Training loss: 3.0744 0.9247 sec/batch\n",
      "Epoch 1/20  Iteration 167/3560 Training loss: 3.0719 0.9537 sec/batch\n",
      "Epoch 1/20  Iteration 168/3560 Training loss: 3.0694 0.9543 sec/batch\n",
      "Epoch 1/20  Iteration 169/3560 Training loss: 3.0669 0.9423 sec/batch\n",
      "Epoch 1/20  Iteration 170/3560 Training loss: 3.0642 0.9679 sec/batch\n",
      "Epoch 1/20  Iteration 171/3560 Training loss: 3.0617 1.0373 sec/batch\n",
      "Epoch 1/20  Iteration 172/3560 Training loss: 3.0593 1.1238 sec/batch\n",
      "Epoch 1/20  Iteration 173/3560 Training loss: 3.0570 0.9909 sec/batch\n",
      "Epoch 1/20  Iteration 174/3560 Training loss: 3.0546 0.9607 sec/batch\n",
      "Epoch 1/20  Iteration 175/3560 Training loss: 3.0523 0.9844 sec/batch\n",
      "Epoch 1/20  Iteration 176/3560 Training loss: 3.0498 0.9556 sec/batch\n",
      "Epoch 1/20  Iteration 177/3560 Training loss: 3.0472 0.9396 sec/batch\n",
      "Epoch 1/20  Iteration 178/3560 Training loss: 3.0446 1.0071 sec/batch\n",
      "Epoch 2/20  Iteration 179/3560 Training loss: 2.6679 1.1494 sec/batch\n",
      "Epoch 2/20  Iteration 180/3560 Training loss: 2.6130 0.9793 sec/batch\n",
      "Epoch 2/20  Iteration 181/3560 Training loss: 2.5969 0.9080 sec/batch\n",
      "Epoch 2/20  Iteration 182/3560 Training loss: 2.5909 0.9820 sec/batch\n",
      "Epoch 2/20  Iteration 183/3560 Training loss: 2.5856 0.9247 sec/batch\n",
      "Epoch 2/20  Iteration 184/3560 Training loss: 2.5816 0.9455 sec/batch\n",
      "Epoch 2/20  Iteration 185/3560 Training loss: 2.5804 0.9238 sec/batch\n",
      "Epoch 2/20  Iteration 186/3560 Training loss: 2.5805 0.9536 sec/batch\n",
      "Epoch 2/20  Iteration 187/3560 Training loss: 2.5800 1.0808 sec/batch\n",
      "Epoch 2/20  Iteration 188/3560 Training loss: 2.5770 0.9914 sec/batch\n",
      "Epoch 2/20  Iteration 189/3560 Training loss: 2.5732 0.9607 sec/batch\n",
      "Epoch 2/20  Iteration 190/3560 Training loss: 2.5719 0.9978 sec/batch\n",
      "Epoch 2/20  Iteration 191/3560 Training loss: 2.5702 0.9334 sec/batch\n",
      "Epoch 2/20  Iteration 192/3560 Training loss: 2.5712 0.9828 sec/batch\n",
      "Epoch 2/20  Iteration 193/3560 Training loss: 2.5699 0.9385 sec/batch\n",
      "Epoch 2/20  Iteration 194/3560 Training loss: 2.5681 0.9564 sec/batch\n",
      "Epoch 2/20  Iteration 195/3560 Training loss: 2.5666 1.0330 sec/batch\n",
      "Epoch 2/20  Iteration 196/3560 Training loss: 2.5666 1.0171 sec/batch\n",
      "Epoch 2/20  Iteration 197/3560 Training loss: 2.5653 0.9164 sec/batch\n",
      "Epoch 2/20  Iteration 198/3560 Training loss: 2.5625 0.8779 sec/batch\n",
      "Epoch 2/20  Iteration 199/3560 Training loss: 2.5604 0.9589 sec/batch\n",
      "Epoch 2/20  Iteration 200/3560 Training loss: 2.5599 0.9362 sec/batch\n",
      "Epoch 2/20  Iteration 201/3560 Training loss: 2.5581 0.8934 sec/batch\n",
      "Epoch 2/20  Iteration 202/3560 Training loss: 2.5559 0.9975 sec/batch\n",
      "Epoch 2/20  Iteration 203/3560 Training loss: 2.5536 1.1103 sec/batch\n",
      "Epoch 2/20  Iteration 204/3560 Training loss: 2.5521 1.0042 sec/batch\n",
      "Epoch 2/20  Iteration 205/3560 Training loss: 2.5502 1.0205 sec/batch\n",
      "Epoch 2/20  Iteration 206/3560 Training loss: 2.5482 0.9230 sec/batch\n",
      "Epoch 2/20  Iteration 207/3560 Training loss: 2.5470 0.9176 sec/batch\n",
      "Epoch 2/20  Iteration 208/3560 Training loss: 2.5456 0.9287 sec/batch\n",
      "Epoch 2/20  Iteration 209/3560 Training loss: 2.5449 0.9861 sec/batch\n",
      "Epoch 2/20  Iteration 210/3560 Training loss: 2.5432 0.9490 sec/batch\n",
      "Epoch 2/20  Iteration 211/3560 Training loss: 2.5410 1.0604 sec/batch\n",
      "Epoch 2/20  Iteration 212/3560 Training loss: 2.5398 0.9354 sec/batch\n",
      "Epoch 2/20  Iteration 213/3560 Training loss: 2.5381 0.9979 sec/batch\n",
      "Epoch 2/20  Iteration 214/3560 Training loss: 2.5369 0.9691 sec/batch\n",
      "Epoch 2/20  Iteration 215/3560 Training loss: 2.5354 0.9193 sec/batch\n",
      "Epoch 2/20  Iteration 216/3560 Training loss: 2.5331 0.9228 sec/batch\n",
      "Epoch 2/20  Iteration 217/3560 Training loss: 2.5311 0.9376 sec/batch\n",
      "Epoch 2/20  Iteration 218/3560 Training loss: 2.5294 0.9524 sec/batch\n",
      "Epoch 2/20  Iteration 219/3560 Training loss: 2.5274 1.0977 sec/batch\n",
      "Epoch 2/20  Iteration 220/3560 Training loss: 2.5256 0.9736 sec/batch\n",
      "Epoch 2/20  Iteration 221/3560 Training loss: 2.5237 0.9768 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 222/3560 Training loss: 2.5219 0.9421 sec/batch\n",
      "Epoch 2/20  Iteration 223/3560 Training loss: 2.5200 0.9579 sec/batch\n",
      "Epoch 2/20  Iteration 224/3560 Training loss: 2.5180 0.9222 sec/batch\n",
      "Epoch 2/20  Iteration 225/3560 Training loss: 2.5168 0.9218 sec/batch\n",
      "Epoch 2/20  Iteration 226/3560 Training loss: 2.5154 0.9776 sec/batch\n",
      "Epoch 2/20  Iteration 227/3560 Training loss: 2.5139 1.0858 sec/batch\n",
      "Epoch 2/20  Iteration 228/3560 Training loss: 2.5130 0.9393 sec/batch\n",
      "Epoch 2/20  Iteration 229/3560 Training loss: 2.5116 0.9534 sec/batch\n",
      "Epoch 2/20  Iteration 230/3560 Training loss: 2.5102 0.9419 sec/batch\n",
      "Epoch 2/20  Iteration 231/3560 Training loss: 2.5086 0.8983 sec/batch\n",
      "Epoch 2/20  Iteration 232/3560 Training loss: 2.5070 0.8866 sec/batch\n",
      "Epoch 2/20  Iteration 233/3560 Training loss: 2.5055 1.0121 sec/batch\n",
      "Epoch 2/20  Iteration 234/3560 Training loss: 2.5041 1.0941 sec/batch\n",
      "Epoch 2/20  Iteration 235/3560 Training loss: 2.5028 0.9728 sec/batch\n",
      "Epoch 2/20  Iteration 236/3560 Training loss: 2.5011 0.9777 sec/batch\n",
      "Epoch 2/20  Iteration 237/3560 Training loss: 2.4999 1.0014 sec/batch\n",
      "Epoch 2/20  Iteration 238/3560 Training loss: 2.4988 0.8993 sec/batch\n",
      "Epoch 2/20  Iteration 239/3560 Training loss: 2.4975 0.9411 sec/batch\n",
      "Epoch 2/20  Iteration 240/3560 Training loss: 2.4964 0.9660 sec/batch\n",
      "Epoch 2/20  Iteration 241/3560 Training loss: 2.4955 0.9529 sec/batch\n",
      "Epoch 2/20  Iteration 242/3560 Training loss: 2.4942 1.0897 sec/batch\n",
      "Epoch 2/20  Iteration 243/3560 Training loss: 2.4928 0.8957 sec/batch\n",
      "Epoch 2/20  Iteration 244/3560 Training loss: 2.4919 0.9956 sec/batch\n",
      "Epoch 2/20  Iteration 245/3560 Training loss: 2.4908 0.9502 sec/batch\n",
      "Epoch 2/20  Iteration 246/3560 Training loss: 2.4891 0.9542 sec/batch\n",
      "Epoch 2/20  Iteration 247/3560 Training loss: 2.4876 0.9192 sec/batch\n",
      "Epoch 2/20  Iteration 248/3560 Training loss: 2.4867 0.9588 sec/batch\n",
      "Epoch 2/20  Iteration 249/3560 Training loss: 2.4856 0.9694 sec/batch\n",
      "Epoch 2/20  Iteration 250/3560 Training loss: 2.4847 1.0189 sec/batch\n",
      "Epoch 2/20  Iteration 251/3560 Training loss: 2.4837 1.0459 sec/batch\n",
      "Epoch 2/20  Iteration 252/3560 Training loss: 2.4823 0.8889 sec/batch\n",
      "Epoch 2/20  Iteration 253/3560 Training loss: 2.4812 0.9321 sec/batch\n",
      "Epoch 2/20  Iteration 254/3560 Training loss: 2.4806 0.9819 sec/batch\n",
      "Epoch 2/20  Iteration 255/3560 Training loss: 2.4794 0.9982 sec/batch\n",
      "Epoch 2/20  Iteration 256/3560 Training loss: 2.4785 0.9385 sec/batch\n",
      "Epoch 2/20  Iteration 257/3560 Training loss: 2.4772 0.9499 sec/batch\n",
      "Epoch 2/20  Iteration 258/3560 Training loss: 2.4760 1.0527 sec/batch\n",
      "Epoch 2/20  Iteration 259/3560 Training loss: 2.4748 0.9846 sec/batch\n",
      "Epoch 2/20  Iteration 260/3560 Training loss: 2.4740 0.9430 sec/batch\n",
      "Epoch 2/20  Iteration 261/3560 Training loss: 2.4728 0.9711 sec/batch\n",
      "Epoch 2/20  Iteration 262/3560 Training loss: 2.4715 0.9630 sec/batch\n",
      "Epoch 2/20  Iteration 263/3560 Training loss: 2.4699 0.9120 sec/batch\n",
      "Epoch 2/20  Iteration 264/3560 Training loss: 2.4686 0.9194 sec/batch\n",
      "Epoch 2/20  Iteration 265/3560 Training loss: 2.4676 0.9969 sec/batch\n",
      "Epoch 2/20  Iteration 266/3560 Training loss: 2.4664 1.1625 sec/batch\n",
      "Epoch 2/20  Iteration 267/3560 Training loss: 2.4652 0.9634 sec/batch\n",
      "Epoch 2/20  Iteration 268/3560 Training loss: 2.4643 0.9833 sec/batch\n",
      "Epoch 2/20  Iteration 269/3560 Training loss: 2.4631 0.9418 sec/batch\n",
      "Epoch 2/20  Iteration 270/3560 Training loss: 2.4621 0.9254 sec/batch\n",
      "Epoch 2/20  Iteration 271/3560 Training loss: 2.4610 0.8807 sec/batch\n",
      "Epoch 2/20  Iteration 272/3560 Training loss: 2.4598 0.9371 sec/batch\n",
      "Epoch 2/20  Iteration 273/3560 Training loss: 2.4585 1.0729 sec/batch\n",
      "Epoch 2/20  Iteration 274/3560 Training loss: 2.4573 1.0766 sec/batch\n",
      "Epoch 2/20  Iteration 275/3560 Training loss: 2.4563 0.9786 sec/batch\n",
      "Epoch 2/20  Iteration 276/3560 Training loss: 2.4552 0.9686 sec/batch\n",
      "Epoch 2/20  Iteration 277/3560 Training loss: 2.4541 0.9327 sec/batch\n",
      "Epoch 2/20  Iteration 278/3560 Training loss: 2.4530 0.9473 sec/batch\n",
      "Epoch 2/20  Iteration 279/3560 Training loss: 2.4521 0.9210 sec/batch\n",
      "Epoch 2/20  Iteration 280/3560 Training loss: 2.4511 0.9972 sec/batch\n",
      "Epoch 2/20  Iteration 281/3560 Training loss: 2.4499 1.0404 sec/batch\n",
      "Epoch 2/20  Iteration 282/3560 Training loss: 2.4487 1.0251 sec/batch\n",
      "Epoch 2/20  Iteration 283/3560 Training loss: 2.4477 0.9340 sec/batch\n",
      "Epoch 2/20  Iteration 284/3560 Training loss: 2.4467 0.9687 sec/batch\n",
      "Epoch 2/20  Iteration 285/3560 Training loss: 2.4459 0.9374 sec/batch\n",
      "Epoch 2/20  Iteration 286/3560 Training loss: 2.4453 0.9618 sec/batch\n",
      "Epoch 2/20  Iteration 287/3560 Training loss: 2.4445 0.9870 sec/batch\n",
      "Epoch 2/20  Iteration 288/3560 Training loss: 2.4435 0.9597 sec/batch\n",
      "Epoch 2/20  Iteration 289/3560 Training loss: 2.4427 1.0122 sec/batch\n",
      "Epoch 2/20  Iteration 290/3560 Training loss: 2.4420 0.9926 sec/batch\n",
      "Epoch 2/20  Iteration 291/3560 Training loss: 2.4411 0.9784 sec/batch\n",
      "Epoch 2/20  Iteration 292/3560 Training loss: 2.4402 0.9449 sec/batch\n",
      "Epoch 2/20  Iteration 293/3560 Training loss: 2.4394 0.9264 sec/batch\n",
      "Epoch 2/20  Iteration 294/3560 Training loss: 2.4382 0.9131 sec/batch\n",
      "Epoch 2/20  Iteration 295/3560 Training loss: 2.4374 1.0232 sec/batch\n",
      "Epoch 2/20  Iteration 296/3560 Training loss: 2.4365 0.9827 sec/batch\n",
      "Epoch 2/20  Iteration 297/3560 Training loss: 2.4358 1.0741 sec/batch\n",
      "Epoch 2/20  Iteration 298/3560 Training loss: 2.4351 0.9627 sec/batch\n",
      "Epoch 2/20  Iteration 299/3560 Training loss: 2.4344 0.9302 sec/batch\n",
      "Epoch 2/20  Iteration 300/3560 Training loss: 2.4336 0.9867 sec/batch\n",
      "Epoch 2/20  Iteration 301/3560 Training loss: 2.4327 0.9429 sec/batch\n",
      "Epoch 2/20  Iteration 302/3560 Training loss: 2.4320 0.9164 sec/batch\n",
      "Epoch 2/20  Iteration 303/3560 Training loss: 2.4312 0.9488 sec/batch\n",
      "Epoch 2/20  Iteration 304/3560 Training loss: 2.4302 1.0510 sec/batch\n",
      "Epoch 2/20  Iteration 305/3560 Training loss: 2.4296 1.0375 sec/batch\n",
      "Epoch 2/20  Iteration 306/3560 Training loss: 2.4290 0.9816 sec/batch\n",
      "Epoch 2/20  Iteration 307/3560 Training loss: 2.4282 0.9643 sec/batch\n",
      "Epoch 2/20  Iteration 308/3560 Training loss: 2.4275 1.0093 sec/batch\n",
      "Epoch 2/20  Iteration 309/3560 Training loss: 2.4266 0.8656 sec/batch\n",
      "Epoch 2/20  Iteration 310/3560 Training loss: 2.4257 0.8701 sec/batch\n",
      "Epoch 2/20  Iteration 311/3560 Training loss: 2.4249 1.0165 sec/batch\n",
      "Epoch 2/20  Iteration 312/3560 Training loss: 2.4242 0.9843 sec/batch\n",
      "Epoch 2/20  Iteration 313/3560 Training loss: 2.4233 1.0830 sec/batch\n",
      "Epoch 2/20  Iteration 314/3560 Training loss: 2.4226 0.9206 sec/batch\n",
      "Epoch 2/20  Iteration 315/3560 Training loss: 2.4219 0.9507 sec/batch\n",
      "Epoch 2/20  Iteration 316/3560 Training loss: 2.4211 0.9725 sec/batch\n",
      "Epoch 2/20  Iteration 317/3560 Training loss: 2.4206 0.8631 sec/batch\n",
      "Epoch 2/20  Iteration 318/3560 Training loss: 2.4198 0.9716 sec/batch\n",
      "Epoch 2/20  Iteration 319/3560 Training loss: 2.4192 0.9166 sec/batch\n",
      "Epoch 2/20  Iteration 320/3560 Training loss: 2.4184 1.0388 sec/batch\n",
      "Epoch 2/20  Iteration 321/3560 Training loss: 2.4176 1.0655 sec/batch\n",
      "Epoch 2/20  Iteration 322/3560 Training loss: 2.4168 0.9864 sec/batch\n",
      "Epoch 2/20  Iteration 323/3560 Training loss: 2.4160 0.9095 sec/batch\n",
      "Epoch 2/20  Iteration 324/3560 Training loss: 2.4154 0.9383 sec/batch\n",
      "Epoch 2/20  Iteration 325/3560 Training loss: 2.4147 0.9996 sec/batch\n",
      "Epoch 2/20  Iteration 326/3560 Training loss: 2.4141 0.9251 sec/batch\n",
      "Epoch 2/20  Iteration 327/3560 Training loss: 2.4133 0.9773 sec/batch\n",
      "Epoch 2/20  Iteration 328/3560 Training loss: 2.4125 1.1881 sec/batch\n",
      "Epoch 2/20  Iteration 329/3560 Training loss: 2.4119 1.0569 sec/batch\n",
      "Epoch 2/20  Iteration 330/3560 Training loss: 2.4115 0.9597 sec/batch\n",
      "Epoch 2/20  Iteration 331/3560 Training loss: 2.4108 1.0474 sec/batch\n",
      "Epoch 2/20  Iteration 332/3560 Training loss: 2.4102 0.9555 sec/batch\n",
      "Epoch 2/20  Iteration 333/3560 Training loss: 2.4094 0.9533 sec/batch\n",
      "Epoch 2/20  Iteration 334/3560 Training loss: 2.4087 0.9226 sec/batch\n",
      "Epoch 2/20  Iteration 335/3560 Training loss: 2.4079 1.0078 sec/batch\n",
      "Epoch 2/20  Iteration 336/3560 Training loss: 2.4072 1.0474 sec/batch\n",
      "Epoch 2/20  Iteration 337/3560 Training loss: 2.4064 1.0251 sec/batch\n",
      "Epoch 2/20  Iteration 338/3560 Training loss: 2.4058 0.9537 sec/batch\n",
      "Epoch 2/20  Iteration 339/3560 Training loss: 2.4052 0.9752 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 340/3560 Training loss: 2.4044 0.8920 sec/batch\n",
      "Epoch 2/20  Iteration 341/3560 Training loss: 2.4036 0.9377 sec/batch\n",
      "Epoch 2/20  Iteration 342/3560 Training loss: 2.4029 0.9001 sec/batch\n",
      "Epoch 2/20  Iteration 343/3560 Training loss: 2.4023 0.9752 sec/batch\n",
      "Epoch 2/20  Iteration 344/3560 Training loss: 2.4016 0.9648 sec/batch\n",
      "Epoch 2/20  Iteration 345/3560 Training loss: 2.4009 1.0453 sec/batch\n",
      "Epoch 2/20  Iteration 346/3560 Training loss: 2.4003 0.9099 sec/batch\n",
      "Epoch 2/20  Iteration 347/3560 Training loss: 2.3996 1.0012 sec/batch\n",
      "Epoch 2/20  Iteration 348/3560 Training loss: 2.3989 0.9451 sec/batch\n",
      "Epoch 2/20  Iteration 349/3560 Training loss: 2.3982 0.9850 sec/batch\n",
      "Epoch 2/20  Iteration 350/3560 Training loss: 2.3977 0.9202 sec/batch\n",
      "Epoch 2/20  Iteration 351/3560 Training loss: 2.3972 1.0582 sec/batch\n",
      "Epoch 2/20  Iteration 352/3560 Training loss: 2.3968 1.0393 sec/batch\n",
      "Epoch 2/20  Iteration 353/3560 Training loss: 2.3963 0.9936 sec/batch\n",
      "Epoch 2/20  Iteration 354/3560 Training loss: 2.3957 0.9648 sec/batch\n",
      "Epoch 2/20  Iteration 355/3560 Training loss: 2.3949 1.0166 sec/batch\n",
      "Epoch 2/20  Iteration 356/3560 Training loss: 2.3942 0.9118 sec/batch\n",
      "Epoch 3/20  Iteration 357/3560 Training loss: 2.3508 0.9414 sec/batch\n",
      "Epoch 3/20  Iteration 358/3560 Training loss: 2.2949 1.0231 sec/batch\n",
      "Epoch 3/20  Iteration 359/3560 Training loss: 2.2798 1.0246 sec/batch\n",
      "Epoch 3/20  Iteration 360/3560 Training loss: 2.2745 1.0418 sec/batch\n",
      "Epoch 3/20  Iteration 361/3560 Training loss: 2.2712 0.9570 sec/batch\n",
      "Epoch 3/20  Iteration 362/3560 Training loss: 2.2684 0.9181 sec/batch\n",
      "Epoch 3/20  Iteration 363/3560 Training loss: 2.2679 0.9333 sec/batch\n",
      "Epoch 3/20  Iteration 364/3560 Training loss: 2.2691 1.0062 sec/batch\n",
      "Epoch 3/20  Iteration 365/3560 Training loss: 2.2711 0.9041 sec/batch\n",
      "Epoch 3/20  Iteration 366/3560 Training loss: 2.2713 0.9461 sec/batch\n",
      "Epoch 3/20  Iteration 367/3560 Training loss: 2.2690 1.0457 sec/batch\n",
      "Epoch 3/20  Iteration 368/3560 Training loss: 2.2679 1.0050 sec/batch\n",
      "Epoch 3/20  Iteration 369/3560 Training loss: 2.2678 0.9674 sec/batch\n",
      "Epoch 3/20  Iteration 370/3560 Training loss: 2.2700 0.9733 sec/batch\n",
      "Epoch 3/20  Iteration 371/3560 Training loss: 2.2695 0.9145 sec/batch\n",
      "Epoch 3/20  Iteration 372/3560 Training loss: 2.2692 0.9364 sec/batch\n",
      "Epoch 3/20  Iteration 373/3560 Training loss: 2.2680 0.9614 sec/batch\n",
      "Epoch 3/20  Iteration 374/3560 Training loss: 2.2695 0.9905 sec/batch\n",
      "Epoch 3/20  Iteration 375/3560 Training loss: 2.2697 1.0263 sec/batch\n",
      "Epoch 3/20  Iteration 376/3560 Training loss: 2.2691 0.9586 sec/batch\n",
      "Epoch 3/20  Iteration 377/3560 Training loss: 2.2681 0.9731 sec/batch\n",
      "Epoch 3/20  Iteration 378/3560 Training loss: 2.2692 0.9448 sec/batch\n",
      "Epoch 3/20  Iteration 379/3560 Training loss: 2.2680 0.9817 sec/batch\n",
      "Epoch 3/20  Iteration 380/3560 Training loss: 2.2667 1.0244 sec/batch\n",
      "Epoch 3/20  Iteration 381/3560 Training loss: 2.2658 0.9481 sec/batch\n",
      "Epoch 3/20  Iteration 382/3560 Training loss: 2.2646 0.9887 sec/batch\n",
      "Epoch 3/20  Iteration 383/3560 Training loss: 2.2634 1.0707 sec/batch\n",
      "Epoch 3/20  Iteration 384/3560 Training loss: 2.2631 0.9990 sec/batch\n",
      "Epoch 3/20  Iteration 385/3560 Training loss: 2.2634 0.9735 sec/batch\n",
      "Epoch 3/20  Iteration 386/3560 Training loss: 2.2631 0.9655 sec/batch\n",
      "Epoch 3/20  Iteration 387/3560 Training loss: 2.2629 0.9007 sec/batch\n",
      "Epoch 3/20  Iteration 388/3560 Training loss: 2.2619 0.9336 sec/batch\n",
      "Epoch 3/20  Iteration 389/3560 Training loss: 2.2609 0.9454 sec/batch\n",
      "Epoch 3/20  Iteration 390/3560 Training loss: 2.2609 1.0787 sec/batch\n",
      "Epoch 3/20  Iteration 391/3560 Training loss: 2.2602 1.0714 sec/batch\n",
      "Epoch 3/20  Iteration 392/3560 Training loss: 2.2598 1.0062 sec/batch\n",
      "Epoch 3/20  Iteration 393/3560 Training loss: 2.2590 0.9164 sec/batch\n",
      "Epoch 3/20  Iteration 394/3560 Training loss: 2.2575 0.8826 sec/batch\n",
      "Epoch 3/20  Iteration 395/3560 Training loss: 2.2564 0.9878 sec/batch\n",
      "Epoch 3/20  Iteration 396/3560 Training loss: 2.2552 0.9509 sec/batch\n",
      "Epoch 3/20  Iteration 397/3560 Training loss: 2.2544 1.0297 sec/batch\n",
      "Epoch 3/20  Iteration 398/3560 Training loss: 2.2536 0.9989 sec/batch\n",
      "Epoch 3/20  Iteration 399/3560 Training loss: 2.2527 1.0910 sec/batch\n",
      "Epoch 3/20  Iteration 400/3560 Training loss: 2.2518 0.9688 sec/batch\n",
      "Epoch 3/20  Iteration 401/3560 Training loss: 2.2512 1.0187 sec/batch\n",
      "Epoch 3/20  Iteration 402/3560 Training loss: 2.2496 0.9109 sec/batch\n",
      "Epoch 3/20  Iteration 403/3560 Training loss: 2.2494 0.9927 sec/batch\n",
      "Epoch 3/20  Iteration 404/3560 Training loss: 2.2487 0.9359 sec/batch\n",
      "Epoch 3/20  Iteration 405/3560 Training loss: 2.2481 1.0038 sec/batch\n",
      "Epoch 3/20  Iteration 406/3560 Training loss: 2.2483 1.0522 sec/batch\n",
      "Epoch 3/20  Iteration 407/3560 Training loss: 2.2474 1.0637 sec/batch\n",
      "Epoch 3/20  Iteration 408/3560 Training loss: 2.2474 0.9662 sec/batch\n",
      "Epoch 3/20  Iteration 409/3560 Training loss: 2.2467 0.9778 sec/batch\n",
      "Epoch 3/20  Iteration 410/3560 Training loss: 2.2460 0.9935 sec/batch\n",
      "Epoch 3/20  Iteration 411/3560 Training loss: 2.2454 0.9180 sec/batch\n",
      "Epoch 3/20  Iteration 412/3560 Training loss: 2.2452 0.9632 sec/batch\n",
      "Epoch 3/20  Iteration 413/3560 Training loss: 2.2447 0.9975 sec/batch\n",
      "Epoch 3/20  Iteration 414/3560 Training loss: 2.2440 0.9940 sec/batch\n",
      "Epoch 3/20  Iteration 415/3560 Training loss: 2.2433 1.0059 sec/batch\n",
      "Epoch 3/20  Iteration 416/3560 Training loss: 2.2435 0.9071 sec/batch\n",
      "Epoch 3/20  Iteration 417/3560 Training loss: 2.2428 0.9332 sec/batch\n",
      "Epoch 3/20  Iteration 418/3560 Training loss: 2.2427 0.9695 sec/batch\n",
      "Epoch 3/20  Iteration 419/3560 Training loss: 2.2426 0.8709 sec/batch\n",
      "Epoch 3/20  Iteration 420/3560 Training loss: 2.2422 1.0234 sec/batch\n",
      "Epoch 3/20  Iteration 421/3560 Training loss: 2.2417 1.0036 sec/batch\n",
      "Epoch 3/20  Iteration 422/3560 Training loss: 2.2416 1.0497 sec/batch\n",
      "Epoch 3/20  Iteration 423/3560 Training loss: 2.2411 1.0102 sec/batch\n",
      "Epoch 3/20  Iteration 424/3560 Training loss: 2.2403 0.9698 sec/batch\n",
      "Epoch 3/20  Iteration 425/3560 Training loss: 2.2395 0.8835 sec/batch\n",
      "Epoch 3/20  Iteration 426/3560 Training loss: 2.2391 0.9659 sec/batch\n",
      "Epoch 3/20  Iteration 427/3560 Training loss: 2.2389 0.9035 sec/batch\n",
      "Epoch 3/20  Iteration 428/3560 Training loss: 2.2386 0.9147 sec/batch\n",
      "Epoch 3/20  Iteration 429/3560 Training loss: 2.2385 0.9983 sec/batch\n",
      "Epoch 3/20  Iteration 430/3560 Training loss: 2.2377 1.0592 sec/batch\n",
      "Epoch 3/20  Iteration 431/3560 Training loss: 2.2372 1.0454 sec/batch\n",
      "Epoch 3/20  Iteration 432/3560 Training loss: 2.2374 0.9124 sec/batch\n",
      "Epoch 3/20  Iteration 433/3560 Training loss: 2.2368 0.9397 sec/batch\n",
      "Epoch 3/20  Iteration 434/3560 Training loss: 2.2367 0.9335 sec/batch\n",
      "Epoch 3/20  Iteration 435/3560 Training loss: 2.2360 0.9779 sec/batch\n",
      "Epoch 3/20  Iteration 436/3560 Training loss: 2.2355 1.0383 sec/batch\n",
      "Epoch 3/20  Iteration 437/3560 Training loss: 2.2348 1.0695 sec/batch\n",
      "Epoch 3/20  Iteration 438/3560 Training loss: 2.2346 0.9347 sec/batch\n",
      "Epoch 3/20  Iteration 439/3560 Training loss: 2.2339 1.0022 sec/batch\n",
      "Epoch 3/20  Iteration 440/3560 Training loss: 2.2333 0.9151 sec/batch\n",
      "Epoch 3/20  Iteration 441/3560 Training loss: 2.2323 1.0305 sec/batch\n",
      "Epoch 3/20  Iteration 442/3560 Training loss: 2.2317 0.9675 sec/batch\n",
      "Epoch 3/20  Iteration 443/3560 Training loss: 2.2313 0.9965 sec/batch\n",
      "Epoch 3/20  Iteration 444/3560 Training loss: 2.2309 0.9768 sec/batch\n",
      "Epoch 3/20  Iteration 445/3560 Training loss: 2.2301 0.9930 sec/batch\n",
      "Epoch 3/20  Iteration 446/3560 Training loss: 2.2298 1.1023 sec/batch\n",
      "Epoch 3/20  Iteration 447/3560 Training loss: 2.2293 0.9672 sec/batch\n",
      "Epoch 3/20  Iteration 448/3560 Training loss: 2.2288 0.9119 sec/batch\n",
      "Epoch 3/20  Iteration 449/3560 Training loss: 2.2281 0.9775 sec/batch\n",
      "Epoch 3/20  Iteration 450/3560 Training loss: 2.2274 0.9214 sec/batch\n",
      "Epoch 3/20  Iteration 451/3560 Training loss: 2.2268 1.0211 sec/batch\n",
      "Epoch 3/20  Iteration 452/3560 Training loss: 2.2262 1.0310 sec/batch\n",
      "Epoch 3/20  Iteration 453/3560 Training loss: 2.2257 1.0246 sec/batch\n",
      "Epoch 3/20  Iteration 454/3560 Training loss: 2.2252 0.9721 sec/batch\n",
      "Epoch 3/20  Iteration 455/3560 Training loss: 2.2245 1.0031 sec/batch\n",
      "Epoch 3/20  Iteration 456/3560 Training loss: 2.2237 0.9233 sec/batch\n",
      "Epoch 3/20  Iteration 457/3560 Training loss: 2.2234 0.9842 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 458/3560 Training loss: 2.2230 0.9715 sec/batch\n",
      "Epoch 3/20  Iteration 459/3560 Training loss: 2.2224 1.0268 sec/batch\n",
      "Epoch 3/20  Iteration 460/3560 Training loss: 2.2219 0.9148 sec/batch\n",
      "Epoch 3/20  Iteration 461/3560 Training loss: 2.2214 0.9757 sec/batch\n",
      "Epoch 3/20  Iteration 462/3560 Training loss: 2.2209 0.9458 sec/batch\n",
      "Epoch 3/20  Iteration 463/3560 Training loss: 2.2205 1.0161 sec/batch\n",
      "Epoch 3/20  Iteration 464/3560 Training loss: 2.2202 0.9711 sec/batch\n",
      "Epoch 3/20  Iteration 465/3560 Training loss: 2.2199 0.8933 sec/batch\n",
      "Epoch 3/20  Iteration 466/3560 Training loss: 2.2194 0.9665 sec/batch\n",
      "Epoch 3/20  Iteration 467/3560 Training loss: 2.2189 1.0315 sec/batch\n",
      "Epoch 3/20  Iteration 468/3560 Training loss: 2.2186 0.9430 sec/batch\n",
      "Epoch 3/20  Iteration 469/3560 Training loss: 2.2181 1.0494 sec/batch\n",
      "Epoch 3/20  Iteration 470/3560 Training loss: 2.2176 0.9788 sec/batch\n",
      "Epoch 3/20  Iteration 471/3560 Training loss: 2.2171 0.9747 sec/batch\n",
      "Epoch 3/20  Iteration 472/3560 Training loss: 2.2163 0.9646 sec/batch\n",
      "Epoch 3/20  Iteration 473/3560 Training loss: 2.2159 0.9002 sec/batch\n",
      "Epoch 3/20  Iteration 474/3560 Training loss: 2.2155 1.0689 sec/batch\n",
      "Epoch 3/20  Iteration 475/3560 Training loss: 2.2152 1.0048 sec/batch\n",
      "Epoch 3/20  Iteration 476/3560 Training loss: 2.2148 0.9060 sec/batch\n",
      "Epoch 3/20  Iteration 477/3560 Training loss: 2.2146 1.1135 sec/batch\n",
      "Epoch 3/20  Iteration 478/3560 Training loss: 2.2141 0.8913 sec/batch\n",
      "Epoch 3/20  Iteration 479/3560 Training loss: 2.2135 0.9503 sec/batch\n",
      "Epoch 3/20  Iteration 480/3560 Training loss: 2.2133 0.9285 sec/batch\n",
      "Epoch 3/20  Iteration 481/3560 Training loss: 2.2129 0.9464 sec/batch\n",
      "Epoch 3/20  Iteration 482/3560 Training loss: 2.2123 1.1140 sec/batch\n",
      "Epoch 3/20  Iteration 483/3560 Training loss: 2.2120 0.9702 sec/batch\n",
      "Epoch 3/20  Iteration 484/3560 Training loss: 2.2117 1.0251 sec/batch\n",
      "Epoch 3/20  Iteration 485/3560 Training loss: 2.2113 0.9763 sec/batch\n",
      "Epoch 3/20  Iteration 486/3560 Training loss: 2.2110 0.9743 sec/batch\n",
      "Epoch 3/20  Iteration 487/3560 Training loss: 2.2106 0.9409 sec/batch\n",
      "Epoch 3/20  Iteration 488/3560 Training loss: 2.2099 1.0280 sec/batch\n",
      "Epoch 3/20  Iteration 489/3560 Training loss: 2.2096 0.9559 sec/batch\n",
      "Epoch 3/20  Iteration 490/3560 Training loss: 2.2094 1.0287 sec/batch\n",
      "Epoch 3/20  Iteration 491/3560 Training loss: 2.2090 1.1164 sec/batch\n",
      "Epoch 3/20  Iteration 492/3560 Training loss: 2.2088 1.0158 sec/batch\n",
      "Epoch 3/20  Iteration 493/3560 Training loss: 2.2085 1.0135 sec/batch\n",
      "Epoch 3/20  Iteration 494/3560 Training loss: 2.2083 0.9357 sec/batch\n",
      "Epoch 3/20  Iteration 495/3560 Training loss: 2.2082 0.9399 sec/batch\n",
      "Epoch 3/20  Iteration 496/3560 Training loss: 2.2078 0.8865 sec/batch\n",
      "Epoch 3/20  Iteration 497/3560 Training loss: 2.2076 0.9335 sec/batch\n",
      "Epoch 3/20  Iteration 498/3560 Training loss: 2.2073 1.0881 sec/batch\n",
      "Epoch 3/20  Iteration 499/3560 Training loss: 2.2070 0.9582 sec/batch\n",
      "Epoch 3/20  Iteration 500/3560 Training loss: 2.2066 0.9985 sec/batch\n",
      "Epoch 3/20  Iteration 501/3560 Training loss: 2.2062 0.9928 sec/batch\n",
      "Epoch 3/20  Iteration 502/3560 Training loss: 2.2061 0.9710 sec/batch\n",
      "Epoch 3/20  Iteration 503/3560 Training loss: 2.2058 0.9719 sec/batch\n",
      "Epoch 3/20  Iteration 504/3560 Training loss: 2.2056 0.9205 sec/batch\n",
      "Epoch 3/20  Iteration 505/3560 Training loss: 2.2054 0.9997 sec/batch\n",
      "Epoch 3/20  Iteration 506/3560 Training loss: 2.2049 0.9722 sec/batch\n",
      "Epoch 3/20  Iteration 507/3560 Training loss: 2.2046 1.0019 sec/batch\n",
      "Epoch 3/20  Iteration 508/3560 Training loss: 2.2045 1.0277 sec/batch\n",
      "Epoch 3/20  Iteration 509/3560 Training loss: 2.2042 0.9117 sec/batch\n",
      "Epoch 3/20  Iteration 510/3560 Training loss: 2.2040 0.9829 sec/batch\n",
      "Epoch 3/20  Iteration 511/3560 Training loss: 2.2036 0.8685 sec/batch\n",
      "Epoch 3/20  Iteration 512/3560 Training loss: 2.2032 0.8787 sec/batch\n",
      "Epoch 3/20  Iteration 513/3560 Training loss: 2.2028 1.0984 sec/batch\n",
      "Epoch 3/20  Iteration 514/3560 Training loss: 2.2025 0.9971 sec/batch\n",
      "Epoch 3/20  Iteration 515/3560 Training loss: 2.2019 1.0351 sec/batch\n",
      "Epoch 3/20  Iteration 516/3560 Training loss: 2.2019 1.0029 sec/batch\n",
      "Epoch 3/20  Iteration 517/3560 Training loss: 2.2016 1.0034 sec/batch\n",
      "Epoch 3/20  Iteration 518/3560 Training loss: 2.2012 0.9973 sec/batch\n",
      "Epoch 3/20  Iteration 519/3560 Training loss: 2.2009 0.8435 sec/batch\n",
      "Epoch 3/20  Iteration 520/3560 Training loss: 2.2005 0.9884 sec/batch\n",
      "Epoch 3/20  Iteration 521/3560 Training loss: 2.2002 0.8952 sec/batch\n",
      "Epoch 3/20  Iteration 522/3560 Training loss: 2.1998 0.9707 sec/batch\n",
      "Epoch 3/20  Iteration 523/3560 Training loss: 2.1996 0.9574 sec/batch\n",
      "Epoch 3/20  Iteration 524/3560 Training loss: 2.1994 1.0371 sec/batch\n",
      "Epoch 3/20  Iteration 525/3560 Training loss: 2.1991 0.9031 sec/batch\n",
      "Epoch 3/20  Iteration 526/3560 Training loss: 2.1988 0.9349 sec/batch\n",
      "Epoch 3/20  Iteration 527/3560 Training loss: 2.1984 0.9663 sec/batch\n",
      "Epoch 3/20  Iteration 528/3560 Training loss: 2.1981 0.8695 sec/batch\n",
      "Epoch 3/20  Iteration 529/3560 Training loss: 2.1979 1.0120 sec/batch\n",
      "Epoch 3/20  Iteration 530/3560 Training loss: 2.1977 1.0246 sec/batch\n",
      "Epoch 3/20  Iteration 531/3560 Training loss: 2.1975 0.9937 sec/batch\n",
      "Epoch 3/20  Iteration 532/3560 Training loss: 2.1976 0.9681 sec/batch\n",
      "Epoch 3/20  Iteration 533/3560 Training loss: 2.1975 1.0451 sec/batch\n",
      "Epoch 3/20  Iteration 534/3560 Training loss: 2.1976 0.8955 sec/batch\n",
      "Epoch 4/20  Iteration 535/3560 Training loss: 2.2179 0.9610 sec/batch\n",
      "Epoch 4/20  Iteration 536/3560 Training loss: 2.1652 1.0549 sec/batch\n",
      "Epoch 4/20  Iteration 537/3560 Training loss: 2.1503 0.9936 sec/batch\n",
      "Epoch 4/20  Iteration 538/3560 Training loss: 2.1426 0.9887 sec/batch\n",
      "Epoch 4/20  Iteration 539/3560 Training loss: 2.1384 0.9942 sec/batch\n",
      "Epoch 4/20  Iteration 540/3560 Training loss: 2.1330 0.8875 sec/batch\n",
      "Epoch 4/20  Iteration 541/3560 Training loss: 2.1352 1.0291 sec/batch\n",
      "Epoch 4/20  Iteration 542/3560 Training loss: 2.1362 0.9093 sec/batch\n",
      "Epoch 4/20  Iteration 543/3560 Training loss: 2.1373 0.9524 sec/batch\n",
      "Epoch 4/20  Iteration 544/3560 Training loss: 2.1358 0.9533 sec/batch\n",
      "Epoch 4/20  Iteration 545/3560 Training loss: 2.1330 1.0267 sec/batch\n",
      "Epoch 4/20  Iteration 546/3560 Training loss: 2.1313 0.9519 sec/batch\n",
      "Epoch 4/20  Iteration 547/3560 Training loss: 2.1316 1.0455 sec/batch\n",
      "Epoch 4/20  Iteration 548/3560 Training loss: 2.1333 0.9748 sec/batch\n",
      "Epoch 4/20  Iteration 549/3560 Training loss: 2.1325 0.9369 sec/batch\n",
      "Epoch 4/20  Iteration 550/3560 Training loss: 2.1313 0.9437 sec/batch\n",
      "Epoch 4/20  Iteration 551/3560 Training loss: 2.1311 0.9344 sec/batch\n",
      "Epoch 4/20  Iteration 552/3560 Training loss: 2.1329 0.9761 sec/batch\n",
      "Epoch 4/20  Iteration 553/3560 Training loss: 2.1332 0.9339 sec/batch\n",
      "Epoch 4/20  Iteration 554/3560 Training loss: 2.1320 1.0847 sec/batch\n",
      "Epoch 4/20  Iteration 555/3560 Training loss: 2.1312 0.9813 sec/batch\n",
      "Epoch 4/20  Iteration 556/3560 Training loss: 2.1325 0.9400 sec/batch\n",
      "Epoch 4/20  Iteration 557/3560 Training loss: 2.1317 0.9094 sec/batch\n",
      "Epoch 4/20  Iteration 558/3560 Training loss: 2.1309 0.9404 sec/batch\n",
      "Epoch 4/20  Iteration 559/3560 Training loss: 2.1305 0.8708 sec/batch\n",
      "Epoch 4/20  Iteration 560/3560 Training loss: 2.1290 0.9844 sec/batch\n",
      "Epoch 4/20  Iteration 561/3560 Training loss: 2.1281 1.0623 sec/batch\n",
      "Epoch 4/20  Iteration 562/3560 Training loss: 2.1281 0.9508 sec/batch\n",
      "Epoch 4/20  Iteration 563/3560 Training loss: 2.1286 1.0073 sec/batch\n",
      "Epoch 4/20  Iteration 564/3560 Training loss: 2.1282 0.9069 sec/batch\n",
      "Epoch 4/20  Iteration 565/3560 Training loss: 2.1279 0.9756 sec/batch\n",
      "Epoch 4/20  Iteration 566/3560 Training loss: 2.1270 0.9683 sec/batch\n",
      "Epoch 4/20  Iteration 567/3560 Training loss: 2.1268 1.0004 sec/batch\n",
      "Epoch 4/20  Iteration 568/3560 Training loss: 2.1272 1.0090 sec/batch\n",
      "Epoch 4/20  Iteration 569/3560 Training loss: 2.1265 1.0140 sec/batch\n",
      "Epoch 4/20  Iteration 570/3560 Training loss: 2.1259 0.9326 sec/batch\n",
      "Epoch 4/20  Iteration 571/3560 Training loss: 2.1255 0.9103 sec/batch\n",
      "Epoch 4/20  Iteration 572/3560 Training loss: 2.1240 0.9309 sec/batch\n",
      "Epoch 4/20  Iteration 573/3560 Training loss: 2.1229 0.8863 sec/batch\n",
      "Epoch 4/20  Iteration 574/3560 Training loss: 2.1218 0.9194 sec/batch\n",
      "Epoch 4/20  Iteration 575/3560 Training loss: 2.1212 1.0150 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 576/3560 Training loss: 2.1208 0.9639 sec/batch\n",
      "Epoch 4/20  Iteration 577/3560 Training loss: 2.1200 0.9950 sec/batch\n",
      "Epoch 4/20  Iteration 578/3560 Training loss: 2.1192 0.9561 sec/batch\n",
      "Epoch 4/20  Iteration 579/3560 Training loss: 2.1191 1.0331 sec/batch\n",
      "Epoch 4/20  Iteration 580/3560 Training loss: 2.1176 1.0449 sec/batch\n",
      "Epoch 4/20  Iteration 581/3560 Training loss: 2.1173 0.8790 sec/batch\n",
      "Epoch 4/20  Iteration 582/3560 Training loss: 2.1167 1.0155 sec/batch\n",
      "Epoch 4/20  Iteration 583/3560 Training loss: 2.1164 0.9437 sec/batch\n",
      "Epoch 4/20  Iteration 584/3560 Training loss: 2.1169 0.9816 sec/batch\n",
      "Epoch 4/20  Iteration 585/3560 Training loss: 2.1161 1.0377 sec/batch\n",
      "Epoch 4/20  Iteration 586/3560 Training loss: 2.1164 1.0144 sec/batch\n",
      "Epoch 4/20  Iteration 587/3560 Training loss: 2.1158 0.9991 sec/batch\n",
      "Epoch 4/20  Iteration 588/3560 Training loss: 2.1153 0.8905 sec/batch\n",
      "Epoch 4/20  Iteration 589/3560 Training loss: 2.1149 0.9138 sec/batch\n",
      "Epoch 4/20  Iteration 590/3560 Training loss: 2.1148 1.0131 sec/batch\n",
      "Epoch 4/20  Iteration 591/3560 Training loss: 2.1147 1.0329 sec/batch\n",
      "Epoch 4/20  Iteration 592/3560 Training loss: 2.1141 1.0383 sec/batch\n",
      "Epoch 4/20  Iteration 593/3560 Training loss: 2.1135 0.9298 sec/batch\n",
      "Epoch 4/20  Iteration 594/3560 Training loss: 2.1137 0.9620 sec/batch\n",
      "Epoch 4/20  Iteration 595/3560 Training loss: 2.1136 0.9531 sec/batch\n",
      "Epoch 4/20  Iteration 596/3560 Training loss: 2.1138 0.9622 sec/batch\n",
      "Epoch 4/20  Iteration 597/3560 Training loss: 2.1142 1.0063 sec/batch\n",
      "Epoch 4/20  Iteration 598/3560 Training loss: 2.1140 0.9945 sec/batch\n",
      "Epoch 4/20  Iteration 599/3560 Training loss: 2.1137 1.0163 sec/batch\n",
      "Epoch 4/20  Iteration 600/3560 Training loss: 2.1137 0.9208 sec/batch\n",
      "Epoch 4/20  Iteration 601/3560 Training loss: 2.1135 0.9264 sec/batch\n",
      "Epoch 4/20  Iteration 602/3560 Training loss: 2.1129 0.9278 sec/batch\n",
      "Epoch 4/20  Iteration 603/3560 Training loss: 2.1124 0.9768 sec/batch\n",
      "Epoch 4/20  Iteration 604/3560 Training loss: 2.1121 0.8727 sec/batch\n",
      "Epoch 4/20  Iteration 605/3560 Training loss: 2.1122 1.0060 sec/batch\n",
      "Epoch 4/20  Iteration 606/3560 Training loss: 2.1120 0.8726 sec/batch\n",
      "Epoch 4/20  Iteration 607/3560 Training loss: 2.1120 1.1365 sec/batch\n",
      "Epoch 4/20  Iteration 608/3560 Training loss: 2.1114 1.0248 sec/batch\n",
      "Epoch 4/20  Iteration 609/3560 Training loss: 2.1111 0.9249 sec/batch\n",
      "Epoch 4/20  Iteration 610/3560 Training loss: 2.1113 1.0117 sec/batch\n",
      "Epoch 4/20  Iteration 611/3560 Training loss: 2.1109 0.9857 sec/batch\n",
      "Epoch 4/20  Iteration 612/3560 Training loss: 2.1107 0.9566 sec/batch\n",
      "Epoch 4/20  Iteration 613/3560 Training loss: 2.1101 0.9587 sec/batch\n",
      "Epoch 4/20  Iteration 614/3560 Training loss: 2.1096 0.9800 sec/batch\n",
      "Epoch 4/20  Iteration 615/3560 Training loss: 2.1090 1.0079 sec/batch\n",
      "Epoch 4/20  Iteration 616/3560 Training loss: 2.1089 1.0397 sec/batch\n",
      "Epoch 4/20  Iteration 617/3560 Training loss: 2.1082 1.0514 sec/batch\n",
      "Epoch 4/20  Iteration 618/3560 Training loss: 2.1077 0.9956 sec/batch\n",
      "Epoch 4/20  Iteration 619/3560 Training loss: 2.1069 0.9564 sec/batch\n",
      "Epoch 4/20  Iteration 620/3560 Training loss: 2.1063 0.9605 sec/batch\n",
      "Epoch 4/20  Iteration 621/3560 Training loss: 2.1060 0.9620 sec/batch\n",
      "Epoch 4/20  Iteration 622/3560 Training loss: 2.1056 1.0477 sec/batch\n",
      "Epoch 4/20  Iteration 623/3560 Training loss: 2.1049 1.0634 sec/batch\n",
      "Epoch 4/20  Iteration 624/3560 Training loss: 2.1047 1.0304 sec/batch\n",
      "Epoch 4/20  Iteration 625/3560 Training loss: 2.1043 0.9640 sec/batch\n",
      "Epoch 4/20  Iteration 626/3560 Training loss: 2.1039 1.0106 sec/batch\n",
      "Epoch 4/20  Iteration 627/3560 Training loss: 2.1032 1.0215 sec/batch\n",
      "Epoch 4/20  Iteration 628/3560 Training loss: 2.1027 0.9997 sec/batch\n",
      "Epoch 4/20  Iteration 629/3560 Training loss: 2.1022 0.9433 sec/batch\n",
      "Epoch 4/20  Iteration 630/3560 Training loss: 2.1018 0.9837 sec/batch\n",
      "Epoch 4/20  Iteration 631/3560 Training loss: 2.1014 0.9882 sec/batch\n",
      "Epoch 4/20  Iteration 632/3560 Training loss: 2.1009 0.9797 sec/batch\n",
      "Epoch 4/20  Iteration 633/3560 Training loss: 2.1004 0.9505 sec/batch\n",
      "Epoch 4/20  Iteration 634/3560 Training loss: 2.0997 1.0036 sec/batch\n",
      "Epoch 4/20  Iteration 635/3560 Training loss: 2.0996 0.9987 sec/batch\n",
      "Epoch 4/20  Iteration 636/3560 Training loss: 2.0992 0.9958 sec/batch\n",
      "Epoch 4/20  Iteration 637/3560 Training loss: 2.0988 1.0145 sec/batch\n",
      "Epoch 4/20  Iteration 638/3560 Training loss: 2.0984 0.9918 sec/batch\n",
      "Epoch 4/20  Iteration 639/3560 Training loss: 2.0979 1.0081 sec/batch\n",
      "Epoch 4/20  Iteration 640/3560 Training loss: 2.0977 0.9458 sec/batch\n",
      "Epoch 4/20  Iteration 641/3560 Training loss: 2.0974 0.9393 sec/batch\n",
      "Epoch 4/20  Iteration 642/3560 Training loss: 2.0972 1.0632 sec/batch\n",
      "Epoch 4/20  Iteration 643/3560 Training loss: 2.0970 0.8999 sec/batch\n",
      "Epoch 4/20  Iteration 644/3560 Training loss: 2.0967 0.9793 sec/batch\n",
      "Epoch 4/20  Iteration 645/3560 Training loss: 2.0964 0.9420 sec/batch\n",
      "Epoch 4/20  Iteration 646/3560 Training loss: 2.0961 1.0571 sec/batch\n",
      "Epoch 4/20  Iteration 647/3560 Training loss: 2.0958 0.9565 sec/batch\n",
      "Epoch 4/20  Iteration 648/3560 Training loss: 2.0955 0.9552 sec/batch\n",
      "Epoch 4/20  Iteration 649/3560 Training loss: 2.0950 0.9729 sec/batch\n",
      "Epoch 4/20  Iteration 650/3560 Training loss: 2.0944 1.0053 sec/batch\n",
      "Epoch 4/20  Iteration 651/3560 Training loss: 2.0941 1.0336 sec/batch\n",
      "Epoch 4/20  Iteration 652/3560 Training loss: 2.0938 1.0335 sec/batch\n",
      "Epoch 4/20  Iteration 653/3560 Training loss: 2.0936 0.9639 sec/batch\n",
      "Epoch 4/20  Iteration 654/3560 Training loss: 2.0933 1.0247 sec/batch\n",
      "Epoch 4/20  Iteration 655/3560 Training loss: 2.0931 0.9994 sec/batch\n",
      "Epoch 4/20  Iteration 656/3560 Training loss: 2.0926 0.9805 sec/batch\n",
      "Epoch 4/20  Iteration 657/3560 Training loss: 2.0922 0.9225 sec/batch\n",
      "Epoch 4/20  Iteration 658/3560 Training loss: 2.0921 1.1111 sec/batch\n",
      "Epoch 4/20  Iteration 659/3560 Training loss: 2.0918 0.9596 sec/batch\n",
      "Epoch 4/20  Iteration 660/3560 Training loss: 2.0912 0.9078 sec/batch\n",
      "Epoch 4/20  Iteration 661/3560 Training loss: 2.0910 1.0103 sec/batch\n",
      "Epoch 4/20  Iteration 662/3560 Training loss: 2.0908 0.9800 sec/batch\n",
      "Epoch 4/20  Iteration 663/3560 Training loss: 2.0905 0.9783 sec/batch\n",
      "Epoch 4/20  Iteration 664/3560 Training loss: 2.0902 0.8883 sec/batch\n",
      "Epoch 4/20  Iteration 665/3560 Training loss: 2.0898 0.9114 sec/batch\n",
      "Epoch 4/20  Iteration 666/3560 Training loss: 2.0893 0.9866 sec/batch\n",
      "Epoch 4/20  Iteration 667/3560 Training loss: 2.0891 0.9869 sec/batch\n",
      "Epoch 4/20  Iteration 668/3560 Training loss: 2.0889 0.9440 sec/batch\n",
      "Epoch 4/20  Iteration 669/3560 Training loss: 2.0886 1.0344 sec/batch\n",
      "Epoch 4/20  Iteration 670/3560 Training loss: 2.0884 0.9709 sec/batch\n",
      "Epoch 4/20  Iteration 671/3560 Training loss: 2.0883 0.9315 sec/batch\n",
      "Epoch 4/20  Iteration 672/3560 Training loss: 2.0881 0.9374 sec/batch\n",
      "Epoch 4/20  Iteration 673/3560 Training loss: 2.0881 0.9995 sec/batch\n",
      "Epoch 4/20  Iteration 674/3560 Training loss: 2.0878 1.0476 sec/batch\n",
      "Epoch 4/20  Iteration 675/3560 Training loss: 2.0877 0.9693 sec/batch\n",
      "Epoch 4/20  Iteration 676/3560 Training loss: 2.0875 0.9594 sec/batch\n",
      "Epoch 4/20  Iteration 677/3560 Training loss: 2.0873 1.0926 sec/batch\n",
      "Epoch 4/20  Iteration 678/3560 Training loss: 2.0871 0.9564 sec/batch\n",
      "Epoch 4/20  Iteration 679/3560 Training loss: 2.0867 0.9541 sec/batch\n",
      "Epoch 4/20  Iteration 680/3560 Training loss: 2.0867 0.9829 sec/batch\n",
      "Epoch 4/20  Iteration 681/3560 Training loss: 2.0865 1.0265 sec/batch\n",
      "Epoch 4/20  Iteration 682/3560 Training loss: 2.0865 0.9228 sec/batch\n",
      "Epoch 4/20  Iteration 683/3560 Training loss: 2.0863 0.9868 sec/batch\n",
      "Epoch 4/20  Iteration 684/3560 Training loss: 2.0859 0.8844 sec/batch\n",
      "Epoch 4/20  Iteration 685/3560 Training loss: 2.0856 0.9844 sec/batch\n",
      "Epoch 4/20  Iteration 686/3560 Training loss: 2.0856 1.0137 sec/batch\n",
      "Epoch 4/20  Iteration 687/3560 Training loss: 2.0855 0.9369 sec/batch\n",
      "Epoch 4/20  Iteration 688/3560 Training loss: 2.0853 0.9521 sec/batch\n",
      "Epoch 4/20  Iteration 689/3560 Training loss: 2.0851 1.0758 sec/batch\n",
      "Epoch 4/20  Iteration 690/3560 Training loss: 2.0849 1.0122 sec/batch\n",
      "Epoch 4/20  Iteration 691/3560 Training loss: 2.0846 1.0005 sec/batch\n",
      "Epoch 4/20  Iteration 692/3560 Training loss: 2.0843 0.9093 sec/batch\n",
      "Epoch 4/20  Iteration 693/3560 Training loss: 2.0839 1.0229 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 694/3560 Training loss: 2.0839 0.9988 sec/batch\n",
      "Epoch 4/20  Iteration 695/3560 Training loss: 2.0838 0.9595 sec/batch\n",
      "Epoch 4/20  Iteration 696/3560 Training loss: 2.0836 1.0329 sec/batch\n",
      "Epoch 4/20  Iteration 697/3560 Training loss: 2.0834 1.0084 sec/batch\n",
      "Epoch 4/20  Iteration 698/3560 Training loss: 2.0832 0.9657 sec/batch\n",
      "Epoch 4/20  Iteration 699/3560 Training loss: 2.0829 1.0472 sec/batch\n",
      "Epoch 4/20  Iteration 700/3560 Training loss: 2.0827 0.9736 sec/batch\n",
      "Epoch 4/20  Iteration 701/3560 Training loss: 2.0826 0.9802 sec/batch\n",
      "Epoch 4/20  Iteration 702/3560 Training loss: 2.0826 1.0028 sec/batch\n",
      "Epoch 4/20  Iteration 703/3560 Training loss: 2.0823 0.9659 sec/batch\n",
      "Epoch 4/20  Iteration 704/3560 Training loss: 2.0821 0.9778 sec/batch\n",
      "Epoch 4/20  Iteration 705/3560 Training loss: 2.0818 0.9640 sec/batch\n",
      "Epoch 4/20  Iteration 706/3560 Training loss: 2.0815 0.9681 sec/batch\n",
      "Epoch 4/20  Iteration 707/3560 Training loss: 2.0814 0.9561 sec/batch\n",
      "Epoch 4/20  Iteration 708/3560 Training loss: 2.0813 0.8734 sec/batch\n",
      "Epoch 4/20  Iteration 709/3560 Training loss: 2.0811 1.0817 sec/batch\n",
      "Epoch 4/20  Iteration 710/3560 Training loss: 2.0809 0.9367 sec/batch\n",
      "Epoch 4/20  Iteration 711/3560 Training loss: 2.0806 0.9508 sec/batch\n",
      "Epoch 4/20  Iteration 712/3560 Training loss: 2.0804 0.9271 sec/batch\n",
      "Epoch 5/20  Iteration 713/3560 Training loss: 2.1096 0.9756 sec/batch\n",
      "Epoch 5/20  Iteration 714/3560 Training loss: 2.0601 0.9520 sec/batch\n",
      "Epoch 5/20  Iteration 715/3560 Training loss: 2.0469 0.9662 sec/batch\n",
      "Epoch 5/20  Iteration 716/3560 Training loss: 2.0391 0.9265 sec/batch\n",
      "Epoch 5/20  Iteration 717/3560 Training loss: 2.0358 1.0010 sec/batch\n",
      "Epoch 5/20  Iteration 718/3560 Training loss: 2.0292 0.9981 sec/batch\n",
      "Epoch 5/20  Iteration 719/3560 Training loss: 2.0301 0.8957 sec/batch\n",
      "Epoch 5/20  Iteration 720/3560 Training loss: 2.0300 1.0312 sec/batch\n",
      "Epoch 5/20  Iteration 721/3560 Training loss: 2.0325 0.9736 sec/batch\n",
      "Epoch 5/20  Iteration 722/3560 Training loss: 2.0322 0.9424 sec/batch\n",
      "Epoch 5/20  Iteration 723/3560 Training loss: 2.0300 0.9391 sec/batch\n",
      "Epoch 5/20  Iteration 724/3560 Training loss: 2.0281 0.9837 sec/batch\n",
      "Epoch 5/20  Iteration 725/3560 Training loss: 2.0284 0.9548 sec/batch\n",
      "Epoch 5/20  Iteration 726/3560 Training loss: 2.0307 0.9950 sec/batch\n",
      "Epoch 5/20  Iteration 727/3560 Training loss: 2.0298 0.9526 sec/batch\n",
      "Epoch 5/20  Iteration 728/3560 Training loss: 2.0288 0.9025 sec/batch\n",
      "Epoch 5/20  Iteration 729/3560 Training loss: 2.0285 1.0095 sec/batch\n",
      "Epoch 5/20  Iteration 730/3560 Training loss: 2.0305 0.9043 sec/batch\n",
      "Epoch 5/20  Iteration 731/3560 Training loss: 2.0307 0.9674 sec/batch\n",
      "Epoch 5/20  Iteration 732/3560 Training loss: 2.0308 0.9935 sec/batch\n",
      "Epoch 5/20  Iteration 733/3560 Training loss: 2.0301 0.9638 sec/batch\n",
      "Epoch 5/20  Iteration 734/3560 Training loss: 2.0313 0.9327 sec/batch\n",
      "Epoch 5/20  Iteration 735/3560 Training loss: 2.0310 0.9458 sec/batch\n",
      "Epoch 5/20  Iteration 736/3560 Training loss: 2.0306 0.9807 sec/batch\n",
      "Epoch 5/20  Iteration 737/3560 Training loss: 2.0305 1.0796 sec/batch\n",
      "Epoch 5/20  Iteration 738/3560 Training loss: 2.0293 0.9038 sec/batch\n",
      "Epoch 5/20  Iteration 739/3560 Training loss: 2.0285 0.9314 sec/batch\n",
      "Epoch 5/20  Iteration 740/3560 Training loss: 2.0289 1.0518 sec/batch\n",
      "Epoch 5/20  Iteration 741/3560 Training loss: 2.0299 1.0701 sec/batch\n",
      "Epoch 5/20  Iteration 742/3560 Training loss: 2.0297 1.0731 sec/batch\n",
      "Epoch 5/20  Iteration 743/3560 Training loss: 2.0294 0.9396 sec/batch\n",
      "Epoch 5/20  Iteration 744/3560 Training loss: 2.0285 1.0251 sec/batch\n",
      "Epoch 5/20  Iteration 745/3560 Training loss: 2.0284 1.0126 sec/batch\n",
      "Epoch 5/20  Iteration 746/3560 Training loss: 2.0292 0.9413 sec/batch\n",
      "Epoch 5/20  Iteration 747/3560 Training loss: 2.0290 0.8736 sec/batch\n",
      "Epoch 5/20  Iteration 748/3560 Training loss: 2.0286 0.9852 sec/batch\n",
      "Epoch 5/20  Iteration 749/3560 Training loss: 2.0281 0.9196 sec/batch\n",
      "Epoch 5/20  Iteration 750/3560 Training loss: 2.0266 1.0213 sec/batch\n",
      "Epoch 5/20  Iteration 751/3560 Training loss: 2.0256 0.9290 sec/batch\n",
      "Epoch 5/20  Iteration 752/3560 Training loss: 2.0246 1.0230 sec/batch\n",
      "Epoch 5/20  Iteration 753/3560 Training loss: 2.0241 1.0140 sec/batch\n",
      "Epoch 5/20  Iteration 754/3560 Training loss: 2.0237 0.9417 sec/batch\n",
      "Epoch 5/20  Iteration 755/3560 Training loss: 2.0230 1.0102 sec/batch\n",
      "Epoch 5/20  Iteration 756/3560 Training loss: 2.0220 0.9101 sec/batch\n",
      "Epoch 5/20  Iteration 757/3560 Training loss: 2.0220 0.9857 sec/batch\n",
      "Epoch 5/20  Iteration 758/3560 Training loss: 2.0205 0.9268 sec/batch\n",
      "Epoch 5/20  Iteration 759/3560 Training loss: 2.0202 0.9316 sec/batch\n",
      "Epoch 5/20  Iteration 760/3560 Training loss: 2.0195 0.9776 sec/batch\n",
      "Epoch 5/20  Iteration 761/3560 Training loss: 2.0192 1.0043 sec/batch\n",
      "Epoch 5/20  Iteration 762/3560 Training loss: 2.0197 0.9808 sec/batch\n",
      "Epoch 5/20  Iteration 763/3560 Training loss: 2.0191 0.8800 sec/batch\n",
      "Epoch 5/20  Iteration 764/3560 Training loss: 2.0196 1.0145 sec/batch\n",
      "Epoch 5/20  Iteration 765/3560 Training loss: 2.0192 0.9250 sec/batch\n",
      "Epoch 5/20  Iteration 766/3560 Training loss: 2.0188 0.9430 sec/batch\n",
      "Epoch 5/20  Iteration 767/3560 Training loss: 2.0184 0.9249 sec/batch\n",
      "Epoch 5/20  Iteration 768/3560 Training loss: 2.0185 0.9961 sec/batch\n",
      "Epoch 5/20  Iteration 769/3560 Training loss: 2.0185 0.9552 sec/batch\n",
      "Epoch 5/20  Iteration 770/3560 Training loss: 2.0181 0.9422 sec/batch\n",
      "Epoch 5/20  Iteration 771/3560 Training loss: 2.0175 0.9629 sec/batch\n",
      "Epoch 5/20  Iteration 772/3560 Training loss: 2.0180 0.9803 sec/batch\n",
      "Epoch 5/20  Iteration 773/3560 Training loss: 2.0176 0.9504 sec/batch\n",
      "Epoch 5/20  Iteration 774/3560 Training loss: 2.0179 0.9634 sec/batch\n",
      "Epoch 5/20  Iteration 775/3560 Training loss: 2.0183 0.9751 sec/batch\n",
      "Epoch 5/20  Iteration 776/3560 Training loss: 2.0183 1.0096 sec/batch\n",
      "Epoch 5/20  Iteration 777/3560 Training loss: 2.0180 0.9782 sec/batch\n",
      "Epoch 5/20  Iteration 778/3560 Training loss: 2.0181 0.8878 sec/batch\n",
      "Epoch 5/20  Iteration 779/3560 Training loss: 2.0179 0.9640 sec/batch\n",
      "Epoch 5/20  Iteration 780/3560 Training loss: 2.0173 1.0577 sec/batch\n",
      "Epoch 5/20  Iteration 781/3560 Training loss: 2.0169 0.9436 sec/batch\n",
      "Epoch 5/20  Iteration 782/3560 Training loss: 2.0168 0.9254 sec/batch\n",
      "Epoch 5/20  Iteration 783/3560 Training loss: 2.0169 0.9353 sec/batch\n",
      "Epoch 5/20  Iteration 784/3560 Training loss: 2.0169 0.9651 sec/batch\n",
      "Epoch 5/20  Iteration 785/3560 Training loss: 2.0169 0.9395 sec/batch\n",
      "Epoch 5/20  Iteration 786/3560 Training loss: 2.0165 0.9298 sec/batch\n",
      "Epoch 5/20  Iteration 787/3560 Training loss: 2.0163 1.0489 sec/batch\n",
      "Epoch 5/20  Iteration 788/3560 Training loss: 2.0165 0.9322 sec/batch\n",
      "Epoch 5/20  Iteration 789/3560 Training loss: 2.0162 0.9561 sec/batch\n",
      "Epoch 5/20  Iteration 790/3560 Training loss: 2.0161 0.8984 sec/batch\n",
      "Epoch 5/20  Iteration 791/3560 Training loss: 2.0156 0.9494 sec/batch\n",
      "Epoch 5/20  Iteration 792/3560 Training loss: 2.0152 1.0742 sec/batch\n",
      "Epoch 5/20  Iteration 793/3560 Training loss: 2.0147 0.9688 sec/batch\n",
      "Epoch 5/20  Iteration 794/3560 Training loss: 2.0147 0.8845 sec/batch\n",
      "Epoch 5/20  Iteration 795/3560 Training loss: 2.0139 1.0596 sec/batch\n",
      "Epoch 5/20  Iteration 796/3560 Training loss: 2.0137 0.9282 sec/batch\n",
      "Epoch 5/20  Iteration 797/3560 Training loss: 2.0129 0.9671 sec/batch\n",
      "Epoch 5/20  Iteration 798/3560 Training loss: 2.0125 0.9265 sec/batch\n",
      "Epoch 5/20  Iteration 799/3560 Training loss: 2.0122 1.0107 sec/batch\n",
      "Epoch 5/20  Iteration 800/3560 Training loss: 2.0118 0.9485 sec/batch\n",
      "Epoch 5/20  Iteration 801/3560 Training loss: 2.0110 0.9342 sec/batch\n",
      "Epoch 5/20  Iteration 802/3560 Training loss: 2.0109 0.9458 sec/batch\n",
      "Epoch 5/20  Iteration 803/3560 Training loss: 2.0105 1.0306 sec/batch\n",
      "Epoch 5/20  Iteration 804/3560 Training loss: 2.0102 1.0029 sec/batch\n",
      "Epoch 5/20  Iteration 805/3560 Training loss: 2.0095 0.9252 sec/batch\n",
      "Epoch 5/20  Iteration 806/3560 Training loss: 2.0090 0.9961 sec/batch\n",
      "Epoch 5/20  Iteration 807/3560 Training loss: 2.0085 0.9873 sec/batch\n",
      "Epoch 5/20  Iteration 808/3560 Training loss: 2.0083 0.9674 sec/batch\n",
      "Epoch 5/20  Iteration 809/3560 Training loss: 2.0079 0.9411 sec/batch\n",
      "Epoch 5/20  Iteration 810/3560 Training loss: 2.0073 1.0434 sec/batch\n",
      "Epoch 5/20  Iteration 811/3560 Training loss: 2.0068 1.0363 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 812/3560 Training loss: 2.0061 0.9577 sec/batch\n",
      "Epoch 5/20  Iteration 813/3560 Training loss: 2.0059 0.9272 sec/batch\n",
      "Epoch 5/20  Iteration 814/3560 Training loss: 2.0057 0.9856 sec/batch\n",
      "Epoch 5/20  Iteration 815/3560 Training loss: 2.0052 0.9168 sec/batch\n",
      "Epoch 5/20  Iteration 816/3560 Training loss: 2.0047 1.0976 sec/batch\n",
      "Epoch 5/20  Iteration 817/3560 Training loss: 2.0043 1.0154 sec/batch\n",
      "Epoch 5/20  Iteration 818/3560 Training loss: 2.0039 0.9859 sec/batch\n",
      "Epoch 5/20  Iteration 819/3560 Training loss: 2.0037 1.0184 sec/batch\n",
      "Epoch 5/20  Iteration 820/3560 Training loss: 2.0035 0.9250 sec/batch\n",
      "Epoch 5/20  Iteration 821/3560 Training loss: 2.0033 0.9795 sec/batch\n",
      "Epoch 5/20  Iteration 822/3560 Training loss: 2.0031 0.9918 sec/batch\n",
      "Epoch 5/20  Iteration 823/3560 Training loss: 2.0028 1.0693 sec/batch\n",
      "Epoch 5/20  Iteration 824/3560 Training loss: 2.0026 0.9555 sec/batch\n",
      "Epoch 5/20  Iteration 825/3560 Training loss: 2.0023 0.9944 sec/batch\n",
      "Epoch 5/20  Iteration 826/3560 Training loss: 2.0020 0.9937 sec/batch\n",
      "Epoch 5/20  Iteration 827/3560 Training loss: 2.0015 0.9486 sec/batch\n",
      "Epoch 5/20  Iteration 828/3560 Training loss: 2.0010 0.9345 sec/batch\n",
      "Epoch 5/20  Iteration 829/3560 Training loss: 2.0008 0.9012 sec/batch\n",
      "Epoch 5/20  Iteration 830/3560 Training loss: 2.0005 0.9005 sec/batch\n",
      "Epoch 5/20  Iteration 831/3560 Training loss: 2.0003 1.0528 sec/batch\n",
      "Epoch 5/20  Iteration 832/3560 Training loss: 2.0000 1.0005 sec/batch\n",
      "Epoch 5/20  Iteration 833/3560 Training loss: 1.9998 0.9074 sec/batch\n",
      "Epoch 5/20  Iteration 834/3560 Training loss: 1.9994 1.0839 sec/batch\n",
      "Epoch 5/20  Iteration 835/3560 Training loss: 1.9989 1.0665 sec/batch\n",
      "Epoch 5/20  Iteration 836/3560 Training loss: 1.9989 0.9399 sec/batch\n",
      "Epoch 5/20  Iteration 837/3560 Training loss: 1.9987 0.9622 sec/batch\n",
      "Epoch 5/20  Iteration 838/3560 Training loss: 1.9982 0.9870 sec/batch\n",
      "Epoch 5/20  Iteration 839/3560 Training loss: 1.9981 1.0679 sec/batch\n",
      "Epoch 5/20  Iteration 840/3560 Training loss: 1.9980 0.9530 sec/batch\n",
      "Epoch 5/20  Iteration 841/3560 Training loss: 1.9978 0.8898 sec/batch\n",
      "Epoch 5/20  Iteration 842/3560 Training loss: 1.9975 1.0250 sec/batch\n",
      "Epoch 5/20  Iteration 843/3560 Training loss: 1.9972 0.9249 sec/batch\n",
      "Epoch 5/20  Iteration 844/3560 Training loss: 1.9967 0.9425 sec/batch\n",
      "Epoch 5/20  Iteration 845/3560 Training loss: 1.9965 0.9733 sec/batch\n",
      "Epoch 5/20  Iteration 846/3560 Training loss: 1.9964 0.9763 sec/batch\n",
      "Epoch 5/20  Iteration 847/3560 Training loss: 1.9962 1.0297 sec/batch\n",
      "Epoch 5/20  Iteration 848/3560 Training loss: 1.9961 0.9425 sec/batch\n",
      "Epoch 5/20  Iteration 849/3560 Training loss: 1.9960 0.9656 sec/batch\n",
      "Epoch 5/20  Iteration 850/3560 Training loss: 1.9959 0.9822 sec/batch\n",
      "Epoch 5/20  Iteration 851/3560 Training loss: 1.9960 0.9452 sec/batch\n",
      "Epoch 5/20  Iteration 852/3560 Training loss: 1.9957 0.9219 sec/batch\n",
      "Epoch 5/20  Iteration 853/3560 Training loss: 1.9957 0.9578 sec/batch\n",
      "Epoch 5/20  Iteration 854/3560 Training loss: 1.9955 0.9513 sec/batch\n",
      "Epoch 5/20  Iteration 855/3560 Training loss: 1.9953 1.0675 sec/batch\n",
      "Epoch 5/20  Iteration 856/3560 Training loss: 1.9951 1.0172 sec/batch\n",
      "Epoch 5/20  Iteration 857/3560 Training loss: 1.9947 1.0341 sec/batch\n",
      "Epoch 5/20  Iteration 858/3560 Training loss: 1.9947 0.9471 sec/batch\n",
      "Epoch 5/20  Iteration 859/3560 Training loss: 1.9946 0.9356 sec/batch\n",
      "Epoch 5/20  Iteration 860/3560 Training loss: 1.9945 0.9504 sec/batch\n",
      "Epoch 5/20  Iteration 861/3560 Training loss: 1.9944 0.9083 sec/batch\n",
      "Epoch 5/20  Iteration 862/3560 Training loss: 1.9941 0.9475 sec/batch\n",
      "Epoch 5/20  Iteration 863/3560 Training loss: 1.9938 1.0518 sec/batch\n",
      "Epoch 5/20  Iteration 864/3560 Training loss: 1.9938 0.9769 sec/batch\n",
      "Epoch 5/20  Iteration 865/3560 Training loss: 1.9937 1.0893 sec/batch\n",
      "Epoch 5/20  Iteration 866/3560 Training loss: 1.9936 0.9570 sec/batch\n",
      "Epoch 5/20  Iteration 867/3560 Training loss: 1.9934 0.9700 sec/batch\n",
      "Epoch 5/20  Iteration 868/3560 Training loss: 1.9932 0.8805 sec/batch\n",
      "Epoch 5/20  Iteration 869/3560 Training loss: 1.9931 0.8672 sec/batch\n",
      "Epoch 5/20  Iteration 870/3560 Training loss: 1.9929 1.0259 sec/batch\n",
      "Epoch 5/20  Iteration 871/3560 Training loss: 1.9926 0.9952 sec/batch\n",
      "Epoch 5/20  Iteration 872/3560 Training loss: 1.9926 0.9661 sec/batch\n",
      "Epoch 5/20  Iteration 873/3560 Training loss: 1.9925 1.0244 sec/batch\n",
      "Epoch 5/20  Iteration 874/3560 Training loss: 1.9923 0.9151 sec/batch\n",
      "Epoch 5/20  Iteration 875/3560 Training loss: 1.9922 0.9831 sec/batch\n",
      "Epoch 5/20  Iteration 876/3560 Training loss: 1.9920 0.9732 sec/batch\n",
      "Epoch 5/20  Iteration 877/3560 Training loss: 1.9919 0.9770 sec/batch\n",
      "Epoch 5/20  Iteration 878/3560 Training loss: 1.9916 1.0270 sec/batch\n",
      "Epoch 5/20  Iteration 879/3560 Training loss: 1.9915 0.9858 sec/batch\n",
      "Epoch 5/20  Iteration 880/3560 Training loss: 1.9916 1.0469 sec/batch\n",
      "Epoch 5/20  Iteration 881/3560 Training loss: 1.9915 0.9232 sec/batch\n",
      "Epoch 5/20  Iteration 882/3560 Training loss: 1.9913 0.8888 sec/batch\n",
      "Epoch 5/20  Iteration 883/3560 Training loss: 1.9910 1.0564 sec/batch\n",
      "Epoch 5/20  Iteration 884/3560 Training loss: 1.9908 0.9038 sec/batch\n",
      "Epoch 5/20  Iteration 885/3560 Training loss: 1.9907 1.0387 sec/batch\n",
      "Epoch 5/20  Iteration 886/3560 Training loss: 1.9906 1.0637 sec/batch\n",
      "Epoch 5/20  Iteration 887/3560 Training loss: 1.9905 0.9820 sec/batch\n",
      "Epoch 5/20  Iteration 888/3560 Training loss: 1.9903 0.9393 sec/batch\n",
      "Epoch 5/20  Iteration 889/3560 Training loss: 1.9900 1.0279 sec/batch\n",
      "Epoch 5/20  Iteration 890/3560 Training loss: 1.9899 0.9109 sec/batch\n",
      "Epoch 6/20  Iteration 891/3560 Training loss: 2.0339 0.9106 sec/batch\n",
      "Epoch 6/20  Iteration 892/3560 Training loss: 1.9834 0.9200 sec/batch\n",
      "Epoch 6/20  Iteration 893/3560 Training loss: 1.9727 1.0126 sec/batch\n",
      "Epoch 6/20  Iteration 894/3560 Training loss: 1.9635 1.0073 sec/batch\n",
      "Epoch 6/20  Iteration 895/3560 Training loss: 1.9594 0.9380 sec/batch\n",
      "Epoch 6/20  Iteration 896/3560 Training loss: 1.9509 1.0634 sec/batch\n",
      "Epoch 6/20  Iteration 897/3560 Training loss: 1.9514 0.9765 sec/batch\n",
      "Epoch 6/20  Iteration 898/3560 Training loss: 1.9511 1.0411 sec/batch\n",
      "Epoch 6/20  Iteration 899/3560 Training loss: 1.9537 0.9854 sec/batch\n",
      "Epoch 6/20  Iteration 900/3560 Training loss: 1.9520 0.9368 sec/batch\n",
      "Epoch 6/20  Iteration 901/3560 Training loss: 1.9490 1.0076 sec/batch\n",
      "Epoch 6/20  Iteration 902/3560 Training loss: 1.9472 0.9726 sec/batch\n",
      "Epoch 6/20  Iteration 903/3560 Training loss: 1.9476 0.9708 sec/batch\n",
      "Epoch 6/20  Iteration 904/3560 Training loss: 1.9497 0.9835 sec/batch\n",
      "Epoch 6/20  Iteration 905/3560 Training loss: 1.9493 1.0080 sec/batch\n",
      "Epoch 6/20  Iteration 906/3560 Training loss: 1.9475 0.9805 sec/batch\n",
      "Epoch 6/20  Iteration 907/3560 Training loss: 1.9470 0.9555 sec/batch\n",
      "Epoch 6/20  Iteration 908/3560 Training loss: 1.9489 0.9941 sec/batch\n",
      "Epoch 6/20  Iteration 909/3560 Training loss: 1.9493 1.0364 sec/batch\n",
      "Epoch 6/20  Iteration 910/3560 Training loss: 1.9494 0.9496 sec/batch\n",
      "Epoch 6/20  Iteration 911/3560 Training loss: 1.9487 0.9441 sec/batch\n",
      "Epoch 6/20  Iteration 912/3560 Training loss: 1.9494 0.9868 sec/batch\n",
      "Epoch 6/20  Iteration 913/3560 Training loss: 1.9492 0.9169 sec/batch\n",
      "Epoch 6/20  Iteration 914/3560 Training loss: 1.9483 0.9907 sec/batch\n",
      "Epoch 6/20  Iteration 915/3560 Training loss: 1.9477 0.8893 sec/batch\n",
      "Epoch 6/20  Iteration 916/3560 Training loss: 1.9465 1.0851 sec/batch\n",
      "Epoch 6/20  Iteration 917/3560 Training loss: 1.9452 1.0502 sec/batch\n",
      "Epoch 6/20  Iteration 918/3560 Training loss: 1.9455 0.9677 sec/batch\n",
      "Epoch 6/20  Iteration 919/3560 Training loss: 1.9459 0.9385 sec/batch\n",
      "Epoch 6/20  Iteration 920/3560 Training loss: 1.9462 0.9609 sec/batch\n",
      "Epoch 6/20  Iteration 921/3560 Training loss: 1.9459 0.9466 sec/batch\n",
      "Epoch 6/20  Iteration 922/3560 Training loss: 1.9449 0.9792 sec/batch\n",
      "Epoch 6/20  Iteration 923/3560 Training loss: 1.9451 0.9470 sec/batch\n",
      "Epoch 6/20  Iteration 924/3560 Training loss: 1.9455 1.0108 sec/batch\n",
      "Epoch 6/20  Iteration 925/3560 Training loss: 1.9452 1.0684 sec/batch\n",
      "Epoch 6/20  Iteration 926/3560 Training loss: 1.9449 1.0204 sec/batch\n",
      "Epoch 6/20  Iteration 927/3560 Training loss: 1.9443 1.0254 sec/batch\n",
      "Epoch 6/20  Iteration 928/3560 Training loss: 1.9428 0.9972 sec/batch\n",
      "Epoch 6/20  Iteration 929/3560 Training loss: 1.9416 0.9085 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 930/3560 Training loss: 1.9409 0.9825 sec/batch\n",
      "Epoch 6/20  Iteration 931/3560 Training loss: 1.9403 0.9535 sec/batch\n",
      "Epoch 6/20  Iteration 932/3560 Training loss: 1.9404 0.9735 sec/batch\n",
      "Epoch 6/20  Iteration 933/3560 Training loss: 1.9398 0.9785 sec/batch\n",
      "Epoch 6/20  Iteration 934/3560 Training loss: 1.9390 1.0033 sec/batch\n",
      "Epoch 6/20  Iteration 935/3560 Training loss: 1.9392 0.9621 sec/batch\n",
      "Epoch 6/20  Iteration 936/3560 Training loss: 1.9378 1.0017 sec/batch\n",
      "Epoch 6/20  Iteration 937/3560 Training loss: 1.9374 1.0058 sec/batch\n",
      "Epoch 6/20  Iteration 938/3560 Training loss: 1.9367 0.9801 sec/batch\n",
      "Epoch 6/20  Iteration 939/3560 Training loss: 1.9363 1.0102 sec/batch\n",
      "Epoch 6/20  Iteration 940/3560 Training loss: 1.9371 1.0599 sec/batch\n",
      "Epoch 6/20  Iteration 941/3560 Training loss: 1.9364 0.9712 sec/batch\n",
      "Epoch 6/20  Iteration 942/3560 Training loss: 1.9370 0.9818 sec/batch\n",
      "Epoch 6/20  Iteration 943/3560 Training loss: 1.9367 0.9769 sec/batch\n",
      "Epoch 6/20  Iteration 944/3560 Training loss: 1.9365 0.9773 sec/batch\n",
      "Epoch 6/20  Iteration 945/3560 Training loss: 1.9362 0.9797 sec/batch\n",
      "Epoch 6/20  Iteration 946/3560 Training loss: 1.9362 0.9459 sec/batch\n",
      "Epoch 6/20  Iteration 947/3560 Training loss: 1.9363 1.0162 sec/batch\n",
      "Epoch 6/20  Iteration 948/3560 Training loss: 1.9359 1.0661 sec/batch\n",
      "Epoch 6/20  Iteration 949/3560 Training loss: 1.9354 1.0026 sec/batch\n",
      "Epoch 6/20  Iteration 950/3560 Training loss: 1.9358 0.9970 sec/batch\n",
      "Epoch 6/20  Iteration 951/3560 Training loss: 1.9356 0.9595 sec/batch\n",
      "Epoch 6/20  Iteration 952/3560 Training loss: 1.9361 0.9825 sec/batch\n",
      "Epoch 6/20  Iteration 953/3560 Training loss: 1.9364 0.9001 sec/batch\n",
      "Epoch 6/20  Iteration 954/3560 Training loss: 1.9366 1.0223 sec/batch\n",
      "Epoch 6/20  Iteration 955/3560 Training loss: 1.9362 1.0403 sec/batch\n",
      "Epoch 6/20  Iteration 956/3560 Training loss: 1.9364 1.0132 sec/batch\n",
      "Epoch 6/20  Iteration 957/3560 Training loss: 1.9363 0.9266 sec/batch\n",
      "Epoch 6/20  Iteration 958/3560 Training loss: 1.9357 0.9226 sec/batch\n",
      "Epoch 6/20  Iteration 959/3560 Training loss: 1.9354 1.0618 sec/batch\n",
      "Epoch 6/20  Iteration 960/3560 Training loss: 1.9353 0.9936 sec/batch\n",
      "Epoch 6/20  Iteration 961/3560 Training loss: 1.9356 0.8785 sec/batch\n",
      "Epoch 6/20  Iteration 962/3560 Training loss: 1.9356 0.9786 sec/batch\n",
      "Epoch 6/20  Iteration 963/3560 Training loss: 1.9359 1.0346 sec/batch\n",
      "Epoch 6/20  Iteration 964/3560 Training loss: 1.9354 0.9502 sec/batch\n",
      "Epoch 6/20  Iteration 965/3560 Training loss: 1.9351 0.9550 sec/batch\n",
      "Epoch 6/20  Iteration 966/3560 Training loss: 1.9355 0.9628 sec/batch\n",
      "Epoch 6/20  Iteration 967/3560 Training loss: 1.9353 1.0213 sec/batch\n",
      "Epoch 6/20  Iteration 968/3560 Training loss: 1.9352 0.9762 sec/batch\n",
      "Epoch 6/20  Iteration 969/3560 Training loss: 1.9346 0.9272 sec/batch\n",
      "Epoch 6/20  Iteration 970/3560 Training loss: 1.9341 1.0477 sec/batch\n",
      "Epoch 6/20  Iteration 971/3560 Training loss: 1.9336 1.0379 sec/batch\n",
      "Epoch 6/20  Iteration 972/3560 Training loss: 1.9336 0.9951 sec/batch\n",
      "Epoch 6/20  Iteration 973/3560 Training loss: 1.9329 0.9124 sec/batch\n",
      "Epoch 6/20  Iteration 974/3560 Training loss: 1.9326 0.9915 sec/batch\n",
      "Epoch 6/20  Iteration 975/3560 Training loss: 1.9319 1.0889 sec/batch\n",
      "Epoch 6/20  Iteration 976/3560 Training loss: 1.9315 0.9066 sec/batch\n",
      "Epoch 6/20  Iteration 977/3560 Training loss: 1.9312 0.9361 sec/batch\n",
      "Epoch 6/20  Iteration 978/3560 Training loss: 1.9308 1.0806 sec/batch\n",
      "Epoch 6/20  Iteration 979/3560 Training loss: 1.9302 1.0786 sec/batch\n",
      "Epoch 6/20  Iteration 980/3560 Training loss: 1.9302 0.9420 sec/batch\n",
      "Epoch 6/20  Iteration 981/3560 Training loss: 1.9298 0.9245 sec/batch\n",
      "Epoch 6/20  Iteration 982/3560 Training loss: 1.9296 0.9813 sec/batch\n",
      "Epoch 6/20  Iteration 983/3560 Training loss: 1.9290 1.0204 sec/batch\n",
      "Epoch 6/20  Iteration 984/3560 Training loss: 1.9286 0.8607 sec/batch\n",
      "Epoch 6/20  Iteration 985/3560 Training loss: 1.9281 0.9816 sec/batch\n",
      "Epoch 6/20  Iteration 986/3560 Training loss: 1.9279 0.9547 sec/batch\n",
      "Epoch 6/20  Iteration 987/3560 Training loss: 1.9277 1.0156 sec/batch\n",
      "Epoch 6/20  Iteration 988/3560 Training loss: 1.9272 0.9494 sec/batch\n",
      "Epoch 6/20  Iteration 989/3560 Training loss: 1.9267 1.0360 sec/batch\n",
      "Epoch 6/20  Iteration 990/3560 Training loss: 1.9260 0.9435 sec/batch\n",
      "Epoch 6/20  Iteration 991/3560 Training loss: 1.9259 1.0215 sec/batch\n",
      "Epoch 6/20  Iteration 992/3560 Training loss: 1.9257 0.9800 sec/batch\n",
      "Epoch 6/20  Iteration 993/3560 Training loss: 1.9253 0.9545 sec/batch\n",
      "Epoch 6/20  Iteration 994/3560 Training loss: 1.9250 1.0420 sec/batch\n",
      "Epoch 6/20  Iteration 995/3560 Training loss: 1.9247 0.9659 sec/batch\n",
      "Epoch 6/20  Iteration 996/3560 Training loss: 1.9245 0.8930 sec/batch\n",
      "Epoch 6/20  Iteration 997/3560 Training loss: 1.9242 0.9546 sec/batch\n",
      "Epoch 6/20  Iteration 998/3560 Training loss: 1.9241 0.9696 sec/batch\n",
      "Epoch 6/20  Iteration 999/3560 Training loss: 1.9241 0.9845 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3560 Training loss: 1.9239 0.9794 sec/batch\n",
      "Epoch 6/20  Iteration 1001/3560 Training loss: 1.9238 0.8743 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3560 Training loss: 1.9235 1.0097 sec/batch\n",
      "Epoch 6/20  Iteration 1003/3560 Training loss: 1.9232 0.9322 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3560 Training loss: 1.9230 0.9644 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3560 Training loss: 1.9226 0.9672 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3560 Training loss: 1.9222 1.0017 sec/batch\n",
      "Epoch 6/20  Iteration 1007/3560 Training loss: 1.9220 0.9297 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3560 Training loss: 1.9218 0.9292 sec/batch\n",
      "Epoch 6/20  Iteration 1009/3560 Training loss: 1.9216 0.9361 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3560 Training loss: 1.9214 1.0765 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3560 Training loss: 1.9214 0.9922 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3560 Training loss: 1.9210 0.9511 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3560 Training loss: 1.9206 0.9647 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3560 Training loss: 1.9206 1.0367 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3560 Training loss: 1.9205 0.9326 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3560 Training loss: 1.9200 0.9995 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3560 Training loss: 1.9199 1.0299 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3560 Training loss: 1.9198 1.0240 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3560 Training loss: 1.9195 1.0158 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3560 Training loss: 1.9194 0.9078 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3560 Training loss: 1.9190 1.0914 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3560 Training loss: 1.9186 1.0213 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3560 Training loss: 1.9185 0.9279 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3560 Training loss: 1.9184 0.9161 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3560 Training loss: 1.9183 1.0305 sec/batch\n",
      "Epoch 6/20  Iteration 1026/3560 Training loss: 1.9182 0.9637 sec/batch\n",
      "Epoch 6/20  Iteration 1027/3560 Training loss: 1.9182 0.9603 sec/batch\n",
      "Epoch 6/20  Iteration 1028/3560 Training loss: 1.9181 0.9513 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3560 Training loss: 1.9182 1.0477 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3560 Training loss: 1.9178 1.0431 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3560 Training loss: 1.9179 0.9283 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3560 Training loss: 1.9177 0.9928 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3560 Training loss: 1.9176 0.9881 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3560 Training loss: 1.9174 0.9862 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3560 Training loss: 1.9171 0.9746 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3560 Training loss: 1.9171 0.9539 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3560 Training loss: 1.9170 1.0131 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3560 Training loss: 1.9171 0.9949 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3560 Training loss: 1.9170 0.9631 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3560 Training loss: 1.9167 1.0257 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3560 Training loss: 1.9164 0.9633 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3560 Training loss: 1.9164 0.9685 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3560 Training loss: 1.9164 0.9860 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3560 Training loss: 1.9163 0.8863 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3560 Training loss: 1.9162 1.0026 sec/batch\n",
      "Epoch 6/20  Iteration 1046/3560 Training loss: 1.9160 0.9709 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 1047/3560 Training loss: 1.9159 0.9814 sec/batch\n",
      "Epoch 6/20  Iteration 1048/3560 Training loss: 1.9158 0.9683 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3560 Training loss: 1.9154 0.9305 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3560 Training loss: 1.9155 1.1152 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3560 Training loss: 1.9156 0.8702 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3560 Training loss: 1.9154 0.9647 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3560 Training loss: 1.9153 1.0215 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3560 Training loss: 1.9152 0.9490 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3560 Training loss: 1.9150 0.9407 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3560 Training loss: 1.9148 0.9744 sec/batch\n",
      "Epoch 6/20  Iteration 1057/3560 Training loss: 1.9148 1.0000 sec/batch\n",
      "Epoch 6/20  Iteration 1058/3560 Training loss: 1.9149 0.9483 sec/batch\n",
      "Epoch 6/20  Iteration 1059/3560 Training loss: 1.9148 1.0232 sec/batch\n",
      "Epoch 6/20  Iteration 1060/3560 Training loss: 1.9146 1.0933 sec/batch\n",
      "Epoch 6/20  Iteration 1061/3560 Training loss: 1.9144 1.0504 sec/batch\n",
      "Epoch 6/20  Iteration 1062/3560 Training loss: 1.9142 1.1145 sec/batch\n",
      "Epoch 6/20  Iteration 1063/3560 Training loss: 1.9141 0.9362 sec/batch\n",
      "Epoch 6/20  Iteration 1064/3560 Training loss: 1.9140 1.1971 sec/batch\n",
      "Epoch 6/20  Iteration 1065/3560 Training loss: 1.9139 1.0200 sec/batch\n",
      "Epoch 6/20  Iteration 1066/3560 Training loss: 1.9138 1.2831 sec/batch\n",
      "Epoch 6/20  Iteration 1067/3560 Training loss: 1.9135 1.1191 sec/batch\n",
      "Epoch 6/20  Iteration 1068/3560 Training loss: 1.9135 1.0434 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3560 Training loss: 1.9663 1.0861 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3560 Training loss: 1.9187 1.0705 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3560 Training loss: 1.9065 1.0174 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3560 Training loss: 1.8964 1.5759 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3560 Training loss: 1.8933 1.6779 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3560 Training loss: 1.8842 1.3915 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3560 Training loss: 1.8843 1.2405 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3560 Training loss: 1.8837 1.0184 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3560 Training loss: 1.8861 1.0769 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3560 Training loss: 1.8843 0.9246 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3560 Training loss: 1.8811 1.1795 sec/batch\n",
      "Epoch 7/20  Iteration 1080/3560 Training loss: 1.8794 1.1047 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3560 Training loss: 1.8793 0.9596 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3560 Training loss: 1.8819 1.0045 sec/batch\n",
      "Epoch 7/20  Iteration 1083/3560 Training loss: 1.8814 1.1699 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3560 Training loss: 1.8797 1.0222 sec/batch\n",
      "Epoch 7/20  Iteration 1085/3560 Training loss: 1.8795 1.0087 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3560 Training loss: 1.8810 0.9787 sec/batch\n",
      "Epoch 7/20  Iteration 1087/3560 Training loss: 1.8812 1.0392 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3560 Training loss: 1.8811 1.3216 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3560 Training loss: 1.8803 1.0740 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3560 Training loss: 1.8811 1.1188 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3560 Training loss: 1.8808 1.2059 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3560 Training loss: 1.8800 1.0801 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3560 Training loss: 1.8797 1.0387 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3560 Training loss: 1.8785 0.9336 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3560 Training loss: 1.8771 0.9918 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3560 Training loss: 1.8778 0.9300 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3560 Training loss: 1.8788 0.9477 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3560 Training loss: 1.8792 1.0355 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3560 Training loss: 1.8790 0.9991 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3560 Training loss: 1.8779 0.9855 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3560 Training loss: 1.8780 0.9962 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3560 Training loss: 1.8790 1.0021 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3560 Training loss: 1.8786 1.0403 sec/batch\n",
      "Epoch 7/20  Iteration 1104/3560 Training loss: 1.8781 1.0694 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3560 Training loss: 1.8774 0.9270 sec/batch\n",
      "Epoch 7/20  Iteration 1106/3560 Training loss: 1.8763 1.0890 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3560 Training loss: 1.8748 1.2165 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3560 Training loss: 1.8742 1.0695 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3560 Training loss: 1.8737 0.9834 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3560 Training loss: 1.8740 1.3992 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3560 Training loss: 1.8733 1.3043 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3560 Training loss: 1.8723 0.9676 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3560 Training loss: 1.8724 1.4752 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3560 Training loss: 1.8710 1.3636 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3560 Training loss: 1.8708 1.0504 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3560 Training loss: 1.8702 0.9683 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3560 Training loss: 1.8699 1.0135 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3560 Training loss: 1.8704 0.9500 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3560 Training loss: 1.8698 1.0067 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3560 Training loss: 1.8704 1.1874 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3560 Training loss: 1.8701 1.1506 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3560 Training loss: 1.8700 1.2368 sec/batch\n",
      "Epoch 7/20  Iteration 1123/3560 Training loss: 1.8696 1.2527 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3560 Training loss: 1.8697 1.2723 sec/batch\n",
      "Epoch 7/20  Iteration 1125/3560 Training loss: 1.8698 1.2576 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3560 Training loss: 1.8694 1.3270 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3560 Training loss: 1.8688 1.3895 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3560 Training loss: 1.8691 1.0051 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3560 Training loss: 1.8689 1.2010 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3560 Training loss: 1.8694 1.2084 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3560 Training loss: 1.8696 1.2302 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3560 Training loss: 1.8699 1.2627 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3560 Training loss: 1.8696 1.0021 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3560 Training loss: 1.8698 1.2149 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3560 Training loss: 1.8698 1.1047 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3560 Training loss: 1.8694 1.0504 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3560 Training loss: 1.8693 1.0759 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3560 Training loss: 1.8690 1.1628 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3560 Training loss: 1.8694 1.0418 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3560 Training loss: 1.8695 0.9954 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3560 Training loss: 1.8700 1.1726 sec/batch\n",
      "Epoch 7/20  Iteration 1142/3560 Training loss: 1.8695 1.2755 sec/batch\n",
      "Epoch 7/20  Iteration 1143/3560 Training loss: 1.8693 1.2037 sec/batch\n",
      "Epoch 7/20  Iteration 1144/3560 Training loss: 1.8696 1.0961 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3560 Training loss: 1.8695 1.3070 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3560 Training loss: 1.8696 1.4244 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3560 Training loss: 1.8690 1.3623 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3560 Training loss: 1.8687 1.3238 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3560 Training loss: 1.8682 1.2550 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3560 Training loss: 1.8684 1.1930 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3560 Training loss: 1.8678 1.1194 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3560 Training loss: 1.8675 0.9963 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3560 Training loss: 1.8669 1.0295 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3560 Training loss: 1.8666 1.0950 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3560 Training loss: 1.8662 1.0117 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3560 Training loss: 1.8658 0.9115 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3560 Training loss: 1.8652 1.2754 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3560 Training loss: 1.8652 1.3538 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3560 Training loss: 1.8648 1.2820 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3560 Training loss: 1.8645 1.1943 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3560 Training loss: 1.8640 1.0730 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3560 Training loss: 1.8637 1.0532 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1163/3560 Training loss: 1.8632 1.0338 sec/batch\n",
      "Epoch 7/20  Iteration 1164/3560 Training loss: 1.8632 1.1007 sec/batch\n",
      "Epoch 7/20  Iteration 1165/3560 Training loss: 1.8629 1.0439 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3560 Training loss: 1.8624 0.9933 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3560 Training loss: 1.8619 1.0580 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3560 Training loss: 1.8613 1.1166 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3560 Training loss: 1.8611 1.0589 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3560 Training loss: 1.8609 0.9546 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3560 Training loss: 1.8605 1.1170 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3560 Training loss: 1.8603 0.9680 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3560 Training loss: 1.8599 0.9818 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3560 Training loss: 1.8597 0.9886 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3560 Training loss: 1.8596 1.1439 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3560 Training loss: 1.8595 0.9881 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3560 Training loss: 1.8594 1.1686 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3560 Training loss: 1.8593 1.0333 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3560 Training loss: 1.8592 1.1444 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3560 Training loss: 1.8590 1.0500 sec/batch\n",
      "Epoch 7/20  Iteration 1181/3560 Training loss: 1.8587 1.2291 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3560 Training loss: 1.8585 1.2673 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3560 Training loss: 1.8581 1.3208 sec/batch\n",
      "Epoch 7/20  Iteration 1184/3560 Training loss: 1.8577 1.1595 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3560 Training loss: 1.8576 1.1168 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3560 Training loss: 1.8574 1.1745 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3560 Training loss: 1.8572 1.1461 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3560 Training loss: 1.8570 1.0749 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3560 Training loss: 1.8569 1.0219 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3560 Training loss: 1.8564 1.0552 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3560 Training loss: 1.8560 1.0984 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3560 Training loss: 1.8560 1.0509 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3560 Training loss: 1.8558 1.1599 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3560 Training loss: 1.8553 1.1007 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3560 Training loss: 1.8554 1.0913 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3560 Training loss: 1.8553 0.9711 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3560 Training loss: 1.8551 1.2200 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3560 Training loss: 1.8549 1.3017 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3560 Training loss: 1.8546 1.3064 sec/batch\n",
      "Epoch 7/20  Iteration 1200/3560 Training loss: 1.8542 1.3015 sec/batch\n",
      "Epoch 7/20  Iteration 1201/3560 Training loss: 1.8541 1.3258 sec/batch\n",
      "Epoch 7/20  Iteration 1202/3560 Training loss: 1.8541 1.3217 sec/batch\n",
      "Epoch 7/20  Iteration 1203/3560 Training loss: 1.8540 1.0627 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3560 Training loss: 1.8539 1.2336 sec/batch\n",
      "Epoch 7/20  Iteration 1205/3560 Training loss: 1.8538 1.3283 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3560 Training loss: 1.8538 1.2931 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3560 Training loss: 1.8539 1.2992 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3560 Training loss: 1.8537 1.3696 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3560 Training loss: 1.8538 1.3506 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3560 Training loss: 1.8536 1.2463 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3560 Training loss: 1.8535 1.3235 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3560 Training loss: 1.8534 1.1096 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3560 Training loss: 1.8531 1.3823 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3560 Training loss: 1.8531 1.1697 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3560 Training loss: 1.8531 1.2508 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3560 Training loss: 1.8532 1.1282 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3560 Training loss: 1.8531 1.2450 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3560 Training loss: 1.8528 1.2158 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3560 Training loss: 1.8525 1.2667 sec/batch\n",
      "Epoch 7/20  Iteration 1220/3560 Training loss: 1.8525 0.9670 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3560 Training loss: 1.8524 1.1738 sec/batch\n",
      "Epoch 7/20  Iteration 1222/3560 Training loss: 1.8524 1.3050 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3560 Training loss: 1.8523 1.1573 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3560 Training loss: 1.8522 1.1987 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3560 Training loss: 1.8521 1.1866 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3560 Training loss: 1.8520 1.1032 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3560 Training loss: 1.8517 1.2115 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3560 Training loss: 1.8517 1.0374 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3560 Training loss: 1.8519 1.2993 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3560 Training loss: 1.8517 1.1676 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3560 Training loss: 1.8517 1.1604 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3560 Training loss: 1.8517 1.1419 sec/batch\n",
      "Epoch 7/20  Iteration 1233/3560 Training loss: 1.8515 1.2249 sec/batch\n",
      "Epoch 7/20  Iteration 1234/3560 Training loss: 1.8514 1.0778 sec/batch\n",
      "Epoch 7/20  Iteration 1235/3560 Training loss: 1.8514 1.0969 sec/batch\n",
      "Epoch 7/20  Iteration 1236/3560 Training loss: 1.8516 1.0961 sec/batch\n",
      "Epoch 7/20  Iteration 1237/3560 Training loss: 1.8514 1.2880 sec/batch\n",
      "Epoch 7/20  Iteration 1238/3560 Training loss: 1.8513 1.1437 sec/batch\n",
      "Epoch 7/20  Iteration 1239/3560 Training loss: 1.8511 1.2077 sec/batch\n",
      "Epoch 7/20  Iteration 1240/3560 Training loss: 1.8509 1.3294 sec/batch\n",
      "Epoch 7/20  Iteration 1241/3560 Training loss: 1.8510 1.4746 sec/batch\n",
      "Epoch 7/20  Iteration 1242/3560 Training loss: 1.8509 1.1033 sec/batch\n",
      "Epoch 7/20  Iteration 1243/3560 Training loss: 1.8509 1.0977 sec/batch\n",
      "Epoch 7/20  Iteration 1244/3560 Training loss: 1.8508 1.2347 sec/batch\n",
      "Epoch 7/20  Iteration 1245/3560 Training loss: 1.8506 1.0356 sec/batch\n",
      "Epoch 7/20  Iteration 1246/3560 Training loss: 1.8505 1.3443 sec/batch\n",
      "Epoch 8/20  Iteration 1247/3560 Training loss: 1.9165 1.1604 sec/batch\n",
      "Epoch 8/20  Iteration 1248/3560 Training loss: 1.8702 1.5351 sec/batch\n",
      "Epoch 8/20  Iteration 1249/3560 Training loss: 1.8531 1.2026 sec/batch\n",
      "Epoch 8/20  Iteration 1250/3560 Training loss: 1.8446 1.2224 sec/batch\n",
      "Epoch 8/20  Iteration 1251/3560 Training loss: 1.8389 1.1132 sec/batch\n",
      "Epoch 8/20  Iteration 1252/3560 Training loss: 1.8290 1.1812 sec/batch\n",
      "Epoch 8/20  Iteration 1253/3560 Training loss: 1.8284 1.1895 sec/batch\n",
      "Epoch 8/20  Iteration 1254/3560 Training loss: 1.8274 1.2064 sec/batch\n",
      "Epoch 8/20  Iteration 1255/3560 Training loss: 1.8291 1.2435 sec/batch\n",
      "Epoch 8/20  Iteration 1256/3560 Training loss: 1.8279 1.2744 sec/batch\n",
      "Epoch 8/20  Iteration 1257/3560 Training loss: 1.8243 1.1840 sec/batch\n",
      "Epoch 8/20  Iteration 1258/3560 Training loss: 1.8226 1.1520 sec/batch\n",
      "Epoch 8/20  Iteration 1259/3560 Training loss: 1.8219 1.1443 sec/batch\n",
      "Epoch 8/20  Iteration 1260/3560 Training loss: 1.8244 1.1041 sec/batch\n",
      "Epoch 8/20  Iteration 1261/3560 Training loss: 1.8241 1.1462 sec/batch\n",
      "Epoch 8/20  Iteration 1262/3560 Training loss: 1.8217 1.2718 sec/batch\n",
      "Epoch 8/20  Iteration 1263/3560 Training loss: 1.8217 1.1676 sec/batch\n",
      "Epoch 8/20  Iteration 1264/3560 Training loss: 1.8233 1.3509 sec/batch\n",
      "Epoch 8/20  Iteration 1265/3560 Training loss: 1.8241 1.3146 sec/batch\n",
      "Epoch 8/20  Iteration 1266/3560 Training loss: 1.8246 1.1353 sec/batch\n",
      "Epoch 8/20  Iteration 1267/3560 Training loss: 1.8236 1.2085 sec/batch\n",
      "Epoch 8/20  Iteration 1268/3560 Training loss: 1.8243 1.2214 sec/batch\n",
      "Epoch 8/20  Iteration 1269/3560 Training loss: 1.8238 1.1497 sec/batch\n",
      "Epoch 8/20  Iteration 1270/3560 Training loss: 1.8235 1.2880 sec/batch\n",
      "Epoch 8/20  Iteration 1271/3560 Training loss: 1.8231 1.3286 sec/batch\n",
      "Epoch 8/20  Iteration 1272/3560 Training loss: 1.8219 1.3346 sec/batch\n",
      "Epoch 8/20  Iteration 1273/3560 Training loss: 1.8209 1.2881 sec/batch\n",
      "Epoch 8/20  Iteration 1274/3560 Training loss: 1.8217 1.4877 sec/batch\n",
      "Epoch 8/20  Iteration 1275/3560 Training loss: 1.8224 1.4894 sec/batch\n",
      "Epoch 8/20  Iteration 1276/3560 Training loss: 1.8226 1.2649 sec/batch\n",
      "Epoch 8/20  Iteration 1277/3560 Training loss: 1.8223 1.1170 sec/batch\n",
      "Epoch 8/20  Iteration 1278/3560 Training loss: 1.8215 1.3698 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1279/3560 Training loss: 1.8215 1.0983 sec/batch\n",
      "Epoch 8/20  Iteration 1280/3560 Training loss: 1.8220 1.3767 sec/batch\n",
      "Epoch 8/20  Iteration 1281/3560 Training loss: 1.8217 1.2370 sec/batch\n",
      "Epoch 8/20  Iteration 1282/3560 Training loss: 1.8213 1.0893 sec/batch\n",
      "Epoch 8/20  Iteration 1283/3560 Training loss: 1.8208 1.0962 sec/batch\n",
      "Epoch 8/20  Iteration 1284/3560 Training loss: 1.8194 1.1200 sec/batch\n",
      "Epoch 8/20  Iteration 1285/3560 Training loss: 1.8182 1.1323 sec/batch\n",
      "Epoch 8/20  Iteration 1286/3560 Training loss: 1.8173 1.2498 sec/batch\n",
      "Epoch 8/20  Iteration 1287/3560 Training loss: 1.8168 1.2491 sec/batch\n",
      "Epoch 8/20  Iteration 1288/3560 Training loss: 1.8170 1.4041 sec/batch\n",
      "Epoch 8/20  Iteration 1289/3560 Training loss: 1.8165 1.2750 sec/batch\n",
      "Epoch 8/20  Iteration 1290/3560 Training loss: 1.8156 1.0910 sec/batch\n",
      "Epoch 8/20  Iteration 1291/3560 Training loss: 1.8158 1.1513 sec/batch\n",
      "Epoch 8/20  Iteration 1292/3560 Training loss: 1.8146 1.2488 sec/batch\n",
      "Epoch 8/20  Iteration 1293/3560 Training loss: 1.8143 1.1473 sec/batch\n",
      "Epoch 8/20  Iteration 1294/3560 Training loss: 1.8137 1.2899 sec/batch\n",
      "Epoch 8/20  Iteration 1295/3560 Training loss: 1.8135 1.1266 sec/batch\n",
      "Epoch 8/20  Iteration 1296/3560 Training loss: 1.8143 1.2761 sec/batch\n",
      "Epoch 8/20  Iteration 1297/3560 Training loss: 1.8138 1.1152 sec/batch\n",
      "Epoch 8/20  Iteration 1298/3560 Training loss: 1.8146 1.1721 sec/batch\n",
      "Epoch 8/20  Iteration 1299/3560 Training loss: 1.8144 1.2162 sec/batch\n",
      "Epoch 8/20  Iteration 1300/3560 Training loss: 1.8142 1.1477 sec/batch\n",
      "Epoch 8/20  Iteration 1301/3560 Training loss: 1.8139 1.1344 sec/batch\n",
      "Epoch 8/20  Iteration 1302/3560 Training loss: 1.8140 1.2367 sec/batch\n",
      "Epoch 8/20  Iteration 1303/3560 Training loss: 1.8142 1.2627 sec/batch\n",
      "Epoch 8/20  Iteration 1304/3560 Training loss: 1.8138 1.1686 sec/batch\n",
      "Epoch 8/20  Iteration 1305/3560 Training loss: 1.8133 1.2921 sec/batch\n",
      "Epoch 8/20  Iteration 1306/3560 Training loss: 1.8138 1.2050 sec/batch\n",
      "Epoch 8/20  Iteration 1307/3560 Training loss: 1.8137 1.0960 sec/batch\n",
      "Epoch 8/20  Iteration 1308/3560 Training loss: 1.8143 1.2706 sec/batch\n",
      "Epoch 8/20  Iteration 1309/3560 Training loss: 1.8146 1.2227 sec/batch\n",
      "Epoch 8/20  Iteration 1310/3560 Training loss: 1.8149 1.0478 sec/batch\n",
      "Epoch 8/20  Iteration 1311/3560 Training loss: 1.8147 1.3228 sec/batch\n",
      "Epoch 8/20  Iteration 1312/3560 Training loss: 1.8149 1.3090 sec/batch\n",
      "Epoch 8/20  Iteration 1313/3560 Training loss: 1.8152 1.4540 sec/batch\n",
      "Epoch 8/20  Iteration 1314/3560 Training loss: 1.8148 1.1043 sec/batch\n",
      "Epoch 8/20  Iteration 1315/3560 Training loss: 1.8146 1.0815 sec/batch\n",
      "Epoch 8/20  Iteration 1316/3560 Training loss: 1.8145 1.1291 sec/batch\n",
      "Epoch 8/20  Iteration 1317/3560 Training loss: 1.8150 1.1021 sec/batch\n",
      "Epoch 8/20  Iteration 1318/3560 Training loss: 1.8152 1.2183 sec/batch\n",
      "Epoch 8/20  Iteration 1319/3560 Training loss: 1.8156 1.1420 sec/batch\n",
      "Epoch 8/20  Iteration 1320/3560 Training loss: 1.8153 1.2983 sec/batch\n",
      "Epoch 8/20  Iteration 1321/3560 Training loss: 1.8151 1.3309 sec/batch\n",
      "Epoch 8/20  Iteration 1322/3560 Training loss: 1.8154 1.3334 sec/batch\n",
      "Epoch 8/20  Iteration 1323/3560 Training loss: 1.8152 1.1329 sec/batch\n",
      "Epoch 8/20  Iteration 1324/3560 Training loss: 1.8153 1.1401 sec/batch\n",
      "Epoch 8/20  Iteration 1325/3560 Training loss: 1.8146 1.1397 sec/batch\n",
      "Epoch 8/20  Iteration 1326/3560 Training loss: 1.8144 1.2442 sec/batch\n",
      "Epoch 8/20  Iteration 1327/3560 Training loss: 1.8139 1.3372 sec/batch\n",
      "Epoch 8/20  Iteration 1328/3560 Training loss: 1.8140 1.2509 sec/batch\n",
      "Epoch 8/20  Iteration 1329/3560 Training loss: 1.8134 1.3092 sec/batch\n",
      "Epoch 8/20  Iteration 1330/3560 Training loss: 1.8131 1.0806 sec/batch\n",
      "Epoch 8/20  Iteration 1331/3560 Training loss: 1.8126 1.2159 sec/batch\n",
      "Epoch 8/20  Iteration 1332/3560 Training loss: 1.8122 1.1016 sec/batch\n",
      "Epoch 8/20  Iteration 1333/3560 Training loss: 1.8119 1.0478 sec/batch\n",
      "Epoch 8/20  Iteration 1334/3560 Training loss: 1.8116 1.2436 sec/batch\n",
      "Epoch 8/20  Iteration 1335/3560 Training loss: 1.8110 1.1404 sec/batch\n",
      "Epoch 8/20  Iteration 1336/3560 Training loss: 1.8110 1.3678 sec/batch\n",
      "Epoch 8/20  Iteration 1337/3560 Training loss: 1.8106 1.2644 sec/batch\n",
      "Epoch 8/20  Iteration 1338/3560 Training loss: 1.8104 1.2422 sec/batch\n",
      "Epoch 8/20  Iteration 1339/3560 Training loss: 1.8098 1.2112 sec/batch\n",
      "Epoch 8/20  Iteration 1340/3560 Training loss: 1.8094 1.1889 sec/batch\n",
      "Epoch 8/20  Iteration 1341/3560 Training loss: 1.8091 1.1827 sec/batch\n",
      "Epoch 8/20  Iteration 1342/3560 Training loss: 1.8090 1.1632 sec/batch\n",
      "Epoch 8/20  Iteration 1343/3560 Training loss: 1.8087 1.2522 sec/batch\n",
      "Epoch 8/20  Iteration 1344/3560 Training loss: 1.8084 1.1583 sec/batch\n",
      "Epoch 8/20  Iteration 1345/3560 Training loss: 1.8079 1.1824 sec/batch\n",
      "Epoch 8/20  Iteration 1346/3560 Training loss: 1.8073 1.1679 sec/batch\n",
      "Epoch 8/20  Iteration 1347/3560 Training loss: 1.8072 1.1385 sec/batch\n",
      "Epoch 8/20  Iteration 1348/3560 Training loss: 1.8070 1.0389 sec/batch\n",
      "Epoch 8/20  Iteration 1349/3560 Training loss: 1.8067 1.1909 sec/batch\n",
      "Epoch 8/20  Iteration 1350/3560 Training loss: 1.8065 1.1712 sec/batch\n",
      "Epoch 8/20  Iteration 1351/3560 Training loss: 1.8061 1.2765 sec/batch\n",
      "Epoch 8/20  Iteration 1352/3560 Training loss: 1.8060 1.2341 sec/batch\n",
      "Epoch 8/20  Iteration 1353/3560 Training loss: 1.8058 1.2379 sec/batch\n",
      "Epoch 8/20  Iteration 1354/3560 Training loss: 1.8056 1.1431 sec/batch\n",
      "Epoch 8/20  Iteration 1355/3560 Training loss: 1.8055 1.2476 sec/batch\n",
      "Epoch 8/20  Iteration 1356/3560 Training loss: 1.8055 1.2559 sec/batch\n",
      "Epoch 8/20  Iteration 1357/3560 Training loss: 1.8053 1.1571 sec/batch\n",
      "Epoch 8/20  Iteration 1358/3560 Training loss: 1.8052 1.2227 sec/batch\n",
      "Epoch 8/20  Iteration 1359/3560 Training loss: 1.8049 1.3019 sec/batch\n",
      "Epoch 8/20  Iteration 1360/3560 Training loss: 1.8048 1.2883 sec/batch\n",
      "Epoch 8/20  Iteration 1361/3560 Training loss: 1.8045 1.3060 sec/batch\n",
      "Epoch 8/20  Iteration 1362/3560 Training loss: 1.8040 1.0845 sec/batch\n",
      "Epoch 8/20  Iteration 1363/3560 Training loss: 1.8039 1.1609 sec/batch\n",
      "Epoch 8/20  Iteration 1364/3560 Training loss: 1.8037 1.1373 sec/batch\n",
      "Epoch 8/20  Iteration 1365/3560 Training loss: 1.8035 1.1933 sec/batch\n",
      "Epoch 8/20  Iteration 1366/3560 Training loss: 1.8034 1.3146 sec/batch\n",
      "Epoch 8/20  Iteration 1367/3560 Training loss: 1.8033 1.1885 sec/batch\n",
      "Epoch 8/20  Iteration 1368/3560 Training loss: 1.8029 1.1058 sec/batch\n",
      "Epoch 8/20  Iteration 1369/3560 Training loss: 1.8025 1.1560 sec/batch\n",
      "Epoch 8/20  Iteration 1370/3560 Training loss: 1.8024 1.0244 sec/batch\n",
      "Epoch 8/20  Iteration 1371/3560 Training loss: 1.8024 1.1625 sec/batch\n",
      "Epoch 8/20  Iteration 1372/3560 Training loss: 1.8018 1.0941 sec/batch\n",
      "Epoch 8/20  Iteration 1373/3560 Training loss: 1.8020 1.2769 sec/batch\n",
      "Epoch 8/20  Iteration 1374/3560 Training loss: 1.8020 1.2656 sec/batch\n",
      "Epoch 8/20  Iteration 1375/3560 Training loss: 1.8018 1.0848 sec/batch\n",
      "Epoch 8/20  Iteration 1376/3560 Training loss: 1.8016 1.1329 sec/batch\n",
      "Epoch 8/20  Iteration 1377/3560 Training loss: 1.8012 1.1755 sec/batch\n",
      "Epoch 8/20  Iteration 1378/3560 Training loss: 1.8008 1.1753 sec/batch\n",
      "Epoch 8/20  Iteration 1379/3560 Training loss: 1.8008 1.0001 sec/batch\n",
      "Epoch 8/20  Iteration 1380/3560 Training loss: 1.8007 1.1751 sec/batch\n",
      "Epoch 8/20  Iteration 1381/3560 Training loss: 1.8006 1.1723 sec/batch\n",
      "Epoch 8/20  Iteration 1382/3560 Training loss: 1.8005 1.2310 sec/batch\n",
      "Epoch 8/20  Iteration 1383/3560 Training loss: 1.8006 1.1272 sec/batch\n",
      "Epoch 8/20  Iteration 1384/3560 Training loss: 1.8005 1.2196 sec/batch\n",
      "Epoch 8/20  Iteration 1385/3560 Training loss: 1.8006 1.1394 sec/batch\n",
      "Epoch 8/20  Iteration 1386/3560 Training loss: 1.8004 1.1204 sec/batch\n",
      "Epoch 8/20  Iteration 1387/3560 Training loss: 1.8006 1.0857 sec/batch\n",
      "Epoch 8/20  Iteration 1388/3560 Training loss: 1.8004 1.1926 sec/batch\n",
      "Epoch 8/20  Iteration 1389/3560 Training loss: 1.8004 1.2581 sec/batch\n",
      "Epoch 8/20  Iteration 1390/3560 Training loss: 1.8004 1.1422 sec/batch\n",
      "Epoch 8/20  Iteration 1391/3560 Training loss: 1.8001 1.1047 sec/batch\n",
      "Epoch 8/20  Iteration 1392/3560 Training loss: 1.8002 1.1663 sec/batch\n",
      "Epoch 8/20  Iteration 1393/3560 Training loss: 1.8002 1.1530 sec/batch\n",
      "Epoch 8/20  Iteration 1394/3560 Training loss: 1.8003 1.1076 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1395/3560 Training loss: 1.8001 1.1847 sec/batch\n",
      "Epoch 8/20  Iteration 1396/3560 Training loss: 1.8000 1.0142 sec/batch\n",
      "Epoch 8/20  Iteration 1397/3560 Training loss: 1.7997 1.2101 sec/batch\n",
      "Epoch 8/20  Iteration 1398/3560 Training loss: 1.7997 1.2915 sec/batch\n",
      "Epoch 8/20  Iteration 1399/3560 Training loss: 1.7997 1.2063 sec/batch\n",
      "Epoch 8/20  Iteration 1400/3560 Training loss: 1.7997 1.2724 sec/batch\n",
      "Epoch 8/20  Iteration 1401/3560 Training loss: 1.7995 1.1086 sec/batch\n",
      "Epoch 8/20  Iteration 1402/3560 Training loss: 1.7994 1.2567 sec/batch\n",
      "Epoch 8/20  Iteration 1403/3560 Training loss: 1.7994 1.2756 sec/batch\n",
      "Epoch 8/20  Iteration 1404/3560 Training loss: 1.7994 1.0971 sec/batch\n",
      "Epoch 8/20  Iteration 1405/3560 Training loss: 1.7990 1.2719 sec/batch\n",
      "Epoch 8/20  Iteration 1406/3560 Training loss: 1.7991 1.2809 sec/batch\n",
      "Epoch 8/20  Iteration 1407/3560 Training loss: 1.7991 1.4009 sec/batch\n",
      "Epoch 8/20  Iteration 1408/3560 Training loss: 1.7990 1.9932 sec/batch\n",
      "Epoch 8/20  Iteration 1409/3560 Training loss: 1.7990 1.9231 sec/batch\n",
      "Epoch 8/20  Iteration 1410/3560 Training loss: 1.7990 1.3966 sec/batch\n",
      "Epoch 8/20  Iteration 1411/3560 Training loss: 1.7990 1.3134 sec/batch\n",
      "Epoch 8/20  Iteration 1412/3560 Training loss: 1.7989 1.3060 sec/batch\n",
      "Epoch 8/20  Iteration 1413/3560 Training loss: 1.7989 1.4138 sec/batch\n",
      "Epoch 8/20  Iteration 1414/3560 Training loss: 1.7992 1.2854 sec/batch\n",
      "Epoch 8/20  Iteration 1415/3560 Training loss: 1.7991 1.3795 sec/batch\n",
      "Epoch 8/20  Iteration 1416/3560 Training loss: 1.7990 1.3157 sec/batch\n",
      "Epoch 8/20  Iteration 1417/3560 Training loss: 1.7988 1.0854 sec/batch\n",
      "Epoch 8/20  Iteration 1418/3560 Training loss: 1.7987 1.1556 sec/batch\n",
      "Epoch 8/20  Iteration 1419/3560 Training loss: 1.7987 1.0446 sec/batch\n",
      "Epoch 8/20  Iteration 1420/3560 Training loss: 1.7986 1.0600 sec/batch\n",
      "Epoch 8/20  Iteration 1421/3560 Training loss: 1.7986 1.3257 sec/batch\n",
      "Epoch 8/20  Iteration 1422/3560 Training loss: 1.7985 1.2579 sec/batch\n",
      "Epoch 8/20  Iteration 1423/3560 Training loss: 1.7984 1.1690 sec/batch\n",
      "Epoch 8/20  Iteration 1424/3560 Training loss: 1.7983 1.1318 sec/batch\n",
      "Epoch 9/20  Iteration 1425/3560 Training loss: 1.8630 1.1363 sec/batch\n",
      "Epoch 9/20  Iteration 1426/3560 Training loss: 1.8177 1.0643 sec/batch\n",
      "Epoch 9/20  Iteration 1427/3560 Training loss: 1.8038 1.1170 sec/batch\n",
      "Epoch 9/20  Iteration 1428/3560 Training loss: 1.7961 1.2230 sec/batch\n",
      "Epoch 9/20  Iteration 1429/3560 Training loss: 1.7925 1.0321 sec/batch\n",
      "Epoch 9/20  Iteration 1430/3560 Training loss: 1.7815 1.2112 sec/batch\n",
      "Epoch 9/20  Iteration 1431/3560 Training loss: 1.7820 1.1100 sec/batch\n",
      "Epoch 9/20  Iteration 1432/3560 Training loss: 1.7802 1.2720 sec/batch\n",
      "Epoch 9/20  Iteration 1433/3560 Training loss: 1.7815 1.0518 sec/batch\n",
      "Epoch 9/20  Iteration 1434/3560 Training loss: 1.7808 1.0940 sec/batch\n",
      "Epoch 9/20  Iteration 1435/3560 Training loss: 1.7774 1.0812 sec/batch\n",
      "Epoch 9/20  Iteration 1436/3560 Training loss: 1.7756 1.1308 sec/batch\n",
      "Epoch 9/20  Iteration 1437/3560 Training loss: 1.7758 1.1656 sec/batch\n",
      "Epoch 9/20  Iteration 1438/3560 Training loss: 1.7784 1.2596 sec/batch\n",
      "Epoch 9/20  Iteration 1439/3560 Training loss: 1.7780 1.1786 sec/batch\n",
      "Epoch 9/20  Iteration 1440/3560 Training loss: 1.7762 1.1300 sec/batch\n",
      "Epoch 9/20  Iteration 1441/3560 Training loss: 1.7761 1.1033 sec/batch\n",
      "Epoch 9/20  Iteration 1442/3560 Training loss: 1.7777 1.1560 sec/batch\n",
      "Epoch 9/20  Iteration 1443/3560 Training loss: 1.7780 1.1615 sec/batch\n",
      "Epoch 9/20  Iteration 1444/3560 Training loss: 1.7788 1.1366 sec/batch\n",
      "Epoch 9/20  Iteration 1445/3560 Training loss: 1.7779 1.0852 sec/batch\n",
      "Epoch 9/20  Iteration 1446/3560 Training loss: 1.7782 1.3314 sec/batch\n",
      "Epoch 9/20  Iteration 1447/3560 Training loss: 1.7774 1.1657 sec/batch\n",
      "Epoch 9/20  Iteration 1448/3560 Training loss: 1.7771 1.1184 sec/batch\n",
      "Epoch 9/20  Iteration 1449/3560 Training loss: 1.7768 1.1698 sec/batch\n",
      "Epoch 9/20  Iteration 1450/3560 Training loss: 1.7757 1.1026 sec/batch\n",
      "Epoch 9/20  Iteration 1451/3560 Training loss: 1.7746 1.1029 sec/batch\n",
      "Epoch 9/20  Iteration 1452/3560 Training loss: 1.7749 1.0204 sec/batch\n",
      "Epoch 9/20  Iteration 1453/3560 Training loss: 1.7757 1.1855 sec/batch\n",
      "Epoch 9/20  Iteration 1454/3560 Training loss: 1.7759 1.1286 sec/batch\n",
      "Epoch 9/20  Iteration 1455/3560 Training loss: 1.7759 1.2628 sec/batch\n",
      "Epoch 9/20  Iteration 1456/3560 Training loss: 1.7747 1.0641 sec/batch\n",
      "Epoch 9/20  Iteration 1457/3560 Training loss: 1.7748 1.2068 sec/batch\n",
      "Epoch 9/20  Iteration 1458/3560 Training loss: 1.7754 1.2308 sec/batch\n",
      "Epoch 9/20  Iteration 1459/3560 Training loss: 1.7751 1.0691 sec/batch\n",
      "Epoch 9/20  Iteration 1460/3560 Training loss: 1.7750 1.1287 sec/batch\n",
      "Epoch 9/20  Iteration 1461/3560 Training loss: 1.7742 1.1998 sec/batch\n",
      "Epoch 9/20  Iteration 1462/3560 Training loss: 1.7730 1.1813 sec/batch\n",
      "Epoch 9/20  Iteration 1463/3560 Training loss: 1.7718 1.2583 sec/batch\n",
      "Epoch 9/20  Iteration 1464/3560 Training loss: 1.7712 1.2047 sec/batch\n",
      "Epoch 9/20  Iteration 1465/3560 Training loss: 1.7707 1.0744 sec/batch\n",
      "Epoch 9/20  Iteration 1466/3560 Training loss: 1.7710 1.1917 sec/batch\n",
      "Epoch 9/20  Iteration 1467/3560 Training loss: 1.7705 1.0255 sec/batch\n",
      "Epoch 9/20  Iteration 1468/3560 Training loss: 1.7694 1.0908 sec/batch\n",
      "Epoch 9/20  Iteration 1469/3560 Training loss: 1.7695 1.1233 sec/batch\n",
      "Epoch 9/20  Iteration 1470/3560 Training loss: 1.7680 1.0414 sec/batch\n",
      "Epoch 9/20  Iteration 1471/3560 Training loss: 1.7679 1.0798 sec/batch\n",
      "Epoch 9/20  Iteration 1472/3560 Training loss: 1.7673 1.0112 sec/batch\n",
      "Epoch 9/20  Iteration 1473/3560 Training loss: 1.7671 0.9314 sec/batch\n",
      "Epoch 9/20  Iteration 1474/3560 Training loss: 1.7678 1.0282 sec/batch\n",
      "Epoch 9/20  Iteration 1475/3560 Training loss: 1.7673 1.0713 sec/batch\n",
      "Epoch 9/20  Iteration 1476/3560 Training loss: 1.7679 1.0067 sec/batch\n",
      "Epoch 9/20  Iteration 1477/3560 Training loss: 1.7678 1.1800 sec/batch\n",
      "Epoch 9/20  Iteration 1478/3560 Training loss: 1.7676 1.1578 sec/batch\n",
      "Epoch 9/20  Iteration 1479/3560 Training loss: 1.7674 1.0397 sec/batch\n",
      "Epoch 9/20  Iteration 1480/3560 Training loss: 1.7673 1.0861 sec/batch\n",
      "Epoch 9/20  Iteration 1481/3560 Training loss: 1.7676 0.9812 sec/batch\n",
      "Epoch 9/20  Iteration 1482/3560 Training loss: 1.7673 1.1553 sec/batch\n",
      "Epoch 9/20  Iteration 1483/3560 Training loss: 1.7667 1.0549 sec/batch\n",
      "Epoch 9/20  Iteration 1484/3560 Training loss: 1.7672 0.9311 sec/batch\n",
      "Epoch 9/20  Iteration 1485/3560 Training loss: 1.7670 1.0085 sec/batch\n",
      "Epoch 9/20  Iteration 1486/3560 Training loss: 1.7676 1.0776 sec/batch\n",
      "Epoch 9/20  Iteration 1487/3560 Training loss: 1.7680 1.1470 sec/batch\n",
      "Epoch 9/20  Iteration 1488/3560 Training loss: 1.7683 1.1916 sec/batch\n",
      "Epoch 9/20  Iteration 1489/3560 Training loss: 1.7680 1.0019 sec/batch\n",
      "Epoch 9/20  Iteration 1490/3560 Training loss: 1.7684 1.0430 sec/batch\n",
      "Epoch 9/20  Iteration 1491/3560 Training loss: 1.7683 0.9600 sec/batch\n",
      "Epoch 9/20  Iteration 1492/3560 Training loss: 1.7680 1.0815 sec/batch\n",
      "Epoch 9/20  Iteration 1493/3560 Training loss: 1.7679 1.2639 sec/batch\n",
      "Epoch 9/20  Iteration 1494/3560 Training loss: 1.7678 1.1890 sec/batch\n",
      "Epoch 9/20  Iteration 1495/3560 Training loss: 1.7684 1.2393 sec/batch\n",
      "Epoch 9/20  Iteration 1496/3560 Training loss: 1.7686 1.1505 sec/batch\n",
      "Epoch 9/20  Iteration 1497/3560 Training loss: 1.7689 1.0637 sec/batch\n",
      "Epoch 9/20  Iteration 1498/3560 Training loss: 1.7686 1.1598 sec/batch\n",
      "Epoch 9/20  Iteration 1499/3560 Training loss: 1.7686 0.9910 sec/batch\n",
      "Epoch 9/20  Iteration 1500/3560 Training loss: 1.7691 1.0018 sec/batch\n",
      "Epoch 9/20  Iteration 1501/3560 Training loss: 1.7690 1.0006 sec/batch\n",
      "Epoch 9/20  Iteration 1502/3560 Training loss: 1.7690 1.1491 sec/batch\n",
      "Epoch 9/20  Iteration 1503/3560 Training loss: 1.7683 1.3544 sec/batch\n",
      "Epoch 9/20  Iteration 1504/3560 Training loss: 1.7682 1.3531 sec/batch\n",
      "Epoch 9/20  Iteration 1505/3560 Training loss: 1.7677 1.2667 sec/batch\n",
      "Epoch 9/20  Iteration 1506/3560 Training loss: 1.7677 1.3735 sec/batch\n",
      "Epoch 9/20  Iteration 1507/3560 Training loss: 1.7671 1.4131 sec/batch\n",
      "Epoch 9/20  Iteration 1508/3560 Training loss: 1.7670 1.3727 sec/batch\n",
      "Epoch 9/20  Iteration 1509/3560 Training loss: 1.7666 1.3547 sec/batch\n",
      "Epoch 9/20  Iteration 1510/3560 Training loss: 1.7662 1.3491 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1511/3560 Training loss: 1.7660 1.2822 sec/batch\n",
      "Epoch 9/20  Iteration 1512/3560 Training loss: 1.7656 1.1103 sec/batch\n",
      "Epoch 9/20  Iteration 1513/3560 Training loss: 1.7651 1.1884 sec/batch\n",
      "Epoch 9/20  Iteration 1514/3560 Training loss: 1.7651 1.0467 sec/batch\n",
      "Epoch 9/20  Iteration 1515/3560 Training loss: 1.7648 1.2284 sec/batch\n",
      "Epoch 9/20  Iteration 1516/3560 Training loss: 1.7646 1.0710 sec/batch\n",
      "Epoch 9/20  Iteration 1517/3560 Training loss: 1.7641 1.2194 sec/batch\n",
      "Epoch 9/20  Iteration 1518/3560 Training loss: 1.7638 1.3512 sec/batch\n",
      "Epoch 9/20  Iteration 1519/3560 Training loss: 1.7634 1.1739 sec/batch\n",
      "Epoch 9/20  Iteration 1520/3560 Training loss: 1.7634 1.2706 sec/batch\n",
      "Epoch 9/20  Iteration 1521/3560 Training loss: 1.7632 1.2330 sec/batch\n",
      "Epoch 9/20  Iteration 1522/3560 Training loss: 1.7627 1.2729 sec/batch\n",
      "Epoch 9/20  Iteration 1523/3560 Training loss: 1.7624 1.1941 sec/batch\n",
      "Epoch 9/20  Iteration 1524/3560 Training loss: 1.7619 1.1679 sec/batch\n",
      "Epoch 9/20  Iteration 1525/3560 Training loss: 1.7618 1.0704 sec/batch\n",
      "Epoch 9/20  Iteration 1526/3560 Training loss: 1.7616 1.1434 sec/batch\n",
      "Epoch 9/20  Iteration 1527/3560 Training loss: 1.7613 1.0636 sec/batch\n",
      "Epoch 9/20  Iteration 1528/3560 Training loss: 1.7610 1.0061 sec/batch\n",
      "Epoch 9/20  Iteration 1529/3560 Training loss: 1.7607 1.0041 sec/batch\n",
      "Epoch 9/20  Iteration 1530/3560 Training loss: 1.7605 0.9931 sec/batch\n",
      "Epoch 9/20  Iteration 1531/3560 Training loss: 1.7604 0.9838 sec/batch\n",
      "Epoch 9/20  Iteration 1532/3560 Training loss: 1.7602 1.1723 sec/batch\n",
      "Epoch 9/20  Iteration 1533/3560 Training loss: 1.7601 1.2461 sec/batch\n",
      "Epoch 9/20  Iteration 1534/3560 Training loss: 1.7600 1.2799 sec/batch\n",
      "Epoch 9/20  Iteration 1535/3560 Training loss: 1.7599 1.2297 sec/batch\n",
      "Epoch 9/20  Iteration 1536/3560 Training loss: 1.7597 1.1151 sec/batch\n",
      "Epoch 9/20  Iteration 1537/3560 Training loss: 1.7595 0.9757 sec/batch\n",
      "Epoch 9/20  Iteration 1538/3560 Training loss: 1.7592 1.1089 sec/batch\n",
      "Epoch 9/20  Iteration 1539/3560 Training loss: 1.7588 0.9687 sec/batch\n",
      "Epoch 9/20  Iteration 1540/3560 Training loss: 1.7583 1.1276 sec/batch\n",
      "Epoch 9/20  Iteration 1541/3560 Training loss: 1.7583 1.1697 sec/batch\n",
      "Epoch 9/20  Iteration 1542/3560 Training loss: 1.7581 1.2396 sec/batch\n",
      "Epoch 9/20  Iteration 1543/3560 Training loss: 1.7579 1.0387 sec/batch\n",
      "Epoch 9/20  Iteration 1544/3560 Training loss: 1.7577 1.0188 sec/batch\n",
      "Epoch 9/20  Iteration 1545/3560 Training loss: 1.7577 1.0676 sec/batch\n",
      "Epoch 9/20  Iteration 1546/3560 Training loss: 1.7572 1.0479 sec/batch\n",
      "Epoch 9/20  Iteration 1547/3560 Training loss: 1.7567 1.0624 sec/batch\n",
      "Epoch 9/20  Iteration 1548/3560 Training loss: 1.7568 1.0532 sec/batch\n",
      "Epoch 9/20  Iteration 1549/3560 Training loss: 1.7567 1.1803 sec/batch\n",
      "Epoch 9/20  Iteration 1550/3560 Training loss: 1.7563 1.1970 sec/batch\n",
      "Epoch 9/20  Iteration 1551/3560 Training loss: 1.7563 1.1631 sec/batch\n",
      "Epoch 9/20  Iteration 1552/3560 Training loss: 1.7563 0.9423 sec/batch\n",
      "Epoch 9/20  Iteration 1553/3560 Training loss: 1.7561 1.0290 sec/batch\n",
      "Epoch 9/20  Iteration 1554/3560 Training loss: 1.7559 1.0740 sec/batch\n",
      "Epoch 9/20  Iteration 1555/3560 Training loss: 1.7555 1.0709 sec/batch\n",
      "Epoch 9/20  Iteration 1556/3560 Training loss: 1.7552 0.9883 sec/batch\n",
      "Epoch 9/20  Iteration 1557/3560 Training loss: 1.7552 1.0780 sec/batch\n",
      "Epoch 9/20  Iteration 1558/3560 Training loss: 1.7551 1.1785 sec/batch\n",
      "Epoch 9/20  Iteration 1559/3560 Training loss: 1.7551 0.9433 sec/batch\n",
      "Epoch 9/20  Iteration 1560/3560 Training loss: 1.7550 1.0308 sec/batch\n",
      "Epoch 9/20  Iteration 1561/3560 Training loss: 1.7551 1.0035 sec/batch\n",
      "Epoch 9/20  Iteration 1562/3560 Training loss: 1.7551 1.0087 sec/batch\n",
      "Epoch 9/20  Iteration 1563/3560 Training loss: 1.7552 1.0203 sec/batch\n",
      "Epoch 9/20  Iteration 1564/3560 Training loss: 1.7551 1.0909 sec/batch\n",
      "Epoch 9/20  Iteration 1565/3560 Training loss: 1.7553 1.0644 sec/batch\n",
      "Epoch 9/20  Iteration 1566/3560 Training loss: 1.7552 1.3135 sec/batch\n",
      "Epoch 9/20  Iteration 1567/3560 Training loss: 1.7552 1.0630 sec/batch\n",
      "Epoch 9/20  Iteration 1568/3560 Training loss: 1.7552 1.0249 sec/batch\n",
      "Epoch 9/20  Iteration 1569/3560 Training loss: 1.7549 0.9490 sec/batch\n",
      "Epoch 9/20  Iteration 1570/3560 Training loss: 1.7550 1.0033 sec/batch\n",
      "Epoch 9/20  Iteration 1571/3560 Training loss: 1.7550 1.1022 sec/batch\n",
      "Epoch 9/20  Iteration 1572/3560 Training loss: 1.7553 0.8983 sec/batch\n",
      "Epoch 9/20  Iteration 1573/3560 Training loss: 1.7552 1.1218 sec/batch\n",
      "Epoch 9/20  Iteration 1574/3560 Training loss: 1.7551 1.2282 sec/batch\n",
      "Epoch 9/20  Iteration 1575/3560 Training loss: 1.7548 1.0719 sec/batch\n",
      "Epoch 9/20  Iteration 1576/3560 Training loss: 1.7549 0.9668 sec/batch\n",
      "Epoch 9/20  Iteration 1577/3560 Training loss: 1.7549 0.9864 sec/batch\n",
      "Epoch 9/20  Iteration 1578/3560 Training loss: 1.7549 0.9919 sec/batch\n",
      "Epoch 9/20  Iteration 1579/3560 Training loss: 1.7549 1.0137 sec/batch\n",
      "Epoch 9/20  Iteration 1580/3560 Training loss: 1.7548 0.9893 sec/batch\n",
      "Epoch 9/20  Iteration 1581/3560 Training loss: 1.7548 1.1301 sec/batch\n",
      "Epoch 9/20  Iteration 1582/3560 Training loss: 1.7547 1.1628 sec/batch\n",
      "Epoch 9/20  Iteration 1583/3560 Training loss: 1.7545 1.1599 sec/batch\n",
      "Epoch 9/20  Iteration 1584/3560 Training loss: 1.7546 1.0726 sec/batch\n",
      "Epoch 9/20  Iteration 1585/3560 Training loss: 1.7547 0.9809 sec/batch\n",
      "Epoch 9/20  Iteration 1586/3560 Training loss: 1.7546 0.9566 sec/batch\n",
      "Epoch 9/20  Iteration 1587/3560 Training loss: 1.7547 0.9775 sec/batch\n",
      "Epoch 9/20  Iteration 1588/3560 Training loss: 1.7547 1.0450 sec/batch\n",
      "Epoch 9/20  Iteration 1589/3560 Training loss: 1.7546 1.2158 sec/batch\n",
      "Epoch 9/20  Iteration 1590/3560 Training loss: 1.7545 1.2182 sec/batch\n",
      "Epoch 9/20  Iteration 1591/3560 Training loss: 1.7545 0.9019 sec/batch\n",
      "Epoch 9/20  Iteration 1592/3560 Training loss: 1.7548 1.0461 sec/batch\n",
      "Epoch 9/20  Iteration 1593/3560 Training loss: 1.7547 1.0396 sec/batch\n",
      "Epoch 9/20  Iteration 1594/3560 Training loss: 1.7546 0.9817 sec/batch\n",
      "Epoch 9/20  Iteration 1595/3560 Training loss: 1.7545 0.8783 sec/batch\n",
      "Epoch 9/20  Iteration 1596/3560 Training loss: 1.7543 0.9801 sec/batch\n",
      "Epoch 9/20  Iteration 1597/3560 Training loss: 1.7543 1.0850 sec/batch\n",
      "Epoch 9/20  Iteration 1598/3560 Training loss: 1.7543 1.1141 sec/batch\n",
      "Epoch 9/20  Iteration 1599/3560 Training loss: 1.7543 1.0501 sec/batch\n",
      "Epoch 9/20  Iteration 1600/3560 Training loss: 1.7542 0.9057 sec/batch\n",
      "Epoch 9/20  Iteration 1601/3560 Training loss: 1.7540 0.9238 sec/batch\n",
      "Epoch 9/20  Iteration 1602/3560 Training loss: 1.7540 0.9927 sec/batch\n",
      "Epoch 10/20  Iteration 1603/3560 Training loss: 1.8216 0.9589 sec/batch\n",
      "Epoch 10/20  Iteration 1604/3560 Training loss: 1.7785 0.9615 sec/batch\n",
      "Epoch 10/20  Iteration 1605/3560 Training loss: 1.7660 1.0231 sec/batch\n",
      "Epoch 10/20  Iteration 1606/3560 Training loss: 1.7576 1.0061 sec/batch\n",
      "Epoch 10/20  Iteration 1607/3560 Training loss: 1.7533 1.0391 sec/batch\n",
      "Epoch 10/20  Iteration 1608/3560 Training loss: 1.7436 0.9626 sec/batch\n",
      "Epoch 10/20  Iteration 1609/3560 Training loss: 1.7439 1.0367 sec/batch\n",
      "Epoch 10/20  Iteration 1610/3560 Training loss: 1.7411 0.8665 sec/batch\n",
      "Epoch 10/20  Iteration 1611/3560 Training loss: 1.7416 0.9315 sec/batch\n",
      "Epoch 10/20  Iteration 1612/3560 Training loss: 1.7409 0.9322 sec/batch\n",
      "Epoch 10/20  Iteration 1613/3560 Training loss: 1.7373 1.0080 sec/batch\n",
      "Epoch 10/20  Iteration 1614/3560 Training loss: 1.7356 1.0233 sec/batch\n",
      "Epoch 10/20  Iteration 1615/3560 Training loss: 1.7356 1.0733 sec/batch\n",
      "Epoch 10/20  Iteration 1616/3560 Training loss: 1.7380 0.9471 sec/batch\n",
      "Epoch 10/20  Iteration 1617/3560 Training loss: 1.7372 1.0323 sec/batch\n",
      "Epoch 10/20  Iteration 1618/3560 Training loss: 1.7352 0.9151 sec/batch\n",
      "Epoch 10/20  Iteration 1619/3560 Training loss: 1.7356 0.9259 sec/batch\n",
      "Epoch 10/20  Iteration 1620/3560 Training loss: 1.7373 1.0082 sec/batch\n",
      "Epoch 10/20  Iteration 1621/3560 Training loss: 1.7374 0.9670 sec/batch\n",
      "Epoch 10/20  Iteration 1622/3560 Training loss: 1.7382 1.0682 sec/batch\n",
      "Epoch 10/20  Iteration 1623/3560 Training loss: 1.7377 0.9977 sec/batch\n",
      "Epoch 10/20  Iteration 1624/3560 Training loss: 1.7382 1.0620 sec/batch\n",
      "Epoch 10/20  Iteration 1625/3560 Training loss: 1.7379 1.0128 sec/batch\n",
      "Epoch 10/20  Iteration 1626/3560 Training loss: 1.7371 0.9583 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1627/3560 Training loss: 1.7366 0.9904 sec/batch\n",
      "Epoch 10/20  Iteration 1628/3560 Training loss: 1.7350 0.9953 sec/batch\n",
      "Epoch 10/20  Iteration 1629/3560 Training loss: 1.7336 0.9129 sec/batch\n",
      "Epoch 10/20  Iteration 1630/3560 Training loss: 1.7340 1.1545 sec/batch\n",
      "Epoch 10/20  Iteration 1631/3560 Training loss: 1.7345 0.9665 sec/batch\n",
      "Epoch 10/20  Iteration 1632/3560 Training loss: 1.7347 0.9618 sec/batch\n",
      "Epoch 10/20  Iteration 1633/3560 Training loss: 1.7346 0.9816 sec/batch\n",
      "Epoch 10/20  Iteration 1634/3560 Training loss: 1.7333 0.8529 sec/batch\n",
      "Epoch 10/20  Iteration 1635/3560 Training loss: 1.7334 0.9816 sec/batch\n",
      "Epoch 10/20  Iteration 1636/3560 Training loss: 1.7340 0.9144 sec/batch\n",
      "Epoch 10/20  Iteration 1637/3560 Training loss: 1.7337 0.9891 sec/batch\n",
      "Epoch 10/20  Iteration 1638/3560 Training loss: 1.7335 0.9746 sec/batch\n",
      "Epoch 10/20  Iteration 1639/3560 Training loss: 1.7327 1.0060 sec/batch\n",
      "Epoch 10/20  Iteration 1640/3560 Training loss: 1.7315 0.9740 sec/batch\n",
      "Epoch 10/20  Iteration 1641/3560 Training loss: 1.7302 0.9309 sec/batch\n",
      "Epoch 10/20  Iteration 1642/3560 Training loss: 1.7293 0.8940 sec/batch\n",
      "Epoch 10/20  Iteration 1643/3560 Training loss: 1.7286 0.9462 sec/batch\n",
      "Epoch 10/20  Iteration 1644/3560 Training loss: 1.7292 0.9981 sec/batch\n",
      "Epoch 10/20  Iteration 1645/3560 Training loss: 1.7284 1.0399 sec/batch\n",
      "Epoch 10/20  Iteration 1646/3560 Training loss: 1.7276 1.0516 sec/batch\n",
      "Epoch 10/20  Iteration 1647/3560 Training loss: 1.7277 1.0681 sec/batch\n",
      "Epoch 10/20  Iteration 1648/3560 Training loss: 1.7264 1.0029 sec/batch\n",
      "Epoch 10/20  Iteration 1649/3560 Training loss: 1.7261 0.9386 sec/batch\n",
      "Epoch 10/20  Iteration 1650/3560 Training loss: 1.7256 0.9792 sec/batch\n",
      "Epoch 10/20  Iteration 1651/3560 Training loss: 1.7256 0.9559 sec/batch\n",
      "Epoch 10/20  Iteration 1652/3560 Training loss: 1.7264 0.9780 sec/batch\n",
      "Epoch 10/20  Iteration 1653/3560 Training loss: 1.7258 0.9413 sec/batch\n",
      "Epoch 10/20  Iteration 1654/3560 Training loss: 1.7266 1.1518 sec/batch\n",
      "Epoch 10/20  Iteration 1655/3560 Training loss: 1.7265 1.0289 sec/batch\n",
      "Epoch 10/20  Iteration 1656/3560 Training loss: 1.7264 1.0663 sec/batch\n",
      "Epoch 10/20  Iteration 1657/3560 Training loss: 1.7263 0.9632 sec/batch\n",
      "Epoch 10/20  Iteration 1658/3560 Training loss: 1.7264 0.9124 sec/batch\n",
      "Epoch 10/20  Iteration 1659/3560 Training loss: 1.7268 0.9842 sec/batch\n",
      "Epoch 10/20  Iteration 1660/3560 Training loss: 1.7263 0.9890 sec/batch\n",
      "Epoch 10/20  Iteration 1661/3560 Training loss: 1.7257 1.0105 sec/batch\n",
      "Epoch 10/20  Iteration 1662/3560 Training loss: 1.7263 1.0287 sec/batch\n",
      "Epoch 10/20  Iteration 1663/3560 Training loss: 1.7261 1.0468 sec/batch\n",
      "Epoch 10/20  Iteration 1664/3560 Training loss: 1.7269 0.9607 sec/batch\n",
      "Epoch 10/20  Iteration 1665/3560 Training loss: 1.7272 1.0108 sec/batch\n",
      "Epoch 10/20  Iteration 1666/3560 Training loss: 1.7275 0.9836 sec/batch\n",
      "Epoch 10/20  Iteration 1667/3560 Training loss: 1.7271 0.8730 sec/batch\n",
      "Epoch 10/20  Iteration 1668/3560 Training loss: 1.7274 0.8838 sec/batch\n",
      "Epoch 10/20  Iteration 1669/3560 Training loss: 1.7276 0.9828 sec/batch\n",
      "Epoch 10/20  Iteration 1670/3560 Training loss: 1.7272 1.1013 sec/batch\n",
      "Epoch 10/20  Iteration 1671/3560 Training loss: 1.7270 1.0283 sec/batch\n",
      "Epoch 10/20  Iteration 1672/3560 Training loss: 1.7269 1.0268 sec/batch\n",
      "Epoch 10/20  Iteration 1673/3560 Training loss: 1.7274 0.9110 sec/batch\n",
      "Epoch 10/20  Iteration 1674/3560 Training loss: 1.7276 1.0208 sec/batch\n",
      "Epoch 10/20  Iteration 1675/3560 Training loss: 1.7280 1.0477 sec/batch\n",
      "Epoch 10/20  Iteration 1676/3560 Training loss: 1.7277 0.9806 sec/batch\n",
      "Epoch 10/20  Iteration 1677/3560 Training loss: 1.7275 0.9295 sec/batch\n",
      "Epoch 10/20  Iteration 1678/3560 Training loss: 1.7278 1.0949 sec/batch\n",
      "Epoch 10/20  Iteration 1679/3560 Training loss: 1.7277 1.0846 sec/batch\n",
      "Epoch 10/20  Iteration 1680/3560 Training loss: 1.7278 0.9460 sec/batch\n",
      "Epoch 10/20  Iteration 1681/3560 Training loss: 1.7272 0.9345 sec/batch\n",
      "Epoch 10/20  Iteration 1682/3560 Training loss: 1.7270 0.9168 sec/batch\n",
      "Epoch 10/20  Iteration 1683/3560 Training loss: 1.7265 0.9312 sec/batch\n",
      "Epoch 10/20  Iteration 1684/3560 Training loss: 1.7266 0.9462 sec/batch\n",
      "Epoch 10/20  Iteration 1685/3560 Training loss: 1.7260 1.0327 sec/batch\n",
      "Epoch 10/20  Iteration 1686/3560 Training loss: 1.7259 1.0099 sec/batch\n",
      "Epoch 10/20  Iteration 1687/3560 Training loss: 1.7255 1.1242 sec/batch\n",
      "Epoch 10/20  Iteration 1688/3560 Training loss: 1.7252 0.8630 sec/batch\n",
      "Epoch 10/20  Iteration 1689/3560 Training loss: 1.7249 0.9801 sec/batch\n",
      "Epoch 10/20  Iteration 1690/3560 Training loss: 1.7246 0.9039 sec/batch\n",
      "Epoch 10/20  Iteration 1691/3560 Training loss: 1.7240 0.9806 sec/batch\n",
      "Epoch 10/20  Iteration 1692/3560 Training loss: 1.7241 0.8993 sec/batch\n",
      "Epoch 10/20  Iteration 1693/3560 Training loss: 1.7238 0.9832 sec/batch\n",
      "Epoch 10/20  Iteration 1694/3560 Training loss: 1.7236 1.0621 sec/batch\n",
      "Epoch 10/20  Iteration 1695/3560 Training loss: 1.7232 1.0335 sec/batch\n",
      "Epoch 10/20  Iteration 1696/3560 Training loss: 1.7229 1.0481 sec/batch\n",
      "Epoch 10/20  Iteration 1697/3560 Training loss: 1.7225 1.0664 sec/batch\n",
      "Epoch 10/20  Iteration 1698/3560 Training loss: 1.7224 0.9467 sec/batch\n",
      "Epoch 10/20  Iteration 1699/3560 Training loss: 1.7223 0.9846 sec/batch\n",
      "Epoch 10/20  Iteration 1700/3560 Training loss: 1.7219 0.9307 sec/batch\n",
      "Epoch 10/20  Iteration 1701/3560 Training loss: 1.7216 1.0783 sec/batch\n",
      "Epoch 10/20  Iteration 1702/3560 Training loss: 1.7211 1.1552 sec/batch\n",
      "Epoch 10/20  Iteration 1703/3560 Training loss: 1.7211 1.0012 sec/batch\n",
      "Epoch 10/20  Iteration 1704/3560 Training loss: 1.7210 1.0306 sec/batch\n",
      "Epoch 10/20  Iteration 1705/3560 Training loss: 1.7206 1.0048 sec/batch\n",
      "Epoch 10/20  Iteration 1706/3560 Training loss: 1.7204 0.9503 sec/batch\n",
      "Epoch 10/20  Iteration 1707/3560 Training loss: 1.7202 0.8748 sec/batch\n",
      "Epoch 10/20  Iteration 1708/3560 Training loss: 1.7200 1.0056 sec/batch\n",
      "Epoch 10/20  Iteration 1709/3560 Training loss: 1.7199 0.9929 sec/batch\n",
      "Epoch 10/20  Iteration 1710/3560 Training loss: 1.7198 1.0598 sec/batch\n",
      "Epoch 10/20  Iteration 1711/3560 Training loss: 1.7197 1.0423 sec/batch\n",
      "Epoch 10/20  Iteration 1712/3560 Training loss: 1.7196 0.9729 sec/batch\n",
      "Epoch 10/20  Iteration 1713/3560 Training loss: 1.7195 0.9530 sec/batch\n",
      "Epoch 10/20  Iteration 1714/3560 Training loss: 1.7193 0.9455 sec/batch\n",
      "Epoch 10/20  Iteration 1715/3560 Training loss: 1.7191 0.9061 sec/batch\n",
      "Epoch 10/20  Iteration 1716/3560 Training loss: 1.7189 1.1569 sec/batch\n",
      "Epoch 10/20  Iteration 1717/3560 Training loss: 1.7186 0.9159 sec/batch\n",
      "Epoch 10/20  Iteration 1718/3560 Training loss: 1.7182 1.0556 sec/batch\n",
      "Epoch 10/20  Iteration 1719/3560 Training loss: 1.7182 1.0033 sec/batch\n",
      "Epoch 10/20  Iteration 1720/3560 Training loss: 1.7181 0.8743 sec/batch\n",
      "Epoch 10/20  Iteration 1721/3560 Training loss: 1.7179 0.8902 sec/batch\n",
      "Epoch 10/20  Iteration 1722/3560 Training loss: 1.7178 0.9682 sec/batch\n",
      "Epoch 10/20  Iteration 1723/3560 Training loss: 1.7176 0.9491 sec/batch\n",
      "Epoch 10/20  Iteration 1724/3560 Training loss: 1.7172 0.8843 sec/batch\n",
      "Epoch 10/20  Iteration 1725/3560 Training loss: 1.7168 1.0604 sec/batch\n",
      "Epoch 10/20  Iteration 1726/3560 Training loss: 1.7169 1.1419 sec/batch\n",
      "Epoch 10/20  Iteration 1727/3560 Training loss: 1.7168 1.0873 sec/batch\n",
      "Epoch 10/20  Iteration 1728/3560 Training loss: 1.7163 0.8773 sec/batch\n",
      "Epoch 10/20  Iteration 1729/3560 Training loss: 1.7164 0.9513 sec/batch\n",
      "Epoch 10/20  Iteration 1730/3560 Training loss: 1.7164 0.9209 sec/batch\n",
      "Epoch 10/20  Iteration 1731/3560 Training loss: 1.7162 0.9774 sec/batch\n",
      "Epoch 10/20  Iteration 1732/3560 Training loss: 1.7160 0.9195 sec/batch\n",
      "Epoch 10/20  Iteration 1733/3560 Training loss: 1.7156 1.0493 sec/batch\n",
      "Epoch 10/20  Iteration 1734/3560 Training loss: 1.7152 1.2577 sec/batch\n",
      "Epoch 10/20  Iteration 1735/3560 Training loss: 1.7152 0.9756 sec/batch\n",
      "Epoch 10/20  Iteration 1736/3560 Training loss: 1.7152 1.0406 sec/batch\n",
      "Epoch 10/20  Iteration 1737/3560 Training loss: 1.7151 0.9202 sec/batch\n",
      "Epoch 10/20  Iteration 1738/3560 Training loss: 1.7152 1.0004 sec/batch\n",
      "Epoch 10/20  Iteration 1739/3560 Training loss: 1.7153 0.9109 sec/batch\n",
      "Epoch 10/20  Iteration 1740/3560 Training loss: 1.7154 1.0083 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1741/3560 Training loss: 1.7154 0.9738 sec/batch\n",
      "Epoch 10/20  Iteration 1742/3560 Training loss: 1.7152 1.1104 sec/batch\n",
      "Epoch 10/20  Iteration 1743/3560 Training loss: 1.7155 0.9474 sec/batch\n",
      "Epoch 10/20  Iteration 1744/3560 Training loss: 1.7154 0.9310 sec/batch\n",
      "Epoch 10/20  Iteration 1745/3560 Training loss: 1.7153 0.8783 sec/batch\n",
      "Epoch 10/20  Iteration 1746/3560 Training loss: 1.7153 1.0179 sec/batch\n",
      "Epoch 10/20  Iteration 1747/3560 Training loss: 1.7152 0.9266 sec/batch\n",
      "Epoch 10/20  Iteration 1748/3560 Training loss: 1.7152 0.9441 sec/batch\n",
      "Epoch 10/20  Iteration 1749/3560 Training loss: 1.7152 1.0456 sec/batch\n",
      "Epoch 10/20  Iteration 1750/3560 Training loss: 1.7153 1.0445 sec/batch\n",
      "Epoch 10/20  Iteration 1751/3560 Training loss: 1.7153 0.9549 sec/batch\n",
      "Epoch 10/20  Iteration 1752/3560 Training loss: 1.7151 1.0195 sec/batch\n",
      "Epoch 10/20  Iteration 1753/3560 Training loss: 1.7148 0.9626 sec/batch\n",
      "Epoch 10/20  Iteration 1754/3560 Training loss: 1.7148 0.9337 sec/batch\n",
      "Epoch 10/20  Iteration 1755/3560 Training loss: 1.7148 0.9535 sec/batch\n",
      "Epoch 10/20  Iteration 1756/3560 Training loss: 1.7149 0.9075 sec/batch\n",
      "Epoch 10/20  Iteration 1757/3560 Training loss: 1.7149 1.0277 sec/batch\n",
      "Epoch 10/20  Iteration 1758/3560 Training loss: 1.7148 1.1109 sec/batch\n",
      "Epoch 10/20  Iteration 1759/3560 Training loss: 1.7148 0.9739 sec/batch\n",
      "Epoch 10/20  Iteration 1760/3560 Training loss: 1.7148 0.9013 sec/batch\n",
      "Epoch 10/20  Iteration 1761/3560 Training loss: 1.7146 0.9484 sec/batch\n",
      "Epoch 10/20  Iteration 1762/3560 Training loss: 1.7147 1.0268 sec/batch\n",
      "Epoch 10/20  Iteration 1763/3560 Training loss: 1.7148 0.8560 sec/batch\n",
      "Epoch 10/20  Iteration 1764/3560 Training loss: 1.7148 0.9006 sec/batch\n",
      "Epoch 10/20  Iteration 1765/3560 Training loss: 1.7148 1.0329 sec/batch\n",
      "Epoch 10/20  Iteration 1766/3560 Training loss: 1.7148 1.1702 sec/batch\n",
      "Epoch 10/20  Iteration 1767/3560 Training loss: 1.7147 0.9931 sec/batch\n",
      "Epoch 10/20  Iteration 1768/3560 Training loss: 1.7146 0.9211 sec/batch\n",
      "Epoch 10/20  Iteration 1769/3560 Training loss: 1.7147 0.9822 sec/batch\n",
      "Epoch 10/20  Iteration 1770/3560 Training loss: 1.7150 0.9594 sec/batch\n",
      "Epoch 10/20  Iteration 1771/3560 Training loss: 1.7150 0.9071 sec/batch\n",
      "Epoch 10/20  Iteration 1772/3560 Training loss: 1.7150 0.9020 sec/batch\n",
      "Epoch 10/20  Iteration 1773/3560 Training loss: 1.7148 1.0223 sec/batch\n",
      "Epoch 10/20  Iteration 1774/3560 Training loss: 1.7147 1.0796 sec/batch\n",
      "Epoch 10/20  Iteration 1775/3560 Training loss: 1.7147 0.9560 sec/batch\n",
      "Epoch 10/20  Iteration 1776/3560 Training loss: 1.7147 0.9165 sec/batch\n",
      "Epoch 10/20  Iteration 1777/3560 Training loss: 1.7147 0.9345 sec/batch\n",
      "Epoch 10/20  Iteration 1778/3560 Training loss: 1.7146 1.0009 sec/batch\n",
      "Epoch 10/20  Iteration 1779/3560 Training loss: 1.7144 0.9383 sec/batch\n",
      "Epoch 10/20  Iteration 1780/3560 Training loss: 1.7144 0.9526 sec/batch\n",
      "Epoch 11/20  Iteration 1781/3560 Training loss: 1.7839 1.0384 sec/batch\n",
      "Epoch 11/20  Iteration 1782/3560 Training loss: 1.7391 1.0872 sec/batch\n",
      "Epoch 11/20  Iteration 1783/3560 Training loss: 1.7259 0.9795 sec/batch\n",
      "Epoch 11/20  Iteration 1784/3560 Training loss: 1.7179 0.9444 sec/batch\n",
      "Epoch 11/20  Iteration 1785/3560 Training loss: 1.7132 0.9403 sec/batch\n",
      "Epoch 11/20  Iteration 1786/3560 Training loss: 1.7037 0.9087 sec/batch\n",
      "Epoch 11/20  Iteration 1787/3560 Training loss: 1.7041 0.9915 sec/batch\n",
      "Epoch 11/20  Iteration 1788/3560 Training loss: 1.7029 0.9360 sec/batch\n",
      "Epoch 11/20  Iteration 1789/3560 Training loss: 1.7048 1.0336 sec/batch\n",
      "Epoch 11/20  Iteration 1790/3560 Training loss: 1.7029 1.1159 sec/batch\n",
      "Epoch 11/20  Iteration 1791/3560 Training loss: 1.6991 0.9081 sec/batch\n",
      "Epoch 11/20  Iteration 1792/3560 Training loss: 1.6976 1.0028 sec/batch\n",
      "Epoch 11/20  Iteration 1793/3560 Training loss: 1.6965 0.9073 sec/batch\n",
      "Epoch 11/20  Iteration 1794/3560 Training loss: 1.6985 0.9917 sec/batch\n",
      "Epoch 11/20  Iteration 1795/3560 Training loss: 1.6975 0.9491 sec/batch\n",
      "Epoch 11/20  Iteration 1796/3560 Training loss: 1.6955 0.8570 sec/batch\n",
      "Epoch 11/20  Iteration 1797/3560 Training loss: 1.6957 1.0145 sec/batch\n",
      "Epoch 11/20  Iteration 1798/3560 Training loss: 1.6979 1.1975 sec/batch\n",
      "Epoch 11/20  Iteration 1799/3560 Training loss: 1.6980 0.9964 sec/batch\n",
      "Epoch 11/20  Iteration 1800/3560 Training loss: 1.6990 0.9333 sec/batch\n",
      "Epoch 11/20  Iteration 1801/3560 Training loss: 1.6986 0.9930 sec/batch\n",
      "Epoch 11/20  Iteration 1802/3560 Training loss: 1.6994 0.9095 sec/batch\n",
      "Epoch 11/20  Iteration 1803/3560 Training loss: 1.6985 0.9658 sec/batch\n",
      "Epoch 11/20  Iteration 1804/3560 Training loss: 1.6984 0.9027 sec/batch\n",
      "Epoch 11/20  Iteration 1805/3560 Training loss: 1.6981 1.1009 sec/batch\n",
      "Epoch 11/20  Iteration 1806/3560 Training loss: 1.6968 1.0970 sec/batch\n",
      "Epoch 11/20  Iteration 1807/3560 Training loss: 1.6955 0.9346 sec/batch\n",
      "Epoch 11/20  Iteration 1808/3560 Training loss: 1.6963 0.9604 sec/batch\n",
      "Epoch 11/20  Iteration 1809/3560 Training loss: 1.6971 0.8541 sec/batch\n",
      "Epoch 11/20  Iteration 1810/3560 Training loss: 1.6975 1.0083 sec/batch\n",
      "Epoch 11/20  Iteration 1811/3560 Training loss: 1.6973 0.9344 sec/batch\n",
      "Epoch 11/20  Iteration 1812/3560 Training loss: 1.6960 0.9146 sec/batch\n",
      "Epoch 11/20  Iteration 1813/3560 Training loss: 1.6963 1.1661 sec/batch\n",
      "Epoch 11/20  Iteration 1814/3560 Training loss: 1.6971 1.0504 sec/batch\n",
      "Epoch 11/20  Iteration 1815/3560 Training loss: 1.6971 0.9266 sec/batch\n",
      "Epoch 11/20  Iteration 1816/3560 Training loss: 1.6969 0.9665 sec/batch\n",
      "Epoch 11/20  Iteration 1817/3560 Training loss: 1.6962 0.9693 sec/batch\n",
      "Epoch 11/20  Iteration 1818/3560 Training loss: 1.6950 1.0124 sec/batch\n",
      "Epoch 11/20  Iteration 1819/3560 Training loss: 1.6937 0.8544 sec/batch\n",
      "Epoch 11/20  Iteration 1820/3560 Training loss: 1.6932 0.9603 sec/batch\n",
      "Epoch 11/20  Iteration 1821/3560 Training loss: 1.6926 1.9025 sec/batch\n",
      "Epoch 11/20  Iteration 1822/3560 Training loss: 1.6932 1.3170 sec/batch\n",
      "Epoch 11/20  Iteration 1823/3560 Training loss: 1.6925 1.4457 sec/batch\n",
      "Epoch 11/20  Iteration 1824/3560 Training loss: 1.6915 0.9696 sec/batch\n",
      "Epoch 11/20  Iteration 1825/3560 Training loss: 1.6918 1.0714 sec/batch\n",
      "Epoch 11/20  Iteration 1826/3560 Training loss: 1.6905 0.9799 sec/batch\n",
      "Epoch 11/20  Iteration 1827/3560 Training loss: 1.6902 1.0198 sec/batch\n",
      "Epoch 11/20  Iteration 1828/3560 Training loss: 1.6898 0.9560 sec/batch\n",
      "Epoch 11/20  Iteration 1829/3560 Training loss: 1.6896 1.0707 sec/batch\n",
      "Epoch 11/20  Iteration 1830/3560 Training loss: 1.6904 1.0218 sec/batch\n",
      "Epoch 11/20  Iteration 1831/3560 Training loss: 1.6900 0.9815 sec/batch\n",
      "Epoch 11/20  Iteration 1832/3560 Training loss: 1.6908 1.0317 sec/batch\n",
      "Epoch 11/20  Iteration 1833/3560 Training loss: 1.6907 0.9914 sec/batch\n",
      "Epoch 11/20  Iteration 1834/3560 Training loss: 1.6909 0.9414 sec/batch\n",
      "Epoch 11/20  Iteration 1835/3560 Training loss: 1.6907 0.9312 sec/batch\n",
      "Epoch 11/20  Iteration 1836/3560 Training loss: 1.6908 1.0334 sec/batch\n",
      "Epoch 11/20  Iteration 1837/3560 Training loss: 1.6912 1.0331 sec/batch\n",
      "Epoch 11/20  Iteration 1838/3560 Training loss: 1.6909 1.0872 sec/batch\n",
      "Epoch 11/20  Iteration 1839/3560 Training loss: 1.6904 0.9696 sec/batch\n",
      "Epoch 11/20  Iteration 1840/3560 Training loss: 1.6908 0.9836 sec/batch\n",
      "Epoch 11/20  Iteration 1841/3560 Training loss: 1.6907 0.8597 sec/batch\n",
      "Epoch 11/20  Iteration 1842/3560 Training loss: 1.6915 1.0256 sec/batch\n",
      "Epoch 11/20  Iteration 1843/3560 Training loss: 1.6919 0.9367 sec/batch\n",
      "Epoch 11/20  Iteration 1844/3560 Training loss: 1.6922 0.9360 sec/batch\n",
      "Epoch 11/20  Iteration 1845/3560 Training loss: 1.6919 1.0255 sec/batch\n",
      "Epoch 11/20  Iteration 1846/3560 Training loss: 1.6921 1.1639 sec/batch\n",
      "Epoch 11/20  Iteration 1847/3560 Training loss: 1.6922 1.0070 sec/batch\n",
      "Epoch 11/20  Iteration 1848/3560 Training loss: 1.6919 0.9813 sec/batch\n",
      "Epoch 11/20  Iteration 1849/3560 Training loss: 1.6920 0.9693 sec/batch\n",
      "Epoch 11/20  Iteration 1850/3560 Training loss: 1.6918 0.9226 sec/batch\n",
      "Epoch 11/20  Iteration 1851/3560 Training loss: 1.6925 0.9832 sec/batch\n",
      "Epoch 11/20  Iteration 1852/3560 Training loss: 1.6928 0.8484 sec/batch\n",
      "Epoch 11/20  Iteration 1853/3560 Training loss: 1.6932 1.1041 sec/batch\n",
      "Epoch 11/20  Iteration 1854/3560 Training loss: 1.6929 1.0329 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1855/3560 Training loss: 1.6927 0.9463 sec/batch\n",
      "Epoch 11/20  Iteration 1856/3560 Training loss: 1.6932 1.0652 sec/batch\n",
      "Epoch 11/20  Iteration 1857/3560 Training loss: 1.6931 1.0078 sec/batch\n",
      "Epoch 11/20  Iteration 1858/3560 Training loss: 1.6931 1.0022 sec/batch\n",
      "Epoch 11/20  Iteration 1859/3560 Training loss: 1.6926 1.0085 sec/batch\n",
      "Epoch 11/20  Iteration 1860/3560 Training loss: 1.6924 0.9401 sec/batch\n",
      "Epoch 11/20  Iteration 1861/3560 Training loss: 1.6919 1.0348 sec/batch\n",
      "Epoch 11/20  Iteration 1862/3560 Training loss: 1.6920 1.1626 sec/batch\n",
      "Epoch 11/20  Iteration 1863/3560 Training loss: 1.6915 0.9438 sec/batch\n",
      "Epoch 11/20  Iteration 1864/3560 Training loss: 1.6913 0.9323 sec/batch\n",
      "Epoch 11/20  Iteration 1865/3560 Training loss: 1.6911 0.9602 sec/batch\n",
      "Epoch 11/20  Iteration 1866/3560 Training loss: 1.6908 1.0085 sec/batch\n",
      "Epoch 11/20  Iteration 1867/3560 Training loss: 1.6906 0.9689 sec/batch\n",
      "Epoch 11/20  Iteration 1868/3560 Training loss: 1.6903 1.0179 sec/batch\n",
      "Epoch 11/20  Iteration 1869/3560 Training loss: 1.6897 1.1293 sec/batch\n",
      "Epoch 11/20  Iteration 1870/3560 Training loss: 1.6899 1.0130 sec/batch\n",
      "Epoch 11/20  Iteration 1871/3560 Training loss: 1.6896 0.9654 sec/batch\n",
      "Epoch 11/20  Iteration 1872/3560 Training loss: 1.6893 0.9083 sec/batch\n",
      "Epoch 11/20  Iteration 1873/3560 Training loss: 1.6889 0.9351 sec/batch\n",
      "Epoch 11/20  Iteration 1874/3560 Training loss: 1.6885 0.9190 sec/batch\n",
      "Epoch 11/20  Iteration 1875/3560 Training loss: 1.6882 0.9155 sec/batch\n",
      "Epoch 11/20  Iteration 1876/3560 Training loss: 1.6882 0.9994 sec/batch\n",
      "Epoch 11/20  Iteration 1877/3560 Training loss: 1.6881 1.0184 sec/batch\n",
      "Epoch 11/20  Iteration 1878/3560 Training loss: 1.6877 1.1159 sec/batch\n",
      "Epoch 11/20  Iteration 1879/3560 Training loss: 1.6873 0.9781 sec/batch\n",
      "Epoch 11/20  Iteration 1880/3560 Training loss: 1.6868 0.9626 sec/batch\n",
      "Epoch 11/20  Iteration 1881/3560 Training loss: 1.6867 0.9326 sec/batch\n",
      "Epoch 11/20  Iteration 1882/3560 Training loss: 1.6866 0.8779 sec/batch\n",
      "Epoch 11/20  Iteration 1883/3560 Training loss: 1.6863 0.9726 sec/batch\n",
      "Epoch 11/20  Iteration 1884/3560 Training loss: 1.6861 0.9013 sec/batch\n",
      "Epoch 11/20  Iteration 1885/3560 Training loss: 1.6858 1.1036 sec/batch\n",
      "Epoch 11/20  Iteration 1886/3560 Training loss: 1.6857 1.0639 sec/batch\n",
      "Epoch 11/20  Iteration 1887/3560 Training loss: 1.6855 1.0778 sec/batch\n",
      "Epoch 11/20  Iteration 1888/3560 Training loss: 1.6854 0.9185 sec/batch\n",
      "Epoch 11/20  Iteration 1889/3560 Training loss: 1.6854 0.9738 sec/batch\n",
      "Epoch 11/20  Iteration 1890/3560 Training loss: 1.6853 1.0227 sec/batch\n",
      "Epoch 11/20  Iteration 1891/3560 Training loss: 1.6851 1.0323 sec/batch\n",
      "Epoch 11/20  Iteration 1892/3560 Training loss: 1.6849 1.0136 sec/batch\n",
      "Epoch 11/20  Iteration 1893/3560 Training loss: 1.6847 1.0907 sec/batch\n",
      "Epoch 11/20  Iteration 1894/3560 Training loss: 1.6845 1.0252 sec/batch\n",
      "Epoch 11/20  Iteration 1895/3560 Training loss: 1.6842 0.9646 sec/batch\n",
      "Epoch 11/20  Iteration 1896/3560 Training loss: 1.6838 0.9484 sec/batch\n",
      "Epoch 11/20  Iteration 1897/3560 Training loss: 1.6838 0.9617 sec/batch\n",
      "Epoch 11/20  Iteration 1898/3560 Training loss: 1.6838 0.9284 sec/batch\n",
      "Epoch 11/20  Iteration 1899/3560 Training loss: 1.6836 0.9247 sec/batch\n",
      "Epoch 11/20  Iteration 1900/3560 Training loss: 1.6835 0.9566 sec/batch\n",
      "Epoch 11/20  Iteration 1901/3560 Training loss: 1.6835 1.0568 sec/batch\n",
      "Epoch 11/20  Iteration 1902/3560 Training loss: 1.6831 1.0392 sec/batch\n",
      "Epoch 11/20  Iteration 1903/3560 Training loss: 1.6828 1.0244 sec/batch\n",
      "Epoch 11/20  Iteration 1904/3560 Training loss: 1.6828 0.9280 sec/batch\n",
      "Epoch 11/20  Iteration 1905/3560 Training loss: 1.6827 0.9442 sec/batch\n",
      "Epoch 11/20  Iteration 1906/3560 Training loss: 1.6824 0.9292 sec/batch\n",
      "Epoch 11/20  Iteration 1907/3560 Training loss: 1.6824 0.8578 sec/batch\n",
      "Epoch 11/20  Iteration 1908/3560 Training loss: 1.6825 0.9532 sec/batch\n",
      "Epoch 11/20  Iteration 1909/3560 Training loss: 1.6823 0.9695 sec/batch\n",
      "Epoch 11/20  Iteration 1910/3560 Training loss: 1.6822 1.1341 sec/batch\n",
      "Epoch 11/20  Iteration 1911/3560 Training loss: 1.6818 0.9684 sec/batch\n",
      "Epoch 11/20  Iteration 1912/3560 Training loss: 1.6815 0.9142 sec/batch\n",
      "Epoch 11/20  Iteration 1913/3560 Training loss: 1.6815 0.9797 sec/batch\n",
      "Epoch 11/20  Iteration 1914/3560 Training loss: 1.6815 0.9132 sec/batch\n",
      "Epoch 11/20  Iteration 1915/3560 Training loss: 1.6814 1.0125 sec/batch\n",
      "Epoch 11/20  Iteration 1916/3560 Training loss: 1.6815 0.9495 sec/batch\n",
      "Epoch 11/20  Iteration 1917/3560 Training loss: 1.6816 0.9811 sec/batch\n",
      "Epoch 11/20  Iteration 1918/3560 Training loss: 1.6816 1.1569 sec/batch\n",
      "Epoch 11/20  Iteration 1919/3560 Training loss: 1.6816 0.9941 sec/batch\n",
      "Epoch 11/20  Iteration 1920/3560 Training loss: 1.6815 0.9570 sec/batch\n",
      "Epoch 11/20  Iteration 1921/3560 Training loss: 1.6818 0.9472 sec/batch\n",
      "Epoch 11/20  Iteration 1922/3560 Training loss: 1.6816 1.0113 sec/batch\n",
      "Epoch 11/20  Iteration 1923/3560 Training loss: 1.6816 0.9874 sec/batch\n",
      "Epoch 11/20  Iteration 1924/3560 Training loss: 1.6816 1.0449 sec/batch\n",
      "Epoch 11/20  Iteration 1925/3560 Training loss: 1.6815 1.0514 sec/batch\n",
      "Epoch 11/20  Iteration 1926/3560 Training loss: 1.6816 1.0326 sec/batch\n",
      "Epoch 11/20  Iteration 1927/3560 Training loss: 1.6816 1.0788 sec/batch\n",
      "Epoch 11/20  Iteration 1928/3560 Training loss: 1.6819 1.0947 sec/batch\n",
      "Epoch 11/20  Iteration 1929/3560 Training loss: 1.6818 0.9731 sec/batch\n",
      "Epoch 11/20  Iteration 1930/3560 Training loss: 1.6816 0.9788 sec/batch\n",
      "Epoch 11/20  Iteration 1931/3560 Training loss: 1.6813 0.9567 sec/batch\n",
      "Epoch 11/20  Iteration 1932/3560 Training loss: 1.6813 0.9970 sec/batch\n",
      "Epoch 11/20  Iteration 1933/3560 Training loss: 1.6813 1.1203 sec/batch\n",
      "Epoch 11/20  Iteration 1934/3560 Training loss: 1.6813 0.9834 sec/batch\n",
      "Epoch 11/20  Iteration 1935/3560 Training loss: 1.6812 0.9362 sec/batch\n",
      "Epoch 11/20  Iteration 1936/3560 Training loss: 1.6811 0.9809 sec/batch\n",
      "Epoch 11/20  Iteration 1937/3560 Training loss: 1.6812 0.9218 sec/batch\n",
      "Epoch 11/20  Iteration 1938/3560 Training loss: 1.6811 0.9531 sec/batch\n",
      "Epoch 11/20  Iteration 1939/3560 Training loss: 1.6809 0.9853 sec/batch\n",
      "Epoch 11/20  Iteration 1940/3560 Training loss: 1.6810 1.1084 sec/batch\n",
      "Epoch 11/20  Iteration 1941/3560 Training loss: 1.6812 1.1879 sec/batch\n",
      "Epoch 11/20  Iteration 1942/3560 Training loss: 1.6812 1.0294 sec/batch\n",
      "Epoch 11/20  Iteration 1943/3560 Training loss: 1.6811 1.0142 sec/batch\n",
      "Epoch 11/20  Iteration 1944/3560 Training loss: 1.6812 0.9244 sec/batch\n",
      "Epoch 11/20  Iteration 1945/3560 Training loss: 1.6812 0.9424 sec/batch\n",
      "Epoch 11/20  Iteration 1946/3560 Training loss: 1.6811 0.9249 sec/batch\n",
      "Epoch 11/20  Iteration 1947/3560 Training loss: 1.6812 1.0146 sec/batch\n",
      "Epoch 11/20  Iteration 1948/3560 Training loss: 1.6815 0.9117 sec/batch\n",
      "Epoch 11/20  Iteration 1949/3560 Training loss: 1.6814 1.1565 sec/batch\n",
      "Epoch 11/20  Iteration 1950/3560 Training loss: 1.6813 0.9693 sec/batch\n",
      "Epoch 11/20  Iteration 1951/3560 Training loss: 1.6813 1.0268 sec/batch\n",
      "Epoch 11/20  Iteration 1952/3560 Training loss: 1.6811 0.9853 sec/batch\n",
      "Epoch 11/20  Iteration 1953/3560 Training loss: 1.6811 0.9621 sec/batch\n",
      "Epoch 11/20  Iteration 1954/3560 Training loss: 1.6811 0.8984 sec/batch\n",
      "Epoch 11/20  Iteration 1955/3560 Training loss: 1.6811 0.9386 sec/batch\n",
      "Epoch 11/20  Iteration 1956/3560 Training loss: 1.6810 1.0289 sec/batch\n",
      "Epoch 11/20  Iteration 1957/3560 Training loss: 1.6808 1.1371 sec/batch\n",
      "Epoch 11/20  Iteration 1958/3560 Training loss: 1.6808 0.9353 sec/batch\n",
      "Epoch 12/20  Iteration 1959/3560 Training loss: 1.7515 0.9558 sec/batch\n",
      "Epoch 12/20  Iteration 1960/3560 Training loss: 1.7117 0.9983 sec/batch\n",
      "Epoch 12/20  Iteration 1961/3560 Training loss: 1.6991 0.8990 sec/batch\n",
      "Epoch 12/20  Iteration 1962/3560 Training loss: 1.6893 0.9144 sec/batch\n",
      "Epoch 12/20  Iteration 1963/3560 Training loss: 1.6846 0.8994 sec/batch\n",
      "Epoch 12/20  Iteration 1964/3560 Training loss: 1.6742 0.9710 sec/batch\n",
      "Epoch 12/20  Iteration 1965/3560 Training loss: 1.6727 1.0756 sec/batch\n",
      "Epoch 12/20  Iteration 1966/3560 Training loss: 1.6701 1.0152 sec/batch\n",
      "Epoch 12/20  Iteration 1967/3560 Training loss: 1.6723 1.0075 sec/batch\n",
      "Epoch 12/20  Iteration 1968/3560 Training loss: 1.6720 0.8582 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 1969/3560 Training loss: 1.6685 1.0101 sec/batch\n",
      "Epoch 12/20  Iteration 1970/3560 Training loss: 1.6674 0.9076 sec/batch\n",
      "Epoch 12/20  Iteration 1971/3560 Training loss: 1.6675 0.9151 sec/batch\n",
      "Epoch 12/20  Iteration 1972/3560 Training loss: 1.6697 1.0810 sec/batch\n",
      "Epoch 12/20  Iteration 1973/3560 Training loss: 1.6687 1.0958 sec/batch\n",
      "Epoch 12/20  Iteration 1974/3560 Training loss: 1.6670 0.9766 sec/batch\n",
      "Epoch 12/20  Iteration 1975/3560 Training loss: 1.6670 0.9111 sec/batch\n",
      "Epoch 12/20  Iteration 1976/3560 Training loss: 1.6685 0.9497 sec/batch\n",
      "Epoch 12/20  Iteration 1977/3560 Training loss: 1.6689 0.8187 sec/batch\n",
      "Epoch 12/20  Iteration 1978/3560 Training loss: 1.6700 0.9950 sec/batch\n",
      "Epoch 12/20  Iteration 1979/3560 Training loss: 1.6690 0.9195 sec/batch\n",
      "Epoch 12/20  Iteration 1980/3560 Training loss: 1.6695 1.0050 sec/batch\n",
      "Epoch 12/20  Iteration 1981/3560 Training loss: 1.6686 1.1513 sec/batch\n",
      "Epoch 12/20  Iteration 1982/3560 Training loss: 1.6683 1.0068 sec/batch\n",
      "Epoch 12/20  Iteration 1983/3560 Training loss: 1.6684 0.9184 sec/batch\n",
      "Epoch 12/20  Iteration 1984/3560 Training loss: 1.6671 0.9114 sec/batch\n",
      "Epoch 12/20  Iteration 1985/3560 Training loss: 1.6659 0.9161 sec/batch\n",
      "Epoch 12/20  Iteration 1986/3560 Training loss: 1.6663 0.9451 sec/batch\n",
      "Epoch 12/20  Iteration 1987/3560 Training loss: 1.6668 0.9865 sec/batch\n",
      "Epoch 12/20  Iteration 1988/3560 Training loss: 1.6671 0.9716 sec/batch\n",
      "Epoch 12/20  Iteration 1989/3560 Training loss: 1.6668 1.1053 sec/batch\n",
      "Epoch 12/20  Iteration 1990/3560 Training loss: 1.6658 1.0173 sec/batch\n",
      "Epoch 12/20  Iteration 1991/3560 Training loss: 1.6659 0.9507 sec/batch\n",
      "Epoch 12/20  Iteration 1992/3560 Training loss: 1.6663 0.9844 sec/batch\n",
      "Epoch 12/20  Iteration 1993/3560 Training loss: 1.6660 0.9339 sec/batch\n",
      "Epoch 12/20  Iteration 1994/3560 Training loss: 1.6655 0.9342 sec/batch\n",
      "Epoch 12/20  Iteration 1995/3560 Training loss: 1.6647 0.9379 sec/batch\n",
      "Epoch 12/20  Iteration 1996/3560 Training loss: 1.6637 0.8905 sec/batch\n",
      "Epoch 12/20  Iteration 1997/3560 Training loss: 1.6623 1.1222 sec/batch\n",
      "Epoch 12/20  Iteration 1998/3560 Training loss: 1.6618 1.0630 sec/batch\n",
      "Epoch 12/20  Iteration 1999/3560 Training loss: 1.6612 0.9049 sec/batch\n",
      "Epoch 12/20  Iteration 2000/3560 Training loss: 1.6615 0.9568 sec/batch\n",
      "Epoch 12/20  Iteration 2001/3560 Training loss: 1.6610 0.8788 sec/batch\n",
      "Epoch 12/20  Iteration 2002/3560 Training loss: 1.6603 0.9041 sec/batch\n",
      "Epoch 12/20  Iteration 2003/3560 Training loss: 1.6605 0.9170 sec/batch\n",
      "Epoch 12/20  Iteration 2004/3560 Training loss: 1.6594 0.9063 sec/batch\n",
      "Epoch 12/20  Iteration 2005/3560 Training loss: 1.6590 1.1546 sec/batch\n",
      "Epoch 12/20  Iteration 2006/3560 Training loss: 1.6585 1.0803 sec/batch\n",
      "Epoch 12/20  Iteration 2007/3560 Training loss: 1.6585 0.9388 sec/batch\n",
      "Epoch 12/20  Iteration 2008/3560 Training loss: 1.6592 0.9158 sec/batch\n",
      "Epoch 12/20  Iteration 2009/3560 Training loss: 1.6586 1.0079 sec/batch\n",
      "Epoch 12/20  Iteration 2010/3560 Training loss: 1.6596 0.9449 sec/batch\n",
      "Epoch 12/20  Iteration 2011/3560 Training loss: 1.6594 0.8994 sec/batch\n",
      "Epoch 12/20  Iteration 2012/3560 Training loss: 1.6596 1.0579 sec/batch\n",
      "Epoch 12/20  Iteration 2013/3560 Training loss: 1.6596 1.0727 sec/batch\n",
      "Epoch 12/20  Iteration 2014/3560 Training loss: 1.6598 1.0358 sec/batch\n",
      "Epoch 12/20  Iteration 2015/3560 Training loss: 1.6601 1.0278 sec/batch\n",
      "Epoch 12/20  Iteration 2016/3560 Training loss: 1.6599 0.9551 sec/batch\n",
      "Epoch 12/20  Iteration 2017/3560 Training loss: 1.6594 0.9312 sec/batch\n",
      "Epoch 12/20  Iteration 2018/3560 Training loss: 1.6600 0.9584 sec/batch\n",
      "Epoch 12/20  Iteration 2019/3560 Training loss: 1.6599 1.0482 sec/batch\n",
      "Epoch 12/20  Iteration 2020/3560 Training loss: 1.6608 0.9326 sec/batch\n",
      "Epoch 12/20  Iteration 2021/3560 Training loss: 1.6612 1.1057 sec/batch\n",
      "Epoch 12/20  Iteration 2022/3560 Training loss: 1.6614 1.0691 sec/batch\n",
      "Epoch 12/20  Iteration 2023/3560 Training loss: 1.6613 1.0178 sec/batch\n",
      "Epoch 12/20  Iteration 2024/3560 Training loss: 1.6614 0.9834 sec/batch\n",
      "Epoch 12/20  Iteration 2025/3560 Training loss: 1.6615 0.9857 sec/batch\n",
      "Epoch 12/20  Iteration 2026/3560 Training loss: 1.6612 0.9843 sec/batch\n",
      "Epoch 12/20  Iteration 2027/3560 Training loss: 1.6611 0.8889 sec/batch\n",
      "Epoch 12/20  Iteration 2028/3560 Training loss: 1.6610 1.1114 sec/batch\n",
      "Epoch 12/20  Iteration 2029/3560 Training loss: 1.6616 1.1900 sec/batch\n",
      "Epoch 12/20  Iteration 2030/3560 Training loss: 1.6618 1.0370 sec/batch\n",
      "Epoch 12/20  Iteration 2031/3560 Training loss: 1.6622 1.0024 sec/batch\n",
      "Epoch 12/20  Iteration 2032/3560 Training loss: 1.6620 0.9296 sec/batch\n",
      "Epoch 12/20  Iteration 2033/3560 Training loss: 1.6618 1.0443 sec/batch\n",
      "Epoch 12/20  Iteration 2034/3560 Training loss: 1.6621 0.9921 sec/batch\n",
      "Epoch 12/20  Iteration 2035/3560 Training loss: 1.6621 0.9470 sec/batch\n",
      "Epoch 12/20  Iteration 2036/3560 Training loss: 1.6622 1.0049 sec/batch\n",
      "Epoch 12/20  Iteration 2037/3560 Training loss: 1.6616 1.0896 sec/batch\n",
      "Epoch 12/20  Iteration 2038/3560 Training loss: 1.6613 1.0169 sec/batch\n",
      "Epoch 12/20  Iteration 2039/3560 Training loss: 1.6608 0.9395 sec/batch\n",
      "Epoch 12/20  Iteration 2040/3560 Training loss: 1.6609 0.9378 sec/batch\n",
      "Epoch 12/20  Iteration 2041/3560 Training loss: 1.6603 0.9637 sec/batch\n",
      "Epoch 12/20  Iteration 2042/3560 Training loss: 1.6603 1.0358 sec/batch\n",
      "Epoch 12/20  Iteration 2043/3560 Training loss: 1.6599 0.9692 sec/batch\n",
      "Epoch 12/20  Iteration 2044/3560 Training loss: 1.6597 1.0582 sec/batch\n",
      "Epoch 12/20  Iteration 2045/3560 Training loss: 1.6594 1.1059 sec/batch\n",
      "Epoch 12/20  Iteration 2046/3560 Training loss: 1.6592 1.0265 sec/batch\n",
      "Epoch 12/20  Iteration 2047/3560 Training loss: 1.6587 0.9322 sec/batch\n",
      "Epoch 12/20  Iteration 2048/3560 Training loss: 1.6588 0.9277 sec/batch\n",
      "Epoch 12/20  Iteration 2049/3560 Training loss: 1.6586 0.9173 sec/batch\n",
      "Epoch 12/20  Iteration 2050/3560 Training loss: 1.6584 0.8943 sec/batch\n",
      "Epoch 12/20  Iteration 2051/3560 Training loss: 1.6579 1.0007 sec/batch\n",
      "Epoch 12/20  Iteration 2052/3560 Training loss: 1.6576 1.0064 sec/batch\n",
      "Epoch 12/20  Iteration 2053/3560 Training loss: 1.6573 1.1356 sec/batch\n",
      "Epoch 12/20  Iteration 2054/3560 Training loss: 1.6574 0.9824 sec/batch\n",
      "Epoch 12/20  Iteration 2055/3560 Training loss: 1.6572 0.9793 sec/batch\n",
      "Epoch 12/20  Iteration 2056/3560 Training loss: 1.6568 0.8644 sec/batch\n",
      "Epoch 12/20  Iteration 2057/3560 Training loss: 1.6565 1.0095 sec/batch\n",
      "Epoch 12/20  Iteration 2058/3560 Training loss: 1.6560 0.9035 sec/batch\n",
      "Epoch 12/20  Iteration 2059/3560 Training loss: 1.6560 0.9006 sec/batch\n",
      "Epoch 12/20  Iteration 2060/3560 Training loss: 1.6558 0.9735 sec/batch\n",
      "Epoch 12/20  Iteration 2061/3560 Training loss: 1.6555 1.0624 sec/batch\n",
      "Epoch 12/20  Iteration 2062/3560 Training loss: 1.6555 0.9920 sec/batch\n",
      "Epoch 12/20  Iteration 2063/3560 Training loss: 1.6553 0.9711 sec/batch\n",
      "Epoch 12/20  Iteration 2064/3560 Training loss: 1.6552 0.9313 sec/batch\n",
      "Epoch 12/20  Iteration 2065/3560 Training loss: 1.6551 0.9664 sec/batch\n",
      "Epoch 12/20  Iteration 2066/3560 Training loss: 1.6551 0.9523 sec/batch\n",
      "Epoch 12/20  Iteration 2067/3560 Training loss: 1.6550 0.9595 sec/batch\n",
      "Epoch 12/20  Iteration 2068/3560 Training loss: 1.6550 0.9392 sec/batch\n",
      "Epoch 12/20  Iteration 2069/3560 Training loss: 1.6547 1.0615 sec/batch\n",
      "Epoch 12/20  Iteration 2070/3560 Training loss: 1.6547 1.0635 sec/batch\n",
      "Epoch 12/20  Iteration 2071/3560 Training loss: 1.6545 0.9460 sec/batch\n",
      "Epoch 12/20  Iteration 2072/3560 Training loss: 1.6543 0.8905 sec/batch\n",
      "Epoch 12/20  Iteration 2073/3560 Training loss: 1.6541 0.9787 sec/batch\n",
      "Epoch 12/20  Iteration 2074/3560 Training loss: 1.6536 0.8711 sec/batch\n",
      "Epoch 12/20  Iteration 2075/3560 Training loss: 1.6536 0.8922 sec/batch\n",
      "Epoch 12/20  Iteration 2076/3560 Training loss: 1.6536 0.9634 sec/batch\n",
      "Epoch 12/20  Iteration 2077/3560 Training loss: 1.6534 1.0380 sec/batch\n",
      "Epoch 12/20  Iteration 2078/3560 Training loss: 1.6533 1.0250 sec/batch\n",
      "Epoch 12/20  Iteration 2079/3560 Training loss: 1.6533 0.9794 sec/batch\n",
      "Epoch 12/20  Iteration 2080/3560 Training loss: 1.6529 0.9434 sec/batch\n",
      "Epoch 12/20  Iteration 2081/3560 Training loss: 1.6525 1.0091 sec/batch\n",
      "Epoch 12/20  Iteration 2082/3560 Training loss: 1.6526 1.0063 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 2083/3560 Training loss: 1.6525 0.9468 sec/batch\n",
      "Epoch 12/20  Iteration 2084/3560 Training loss: 1.6521 1.0092 sec/batch\n",
      "Epoch 12/20  Iteration 2085/3560 Training loss: 1.6523 1.0764 sec/batch\n",
      "Epoch 12/20  Iteration 2086/3560 Training loss: 1.6523 1.1534 sec/batch\n",
      "Epoch 12/20  Iteration 2087/3560 Training loss: 1.6521 0.8467 sec/batch\n",
      "Epoch 12/20  Iteration 2088/3560 Training loss: 1.6520 0.9651 sec/batch\n",
      "Epoch 12/20  Iteration 2089/3560 Training loss: 1.6516 0.8797 sec/batch\n",
      "Epoch 12/20  Iteration 2090/3560 Training loss: 1.6513 0.9875 sec/batch\n",
      "Epoch 12/20  Iteration 2091/3560 Training loss: 1.6513 0.9085 sec/batch\n",
      "Epoch 12/20  Iteration 2092/3560 Training loss: 1.6514 0.9886 sec/batch\n",
      "Epoch 12/20  Iteration 2093/3560 Training loss: 1.6514 1.2082 sec/batch\n",
      "Epoch 12/20  Iteration 2094/3560 Training loss: 1.6514 1.0170 sec/batch\n",
      "Epoch 12/20  Iteration 2095/3560 Training loss: 1.6516 1.0178 sec/batch\n",
      "Epoch 12/20  Iteration 2096/3560 Training loss: 1.6516 0.9085 sec/batch\n",
      "Epoch 12/20  Iteration 2097/3560 Training loss: 1.6517 0.9628 sec/batch\n",
      "Epoch 12/20  Iteration 2098/3560 Training loss: 1.6516 0.9190 sec/batch\n",
      "Epoch 12/20  Iteration 2099/3560 Training loss: 1.6519 0.9071 sec/batch\n",
      "Epoch 12/20  Iteration 2100/3560 Training loss: 1.6518 1.1110 sec/batch\n",
      "Epoch 12/20  Iteration 2101/3560 Training loss: 1.6517 1.0700 sec/batch\n",
      "Epoch 12/20  Iteration 2102/3560 Training loss: 1.6518 0.9855 sec/batch\n",
      "Epoch 12/20  Iteration 2103/3560 Training loss: 1.6516 1.0130 sec/batch\n",
      "Epoch 12/20  Iteration 2104/3560 Training loss: 1.6518 0.9234 sec/batch\n",
      "Epoch 12/20  Iteration 2105/3560 Training loss: 1.6519 0.9076 sec/batch\n",
      "Epoch 12/20  Iteration 2106/3560 Training loss: 1.6521 0.8826 sec/batch\n",
      "Epoch 12/20  Iteration 2107/3560 Training loss: 1.6520 0.8850 sec/batch\n",
      "Epoch 12/20  Iteration 2108/3560 Training loss: 1.6519 1.0207 sec/batch\n",
      "Epoch 12/20  Iteration 2109/3560 Training loss: 1.6516 1.0593 sec/batch\n",
      "Epoch 12/20  Iteration 2110/3560 Training loss: 1.6516 0.9926 sec/batch\n",
      "Epoch 12/20  Iteration 2111/3560 Training loss: 1.6517 1.0528 sec/batch\n",
      "Epoch 12/20  Iteration 2112/3560 Training loss: 1.6517 0.8803 sec/batch\n",
      "Epoch 12/20  Iteration 2113/3560 Training loss: 1.6517 0.9247 sec/batch\n",
      "Epoch 12/20  Iteration 2114/3560 Training loss: 1.6517 1.0483 sec/batch\n",
      "Epoch 12/20  Iteration 2115/3560 Training loss: 1.6517 0.9065 sec/batch\n",
      "Epoch 12/20  Iteration 2116/3560 Training loss: 1.6516 0.9840 sec/batch\n",
      "Epoch 12/20  Iteration 2117/3560 Training loss: 1.6514 1.0816 sec/batch\n",
      "Epoch 12/20  Iteration 2118/3560 Training loss: 1.6515 1.0924 sec/batch\n",
      "Epoch 12/20  Iteration 2119/3560 Training loss: 1.6517 0.9120 sec/batch\n",
      "Epoch 12/20  Iteration 2120/3560 Training loss: 1.6516 0.9966 sec/batch\n",
      "Epoch 12/20  Iteration 2121/3560 Training loss: 1.6516 0.9009 sec/batch\n",
      "Epoch 12/20  Iteration 2122/3560 Training loss: 1.6517 0.9209 sec/batch\n",
      "Epoch 12/20  Iteration 2123/3560 Training loss: 1.6516 0.9157 sec/batch\n",
      "Epoch 12/20  Iteration 2124/3560 Training loss: 1.6515 1.0880 sec/batch\n",
      "Epoch 12/20  Iteration 2125/3560 Training loss: 1.6516 0.9847 sec/batch\n",
      "Epoch 12/20  Iteration 2126/3560 Training loss: 1.6521 1.0881 sec/batch\n",
      "Epoch 12/20  Iteration 2127/3560 Training loss: 1.6520 0.9612 sec/batch\n",
      "Epoch 12/20  Iteration 2128/3560 Training loss: 1.6520 0.8617 sec/batch\n",
      "Epoch 12/20  Iteration 2129/3560 Training loss: 1.6519 0.9883 sec/batch\n",
      "Epoch 12/20  Iteration 2130/3560 Training loss: 1.6517 0.9322 sec/batch\n",
      "Epoch 12/20  Iteration 2131/3560 Training loss: 1.6517 0.9081 sec/batch\n",
      "Epoch 12/20  Iteration 2132/3560 Training loss: 1.6518 1.0717 sec/batch\n",
      "Epoch 12/20  Iteration 2133/3560 Training loss: 1.6518 1.1647 sec/batch\n",
      "Epoch 12/20  Iteration 2134/3560 Training loss: 1.6517 0.9382 sec/batch\n",
      "Epoch 12/20  Iteration 2135/3560 Training loss: 1.6516 1.0635 sec/batch\n",
      "Epoch 12/20  Iteration 2136/3560 Training loss: 1.6517 0.9189 sec/batch\n",
      "Epoch 13/20  Iteration 2137/3560 Training loss: 1.7276 0.9708 sec/batch\n",
      "Epoch 13/20  Iteration 2138/3560 Training loss: 1.6857 0.9977 sec/batch\n",
      "Epoch 13/20  Iteration 2139/3560 Training loss: 1.6714 0.8593 sec/batch\n",
      "Epoch 13/20  Iteration 2140/3560 Training loss: 1.6621 1.0797 sec/batch\n",
      "Epoch 13/20  Iteration 2141/3560 Training loss: 1.6569 1.1260 sec/batch\n",
      "Epoch 13/20  Iteration 2142/3560 Training loss: 1.6457 0.9544 sec/batch\n",
      "Epoch 13/20  Iteration 2143/3560 Training loss: 1.6458 0.9981 sec/batch\n",
      "Epoch 13/20  Iteration 2144/3560 Training loss: 1.6433 0.9313 sec/batch\n",
      "Epoch 13/20  Iteration 2145/3560 Training loss: 1.6447 0.9818 sec/batch\n",
      "Epoch 13/20  Iteration 2146/3560 Training loss: 1.6437 0.8950 sec/batch\n",
      "Epoch 13/20  Iteration 2147/3560 Training loss: 1.6406 0.9608 sec/batch\n",
      "Epoch 13/20  Iteration 2148/3560 Training loss: 1.6387 1.0639 sec/batch\n",
      "Epoch 13/20  Iteration 2149/3560 Training loss: 1.6389 1.0660 sec/batch\n",
      "Epoch 13/20  Iteration 2150/3560 Training loss: 1.6414 0.9289 sec/batch\n",
      "Epoch 13/20  Iteration 2151/3560 Training loss: 1.6411 0.9835 sec/batch\n",
      "Epoch 13/20  Iteration 2152/3560 Training loss: 1.6393 0.8825 sec/batch\n",
      "Epoch 13/20  Iteration 2153/3560 Training loss: 1.6397 0.9727 sec/batch\n",
      "Epoch 13/20  Iteration 2154/3560 Training loss: 1.6412 0.9239 sec/batch\n",
      "Epoch 13/20  Iteration 2155/3560 Training loss: 1.6416 0.9651 sec/batch\n",
      "Epoch 13/20  Iteration 2156/3560 Training loss: 1.6424 1.1076 sec/batch\n",
      "Epoch 13/20  Iteration 2157/3560 Training loss: 1.6419 1.0621 sec/batch\n",
      "Epoch 13/20  Iteration 2158/3560 Training loss: 1.6423 0.9899 sec/batch\n",
      "Epoch 13/20  Iteration 2159/3560 Training loss: 1.6417 0.9581 sec/batch\n",
      "Epoch 13/20  Iteration 2160/3560 Training loss: 1.6411 0.9012 sec/batch\n",
      "Epoch 13/20  Iteration 2161/3560 Training loss: 1.6412 0.9791 sec/batch\n",
      "Epoch 13/20  Iteration 2162/3560 Training loss: 1.6398 0.9228 sec/batch\n",
      "Epoch 13/20  Iteration 2163/3560 Training loss: 1.6383 0.8808 sec/batch\n",
      "Epoch 13/20  Iteration 2164/3560 Training loss: 1.6390 1.0430 sec/batch\n",
      "Epoch 13/20  Iteration 2165/3560 Training loss: 1.6397 1.1091 sec/batch\n",
      "Epoch 13/20  Iteration 2166/3560 Training loss: 1.6403 0.9421 sec/batch\n",
      "Epoch 13/20  Iteration 2167/3560 Training loss: 1.6400 0.9210 sec/batch\n",
      "Epoch 13/20  Iteration 2168/3560 Training loss: 1.6390 0.9666 sec/batch\n",
      "Epoch 13/20  Iteration 2169/3560 Training loss: 1.6392 0.9088 sec/batch\n",
      "Epoch 13/20  Iteration 2170/3560 Training loss: 1.6397 0.9836 sec/batch\n",
      "Epoch 13/20  Iteration 2171/3560 Training loss: 1.6395 0.9373 sec/batch\n",
      "Epoch 13/20  Iteration 2172/3560 Training loss: 1.6392 1.1632 sec/batch\n",
      "Epoch 13/20  Iteration 2173/3560 Training loss: 1.6384 1.0352 sec/batch\n",
      "Epoch 13/20  Iteration 2174/3560 Training loss: 1.6374 0.9516 sec/batch\n",
      "Epoch 13/20  Iteration 2175/3560 Training loss: 1.6361 0.9456 sec/batch\n",
      "Epoch 13/20  Iteration 2176/3560 Training loss: 1.6355 0.9346 sec/batch\n",
      "Epoch 13/20  Iteration 2177/3560 Training loss: 1.6347 1.0296 sec/batch\n",
      "Epoch 13/20  Iteration 2178/3560 Training loss: 1.6354 0.9181 sec/batch\n",
      "Epoch 13/20  Iteration 2179/3560 Training loss: 1.6350 0.9258 sec/batch\n",
      "Epoch 13/20  Iteration 2180/3560 Training loss: 1.6344 1.0677 sec/batch\n",
      "Epoch 13/20  Iteration 2181/3560 Training loss: 1.6347 1.2008 sec/batch\n",
      "Epoch 13/20  Iteration 2182/3560 Training loss: 1.6335 1.0391 sec/batch\n",
      "Epoch 13/20  Iteration 2183/3560 Training loss: 1.6331 0.9336 sec/batch\n",
      "Epoch 13/20  Iteration 2184/3560 Training loss: 1.6327 0.9492 sec/batch\n",
      "Epoch 13/20  Iteration 2185/3560 Training loss: 1.6327 0.9242 sec/batch\n",
      "Epoch 13/20  Iteration 2186/3560 Training loss: 1.6333 1.0912 sec/batch\n",
      "Epoch 13/20  Iteration 2187/3560 Training loss: 1.6328 0.8874 sec/batch\n",
      "Epoch 13/20  Iteration 2188/3560 Training loss: 1.6337 1.0111 sec/batch\n",
      "Epoch 13/20  Iteration 2189/3560 Training loss: 1.6336 1.1054 sec/batch\n",
      "Epoch 13/20  Iteration 2190/3560 Training loss: 1.6335 1.0270 sec/batch\n",
      "Epoch 13/20  Iteration 2191/3560 Training loss: 1.6335 0.9510 sec/batch\n",
      "Epoch 13/20  Iteration 2192/3560 Training loss: 1.6336 0.9751 sec/batch\n",
      "Epoch 13/20  Iteration 2193/3560 Training loss: 1.6340 0.9160 sec/batch\n",
      "Epoch 13/20  Iteration 2194/3560 Training loss: 1.6338 0.9638 sec/batch\n",
      "Epoch 13/20  Iteration 2195/3560 Training loss: 1.6333 0.8893 sec/batch\n",
      "Epoch 13/20  Iteration 2196/3560 Training loss: 1.6338 1.0276 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2197/3560 Training loss: 1.6337 1.2059 sec/batch\n",
      "Epoch 13/20  Iteration 2198/3560 Training loss: 1.6346 0.9717 sec/batch\n",
      "Epoch 13/20  Iteration 2199/3560 Training loss: 1.6350 0.9429 sec/batch\n",
      "Epoch 13/20  Iteration 2200/3560 Training loss: 1.6354 0.9072 sec/batch\n",
      "Epoch 13/20  Iteration 2201/3560 Training loss: 1.6352 0.9966 sec/batch\n",
      "Epoch 13/20  Iteration 2202/3560 Training loss: 1.6355 0.9776 sec/batch\n",
      "Epoch 13/20  Iteration 2203/3560 Training loss: 1.6357 0.9179 sec/batch\n",
      "Epoch 13/20  Iteration 2204/3560 Training loss: 1.6354 1.1428 sec/batch\n",
      "Epoch 13/20  Iteration 2205/3560 Training loss: 1.6354 1.1145 sec/batch\n",
      "Epoch 13/20  Iteration 2206/3560 Training loss: 1.6353 0.9990 sec/batch\n",
      "Epoch 13/20  Iteration 2207/3560 Training loss: 1.6360 0.9696 sec/batch\n",
      "Epoch 13/20  Iteration 2208/3560 Training loss: 1.6362 1.0385 sec/batch\n",
      "Epoch 13/20  Iteration 2209/3560 Training loss: 1.6366 0.9927 sec/batch\n",
      "Epoch 13/20  Iteration 2210/3560 Training loss: 1.6363 0.9152 sec/batch\n",
      "Epoch 13/20  Iteration 2211/3560 Training loss: 1.6363 1.0294 sec/batch\n",
      "Epoch 13/20  Iteration 2212/3560 Training loss: 1.6366 1.0816 sec/batch\n",
      "Epoch 13/20  Iteration 2213/3560 Training loss: 1.6365 1.0089 sec/batch\n",
      "Epoch 13/20  Iteration 2214/3560 Training loss: 1.6366 0.9738 sec/batch\n",
      "Epoch 13/20  Iteration 2215/3560 Training loss: 1.6361 0.8880 sec/batch\n",
      "Epoch 13/20  Iteration 2216/3560 Training loss: 1.6359 0.9476 sec/batch\n",
      "Epoch 13/20  Iteration 2217/3560 Training loss: 1.6354 0.9736 sec/batch\n",
      "Epoch 13/20  Iteration 2218/3560 Training loss: 1.6356 0.9636 sec/batch\n",
      "Epoch 13/20  Iteration 2219/3560 Training loss: 1.6351 1.0572 sec/batch\n",
      "Epoch 13/20  Iteration 2220/3560 Training loss: 1.6350 1.0696 sec/batch\n",
      "Epoch 13/20  Iteration 2221/3560 Training loss: 1.6346 1.0572 sec/batch\n",
      "Epoch 13/20  Iteration 2222/3560 Training loss: 1.6343 1.0146 sec/batch\n",
      "Epoch 13/20  Iteration 2223/3560 Training loss: 1.6341 0.9809 sec/batch\n",
      "Epoch 13/20  Iteration 2224/3560 Training loss: 1.6338 0.9027 sec/batch\n",
      "Epoch 13/20  Iteration 2225/3560 Training loss: 1.6333 0.9522 sec/batch\n",
      "Epoch 13/20  Iteration 2226/3560 Training loss: 1.6334 0.9819 sec/batch\n",
      "Epoch 13/20  Iteration 2227/3560 Training loss: 1.6331 0.9589 sec/batch\n",
      "Epoch 13/20  Iteration 2228/3560 Training loss: 1.6328 1.1385 sec/batch\n",
      "Epoch 13/20  Iteration 2229/3560 Training loss: 1.6324 0.9654 sec/batch\n",
      "Epoch 13/20  Iteration 2230/3560 Training loss: 1.6321 0.9531 sec/batch\n",
      "Epoch 13/20  Iteration 2231/3560 Training loss: 1.6319 0.9741 sec/batch\n",
      "Epoch 13/20  Iteration 2232/3560 Training loss: 1.6319 0.8651 sec/batch\n",
      "Epoch 13/20  Iteration 2233/3560 Training loss: 1.6318 0.9021 sec/batch\n",
      "Epoch 13/20  Iteration 2234/3560 Training loss: 1.6313 0.9920 sec/batch\n",
      "Epoch 13/20  Iteration 2235/3560 Training loss: 1.6310 1.0006 sec/batch\n",
      "Epoch 13/20  Iteration 2236/3560 Training loss: 1.6305 1.0652 sec/batch\n",
      "Epoch 13/20  Iteration 2237/3560 Training loss: 1.6305 1.0509 sec/batch\n",
      "Epoch 13/20  Iteration 2238/3560 Training loss: 1.6303 1.0297 sec/batch\n",
      "Epoch 13/20  Iteration 2239/3560 Training loss: 1.6300 0.9762 sec/batch\n",
      "Epoch 13/20  Iteration 2240/3560 Training loss: 1.6299 0.8951 sec/batch\n",
      "Epoch 13/20  Iteration 2241/3560 Training loss: 1.6296 0.9581 sec/batch\n",
      "Epoch 13/20  Iteration 2242/3560 Training loss: 1.6294 0.8895 sec/batch\n",
      "Epoch 13/20  Iteration 2243/3560 Training loss: 1.6293 1.0424 sec/batch\n",
      "Epoch 13/20  Iteration 2244/3560 Training loss: 1.6292 1.0129 sec/batch\n",
      "Epoch 13/20  Iteration 2245/3560 Training loss: 1.6290 1.0556 sec/batch\n",
      "Epoch 13/20  Iteration 2246/3560 Training loss: 1.6290 0.9299 sec/batch\n",
      "Epoch 13/20  Iteration 2247/3560 Training loss: 1.6289 0.9797 sec/batch\n",
      "Epoch 13/20  Iteration 2248/3560 Training loss: 1.6288 0.8985 sec/batch\n",
      "Epoch 13/20  Iteration 2249/3560 Training loss: 1.6285 0.9893 sec/batch\n",
      "Epoch 13/20  Iteration 2250/3560 Training loss: 1.6283 0.8956 sec/batch\n",
      "Epoch 13/20  Iteration 2251/3560 Training loss: 1.6280 1.0146 sec/batch\n",
      "Epoch 13/20  Iteration 2252/3560 Training loss: 1.6277 1.0432 sec/batch\n",
      "Epoch 13/20  Iteration 2253/3560 Training loss: 1.6277 1.0951 sec/batch\n",
      "Epoch 13/20  Iteration 2254/3560 Training loss: 1.6276 0.9305 sec/batch\n",
      "Epoch 13/20  Iteration 2255/3560 Training loss: 1.6276 0.9125 sec/batch\n",
      "Epoch 13/20  Iteration 2256/3560 Training loss: 1.6275 1.0088 sec/batch\n",
      "Epoch 13/20  Iteration 2257/3560 Training loss: 1.6274 0.8879 sec/batch\n",
      "Epoch 13/20  Iteration 2258/3560 Training loss: 1.6271 0.8848 sec/batch\n",
      "Epoch 13/20  Iteration 2259/3560 Training loss: 1.6267 1.0224 sec/batch\n",
      "Epoch 13/20  Iteration 2260/3560 Training loss: 1.6267 1.1321 sec/batch\n",
      "Epoch 13/20  Iteration 2261/3560 Training loss: 1.6267 1.0263 sec/batch\n",
      "Epoch 13/20  Iteration 2262/3560 Training loss: 1.6263 1.0078 sec/batch\n",
      "Epoch 13/20  Iteration 2263/3560 Training loss: 1.6264 0.9425 sec/batch\n",
      "Epoch 13/20  Iteration 2264/3560 Training loss: 1.6263 1.0590 sec/batch\n",
      "Epoch 13/20  Iteration 2265/3560 Training loss: 1.6263 0.9177 sec/batch\n",
      "Epoch 13/20  Iteration 2266/3560 Training loss: 1.6261 1.0370 sec/batch\n",
      "Epoch 13/20  Iteration 2267/3560 Training loss: 1.6257 0.9556 sec/batch\n",
      "Epoch 13/20  Iteration 2268/3560 Training loss: 1.6254 0.9108 sec/batch\n",
      "Epoch 13/20  Iteration 2269/3560 Training loss: 1.6255 1.0768 sec/batch\n",
      "Epoch 13/20  Iteration 2270/3560 Training loss: 1.6255 0.9889 sec/batch\n",
      "Epoch 13/20  Iteration 2271/3560 Training loss: 1.6255 0.9288 sec/batch\n",
      "Epoch 13/20  Iteration 2272/3560 Training loss: 1.6256 0.9752 sec/batch\n",
      "Epoch 13/20  Iteration 2273/3560 Training loss: 1.6257 0.9175 sec/batch\n",
      "Epoch 13/20  Iteration 2274/3560 Training loss: 1.6258 1.0234 sec/batch\n",
      "Epoch 13/20  Iteration 2275/3560 Training loss: 1.6259 1.0708 sec/batch\n",
      "Epoch 13/20  Iteration 2276/3560 Training loss: 1.6258 0.9963 sec/batch\n",
      "Epoch 13/20  Iteration 2277/3560 Training loss: 1.6261 0.9666 sec/batch\n",
      "Epoch 13/20  Iteration 2278/3560 Training loss: 1.6260 0.9675 sec/batch\n",
      "Epoch 13/20  Iteration 2279/3560 Training loss: 1.6259 0.9430 sec/batch\n",
      "Epoch 13/20  Iteration 2280/3560 Training loss: 1.6260 0.9609 sec/batch\n",
      "Epoch 13/20  Iteration 2281/3560 Training loss: 1.6258 1.0477 sec/batch\n",
      "Epoch 13/20  Iteration 2282/3560 Training loss: 1.6259 1.0146 sec/batch\n",
      "Epoch 13/20  Iteration 2283/3560 Training loss: 1.6260 0.9952 sec/batch\n",
      "Epoch 13/20  Iteration 2284/3560 Training loss: 1.6262 1.0619 sec/batch\n",
      "Epoch 13/20  Iteration 2285/3560 Training loss: 1.6262 0.9254 sec/batch\n",
      "Epoch 13/20  Iteration 2286/3560 Training loss: 1.6260 1.0178 sec/batch\n",
      "Epoch 13/20  Iteration 2287/3560 Training loss: 1.6257 0.9334 sec/batch\n",
      "Epoch 13/20  Iteration 2288/3560 Training loss: 1.6257 0.9577 sec/batch\n",
      "Epoch 13/20  Iteration 2289/3560 Training loss: 1.6258 1.0642 sec/batch\n",
      "Epoch 13/20  Iteration 2290/3560 Training loss: 1.6258 0.9288 sec/batch\n",
      "Epoch 13/20  Iteration 2291/3560 Training loss: 1.6258 1.0333 sec/batch\n",
      "Epoch 13/20  Iteration 2292/3560 Training loss: 1.6258 1.0618 sec/batch\n",
      "Epoch 13/20  Iteration 2293/3560 Training loss: 1.6259 0.9415 sec/batch\n",
      "Epoch 13/20  Iteration 2294/3560 Training loss: 1.6259 0.9474 sec/batch\n",
      "Epoch 13/20  Iteration 2295/3560 Training loss: 1.6257 0.9563 sec/batch\n",
      "Epoch 13/20  Iteration 2296/3560 Training loss: 1.6258 0.9508 sec/batch\n",
      "Epoch 13/20  Iteration 2297/3560 Training loss: 1.6260 0.9857 sec/batch\n",
      "Epoch 13/20  Iteration 2298/3560 Training loss: 1.6259 0.9716 sec/batch\n",
      "Epoch 13/20  Iteration 2299/3560 Training loss: 1.6259 0.9475 sec/batch\n",
      "Epoch 13/20  Iteration 2300/3560 Training loss: 1.6260 1.1476 sec/batch\n",
      "Epoch 13/20  Iteration 2301/3560 Training loss: 1.6260 0.9356 sec/batch\n",
      "Epoch 13/20  Iteration 2302/3560 Training loss: 1.6260 0.9473 sec/batch\n",
      "Epoch 13/20  Iteration 2303/3560 Training loss: 1.6261 0.9699 sec/batch\n",
      "Epoch 13/20  Iteration 2304/3560 Training loss: 1.6265 0.9627 sec/batch\n",
      "Epoch 13/20  Iteration 2305/3560 Training loss: 1.6264 0.9262 sec/batch\n",
      "Epoch 13/20  Iteration 2306/3560 Training loss: 1.6263 0.9920 sec/batch\n",
      "Epoch 13/20  Iteration 2307/3560 Training loss: 1.6262 1.0650 sec/batch\n",
      "Epoch 13/20  Iteration 2308/3560 Training loss: 1.6260 1.0395 sec/batch\n",
      "Epoch 13/20  Iteration 2309/3560 Training loss: 1.6261 1.0675 sec/batch\n",
      "Epoch 13/20  Iteration 2310/3560 Training loss: 1.6261 0.9906 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2311/3560 Training loss: 1.6261 0.9600 sec/batch\n",
      "Epoch 13/20  Iteration 2312/3560 Training loss: 1.6261 1.0173 sec/batch\n",
      "Epoch 13/20  Iteration 2313/3560 Training loss: 1.6259 0.9350 sec/batch\n",
      "Epoch 13/20  Iteration 2314/3560 Training loss: 1.6260 0.9497 sec/batch\n",
      "Epoch 14/20  Iteration 2315/3560 Training loss: 1.7007 0.8728 sec/batch\n",
      "Epoch 14/20  Iteration 2316/3560 Training loss: 1.6589 1.0548 sec/batch\n",
      "Epoch 14/20  Iteration 2317/3560 Training loss: 1.6447 0.9899 sec/batch\n",
      "Epoch 14/20  Iteration 2318/3560 Training loss: 1.6387 0.9722 sec/batch\n",
      "Epoch 14/20  Iteration 2319/3560 Training loss: 1.6326 0.9036 sec/batch\n",
      "Epoch 14/20  Iteration 2320/3560 Training loss: 1.6231 1.0101 sec/batch\n",
      "Epoch 14/20  Iteration 2321/3560 Training loss: 1.6233 0.9925 sec/batch\n",
      "Epoch 14/20  Iteration 2322/3560 Training loss: 1.6209 0.9253 sec/batch\n",
      "Epoch 14/20  Iteration 2323/3560 Training loss: 1.6223 0.9565 sec/batch\n",
      "Epoch 14/20  Iteration 2324/3560 Training loss: 1.6221 1.1102 sec/batch\n",
      "Epoch 14/20  Iteration 2325/3560 Training loss: 1.6190 1.0684 sec/batch\n",
      "Epoch 14/20  Iteration 2326/3560 Training loss: 1.6174 0.9197 sec/batch\n",
      "Epoch 14/20  Iteration 2327/3560 Training loss: 1.6167 1.0183 sec/batch\n",
      "Epoch 14/20  Iteration 2328/3560 Training loss: 1.6187 0.9700 sec/batch\n",
      "Epoch 14/20  Iteration 2329/3560 Training loss: 1.6177 0.8750 sec/batch\n",
      "Epoch 14/20  Iteration 2330/3560 Training loss: 1.6156 0.9584 sec/batch\n",
      "Epoch 14/20  Iteration 2331/3560 Training loss: 1.6159 1.0080 sec/batch\n",
      "Epoch 14/20  Iteration 2332/3560 Training loss: 1.6169 1.0697 sec/batch\n",
      "Epoch 14/20  Iteration 2333/3560 Training loss: 1.6174 1.0407 sec/batch\n",
      "Epoch 14/20  Iteration 2334/3560 Training loss: 1.6185 0.9050 sec/batch\n",
      "Epoch 14/20  Iteration 2335/3560 Training loss: 1.6175 0.9506 sec/batch\n",
      "Epoch 14/20  Iteration 2336/3560 Training loss: 1.6178 0.9632 sec/batch\n",
      "Epoch 14/20  Iteration 2337/3560 Training loss: 1.6172 0.9472 sec/batch\n",
      "Epoch 14/20  Iteration 2338/3560 Training loss: 1.6166 0.9754 sec/batch\n",
      "Epoch 14/20  Iteration 2339/3560 Training loss: 1.6165 0.9767 sec/batch\n",
      "Epoch 14/20  Iteration 2340/3560 Training loss: 1.6151 1.0756 sec/batch\n",
      "Epoch 14/20  Iteration 2341/3560 Training loss: 1.6138 1.0281 sec/batch\n",
      "Epoch 14/20  Iteration 2342/3560 Training loss: 1.6145 0.9962 sec/batch\n",
      "Epoch 14/20  Iteration 2343/3560 Training loss: 1.6152 1.1390 sec/batch\n",
      "Epoch 14/20  Iteration 2344/3560 Training loss: 1.6156 0.9374 sec/batch\n",
      "Epoch 14/20  Iteration 2345/3560 Training loss: 1.6155 1.0395 sec/batch\n",
      "Epoch 14/20  Iteration 2346/3560 Training loss: 1.6147 0.9693 sec/batch\n",
      "Epoch 14/20  Iteration 2347/3560 Training loss: 1.6150 1.0399 sec/batch\n",
      "Epoch 14/20  Iteration 2348/3560 Training loss: 1.6156 1.0348 sec/batch\n",
      "Epoch 14/20  Iteration 2349/3560 Training loss: 1.6154 1.0449 sec/batch\n",
      "Epoch 14/20  Iteration 2350/3560 Training loss: 1.6153 0.9547 sec/batch\n",
      "Epoch 14/20  Iteration 2351/3560 Training loss: 1.6148 1.0297 sec/batch\n",
      "Epoch 14/20  Iteration 2352/3560 Training loss: 1.6137 0.9605 sec/batch\n",
      "Epoch 14/20  Iteration 2353/3560 Training loss: 1.6124 0.8977 sec/batch\n",
      "Epoch 14/20  Iteration 2354/3560 Training loss: 1.6116 0.9467 sec/batch\n",
      "Epoch 14/20  Iteration 2355/3560 Training loss: 1.6112 1.0274 sec/batch\n",
      "Epoch 14/20  Iteration 2356/3560 Training loss: 1.6116 1.0311 sec/batch\n",
      "Epoch 14/20  Iteration 2357/3560 Training loss: 1.6111 1.0403 sec/batch\n",
      "Epoch 14/20  Iteration 2358/3560 Training loss: 1.6103 0.9452 sec/batch\n",
      "Epoch 14/20  Iteration 2359/3560 Training loss: 1.6106 0.9110 sec/batch\n",
      "Epoch 14/20  Iteration 2360/3560 Training loss: 1.6095 0.9602 sec/batch\n",
      "Epoch 14/20  Iteration 2361/3560 Training loss: 1.6091 0.9721 sec/batch\n",
      "Epoch 14/20  Iteration 2362/3560 Training loss: 1.6086 0.9548 sec/batch\n",
      "Epoch 14/20  Iteration 2363/3560 Training loss: 1.6083 0.9906 sec/batch\n",
      "Epoch 14/20  Iteration 2364/3560 Training loss: 1.6089 0.9930 sec/batch\n",
      "Epoch 14/20  Iteration 2365/3560 Training loss: 1.6084 0.9644 sec/batch\n",
      "Epoch 14/20  Iteration 2366/3560 Training loss: 1.6092 0.9922 sec/batch\n",
      "Epoch 14/20  Iteration 2367/3560 Training loss: 1.6091 0.9069 sec/batch\n",
      "Epoch 14/20  Iteration 2368/3560 Training loss: 1.6093 0.9043 sec/batch\n",
      "Epoch 14/20  Iteration 2369/3560 Training loss: 1.6091 0.9514 sec/batch\n",
      "Epoch 14/20  Iteration 2370/3560 Training loss: 1.6091 0.9077 sec/batch\n",
      "Epoch 14/20  Iteration 2371/3560 Training loss: 1.6094 0.9895 sec/batch\n",
      "Epoch 14/20  Iteration 2372/3560 Training loss: 1.6092 1.0226 sec/batch\n",
      "Epoch 14/20  Iteration 2373/3560 Training loss: 1.6086 0.9924 sec/batch\n",
      "Epoch 14/20  Iteration 2374/3560 Training loss: 1.6092 0.9369 sec/batch\n",
      "Epoch 14/20  Iteration 2375/3560 Training loss: 1.6091 0.9700 sec/batch\n",
      "Epoch 14/20  Iteration 2376/3560 Training loss: 1.6098 0.9209 sec/batch\n",
      "Epoch 14/20  Iteration 2377/3560 Training loss: 1.6102 1.0391 sec/batch\n",
      "Epoch 14/20  Iteration 2378/3560 Training loss: 1.6105 0.9063 sec/batch\n",
      "Epoch 14/20  Iteration 2379/3560 Training loss: 1.6102 1.0025 sec/batch\n",
      "Epoch 14/20  Iteration 2380/3560 Training loss: 1.6105 1.0514 sec/batch\n",
      "Epoch 14/20  Iteration 2381/3560 Training loss: 1.6107 0.9533 sec/batch\n",
      "Epoch 14/20  Iteration 2382/3560 Training loss: 1.6103 0.9869 sec/batch\n",
      "Epoch 14/20  Iteration 2383/3560 Training loss: 1.6102 0.9945 sec/batch\n",
      "Epoch 14/20  Iteration 2384/3560 Training loss: 1.6100 0.8681 sec/batch\n",
      "Epoch 14/20  Iteration 2385/3560 Training loss: 1.6107 0.9244 sec/batch\n",
      "Epoch 14/20  Iteration 2386/3560 Training loss: 1.6108 0.8585 sec/batch\n",
      "Epoch 14/20  Iteration 2387/3560 Training loss: 1.6111 1.0155 sec/batch\n",
      "Epoch 14/20  Iteration 2388/3560 Training loss: 1.6110 0.9640 sec/batch\n",
      "Epoch 14/20  Iteration 2389/3560 Training loss: 1.6110 1.0560 sec/batch\n",
      "Epoch 14/20  Iteration 2390/3560 Training loss: 1.6112 0.9567 sec/batch\n",
      "Epoch 14/20  Iteration 2391/3560 Training loss: 1.6112 0.9550 sec/batch\n",
      "Epoch 14/20  Iteration 2392/3560 Training loss: 1.6112 0.9308 sec/batch\n",
      "Epoch 14/20  Iteration 2393/3560 Training loss: 1.6107 0.9957 sec/batch\n",
      "Epoch 14/20  Iteration 2394/3560 Training loss: 1.6106 0.8715 sec/batch\n",
      "Epoch 14/20  Iteration 2395/3560 Training loss: 1.6102 1.0375 sec/batch\n",
      "Epoch 14/20  Iteration 2396/3560 Training loss: 1.6102 1.0261 sec/batch\n",
      "Epoch 14/20  Iteration 2397/3560 Training loss: 1.6096 1.0197 sec/batch\n",
      "Epoch 14/20  Iteration 2398/3560 Training loss: 1.6096 0.9932 sec/batch\n",
      "Epoch 14/20  Iteration 2399/3560 Training loss: 1.6092 0.9223 sec/batch\n",
      "Epoch 14/20  Iteration 2400/3560 Training loss: 1.6091 1.0110 sec/batch\n",
      "Epoch 14/20  Iteration 2401/3560 Training loss: 1.6089 0.9577 sec/batch\n",
      "Epoch 14/20  Iteration 2402/3560 Training loss: 1.6086 0.8728 sec/batch\n",
      "Epoch 14/20  Iteration 2403/3560 Training loss: 1.6081 0.9934 sec/batch\n",
      "Epoch 14/20  Iteration 2404/3560 Training loss: 1.6082 1.1000 sec/batch\n",
      "Epoch 14/20  Iteration 2405/3560 Training loss: 1.6080 0.9899 sec/batch\n",
      "Epoch 14/20  Iteration 2406/3560 Training loss: 1.6077 1.0060 sec/batch\n",
      "Epoch 14/20  Iteration 2407/3560 Training loss: 1.6074 0.9421 sec/batch\n",
      "Epoch 14/20  Iteration 2408/3560 Training loss: 1.6070 0.9338 sec/batch\n",
      "Epoch 14/20  Iteration 2409/3560 Training loss: 1.6067 0.8806 sec/batch\n",
      "Epoch 14/20  Iteration 2410/3560 Training loss: 1.6067 0.9402 sec/batch\n",
      "Epoch 14/20  Iteration 2411/3560 Training loss: 1.6066 1.0510 sec/batch\n",
      "Epoch 14/20  Iteration 2412/3560 Training loss: 1.6061 0.9182 sec/batch\n",
      "Epoch 14/20  Iteration 2413/3560 Training loss: 1.6056 0.9807 sec/batch\n",
      "Epoch 14/20  Iteration 2414/3560 Training loss: 1.6051 1.0586 sec/batch\n",
      "Epoch 14/20  Iteration 2415/3560 Training loss: 1.6050 1.0579 sec/batch\n",
      "Epoch 14/20  Iteration 2416/3560 Training loss: 1.6048 0.9773 sec/batch\n",
      "Epoch 14/20  Iteration 2417/3560 Training loss: 1.6047 0.8777 sec/batch\n",
      "Epoch 14/20  Iteration 2418/3560 Training loss: 1.6045 0.9558 sec/batch\n",
      "Epoch 14/20  Iteration 2419/3560 Training loss: 1.6044 0.8939 sec/batch\n",
      "Epoch 14/20  Iteration 2420/3560 Training loss: 1.6042 1.1082 sec/batch\n",
      "Epoch 14/20  Iteration 2421/3560 Training loss: 1.6041 0.9966 sec/batch\n",
      "Epoch 14/20  Iteration 2422/3560 Training loss: 1.6041 1.0143 sec/batch\n",
      "Epoch 14/20  Iteration 2423/3560 Training loss: 1.6041 1.0888 sec/batch\n",
      "Epoch 14/20  Iteration 2424/3560 Training loss: 1.6042 0.9849 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2425/3560 Training loss: 1.6041 0.9296 sec/batch\n",
      "Epoch 14/20  Iteration 2426/3560 Training loss: 1.6040 0.9353 sec/batch\n",
      "Epoch 14/20  Iteration 2427/3560 Training loss: 1.6038 1.0148 sec/batch\n",
      "Epoch 14/20  Iteration 2428/3560 Training loss: 1.6036 1.0393 sec/batch\n",
      "Epoch 14/20  Iteration 2429/3560 Training loss: 1.6034 0.9893 sec/batch\n",
      "Epoch 14/20  Iteration 2430/3560 Training loss: 1.6031 1.0222 sec/batch\n",
      "Epoch 14/20  Iteration 2431/3560 Training loss: 1.6031 0.9181 sec/batch\n",
      "Epoch 14/20  Iteration 2432/3560 Training loss: 1.6031 0.9376 sec/batch\n",
      "Epoch 14/20  Iteration 2433/3560 Training loss: 1.6029 1.0183 sec/batch\n",
      "Epoch 14/20  Iteration 2434/3560 Training loss: 1.6028 0.9404 sec/batch\n",
      "Epoch 14/20  Iteration 2435/3560 Training loss: 1.6027 1.0127 sec/batch\n",
      "Epoch 14/20  Iteration 2436/3560 Training loss: 1.6025 1.0726 sec/batch\n",
      "Epoch 14/20  Iteration 2437/3560 Training loss: 1.6020 1.0280 sec/batch\n",
      "Epoch 14/20  Iteration 2438/3560 Training loss: 1.6021 0.9827 sec/batch\n",
      "Epoch 14/20  Iteration 2439/3560 Training loss: 1.6020 0.9132 sec/batch\n",
      "Epoch 14/20  Iteration 2440/3560 Training loss: 1.6016 0.9084 sec/batch\n",
      "Epoch 14/20  Iteration 2441/3560 Training loss: 1.6017 0.9613 sec/batch\n",
      "Epoch 14/20  Iteration 2442/3560 Training loss: 1.6017 1.0602 sec/batch\n",
      "Epoch 14/20  Iteration 2443/3560 Training loss: 1.6015 0.9629 sec/batch\n",
      "Epoch 14/20  Iteration 2444/3560 Training loss: 1.6013 1.1749 sec/batch\n",
      "Epoch 14/20  Iteration 2445/3560 Training loss: 1.6009 1.2994 sec/batch\n",
      "Epoch 14/20  Iteration 2446/3560 Training loss: 1.6007 1.0066 sec/batch\n",
      "Epoch 14/20  Iteration 2447/3560 Training loss: 1.6007 1.0144 sec/batch\n",
      "Epoch 14/20  Iteration 2448/3560 Training loss: 1.6007 0.9437 sec/batch\n",
      "Epoch 14/20  Iteration 2449/3560 Training loss: 1.6007 0.9672 sec/batch\n",
      "Epoch 14/20  Iteration 2450/3560 Training loss: 1.6009 1.2163 sec/batch\n",
      "Epoch 14/20  Iteration 2451/3560 Training loss: 1.6010 1.1098 sec/batch\n",
      "Epoch 14/20  Iteration 2452/3560 Training loss: 1.6011 1.1836 sec/batch\n",
      "Epoch 14/20  Iteration 2453/3560 Training loss: 1.6011 1.1268 sec/batch\n",
      "Epoch 14/20  Iteration 2454/3560 Training loss: 1.6009 0.9945 sec/batch\n",
      "Epoch 14/20  Iteration 2455/3560 Training loss: 1.6013 1.0784 sec/batch\n",
      "Epoch 14/20  Iteration 2456/3560 Training loss: 1.6012 1.0619 sec/batch\n",
      "Epoch 14/20  Iteration 2457/3560 Training loss: 1.6012 0.9793 sec/batch\n",
      "Epoch 14/20  Iteration 2458/3560 Training loss: 1.6013 1.0770 sec/batch\n",
      "Epoch 14/20  Iteration 2459/3560 Training loss: 1.6012 0.9666 sec/batch\n",
      "Epoch 14/20  Iteration 2460/3560 Training loss: 1.6013 1.1389 sec/batch\n",
      "Epoch 14/20  Iteration 2461/3560 Training loss: 1.6014 0.9732 sec/batch\n",
      "Epoch 14/20  Iteration 2462/3560 Training loss: 1.6017 1.0984 sec/batch\n",
      "Epoch 14/20  Iteration 2463/3560 Training loss: 1.6017 1.0456 sec/batch\n",
      "Epoch 14/20  Iteration 2464/3560 Training loss: 1.6016 1.0429 sec/batch\n",
      "Epoch 14/20  Iteration 2465/3560 Training loss: 1.6013 1.0915 sec/batch\n",
      "Epoch 14/20  Iteration 2466/3560 Training loss: 1.6013 1.0811 sec/batch\n",
      "Epoch 14/20  Iteration 2467/3560 Training loss: 1.6014 1.0966 sec/batch\n",
      "Epoch 14/20  Iteration 2468/3560 Training loss: 1.6016 1.0685 sec/batch\n",
      "Epoch 14/20  Iteration 2469/3560 Training loss: 1.6015 1.1986 sec/batch\n",
      "Epoch 14/20  Iteration 2470/3560 Training loss: 1.6015 1.0423 sec/batch\n",
      "Epoch 14/20  Iteration 2471/3560 Training loss: 1.6016 1.0037 sec/batch\n",
      "Epoch 14/20  Iteration 2472/3560 Training loss: 1.6016 1.0748 sec/batch\n",
      "Epoch 14/20  Iteration 2473/3560 Training loss: 1.6015 1.0604 sec/batch\n",
      "Epoch 14/20  Iteration 2474/3560 Training loss: 1.6016 1.0805 sec/batch\n",
      "Epoch 14/20  Iteration 2475/3560 Training loss: 1.6017 1.3045 sec/batch\n",
      "Epoch 14/20  Iteration 2476/3560 Training loss: 1.6016 1.2919 sec/batch\n",
      "Epoch 14/20  Iteration 2477/3560 Training loss: 1.6017 1.0128 sec/batch\n",
      "Epoch 14/20  Iteration 2478/3560 Training loss: 1.6018 1.0180 sec/batch\n",
      "Epoch 14/20  Iteration 2479/3560 Training loss: 1.6018 0.9022 sec/batch\n",
      "Epoch 14/20  Iteration 2480/3560 Training loss: 1.6018 0.9944 sec/batch\n",
      "Epoch 14/20  Iteration 2481/3560 Training loss: 1.6019 1.0566 sec/batch\n",
      "Epoch 14/20  Iteration 2482/3560 Training loss: 1.6023 1.0649 sec/batch\n",
      "Epoch 14/20  Iteration 2483/3560 Training loss: 1.6022 1.1422 sec/batch\n",
      "Epoch 14/20  Iteration 2484/3560 Training loss: 1.6022 1.0266 sec/batch\n",
      "Epoch 14/20  Iteration 2485/3560 Training loss: 1.6021 1.0863 sec/batch\n",
      "Epoch 14/20  Iteration 2486/3560 Training loss: 1.6019 0.9691 sec/batch\n",
      "Epoch 14/20  Iteration 2487/3560 Training loss: 1.6020 1.0893 sec/batch\n",
      "Epoch 14/20  Iteration 2488/3560 Training loss: 1.6020 1.0485 sec/batch\n",
      "Epoch 14/20  Iteration 2489/3560 Training loss: 1.6021 0.9613 sec/batch\n",
      "Epoch 14/20  Iteration 2490/3560 Training loss: 1.6019 1.0609 sec/batch\n",
      "Epoch 14/20  Iteration 2491/3560 Training loss: 1.6018 1.1253 sec/batch\n",
      "Epoch 14/20  Iteration 2492/3560 Training loss: 1.6019 1.0556 sec/batch\n",
      "Epoch 15/20  Iteration 2493/3560 Training loss: 1.6819 1.1116 sec/batch\n",
      "Epoch 15/20  Iteration 2494/3560 Training loss: 1.6340 1.0441 sec/batch\n",
      "Epoch 15/20  Iteration 2495/3560 Training loss: 1.6229 1.0663 sec/batch\n",
      "Epoch 15/20  Iteration 2496/3560 Training loss: 1.6150 1.0354 sec/batch\n",
      "Epoch 15/20  Iteration 2497/3560 Training loss: 1.6105 0.9839 sec/batch\n",
      "Epoch 15/20  Iteration 2498/3560 Training loss: 1.6021 1.1452 sec/batch\n",
      "Epoch 15/20  Iteration 2499/3560 Training loss: 1.6012 1.0571 sec/batch\n",
      "Epoch 15/20  Iteration 2500/3560 Training loss: 1.5996 1.0760 sec/batch\n",
      "Epoch 15/20  Iteration 2501/3560 Training loss: 1.6009 1.1104 sec/batch\n",
      "Epoch 15/20  Iteration 2502/3560 Training loss: 1.6002 1.0594 sec/batch\n",
      "Epoch 15/20  Iteration 2503/3560 Training loss: 1.5964 1.0390 sec/batch\n",
      "Epoch 15/20  Iteration 2504/3560 Training loss: 1.5957 0.9878 sec/batch\n",
      "Epoch 15/20  Iteration 2505/3560 Training loss: 1.5950 0.9567 sec/batch\n",
      "Epoch 15/20  Iteration 2506/3560 Training loss: 1.5968 1.1747 sec/batch\n",
      "Epoch 15/20  Iteration 2507/3560 Training loss: 1.5958 1.0469 sec/batch\n",
      "Epoch 15/20  Iteration 2508/3560 Training loss: 1.5935 1.0899 sec/batch\n",
      "Epoch 15/20  Iteration 2509/3560 Training loss: 1.5940 1.0464 sec/batch\n",
      "Epoch 15/20  Iteration 2510/3560 Training loss: 1.5948 1.0822 sec/batch\n",
      "Epoch 15/20  Iteration 2511/3560 Training loss: 1.5948 0.9959 sec/batch\n",
      "Epoch 15/20  Iteration 2512/3560 Training loss: 1.5961 1.1687 sec/batch\n",
      "Epoch 15/20  Iteration 2513/3560 Training loss: 1.5953 0.9607 sec/batch\n",
      "Epoch 15/20  Iteration 2514/3560 Training loss: 1.5953 1.1181 sec/batch\n",
      "Epoch 15/20  Iteration 2515/3560 Training loss: 1.5946 0.9715 sec/batch\n",
      "Epoch 15/20  Iteration 2516/3560 Training loss: 1.5942 1.0945 sec/batch\n",
      "Epoch 15/20  Iteration 2517/3560 Training loss: 1.5941 1.0224 sec/batch\n",
      "Epoch 15/20  Iteration 2518/3560 Training loss: 1.5930 1.0133 sec/batch\n",
      "Epoch 15/20  Iteration 2519/3560 Training loss: 1.5917 1.0126 sec/batch\n",
      "Epoch 15/20  Iteration 2520/3560 Training loss: 1.5923 0.9637 sec/batch\n",
      "Epoch 15/20  Iteration 2521/3560 Training loss: 1.5933 1.0797 sec/batch\n",
      "Epoch 15/20  Iteration 2522/3560 Training loss: 1.5935 1.1400 sec/batch\n",
      "Epoch 15/20  Iteration 2523/3560 Training loss: 1.5933 1.0327 sec/batch\n",
      "Epoch 15/20  Iteration 2524/3560 Training loss: 1.5923 1.0485 sec/batch\n",
      "Epoch 15/20  Iteration 2525/3560 Training loss: 1.5926 1.0133 sec/batch\n",
      "Epoch 15/20  Iteration 2526/3560 Training loss: 1.5931 1.1787 sec/batch\n",
      "Epoch 15/20  Iteration 2527/3560 Training loss: 1.5927 1.0999 sec/batch\n",
      "Epoch 15/20  Iteration 2528/3560 Training loss: 1.5927 1.0876 sec/batch\n",
      "Epoch 15/20  Iteration 2529/3560 Training loss: 1.5921 1.0069 sec/batch\n",
      "Epoch 15/20  Iteration 2530/3560 Training loss: 1.5909 1.0661 sec/batch\n",
      "Epoch 15/20  Iteration 2531/3560 Training loss: 1.5895 1.0287 sec/batch\n",
      "Epoch 15/20  Iteration 2532/3560 Training loss: 1.5889 1.1319 sec/batch\n",
      "Epoch 15/20  Iteration 2533/3560 Training loss: 1.5882 0.9612 sec/batch\n",
      "Epoch 15/20  Iteration 2534/3560 Training loss: 1.5886 1.1144 sec/batch\n",
      "Epoch 15/20  Iteration 2535/3560 Training loss: 1.5881 1.1015 sec/batch\n",
      "Epoch 15/20  Iteration 2536/3560 Training loss: 1.5874 1.0089 sec/batch\n",
      "Epoch 15/20  Iteration 2537/3560 Training loss: 1.5877 1.0069 sec/batch\n",
      "Epoch 15/20  Iteration 2538/3560 Training loss: 1.5866 1.1347 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2539/3560 Training loss: 1.5861 0.9923 sec/batch\n",
      "Epoch 15/20  Iteration 2540/3560 Training loss: 1.5858 1.0775 sec/batch\n",
      "Epoch 15/20  Iteration 2541/3560 Training loss: 1.5856 1.0638 sec/batch\n",
      "Epoch 15/20  Iteration 2542/3560 Training loss: 1.5862 0.9607 sec/batch\n",
      "Epoch 15/20  Iteration 2543/3560 Training loss: 1.5859 1.0744 sec/batch\n",
      "Epoch 15/20  Iteration 2544/3560 Training loss: 1.5867 0.9551 sec/batch\n",
      "Epoch 15/20  Iteration 2545/3560 Training loss: 1.5865 0.9639 sec/batch\n",
      "Epoch 15/20  Iteration 2546/3560 Training loss: 1.5867 0.9999 sec/batch\n",
      "Epoch 15/20  Iteration 2547/3560 Training loss: 1.5866 0.9534 sec/batch\n",
      "Epoch 15/20  Iteration 2548/3560 Training loss: 1.5869 1.1036 sec/batch\n",
      "Epoch 15/20  Iteration 2549/3560 Training loss: 1.5875 1.1028 sec/batch\n",
      "Epoch 15/20  Iteration 2550/3560 Training loss: 1.5871 1.1811 sec/batch\n",
      "Epoch 15/20  Iteration 2551/3560 Training loss: 1.5868 1.0262 sec/batch\n",
      "Epoch 15/20  Iteration 2552/3560 Training loss: 1.5872 0.9752 sec/batch\n",
      "Epoch 15/20  Iteration 2553/3560 Training loss: 1.5872 1.0635 sec/batch\n",
      "Epoch 15/20  Iteration 2554/3560 Training loss: 1.5884 1.0686 sec/batch\n",
      "Epoch 15/20  Iteration 2555/3560 Training loss: 1.5886 0.9506 sec/batch\n",
      "Epoch 15/20  Iteration 2556/3560 Training loss: 1.5888 1.0830 sec/batch\n",
      "Epoch 15/20  Iteration 2557/3560 Training loss: 1.5889 1.0378 sec/batch\n",
      "Epoch 15/20  Iteration 2558/3560 Training loss: 1.5892 1.0012 sec/batch\n",
      "Epoch 15/20  Iteration 2559/3560 Training loss: 1.5894 1.0444 sec/batch\n",
      "Epoch 15/20  Iteration 2560/3560 Training loss: 1.5890 1.2157 sec/batch\n",
      "Epoch 15/20  Iteration 2561/3560 Training loss: 1.5891 1.1419 sec/batch\n",
      "Epoch 15/20  Iteration 2562/3560 Training loss: 1.5891 0.9794 sec/batch\n",
      "Epoch 15/20  Iteration 2563/3560 Training loss: 1.5897 1.0384 sec/batch\n",
      "Epoch 15/20  Iteration 2564/3560 Training loss: 1.5899 1.0440 sec/batch\n",
      "Epoch 15/20  Iteration 2565/3560 Training loss: 1.5903 1.0913 sec/batch\n",
      "Epoch 15/20  Iteration 2566/3560 Training loss: 1.5901 1.0264 sec/batch\n",
      "Epoch 15/20  Iteration 2567/3560 Training loss: 1.5899 0.9884 sec/batch\n",
      "Epoch 15/20  Iteration 2568/3560 Training loss: 1.5902 1.0540 sec/batch\n",
      "Epoch 15/20  Iteration 2569/3560 Training loss: 1.5901 1.0750 sec/batch\n",
      "Epoch 15/20  Iteration 2570/3560 Training loss: 1.5901 1.0115 sec/batch\n",
      "Epoch 15/20  Iteration 2571/3560 Training loss: 1.5894 1.0443 sec/batch\n",
      "Epoch 15/20  Iteration 2572/3560 Training loss: 1.5893 1.0453 sec/batch\n",
      "Epoch 15/20  Iteration 2573/3560 Training loss: 1.5888 1.0796 sec/batch\n",
      "Epoch 15/20  Iteration 2574/3560 Training loss: 1.5888 1.0606 sec/batch\n",
      "Epoch 15/20  Iteration 2575/3560 Training loss: 1.5882 0.9603 sec/batch\n",
      "Epoch 15/20  Iteration 2576/3560 Training loss: 1.5881 1.0497 sec/batch\n",
      "Epoch 15/20  Iteration 2577/3560 Training loss: 1.5878 1.0680 sec/batch\n",
      "Epoch 15/20  Iteration 2578/3560 Training loss: 1.5876 1.1440 sec/batch\n",
      "Epoch 15/20  Iteration 2579/3560 Training loss: 1.5874 1.0342 sec/batch\n",
      "Epoch 15/20  Iteration 2580/3560 Training loss: 1.5873 1.0417 sec/batch\n",
      "Epoch 15/20  Iteration 2581/3560 Training loss: 1.5868 1.1120 sec/batch\n",
      "Epoch 15/20  Iteration 2582/3560 Training loss: 1.5869 1.0964 sec/batch\n",
      "Epoch 15/20  Iteration 2583/3560 Training loss: 1.5865 1.0310 sec/batch\n",
      "Epoch 15/20  Iteration 2584/3560 Training loss: 1.5864 1.0198 sec/batch\n",
      "Epoch 15/20  Iteration 2585/3560 Training loss: 1.5861 1.0225 sec/batch\n",
      "Epoch 15/20  Iteration 2586/3560 Training loss: 1.5857 1.0488 sec/batch\n",
      "Epoch 15/20  Iteration 2587/3560 Training loss: 1.5854 1.1806 sec/batch\n",
      "Epoch 15/20  Iteration 2588/3560 Training loss: 1.5854 0.9773 sec/batch\n",
      "Epoch 15/20  Iteration 2589/3560 Training loss: 1.5853 1.1726 sec/batch\n",
      "Epoch 15/20  Iteration 2590/3560 Training loss: 1.5849 1.1174 sec/batch\n",
      "Epoch 15/20  Iteration 2591/3560 Training loss: 1.5845 1.0482 sec/batch\n",
      "Epoch 15/20  Iteration 2592/3560 Training loss: 1.5840 1.0113 sec/batch\n",
      "Epoch 15/20  Iteration 2593/3560 Training loss: 1.5839 1.0843 sec/batch\n",
      "Epoch 15/20  Iteration 2594/3560 Training loss: 1.5838 0.9834 sec/batch\n",
      "Epoch 15/20  Iteration 2595/3560 Training loss: 1.5837 0.8349 sec/batch\n",
      "Epoch 15/20  Iteration 2596/3560 Training loss: 1.5836 0.9952 sec/batch\n",
      "Epoch 15/20  Iteration 2597/3560 Training loss: 1.5834 1.2204 sec/batch\n",
      "Epoch 15/20  Iteration 2598/3560 Training loss: 1.5831 1.1838 sec/batch\n",
      "Epoch 15/20  Iteration 2599/3560 Training loss: 1.5831 1.0287 sec/batch\n",
      "Epoch 15/20  Iteration 2600/3560 Training loss: 1.5830 0.9892 sec/batch\n",
      "Epoch 15/20  Iteration 2601/3560 Training loss: 1.5828 1.0035 sec/batch\n",
      "Epoch 15/20  Iteration 2602/3560 Training loss: 1.5828 1.1473 sec/batch\n",
      "Epoch 15/20  Iteration 2603/3560 Training loss: 1.5826 0.9378 sec/batch\n",
      "Epoch 15/20  Iteration 2604/3560 Training loss: 1.5825 1.1714 sec/batch\n",
      "Epoch 15/20  Iteration 2605/3560 Training loss: 1.5823 1.1577 sec/batch\n",
      "Epoch 15/20  Iteration 2606/3560 Training loss: 1.5822 1.1906 sec/batch\n",
      "Epoch 15/20  Iteration 2607/3560 Training loss: 1.5818 1.1267 sec/batch\n",
      "Epoch 15/20  Iteration 2608/3560 Training loss: 1.5814 1.0541 sec/batch\n",
      "Epoch 15/20  Iteration 2609/3560 Training loss: 1.5814 1.1134 sec/batch\n",
      "Epoch 15/20  Iteration 2610/3560 Training loss: 1.5814 0.8480 sec/batch\n",
      "Epoch 15/20  Iteration 2611/3560 Training loss: 1.5813 1.0126 sec/batch\n",
      "Epoch 15/20  Iteration 2612/3560 Training loss: 1.5812 1.1514 sec/batch\n",
      "Epoch 15/20  Iteration 2613/3560 Training loss: 1.5810 1.0889 sec/batch\n",
      "Epoch 15/20  Iteration 2614/3560 Training loss: 1.5807 1.1203 sec/batch\n",
      "Epoch 15/20  Iteration 2615/3560 Training loss: 1.5803 0.9654 sec/batch\n",
      "Epoch 15/20  Iteration 2616/3560 Training loss: 1.5803 1.0070 sec/batch\n",
      "Epoch 15/20  Iteration 2617/3560 Training loss: 1.5803 1.0445 sec/batch\n",
      "Epoch 15/20  Iteration 2618/3560 Training loss: 1.5798 1.0107 sec/batch\n",
      "Epoch 15/20  Iteration 2619/3560 Training loss: 1.5799 1.0064 sec/batch\n",
      "Epoch 15/20  Iteration 2620/3560 Training loss: 1.5799 1.1512 sec/batch\n",
      "Epoch 15/20  Iteration 2621/3560 Training loss: 1.5798 1.0970 sec/batch\n",
      "Epoch 15/20  Iteration 2622/3560 Training loss: 1.5795 1.0010 sec/batch\n",
      "Epoch 15/20  Iteration 2623/3560 Training loss: 1.5791 0.9716 sec/batch\n",
      "Epoch 15/20  Iteration 2624/3560 Training loss: 1.5788 1.0341 sec/batch\n",
      "Epoch 15/20  Iteration 2625/3560 Training loss: 1.5789 1.0582 sec/batch\n",
      "Epoch 15/20  Iteration 2626/3560 Training loss: 1.5790 0.9804 sec/batch\n",
      "Epoch 15/20  Iteration 2627/3560 Training loss: 1.5790 1.0590 sec/batch\n",
      "Epoch 15/20  Iteration 2628/3560 Training loss: 1.5791 1.0750 sec/batch\n",
      "Epoch 15/20  Iteration 2629/3560 Training loss: 1.5792 1.1756 sec/batch\n",
      "Epoch 15/20  Iteration 2630/3560 Training loss: 1.5792 1.1614 sec/batch\n",
      "Epoch 15/20  Iteration 2631/3560 Training loss: 1.5792 1.0080 sec/batch\n",
      "Epoch 15/20  Iteration 2632/3560 Training loss: 1.5792 1.0551 sec/batch\n",
      "Epoch 15/20  Iteration 2633/3560 Training loss: 1.5794 0.9652 sec/batch\n",
      "Epoch 15/20  Iteration 2634/3560 Training loss: 1.5794 1.0387 sec/batch\n",
      "Epoch 15/20  Iteration 2635/3560 Training loss: 1.5794 1.0697 sec/batch\n",
      "Epoch 15/20  Iteration 2636/3560 Training loss: 1.5795 1.0725 sec/batch\n",
      "Epoch 15/20  Iteration 2637/3560 Training loss: 1.5794 1.1534 sec/batch\n",
      "Epoch 15/20  Iteration 2638/3560 Training loss: 1.5796 1.0602 sec/batch\n",
      "Epoch 15/20  Iteration 2639/3560 Training loss: 1.5797 1.0334 sec/batch\n",
      "Epoch 15/20  Iteration 2640/3560 Training loss: 1.5799 0.9174 sec/batch\n",
      "Epoch 15/20  Iteration 2641/3560 Training loss: 1.5800 1.0337 sec/batch\n",
      "Epoch 15/20  Iteration 2642/3560 Training loss: 1.5798 1.0206 sec/batch\n",
      "Epoch 15/20  Iteration 2643/3560 Training loss: 1.5795 1.0507 sec/batch\n",
      "Epoch 15/20  Iteration 2644/3560 Training loss: 1.5795 1.0143 sec/batch\n",
      "Epoch 15/20  Iteration 2645/3560 Training loss: 1.5795 1.2216 sec/batch\n",
      "Epoch 15/20  Iteration 2646/3560 Training loss: 1.5795 1.0927 sec/batch\n",
      "Epoch 15/20  Iteration 2647/3560 Training loss: 1.5795 0.9799 sec/batch\n",
      "Epoch 15/20  Iteration 2648/3560 Training loss: 1.5795 0.9944 sec/batch\n",
      "Epoch 15/20  Iteration 2649/3560 Training loss: 1.5796 1.2855 sec/batch\n",
      "Epoch 15/20  Iteration 2650/3560 Training loss: 1.5797 1.1776 sec/batch\n",
      "Epoch 15/20  Iteration 2651/3560 Training loss: 1.5795 0.9935 sec/batch\n",
      "Epoch 15/20  Iteration 2652/3560 Training loss: 1.5796 1.1314 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2653/3560 Training loss: 1.5798 1.0510 sec/batch\n",
      "Epoch 15/20  Iteration 2654/3560 Training loss: 1.5798 1.1011 sec/batch\n",
      "Epoch 15/20  Iteration 2655/3560 Training loss: 1.5799 1.0998 sec/batch\n",
      "Epoch 15/20  Iteration 2656/3560 Training loss: 1.5799 1.1155 sec/batch\n",
      "Epoch 15/20  Iteration 2657/3560 Training loss: 1.5799 0.9933 sec/batch\n",
      "Epoch 15/20  Iteration 2658/3560 Training loss: 1.5799 1.0683 sec/batch\n",
      "Epoch 15/20  Iteration 2659/3560 Training loss: 1.5800 0.9904 sec/batch\n",
      "Epoch 15/20  Iteration 2660/3560 Training loss: 1.5804 1.0643 sec/batch\n",
      "Epoch 15/20  Iteration 2661/3560 Training loss: 1.5804 1.2112 sec/batch\n",
      "Epoch 15/20  Iteration 2662/3560 Training loss: 1.5804 1.0489 sec/batch\n",
      "Epoch 15/20  Iteration 2663/3560 Training loss: 1.5803 1.0465 sec/batch\n",
      "Epoch 15/20  Iteration 2664/3560 Training loss: 1.5802 0.9993 sec/batch\n",
      "Epoch 15/20  Iteration 2665/3560 Training loss: 1.5803 1.0678 sec/batch\n",
      "Epoch 15/20  Iteration 2666/3560 Training loss: 1.5804 1.1029 sec/batch\n",
      "Epoch 15/20  Iteration 2667/3560 Training loss: 1.5804 0.9663 sec/batch\n",
      "Epoch 15/20  Iteration 2668/3560 Training loss: 1.5804 1.0945 sec/batch\n",
      "Epoch 15/20  Iteration 2669/3560 Training loss: 1.5803 1.2107 sec/batch\n",
      "Epoch 15/20  Iteration 2670/3560 Training loss: 1.5804 1.0314 sec/batch\n",
      "Epoch 16/20  Iteration 2671/3560 Training loss: 1.6588 0.9606 sec/batch\n",
      "Epoch 16/20  Iteration 2672/3560 Training loss: 1.6184 1.0713 sec/batch\n",
      "Epoch 16/20  Iteration 2673/3560 Training loss: 1.6063 1.0444 sec/batch\n",
      "Epoch 16/20  Iteration 2674/3560 Training loss: 1.6002 1.0543 sec/batch\n",
      "Epoch 16/20  Iteration 2675/3560 Training loss: 1.5943 1.0375 sec/batch\n",
      "Epoch 16/20  Iteration 2676/3560 Training loss: 1.5837 1.2014 sec/batch\n",
      "Epoch 16/20  Iteration 2677/3560 Training loss: 1.5834 1.2223 sec/batch\n",
      "Epoch 16/20  Iteration 2678/3560 Training loss: 1.5805 1.0431 sec/batch\n",
      "Epoch 16/20  Iteration 2679/3560 Training loss: 1.5815 1.0026 sec/batch\n",
      "Epoch 16/20  Iteration 2680/3560 Training loss: 1.5811 0.9378 sec/batch\n",
      "Epoch 16/20  Iteration 2681/3560 Training loss: 1.5776 1.0667 sec/batch\n",
      "Epoch 16/20  Iteration 2682/3560 Training loss: 1.5767 1.0274 sec/batch\n",
      "Epoch 16/20  Iteration 2683/3560 Training loss: 1.5766 1.0526 sec/batch\n",
      "Epoch 16/20  Iteration 2684/3560 Training loss: 1.5783 1.0248 sec/batch\n",
      "Epoch 16/20  Iteration 2685/3560 Training loss: 1.5770 1.0929 sec/batch\n",
      "Epoch 16/20  Iteration 2686/3560 Training loss: 1.5744 1.1912 sec/batch\n",
      "Epoch 16/20  Iteration 2687/3560 Training loss: 1.5747 0.9798 sec/batch\n",
      "Epoch 16/20  Iteration 2688/3560 Training loss: 1.5758 0.9725 sec/batch\n",
      "Epoch 16/20  Iteration 2689/3560 Training loss: 1.5764 0.9967 sec/batch\n",
      "Epoch 16/20  Iteration 2690/3560 Training loss: 1.5779 0.9766 sec/batch\n",
      "Epoch 16/20  Iteration 2691/3560 Training loss: 1.5770 1.1294 sec/batch\n",
      "Epoch 16/20  Iteration 2692/3560 Training loss: 1.5771 1.1364 sec/batch\n",
      "Epoch 16/20  Iteration 2693/3560 Training loss: 1.5765 1.1313 sec/batch\n",
      "Epoch 16/20  Iteration 2694/3560 Training loss: 1.5759 1.1397 sec/batch\n",
      "Epoch 16/20  Iteration 2695/3560 Training loss: 1.5760 1.0616 sec/batch\n",
      "Epoch 16/20  Iteration 2696/3560 Training loss: 1.5745 1.1034 sec/batch\n",
      "Epoch 16/20  Iteration 2697/3560 Training loss: 1.5730 1.1357 sec/batch\n",
      "Epoch 16/20  Iteration 2698/3560 Training loss: 1.5737 1.1456 sec/batch\n",
      "Epoch 16/20  Iteration 2699/3560 Training loss: 1.5742 1.1228 sec/batch\n",
      "Epoch 16/20  Iteration 2700/3560 Training loss: 1.5746 1.1437 sec/batch\n",
      "Epoch 16/20  Iteration 2701/3560 Training loss: 1.5740 1.1341 sec/batch\n",
      "Epoch 16/20  Iteration 2702/3560 Training loss: 1.5731 1.1343 sec/batch\n",
      "Epoch 16/20  Iteration 2703/3560 Training loss: 1.5731 1.0606 sec/batch\n",
      "Epoch 16/20  Iteration 2704/3560 Training loss: 1.5738 1.0564 sec/batch\n",
      "Epoch 16/20  Iteration 2705/3560 Training loss: 1.5734 1.0309 sec/batch\n",
      "Epoch 16/20  Iteration 2706/3560 Training loss: 1.5731 1.0689 sec/batch\n",
      "Epoch 16/20  Iteration 2707/3560 Training loss: 1.5724 1.0897 sec/batch\n",
      "Epoch 16/20  Iteration 2708/3560 Training loss: 1.5714 1.1360 sec/batch\n",
      "Epoch 16/20  Iteration 2709/3560 Training loss: 1.5700 1.0164 sec/batch\n",
      "Epoch 16/20  Iteration 2710/3560 Training loss: 1.5693 1.0124 sec/batch\n",
      "Epoch 16/20  Iteration 2711/3560 Training loss: 1.5685 0.9935 sec/batch\n",
      "Epoch 16/20  Iteration 2712/3560 Training loss: 1.5690 0.9888 sec/batch\n",
      "Epoch 16/20  Iteration 2713/3560 Training loss: 1.5685 1.0928 sec/batch\n",
      "Epoch 16/20  Iteration 2714/3560 Training loss: 1.5679 1.0332 sec/batch\n",
      "Epoch 16/20  Iteration 2715/3560 Training loss: 1.5682 1.0951 sec/batch\n",
      "Epoch 16/20  Iteration 2716/3560 Training loss: 1.5672 1.0952 sec/batch\n",
      "Epoch 16/20  Iteration 2717/3560 Training loss: 1.5669 0.9958 sec/batch\n",
      "Epoch 16/20  Iteration 2718/3560 Training loss: 1.5667 1.0092 sec/batch\n",
      "Epoch 16/20  Iteration 2719/3560 Training loss: 1.5666 0.9948 sec/batch\n",
      "Epoch 16/20  Iteration 2720/3560 Training loss: 1.5672 0.9727 sec/batch\n",
      "Epoch 16/20  Iteration 2721/3560 Training loss: 1.5667 1.0090 sec/batch\n",
      "Epoch 16/20  Iteration 2722/3560 Training loss: 1.5675 1.0271 sec/batch\n",
      "Epoch 16/20  Iteration 2723/3560 Training loss: 1.5674 1.0110 sec/batch\n",
      "Epoch 16/20  Iteration 2724/3560 Training loss: 1.5674 1.3278 sec/batch\n",
      "Epoch 16/20  Iteration 2725/3560 Training loss: 1.5672 1.1938 sec/batch\n",
      "Epoch 16/20  Iteration 2726/3560 Training loss: 1.5673 1.0048 sec/batch\n",
      "Epoch 16/20  Iteration 2727/3560 Training loss: 1.5679 1.0664 sec/batch\n",
      "Epoch 16/20  Iteration 2728/3560 Training loss: 1.5676 0.9388 sec/batch\n",
      "Epoch 16/20  Iteration 2729/3560 Training loss: 1.5670 1.0793 sec/batch\n",
      "Epoch 16/20  Iteration 2730/3560 Training loss: 1.5676 0.9359 sec/batch\n",
      "Epoch 16/20  Iteration 2731/3560 Training loss: 1.5674 1.1161 sec/batch\n",
      "Epoch 16/20  Iteration 2732/3560 Training loss: 1.5684 1.3129 sec/batch\n",
      "Epoch 16/20  Iteration 2733/3560 Training loss: 1.5687 1.2114 sec/batch\n",
      "Epoch 16/20  Iteration 2734/3560 Training loss: 1.5691 1.1278 sec/batch\n",
      "Epoch 16/20  Iteration 2735/3560 Training loss: 1.5689 1.0403 sec/batch\n",
      "Epoch 16/20  Iteration 2736/3560 Training loss: 1.5692 1.0173 sec/batch\n",
      "Epoch 16/20  Iteration 2737/3560 Training loss: 1.5694 1.0194 sec/batch\n",
      "Epoch 16/20  Iteration 2738/3560 Training loss: 1.5691 0.9679 sec/batch\n",
      "Epoch 16/20  Iteration 2739/3560 Training loss: 1.5691 1.1510 sec/batch\n",
      "Epoch 16/20  Iteration 2740/3560 Training loss: 1.5691 1.1097 sec/batch\n",
      "Epoch 16/20  Iteration 2741/3560 Training loss: 1.5697 1.1044 sec/batch\n",
      "Epoch 16/20  Iteration 2742/3560 Training loss: 1.5700 1.2084 sec/batch\n",
      "Epoch 16/20  Iteration 2743/3560 Training loss: 1.5704 0.9589 sec/batch\n",
      "Epoch 16/20  Iteration 2744/3560 Training loss: 1.5702 0.9768 sec/batch\n",
      "Epoch 16/20  Iteration 2745/3560 Training loss: 1.5702 1.0451 sec/batch\n",
      "Epoch 16/20  Iteration 2746/3560 Training loss: 1.5704 1.0252 sec/batch\n",
      "Epoch 16/20  Iteration 2747/3560 Training loss: 1.5703 1.1315 sec/batch\n",
      "Epoch 16/20  Iteration 2748/3560 Training loss: 1.5704 1.1794 sec/batch\n",
      "Epoch 16/20  Iteration 2749/3560 Training loss: 1.5698 1.0745 sec/batch\n",
      "Epoch 16/20  Iteration 2750/3560 Training loss: 1.5697 1.1940 sec/batch\n",
      "Epoch 16/20  Iteration 2751/3560 Training loss: 1.5692 1.1010 sec/batch\n",
      "Epoch 16/20  Iteration 2752/3560 Training loss: 1.5693 0.9296 sec/batch\n",
      "Epoch 16/20  Iteration 2753/3560 Training loss: 1.5686 1.0842 sec/batch\n",
      "Epoch 16/20  Iteration 2754/3560 Training loss: 1.5685 1.0037 sec/batch\n",
      "Epoch 16/20  Iteration 2755/3560 Training loss: 1.5682 1.1138 sec/batch\n",
      "Epoch 16/20  Iteration 2756/3560 Training loss: 1.5680 1.1845 sec/batch\n",
      "Epoch 16/20  Iteration 2757/3560 Training loss: 1.5678 1.0295 sec/batch\n",
      "Epoch 16/20  Iteration 2758/3560 Training loss: 1.5675 1.0214 sec/batch\n",
      "Epoch 16/20  Iteration 2759/3560 Training loss: 1.5671 1.0452 sec/batch\n",
      "Epoch 16/20  Iteration 2760/3560 Training loss: 1.5672 0.9185 sec/batch\n",
      "Epoch 16/20  Iteration 2761/3560 Training loss: 1.5669 1.1222 sec/batch\n",
      "Epoch 16/20  Iteration 2762/3560 Training loss: 1.5668 0.9944 sec/batch\n",
      "Epoch 16/20  Iteration 2763/3560 Training loss: 1.5663 1.0440 sec/batch\n",
      "Epoch 16/20  Iteration 2764/3560 Training loss: 1.5660 1.0701 sec/batch\n",
      "Epoch 16/20  Iteration 2765/3560 Training loss: 1.5657 1.0851 sec/batch\n",
      "Epoch 16/20  Iteration 2766/3560 Training loss: 1.5658 1.0575 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2767/3560 Training loss: 1.5658 1.0642 sec/batch\n",
      "Epoch 16/20  Iteration 2768/3560 Training loss: 1.5653 0.9557 sec/batch\n",
      "Epoch 16/20  Iteration 2769/3560 Training loss: 1.5650 1.1320 sec/batch\n",
      "Epoch 16/20  Iteration 2770/3560 Training loss: 1.5646 1.0028 sec/batch\n",
      "Epoch 16/20  Iteration 2771/3560 Training loss: 1.5645 1.1465 sec/batch\n",
      "Epoch 16/20  Iteration 2772/3560 Training loss: 1.5643 1.1013 sec/batch\n",
      "Epoch 16/20  Iteration 2773/3560 Training loss: 1.5642 1.1207 sec/batch\n",
      "Epoch 16/20  Iteration 2774/3560 Training loss: 1.5640 1.0120 sec/batch\n",
      "Epoch 16/20  Iteration 2775/3560 Training loss: 1.5638 1.0658 sec/batch\n",
      "Epoch 16/20  Iteration 2776/3560 Training loss: 1.5637 1.0274 sec/batch\n",
      "Epoch 16/20  Iteration 2777/3560 Training loss: 1.5637 1.0311 sec/batch\n",
      "Epoch 16/20  Iteration 2778/3560 Training loss: 1.5636 1.0435 sec/batch\n",
      "Epoch 16/20  Iteration 2779/3560 Training loss: 1.5635 1.1025 sec/batch\n",
      "Epoch 16/20  Iteration 2780/3560 Training loss: 1.5635 1.1316 sec/batch\n",
      "Epoch 16/20  Iteration 2781/3560 Training loss: 1.5634 1.1831 sec/batch\n",
      "Epoch 16/20  Iteration 2782/3560 Training loss: 1.5634 0.9535 sec/batch\n",
      "Epoch 16/20  Iteration 2783/3560 Training loss: 1.5633 0.9367 sec/batch\n",
      "Epoch 16/20  Iteration 2784/3560 Training loss: 1.5631 1.0850 sec/batch\n",
      "Epoch 16/20  Iteration 2785/3560 Training loss: 1.5629 0.9742 sec/batch\n",
      "Epoch 16/20  Iteration 2786/3560 Training loss: 1.5625 0.9829 sec/batch\n",
      "Epoch 16/20  Iteration 2787/3560 Training loss: 1.5625 1.0084 sec/batch\n",
      "Epoch 16/20  Iteration 2788/3560 Training loss: 1.5624 1.1929 sec/batch\n",
      "Epoch 16/20  Iteration 2789/3560 Training loss: 1.5623 1.1084 sec/batch\n",
      "Epoch 16/20  Iteration 2790/3560 Training loss: 1.5622 1.0027 sec/batch\n",
      "Epoch 16/20  Iteration 2791/3560 Training loss: 1.5621 0.9619 sec/batch\n",
      "Epoch 16/20  Iteration 2792/3560 Training loss: 1.5618 0.9718 sec/batch\n",
      "Epoch 16/20  Iteration 2793/3560 Training loss: 1.5614 1.0912 sec/batch\n",
      "Epoch 16/20  Iteration 2794/3560 Training loss: 1.5616 1.0466 sec/batch\n",
      "Epoch 16/20  Iteration 2795/3560 Training loss: 1.5616 1.0879 sec/batch\n",
      "Epoch 16/20  Iteration 2796/3560 Training loss: 1.5613 1.1373 sec/batch\n",
      "Epoch 16/20  Iteration 2797/3560 Training loss: 1.5614 0.9973 sec/batch\n",
      "Epoch 16/20  Iteration 2798/3560 Training loss: 1.5614 1.0087 sec/batch\n",
      "Epoch 16/20  Iteration 2799/3560 Training loss: 1.5613 1.0661 sec/batch\n",
      "Epoch 16/20  Iteration 2800/3560 Training loss: 1.5611 1.1652 sec/batch\n",
      "Epoch 16/20  Iteration 2801/3560 Training loss: 1.5607 1.0125 sec/batch\n",
      "Epoch 16/20  Iteration 2802/3560 Training loss: 1.5604 0.9769 sec/batch\n",
      "Epoch 16/20  Iteration 2803/3560 Training loss: 1.5606 1.2342 sec/batch\n",
      "Epoch 16/20  Iteration 2804/3560 Training loss: 1.5607 1.1020 sec/batch\n",
      "Epoch 16/20  Iteration 2805/3560 Training loss: 1.5607 1.1006 sec/batch\n",
      "Epoch 16/20  Iteration 2806/3560 Training loss: 1.5609 1.0333 sec/batch\n",
      "Epoch 16/20  Iteration 2807/3560 Training loss: 1.5610 1.0987 sec/batch\n",
      "Epoch 16/20  Iteration 2808/3560 Training loss: 1.5611 1.0055 sec/batch\n",
      "Epoch 16/20  Iteration 2809/3560 Training loss: 1.5611 1.0314 sec/batch\n",
      "Epoch 16/20  Iteration 2810/3560 Training loss: 1.5610 1.0459 sec/batch\n",
      "Epoch 16/20  Iteration 2811/3560 Training loss: 1.5614 1.1295 sec/batch\n",
      "Epoch 16/20  Iteration 2812/3560 Training loss: 1.5614 1.1197 sec/batch\n",
      "Epoch 16/20  Iteration 2813/3560 Training loss: 1.5613 1.0794 sec/batch\n",
      "Epoch 16/20  Iteration 2814/3560 Training loss: 1.5615 1.0153 sec/batch\n",
      "Epoch 16/20  Iteration 2815/3560 Training loss: 1.5614 1.2183 sec/batch\n",
      "Epoch 16/20  Iteration 2816/3560 Training loss: 1.5615 0.9212 sec/batch\n",
      "Epoch 16/20  Iteration 2817/3560 Training loss: 1.5616 1.1325 sec/batch\n",
      "Epoch 16/20  Iteration 2818/3560 Training loss: 1.5618 1.1290 sec/batch\n",
      "Epoch 16/20  Iteration 2819/3560 Training loss: 1.5619 1.0705 sec/batch\n",
      "Epoch 16/20  Iteration 2820/3560 Training loss: 1.5618 1.0153 sec/batch\n",
      "Epoch 16/20  Iteration 2821/3560 Training loss: 1.5615 1.1073 sec/batch\n",
      "Epoch 16/20  Iteration 2822/3560 Training loss: 1.5615 1.0852 sec/batch\n",
      "Epoch 16/20  Iteration 2823/3560 Training loss: 1.5616 0.9868 sec/batch\n",
      "Epoch 16/20  Iteration 2824/3560 Training loss: 1.5617 0.9363 sec/batch\n",
      "Epoch 16/20  Iteration 2825/3560 Training loss: 1.5617 1.1090 sec/batch\n",
      "Epoch 16/20  Iteration 2826/3560 Training loss: 1.5617 1.0032 sec/batch\n",
      "Epoch 16/20  Iteration 2827/3560 Training loss: 1.5618 1.2242 sec/batch\n",
      "Epoch 16/20  Iteration 2828/3560 Training loss: 1.5618 1.1358 sec/batch\n",
      "Epoch 16/20  Iteration 2829/3560 Training loss: 1.5616 1.1185 sec/batch\n",
      "Epoch 16/20  Iteration 2830/3560 Training loss: 1.5617 1.0105 sec/batch\n",
      "Epoch 16/20  Iteration 2831/3560 Training loss: 1.5620 1.0294 sec/batch\n",
      "Epoch 16/20  Iteration 2832/3560 Training loss: 1.5619 1.0885 sec/batch\n",
      "Epoch 16/20  Iteration 2833/3560 Training loss: 1.5619 1.0392 sec/batch\n",
      "Epoch 16/20  Iteration 2834/3560 Training loss: 1.5619 1.1156 sec/batch\n",
      "Epoch 16/20  Iteration 2835/3560 Training loss: 1.5619 1.0809 sec/batch\n",
      "Epoch 16/20  Iteration 2836/3560 Training loss: 1.5619 1.2150 sec/batch\n",
      "Epoch 16/20  Iteration 2837/3560 Training loss: 1.5621 1.0416 sec/batch\n",
      "Epoch 16/20  Iteration 2838/3560 Training loss: 1.5625 1.0265 sec/batch\n",
      "Epoch 16/20  Iteration 2839/3560 Training loss: 1.5625 1.0328 sec/batch\n",
      "Epoch 16/20  Iteration 2840/3560 Training loss: 1.5625 1.0313 sec/batch\n",
      "Epoch 16/20  Iteration 2841/3560 Training loss: 1.5624 1.0086 sec/batch\n",
      "Epoch 16/20  Iteration 2842/3560 Training loss: 1.5622 1.2304 sec/batch\n",
      "Epoch 16/20  Iteration 2843/3560 Training loss: 1.5623 1.0839 sec/batch\n",
      "Epoch 16/20  Iteration 2844/3560 Training loss: 1.5623 1.0355 sec/batch\n",
      "Epoch 16/20  Iteration 2845/3560 Training loss: 1.5624 1.0802 sec/batch\n",
      "Epoch 16/20  Iteration 2846/3560 Training loss: 1.5623 1.1543 sec/batch\n",
      "Epoch 16/20  Iteration 2847/3560 Training loss: 1.5622 1.0215 sec/batch\n",
      "Epoch 16/20  Iteration 2848/3560 Training loss: 1.5622 1.0153 sec/batch\n",
      "Epoch 17/20  Iteration 2849/3560 Training loss: 1.6331 1.0107 sec/batch\n",
      "Epoch 17/20  Iteration 2850/3560 Training loss: 1.5955 1.0222 sec/batch\n",
      "Epoch 17/20  Iteration 2851/3560 Training loss: 1.5810 1.0922 sec/batch\n",
      "Epoch 17/20  Iteration 2852/3560 Training loss: 1.5776 1.0796 sec/batch\n",
      "Epoch 17/20  Iteration 2853/3560 Training loss: 1.5710 1.2177 sec/batch\n",
      "Epoch 17/20  Iteration 2854/3560 Training loss: 1.5610 0.9863 sec/batch\n",
      "Epoch 17/20  Iteration 2855/3560 Training loss: 1.5614 1.0630 sec/batch\n",
      "Epoch 17/20  Iteration 2856/3560 Training loss: 1.5592 0.9998 sec/batch\n",
      "Epoch 17/20  Iteration 2857/3560 Training loss: 1.5609 1.0564 sec/batch\n",
      "Epoch 17/20  Iteration 2858/3560 Training loss: 1.5603 1.1218 sec/batch\n",
      "Epoch 17/20  Iteration 2859/3560 Training loss: 1.5571 1.1786 sec/batch\n",
      "Epoch 17/20  Iteration 2860/3560 Training loss: 1.5565 1.1132 sec/batch\n",
      "Epoch 17/20  Iteration 2861/3560 Training loss: 1.5557 1.0754 sec/batch\n",
      "Epoch 17/20  Iteration 2862/3560 Training loss: 1.5576 0.9716 sec/batch\n",
      "Epoch 17/20  Iteration 2863/3560 Training loss: 1.5570 0.9969 sec/batch\n",
      "Epoch 17/20  Iteration 2864/3560 Training loss: 1.5548 1.0828 sec/batch\n",
      "Epoch 17/20  Iteration 2865/3560 Training loss: 1.5554 1.0689 sec/batch\n",
      "Epoch 17/20  Iteration 2866/3560 Training loss: 1.5567 1.0333 sec/batch\n",
      "Epoch 17/20  Iteration 2867/3560 Training loss: 1.5570 1.1708 sec/batch\n",
      "Epoch 17/20  Iteration 2868/3560 Training loss: 1.5579 1.0811 sec/batch\n",
      "Epoch 17/20  Iteration 2869/3560 Training loss: 1.5571 1.0129 sec/batch\n",
      "Epoch 17/20  Iteration 2870/3560 Training loss: 1.5573 1.0478 sec/batch\n",
      "Epoch 17/20  Iteration 2871/3560 Training loss: 1.5563 0.9618 sec/batch\n",
      "Epoch 17/20  Iteration 2872/3560 Training loss: 1.5558 1.0660 sec/batch\n",
      "Epoch 17/20  Iteration 2873/3560 Training loss: 1.5558 1.0620 sec/batch\n",
      "Epoch 17/20  Iteration 2874/3560 Training loss: 1.5546 1.1067 sec/batch\n",
      "Epoch 17/20  Iteration 2875/3560 Training loss: 1.5532 1.1457 sec/batch\n",
      "Epoch 17/20  Iteration 2876/3560 Training loss: 1.5540 1.1073 sec/batch\n",
      "Epoch 17/20  Iteration 2877/3560 Training loss: 1.5544 1.0676 sec/batch\n",
      "Epoch 17/20  Iteration 2878/3560 Training loss: 1.5551 1.0732 sec/batch\n",
      "Epoch 17/20  Iteration 2879/3560 Training loss: 1.5550 1.0706 sec/batch\n",
      "Epoch 17/20  Iteration 2880/3560 Training loss: 1.5539 0.9696 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 2881/3560 Training loss: 1.5541 1.0587 sec/batch\n",
      "Epoch 17/20  Iteration 2882/3560 Training loss: 1.5547 1.0423 sec/batch\n",
      "Epoch 17/20  Iteration 2883/3560 Training loss: 1.5544 1.1766 sec/batch\n",
      "Epoch 17/20  Iteration 2884/3560 Training loss: 1.5541 1.1138 sec/batch\n",
      "Epoch 17/20  Iteration 2885/3560 Training loss: 1.5533 1.0561 sec/batch\n",
      "Epoch 17/20  Iteration 2886/3560 Training loss: 1.5520 1.1297 sec/batch\n",
      "Epoch 17/20  Iteration 2887/3560 Training loss: 1.5506 1.0251 sec/batch\n",
      "Epoch 17/20  Iteration 2888/3560 Training loss: 1.5500 0.9982 sec/batch\n",
      "Epoch 17/20  Iteration 2889/3560 Training loss: 1.5491 1.0673 sec/batch\n",
      "Epoch 17/20  Iteration 2890/3560 Training loss: 1.5496 1.0671 sec/batch\n",
      "Epoch 17/20  Iteration 2891/3560 Training loss: 1.5492 1.1217 sec/batch\n",
      "Epoch 17/20  Iteration 2892/3560 Training loss: 1.5485 1.0575 sec/batch\n",
      "Epoch 17/20  Iteration 2893/3560 Training loss: 1.5487 1.0486 sec/batch\n",
      "Epoch 17/20  Iteration 2894/3560 Training loss: 1.5476 1.0087 sec/batch\n",
      "Epoch 17/20  Iteration 2895/3560 Training loss: 1.5473 1.0398 sec/batch\n",
      "Epoch 17/20  Iteration 2896/3560 Training loss: 1.5469 0.9130 sec/batch\n",
      "Epoch 17/20  Iteration 2897/3560 Training loss: 1.5468 1.1545 sec/batch\n",
      "Epoch 17/20  Iteration 2898/3560 Training loss: 1.5475 1.1214 sec/batch\n",
      "Epoch 17/20  Iteration 2899/3560 Training loss: 1.5471 1.2927 sec/batch\n",
      "Epoch 17/20  Iteration 2900/3560 Training loss: 1.5481 1.0523 sec/batch\n",
      "Epoch 17/20  Iteration 2901/3560 Training loss: 1.5480 1.0404 sec/batch\n",
      "Epoch 17/20  Iteration 2902/3560 Training loss: 1.5484 0.9992 sec/batch\n",
      "Epoch 17/20  Iteration 2903/3560 Training loss: 1.5481 0.9858 sec/batch\n",
      "Epoch 17/20  Iteration 2904/3560 Training loss: 1.5482 1.0715 sec/batch\n",
      "Epoch 17/20  Iteration 2905/3560 Training loss: 1.5487 1.1205 sec/batch\n",
      "Epoch 17/20  Iteration 2906/3560 Training loss: 1.5484 1.1270 sec/batch\n",
      "Epoch 17/20  Iteration 2907/3560 Training loss: 1.5479 1.0927 sec/batch\n",
      "Epoch 17/20  Iteration 2908/3560 Training loss: 1.5485 1.0184 sec/batch\n",
      "Epoch 17/20  Iteration 2909/3560 Training loss: 1.5484 0.9339 sec/batch\n",
      "Epoch 17/20  Iteration 2910/3560 Training loss: 1.5495 1.0665 sec/batch\n",
      "Epoch 17/20  Iteration 2911/3560 Training loss: 1.5498 1.0699 sec/batch\n",
      "Epoch 17/20  Iteration 2912/3560 Training loss: 1.5502 1.0470 sec/batch\n",
      "Epoch 17/20  Iteration 2913/3560 Training loss: 1.5501 1.1652 sec/batch\n",
      "Epoch 17/20  Iteration 2914/3560 Training loss: 1.5503 1.0079 sec/batch\n",
      "Epoch 17/20  Iteration 2915/3560 Training loss: 1.5505 1.2119 sec/batch\n",
      "Epoch 17/20  Iteration 2916/3560 Training loss: 1.5502 1.0374 sec/batch\n",
      "Epoch 17/20  Iteration 2917/3560 Training loss: 1.5501 1.0470 sec/batch\n",
      "Epoch 17/20  Iteration 2918/3560 Training loss: 1.5500 1.0407 sec/batch\n",
      "Epoch 17/20  Iteration 2919/3560 Training loss: 1.5506 1.0509 sec/batch\n",
      "Epoch 17/20  Iteration 2920/3560 Training loss: 1.5508 0.9948 sec/batch\n",
      "Epoch 17/20  Iteration 2921/3560 Training loss: 1.5513 1.0974 sec/batch\n",
      "Epoch 17/20  Iteration 2922/3560 Training loss: 1.5511 1.0968 sec/batch\n",
      "Epoch 17/20  Iteration 2923/3560 Training loss: 1.5510 1.0986 sec/batch\n",
      "Epoch 17/20  Iteration 2924/3560 Training loss: 1.5513 1.0993 sec/batch\n",
      "Epoch 17/20  Iteration 2925/3560 Training loss: 1.5513 1.0623 sec/batch\n",
      "Epoch 17/20  Iteration 2926/3560 Training loss: 1.5513 1.0959 sec/batch\n",
      "Epoch 17/20  Iteration 2927/3560 Training loss: 1.5508 0.9601 sec/batch\n",
      "Epoch 17/20  Iteration 2928/3560 Training loss: 1.5506 1.1684 sec/batch\n",
      "Epoch 17/20  Iteration 2929/3560 Training loss: 1.5501 1.0318 sec/batch\n",
      "Epoch 17/20  Iteration 2930/3560 Training loss: 1.5501 1.1131 sec/batch\n",
      "Epoch 17/20  Iteration 2931/3560 Training loss: 1.5495 1.0498 sec/batch\n",
      "Epoch 17/20  Iteration 2932/3560 Training loss: 1.5495 1.0170 sec/batch\n",
      "Epoch 17/20  Iteration 2933/3560 Training loss: 1.5492 1.0086 sec/batch\n",
      "Epoch 17/20  Iteration 2934/3560 Training loss: 1.5490 1.0586 sec/batch\n",
      "Epoch 17/20  Iteration 2935/3560 Training loss: 1.5488 1.0672 sec/batch\n",
      "Epoch 17/20  Iteration 2936/3560 Training loss: 1.5485 0.9839 sec/batch\n",
      "Epoch 17/20  Iteration 2937/3560 Training loss: 1.5480 1.0233 sec/batch\n",
      "Epoch 17/20  Iteration 2938/3560 Training loss: 1.5482 1.1104 sec/batch\n",
      "Epoch 17/20  Iteration 2939/3560 Training loss: 1.5480 1.1270 sec/batch\n",
      "Epoch 17/20  Iteration 2940/3560 Training loss: 1.5479 1.1496 sec/batch\n",
      "Epoch 17/20  Iteration 2941/3560 Training loss: 1.5475 1.1334 sec/batch\n",
      "Epoch 17/20  Iteration 2942/3560 Training loss: 1.5471 1.0668 sec/batch\n",
      "Epoch 17/20  Iteration 2943/3560 Training loss: 1.5469 1.0720 sec/batch\n",
      "Epoch 17/20  Iteration 2944/3560 Training loss: 1.5469 1.0787 sec/batch\n",
      "Epoch 17/20  Iteration 2945/3560 Training loss: 1.5468 1.1957 sec/batch\n",
      "Epoch 17/20  Iteration 2946/3560 Training loss: 1.5464 1.0782 sec/batch\n",
      "Epoch 17/20  Iteration 2947/3560 Training loss: 1.5461 1.1277 sec/batch\n",
      "Epoch 17/20  Iteration 2948/3560 Training loss: 1.5457 1.0797 sec/batch\n",
      "Epoch 17/20  Iteration 2949/3560 Training loss: 1.5456 1.0076 sec/batch\n",
      "Epoch 17/20  Iteration 2950/3560 Training loss: 1.5455 1.0812 sec/batch\n",
      "Epoch 17/20  Iteration 2951/3560 Training loss: 1.5454 1.0668 sec/batch\n",
      "Epoch 17/20  Iteration 2952/3560 Training loss: 1.5453 1.2341 sec/batch\n",
      "Epoch 17/20  Iteration 2953/3560 Training loss: 1.5452 1.1405 sec/batch\n",
      "Epoch 17/20  Iteration 2954/3560 Training loss: 1.5451 1.0370 sec/batch\n",
      "Epoch 17/20  Iteration 2955/3560 Training loss: 1.5451 1.0879 sec/batch\n",
      "Epoch 17/20  Iteration 2956/3560 Training loss: 1.5451 0.9810 sec/batch\n",
      "Epoch 17/20  Iteration 2957/3560 Training loss: 1.5449 1.0768 sec/batch\n",
      "Epoch 17/20  Iteration 2958/3560 Training loss: 1.5450 1.1526 sec/batch\n",
      "Epoch 17/20  Iteration 2959/3560 Training loss: 1.5449 1.0186 sec/batch\n",
      "Epoch 17/20  Iteration 2960/3560 Training loss: 1.5449 1.1280 sec/batch\n",
      "Epoch 17/20  Iteration 2961/3560 Training loss: 1.5446 1.1816 sec/batch\n",
      "Epoch 17/20  Iteration 2962/3560 Training loss: 1.5445 1.1515 sec/batch\n",
      "Epoch 17/20  Iteration 2963/3560 Training loss: 1.5443 1.0926 sec/batch\n",
      "Epoch 17/20  Iteration 2964/3560 Training loss: 1.5439 0.9606 sec/batch\n",
      "Epoch 17/20  Iteration 2965/3560 Training loss: 1.5439 1.0422 sec/batch\n",
      "Epoch 17/20  Iteration 2966/3560 Training loss: 1.5439 0.9358 sec/batch\n",
      "Epoch 17/20  Iteration 2967/3560 Training loss: 1.5438 1.0071 sec/batch\n",
      "Epoch 17/20  Iteration 2968/3560 Training loss: 1.5437 1.0980 sec/batch\n",
      "Epoch 17/20  Iteration 2969/3560 Training loss: 1.5437 1.1098 sec/batch\n",
      "Epoch 17/20  Iteration 2970/3560 Training loss: 1.5434 0.9970 sec/batch\n",
      "Epoch 17/20  Iteration 2971/3560 Training loss: 1.5429 1.0078 sec/batch\n",
      "Epoch 17/20  Iteration 2972/3560 Training loss: 1.5429 1.2255 sec/batch\n",
      "Epoch 17/20  Iteration 2973/3560 Training loss: 1.5429 0.9376 sec/batch\n",
      "Epoch 17/20  Iteration 2974/3560 Training loss: 1.5426 1.0972 sec/batch\n",
      "Epoch 17/20  Iteration 2975/3560 Training loss: 1.5427 1.0351 sec/batch\n",
      "Epoch 17/20  Iteration 2976/3560 Training loss: 1.5428 1.0812 sec/batch\n",
      "Epoch 17/20  Iteration 2977/3560 Training loss: 1.5426 1.1935 sec/batch\n",
      "Epoch 17/20  Iteration 2978/3560 Training loss: 1.5423 1.0452 sec/batch\n",
      "Epoch 17/20  Iteration 2979/3560 Training loss: 1.5419 1.0087 sec/batch\n",
      "Epoch 17/20  Iteration 2980/3560 Training loss: 1.5417 0.9830 sec/batch\n",
      "Epoch 17/20  Iteration 2981/3560 Training loss: 1.5418 1.1617 sec/batch\n",
      "Epoch 17/20  Iteration 2982/3560 Training loss: 1.5419 0.8865 sec/batch\n",
      "Epoch 17/20  Iteration 2983/3560 Training loss: 1.5419 1.0787 sec/batch\n",
      "Epoch 17/20  Iteration 2984/3560 Training loss: 1.5421 1.0887 sec/batch\n",
      "Epoch 17/20  Iteration 2985/3560 Training loss: 1.5422 1.0502 sec/batch\n",
      "Epoch 17/20  Iteration 2986/3560 Training loss: 1.5423 1.0511 sec/batch\n",
      "Epoch 17/20  Iteration 2987/3560 Training loss: 1.5423 1.0877 sec/batch\n",
      "Epoch 17/20  Iteration 2988/3560 Training loss: 1.5423 1.0335 sec/batch\n",
      "Epoch 17/20  Iteration 2989/3560 Training loss: 1.5426 0.9887 sec/batch\n",
      "Epoch 17/20  Iteration 2990/3560 Training loss: 1.5426 1.2109 sec/batch\n",
      "Epoch 17/20  Iteration 2991/3560 Training loss: 1.5426 1.0731 sec/batch\n",
      "Epoch 17/20  Iteration 2992/3560 Training loss: 1.5428 1.2173 sec/batch\n",
      "Epoch 17/20  Iteration 2993/3560 Training loss: 1.5428 1.0687 sec/batch\n",
      "Epoch 17/20  Iteration 2994/3560 Training loss: 1.5429 1.0518 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 2995/3560 Training loss: 1.5430 1.0459 sec/batch\n",
      "Epoch 17/20  Iteration 2996/3560 Training loss: 1.5432 1.0642 sec/batch\n",
      "Epoch 17/20  Iteration 2997/3560 Training loss: 1.5433 1.0137 sec/batch\n",
      "Epoch 17/20  Iteration 2998/3560 Training loss: 1.5432 0.9988 sec/batch\n",
      "Epoch 17/20  Iteration 2999/3560 Training loss: 1.5429 1.1397 sec/batch\n",
      "Epoch 17/20  Iteration 3000/3560 Training loss: 1.5428 1.0704 sec/batch\n",
      "Epoch 17/20  Iteration 3001/3560 Training loss: 1.5429 1.0297 sec/batch\n",
      "Epoch 17/20  Iteration 3002/3560 Training loss: 1.5430 0.9299 sec/batch\n",
      "Epoch 17/20  Iteration 3003/3560 Training loss: 1.5430 1.1186 sec/batch\n",
      "Epoch 17/20  Iteration 3004/3560 Training loss: 1.5430 0.9977 sec/batch\n",
      "Epoch 17/20  Iteration 3005/3560 Training loss: 1.5431 1.0564 sec/batch\n",
      "Epoch 17/20  Iteration 3006/3560 Training loss: 1.5430 1.0760 sec/batch\n",
      "Epoch 17/20  Iteration 3007/3560 Training loss: 1.5429 0.9776 sec/batch\n",
      "Epoch 17/20  Iteration 3008/3560 Training loss: 1.5431 1.0983 sec/batch\n",
      "Epoch 17/20  Iteration 3009/3560 Training loss: 1.5433 1.0348 sec/batch\n",
      "Epoch 17/20  Iteration 3010/3560 Training loss: 1.5432 1.0566 sec/batch\n",
      "Epoch 17/20  Iteration 3011/3560 Training loss: 1.5433 1.0586 sec/batch\n",
      "Epoch 17/20  Iteration 3012/3560 Training loss: 1.5433 0.9460 sec/batch\n",
      "Epoch 17/20  Iteration 3013/3560 Training loss: 1.5432 1.0603 sec/batch\n",
      "Epoch 17/20  Iteration 3014/3560 Training loss: 1.5432 1.0234 sec/batch\n",
      "Epoch 17/20  Iteration 3015/3560 Training loss: 1.5434 1.1185 sec/batch\n",
      "Epoch 17/20  Iteration 3016/3560 Training loss: 1.5437 1.0004 sec/batch\n",
      "Epoch 17/20  Iteration 3017/3560 Training loss: 1.5438 0.9930 sec/batch\n",
      "Epoch 17/20  Iteration 3018/3560 Training loss: 1.5437 1.0116 sec/batch\n",
      "Epoch 17/20  Iteration 3019/3560 Training loss: 1.5436 1.1053 sec/batch\n",
      "Epoch 17/20  Iteration 3020/3560 Training loss: 1.5435 1.0838 sec/batch\n",
      "Epoch 17/20  Iteration 3021/3560 Training loss: 1.5436 0.9759 sec/batch\n",
      "Epoch 17/20  Iteration 3022/3560 Training loss: 1.5437 1.0034 sec/batch\n",
      "Epoch 17/20  Iteration 3023/3560 Training loss: 1.5438 1.1740 sec/batch\n",
      "Epoch 17/20  Iteration 3024/3560 Training loss: 1.5437 1.0252 sec/batch\n",
      "Epoch 17/20  Iteration 3025/3560 Training loss: 1.5436 1.0102 sec/batch\n",
      "Epoch 17/20  Iteration 3026/3560 Training loss: 1.5437 0.9607 sec/batch\n",
      "Epoch 18/20  Iteration 3027/3560 Training loss: 1.6199 1.1275 sec/batch\n",
      "Epoch 18/20  Iteration 3028/3560 Training loss: 1.5833 1.0570 sec/batch\n",
      "Epoch 18/20  Iteration 3029/3560 Training loss: 1.5692 1.0262 sec/batch\n",
      "Epoch 18/20  Iteration 3030/3560 Training loss: 1.5640 1.0961 sec/batch\n",
      "Epoch 18/20  Iteration 3031/3560 Training loss: 1.5553 1.0569 sec/batch\n",
      "Epoch 18/20  Iteration 3032/3560 Training loss: 1.5455 1.0702 sec/batch\n",
      "Epoch 18/20  Iteration 3033/3560 Training loss: 1.5452 1.0373 sec/batch\n",
      "Epoch 18/20  Iteration 3034/3560 Training loss: 1.5421 1.0199 sec/batch\n",
      "Epoch 18/20  Iteration 3035/3560 Training loss: 1.5438 1.0974 sec/batch\n",
      "Epoch 18/20  Iteration 3036/3560 Training loss: 1.5424 1.0794 sec/batch\n",
      "Epoch 18/20  Iteration 3037/3560 Training loss: 1.5389 1.0246 sec/batch\n",
      "Epoch 18/20  Iteration 3038/3560 Training loss: 1.5381 1.0209 sec/batch\n",
      "Epoch 18/20  Iteration 3039/3560 Training loss: 1.5378 1.1028 sec/batch\n",
      "Epoch 18/20  Iteration 3040/3560 Training loss: 1.5395 1.2430 sec/batch\n",
      "Epoch 18/20  Iteration 3041/3560 Training loss: 1.5382 1.0908 sec/batch\n",
      "Epoch 18/20  Iteration 3042/3560 Training loss: 1.5362 1.0086 sec/batch\n",
      "Epoch 18/20  Iteration 3043/3560 Training loss: 1.5368 1.0662 sec/batch\n",
      "Epoch 18/20  Iteration 3044/3560 Training loss: 1.5378 1.0601 sec/batch\n",
      "Epoch 18/20  Iteration 3045/3560 Training loss: 1.5379 1.0337 sec/batch\n",
      "Epoch 18/20  Iteration 3046/3560 Training loss: 1.5391 1.0234 sec/batch\n",
      "Epoch 18/20  Iteration 3047/3560 Training loss: 1.5385 1.0780 sec/batch\n",
      "Epoch 18/20  Iteration 3048/3560 Training loss: 1.5388 1.0332 sec/batch\n",
      "Epoch 18/20  Iteration 3049/3560 Training loss: 1.5379 1.1103 sec/batch\n",
      "Epoch 18/20  Iteration 3050/3560 Training loss: 1.5374 0.9712 sec/batch\n",
      "Epoch 18/20  Iteration 3051/3560 Training loss: 1.5372 1.1804 sec/batch\n",
      "Epoch 18/20  Iteration 3052/3560 Training loss: 1.5361 1.0673 sec/batch\n",
      "Epoch 18/20  Iteration 3053/3560 Training loss: 1.5350 1.0052 sec/batch\n",
      "Epoch 18/20  Iteration 3054/3560 Training loss: 1.5357 0.9597 sec/batch\n",
      "Epoch 18/20  Iteration 3055/3560 Training loss: 1.5362 1.0454 sec/batch\n",
      "Epoch 18/20  Iteration 3056/3560 Training loss: 1.5365 1.1239 sec/batch\n",
      "Epoch 18/20  Iteration 3057/3560 Training loss: 1.5360 1.0365 sec/batch\n",
      "Epoch 18/20  Iteration 3058/3560 Training loss: 1.5351 1.1542 sec/batch\n",
      "Epoch 18/20  Iteration 3059/3560 Training loss: 1.5355 1.0037 sec/batch\n",
      "Epoch 18/20  Iteration 3060/3560 Training loss: 1.5361 1.0788 sec/batch\n",
      "Epoch 18/20  Iteration 3061/3560 Training loss: 1.5360 1.0601 sec/batch\n",
      "Epoch 18/20  Iteration 3062/3560 Training loss: 1.5361 1.0136 sec/batch\n",
      "Epoch 18/20  Iteration 3063/3560 Training loss: 1.5355 1.0488 sec/batch\n",
      "Epoch 18/20  Iteration 3064/3560 Training loss: 1.5341 1.1390 sec/batch\n",
      "Epoch 18/20  Iteration 3065/3560 Training loss: 1.5327 0.9960 sec/batch\n",
      "Epoch 18/20  Iteration 3066/3560 Training loss: 1.5322 1.0901 sec/batch\n",
      "Epoch 18/20  Iteration 3067/3560 Training loss: 1.5316 1.0596 sec/batch\n",
      "Epoch 18/20  Iteration 3068/3560 Training loss: 1.5322 1.1592 sec/batch\n",
      "Epoch 18/20  Iteration 3069/3560 Training loss: 1.5318 0.9985 sec/batch\n",
      "Epoch 18/20  Iteration 3070/3560 Training loss: 1.5310 1.0472 sec/batch\n",
      "Epoch 18/20  Iteration 3071/3560 Training loss: 1.5312 1.0860 sec/batch\n",
      "Epoch 18/20  Iteration 3072/3560 Training loss: 1.5303 1.0148 sec/batch\n",
      "Epoch 18/20  Iteration 3073/3560 Training loss: 1.5300 0.9955 sec/batch\n",
      "Epoch 18/20  Iteration 3074/3560 Training loss: 1.5296 1.0850 sec/batch\n",
      "Epoch 18/20  Iteration 3075/3560 Training loss: 1.5295 1.1213 sec/batch\n",
      "Epoch 18/20  Iteration 3076/3560 Training loss: 1.5302 1.1032 sec/batch\n",
      "Epoch 18/20  Iteration 3077/3560 Training loss: 1.5298 1.0432 sec/batch\n",
      "Epoch 18/20  Iteration 3078/3560 Training loss: 1.5307 1.0513 sec/batch\n",
      "Epoch 18/20  Iteration 3079/3560 Training loss: 1.5308 1.1023 sec/batch\n",
      "Epoch 18/20  Iteration 3080/3560 Training loss: 1.5311 1.0249 sec/batch\n",
      "Epoch 18/20  Iteration 3081/3560 Training loss: 1.5309 0.9605 sec/batch\n",
      "Epoch 18/20  Iteration 3082/3560 Training loss: 1.5309 1.0748 sec/batch\n",
      "Epoch 18/20  Iteration 3083/3560 Training loss: 1.5315 1.0832 sec/batch\n",
      "Epoch 18/20  Iteration 3084/3560 Training loss: 1.5312 1.0346 sec/batch\n",
      "Epoch 18/20  Iteration 3085/3560 Training loss: 1.5307 1.1194 sec/batch\n",
      "Epoch 18/20  Iteration 3086/3560 Training loss: 1.5314 1.0082 sec/batch\n",
      "Epoch 18/20  Iteration 3087/3560 Training loss: 1.5313 1.1461 sec/batch\n",
      "Epoch 18/20  Iteration 3088/3560 Training loss: 1.5322 1.1044 sec/batch\n",
      "Epoch 18/20  Iteration 3089/3560 Training loss: 1.5325 1.0278 sec/batch\n",
      "Epoch 18/20  Iteration 3090/3560 Training loss: 1.5329 1.2475 sec/batch\n",
      "Epoch 18/20  Iteration 3091/3560 Training loss: 1.5328 1.1312 sec/batch\n",
      "Epoch 18/20  Iteration 3092/3560 Training loss: 1.5330 1.0580 sec/batch\n",
      "Epoch 18/20  Iteration 3093/3560 Training loss: 1.5332 1.0214 sec/batch\n",
      "Epoch 18/20  Iteration 3094/3560 Training loss: 1.5329 1.0481 sec/batch\n",
      "Epoch 18/20  Iteration 3095/3560 Training loss: 1.5329 1.1531 sec/batch\n",
      "Epoch 18/20  Iteration 3096/3560 Training loss: 1.5330 1.1551 sec/batch\n",
      "Epoch 18/20  Iteration 3097/3560 Training loss: 1.5338 1.0759 sec/batch\n",
      "Epoch 18/20  Iteration 3098/3560 Training loss: 1.5340 1.0427 sec/batch\n",
      "Epoch 18/20  Iteration 3099/3560 Training loss: 1.5344 1.1621 sec/batch\n",
      "Epoch 18/20  Iteration 3100/3560 Training loss: 1.5342 1.0094 sec/batch\n",
      "Epoch 18/20  Iteration 3101/3560 Training loss: 1.5341 1.0014 sec/batch\n",
      "Epoch 18/20  Iteration 3102/3560 Training loss: 1.5343 1.2484 sec/batch\n",
      "Epoch 18/20  Iteration 3103/3560 Training loss: 1.5342 1.1222 sec/batch\n",
      "Epoch 18/20  Iteration 3104/3560 Training loss: 1.5343 0.9868 sec/batch\n",
      "Epoch 18/20  Iteration 3105/3560 Training loss: 1.5337 1.0501 sec/batch\n",
      "Epoch 18/20  Iteration 3106/3560 Training loss: 1.5336 1.0426 sec/batch\n",
      "Epoch 18/20  Iteration 3107/3560 Training loss: 1.5332 1.0142 sec/batch\n",
      "Epoch 18/20  Iteration 3108/3560 Training loss: 1.5332 1.0380 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3109/3560 Training loss: 1.5327 1.0001 sec/batch\n",
      "Epoch 18/20  Iteration 3110/3560 Training loss: 1.5326 1.0211 sec/batch\n",
      "Epoch 18/20  Iteration 3111/3560 Training loss: 1.5322 1.0909 sec/batch\n",
      "Epoch 18/20  Iteration 3112/3560 Training loss: 1.5320 1.1574 sec/batch\n",
      "Epoch 18/20  Iteration 3113/3560 Training loss: 1.5318 0.9878 sec/batch\n",
      "Epoch 18/20  Iteration 3114/3560 Training loss: 1.5316 1.0682 sec/batch\n",
      "Epoch 18/20  Iteration 3115/3560 Training loss: 1.5311 0.9964 sec/batch\n",
      "Epoch 18/20  Iteration 3116/3560 Training loss: 1.5312 1.0586 sec/batch\n",
      "Epoch 18/20  Iteration 3117/3560 Training loss: 1.5310 1.0014 sec/batch\n",
      "Epoch 18/20  Iteration 3118/3560 Training loss: 1.5309 1.1180 sec/batch\n",
      "Epoch 18/20  Iteration 3119/3560 Training loss: 1.5305 1.0217 sec/batch\n",
      "Epoch 18/20  Iteration 3120/3560 Training loss: 1.5302 1.0512 sec/batch\n",
      "Epoch 18/20  Iteration 3121/3560 Training loss: 1.5299 1.1249 sec/batch\n",
      "Epoch 18/20  Iteration 3122/3560 Training loss: 1.5299 1.0364 sec/batch\n",
      "Epoch 18/20  Iteration 3123/3560 Training loss: 1.5299 0.9654 sec/batch\n",
      "Epoch 18/20  Iteration 3124/3560 Training loss: 1.5295 1.0451 sec/batch\n",
      "Epoch 18/20  Iteration 3125/3560 Training loss: 1.5292 0.9233 sec/batch\n",
      "Epoch 18/20  Iteration 3126/3560 Training loss: 1.5288 1.1775 sec/batch\n",
      "Epoch 18/20  Iteration 3127/3560 Training loss: 1.5287 1.1032 sec/batch\n",
      "Epoch 18/20  Iteration 3128/3560 Training loss: 1.5286 1.1060 sec/batch\n",
      "Epoch 18/20  Iteration 3129/3560 Training loss: 1.5285 1.0817 sec/batch\n",
      "Epoch 18/20  Iteration 3130/3560 Training loss: 1.5285 0.9947 sec/batch\n",
      "Epoch 18/20  Iteration 3131/3560 Training loss: 1.5284 1.0309 sec/batch\n",
      "Epoch 18/20  Iteration 3132/3560 Training loss: 1.5283 1.0965 sec/batch\n",
      "Epoch 18/20  Iteration 3133/3560 Training loss: 1.5282 1.1360 sec/batch\n",
      "Epoch 18/20  Iteration 3134/3560 Training loss: 1.5282 0.9939 sec/batch\n",
      "Epoch 18/20  Iteration 3135/3560 Training loss: 1.5281 1.1995 sec/batch\n",
      "Epoch 18/20  Iteration 3136/3560 Training loss: 1.5282 1.1222 sec/batch\n",
      "Epoch 18/20  Iteration 3137/3560 Training loss: 1.5280 0.9777 sec/batch\n",
      "Epoch 18/20  Iteration 3138/3560 Training loss: 1.5280 0.9939 sec/batch\n",
      "Epoch 18/20  Iteration 3139/3560 Training loss: 1.5279 1.0925 sec/batch\n",
      "Epoch 18/20  Iteration 3140/3560 Training loss: 1.5277 1.1910 sec/batch\n",
      "Epoch 18/20  Iteration 3141/3560 Training loss: 1.5275 1.1831 sec/batch\n",
      "Epoch 18/20  Iteration 3142/3560 Training loss: 1.5271 1.0475 sec/batch\n",
      "Epoch 18/20  Iteration 3143/3560 Training loss: 1.5271 1.0528 sec/batch\n",
      "Epoch 18/20  Iteration 3144/3560 Training loss: 1.5272 1.1350 sec/batch\n",
      "Epoch 18/20  Iteration 3145/3560 Training loss: 1.5271 1.0420 sec/batch\n",
      "Epoch 18/20  Iteration 3146/3560 Training loss: 1.5270 1.0953 sec/batch\n",
      "Epoch 18/20  Iteration 3147/3560 Training loss: 1.5269 1.0177 sec/batch\n",
      "Epoch 18/20  Iteration 3148/3560 Training loss: 1.5267 1.0350 sec/batch\n",
      "Epoch 18/20  Iteration 3149/3560 Training loss: 1.5263 0.9663 sec/batch\n",
      "Epoch 18/20  Iteration 3150/3560 Training loss: 1.5263 0.9445 sec/batch\n",
      "Epoch 18/20  Iteration 3151/3560 Training loss: 1.5263 1.1467 sec/batch\n",
      "Epoch 18/20  Iteration 3152/3560 Training loss: 1.5259 1.1344 sec/batch\n",
      "Epoch 18/20  Iteration 3153/3560 Training loss: 1.5260 1.0283 sec/batch\n",
      "Epoch 18/20  Iteration 3154/3560 Training loss: 1.5261 1.0277 sec/batch\n",
      "Epoch 18/20  Iteration 3155/3560 Training loss: 1.5259 1.0648 sec/batch\n",
      "Epoch 18/20  Iteration 3156/3560 Training loss: 1.5256 1.1020 sec/batch\n",
      "Epoch 18/20  Iteration 3157/3560 Training loss: 1.5253 0.9686 sec/batch\n",
      "Epoch 18/20  Iteration 3158/3560 Training loss: 1.5251 0.9935 sec/batch\n",
      "Epoch 18/20  Iteration 3159/3560 Training loss: 1.5253 1.0507 sec/batch\n",
      "Epoch 18/20  Iteration 3160/3560 Training loss: 1.5253 1.1288 sec/batch\n",
      "Epoch 18/20  Iteration 3161/3560 Training loss: 1.5253 1.0015 sec/batch\n",
      "Epoch 18/20  Iteration 3162/3560 Training loss: 1.5254 1.0261 sec/batch\n",
      "Epoch 18/20  Iteration 3163/3560 Training loss: 1.5256 1.0872 sec/batch\n",
      "Epoch 18/20  Iteration 3164/3560 Training loss: 1.5257 1.0781 sec/batch\n",
      "Epoch 18/20  Iteration 3165/3560 Training loss: 1.5258 1.0961 sec/batch\n",
      "Epoch 18/20  Iteration 3166/3560 Training loss: 1.5257 0.9718 sec/batch\n",
      "Epoch 18/20  Iteration 3167/3560 Training loss: 1.5261 1.0182 sec/batch\n",
      "Epoch 18/20  Iteration 3168/3560 Training loss: 1.5262 1.1361 sec/batch\n",
      "Epoch 18/20  Iteration 3169/3560 Training loss: 1.5262 0.9930 sec/batch\n",
      "Epoch 18/20  Iteration 3170/3560 Training loss: 1.5263 1.0643 sec/batch\n",
      "Epoch 18/20  Iteration 3171/3560 Training loss: 1.5262 1.0046 sec/batch\n",
      "Epoch 18/20  Iteration 3172/3560 Training loss: 1.5264 1.1249 sec/batch\n",
      "Epoch 18/20  Iteration 3173/3560 Training loss: 1.5265 1.0723 sec/batch\n",
      "Epoch 18/20  Iteration 3174/3560 Training loss: 1.5268 1.1108 sec/batch\n",
      "Epoch 18/20  Iteration 3175/3560 Training loss: 1.5269 1.0243 sec/batch\n",
      "Epoch 18/20  Iteration 3176/3560 Training loss: 1.5268 1.0471 sec/batch\n",
      "Epoch 18/20  Iteration 3177/3560 Training loss: 1.5265 0.9453 sec/batch\n",
      "Epoch 18/20  Iteration 3178/3560 Training loss: 1.5264 1.0415 sec/batch\n",
      "Epoch 18/20  Iteration 3179/3560 Training loss: 1.5265 0.9974 sec/batch\n",
      "Epoch 18/20  Iteration 3180/3560 Training loss: 1.5265 1.1198 sec/batch\n",
      "Epoch 18/20  Iteration 3181/3560 Training loss: 1.5266 1.0781 sec/batch\n",
      "Epoch 18/20  Iteration 3182/3560 Training loss: 1.5265 0.9742 sec/batch\n",
      "Epoch 18/20  Iteration 3183/3560 Training loss: 1.5266 1.1521 sec/batch\n",
      "Epoch 18/20  Iteration 3184/3560 Training loss: 1.5266 1.0603 sec/batch\n",
      "Epoch 18/20  Iteration 3185/3560 Training loss: 1.5264 1.0369 sec/batch\n",
      "Epoch 18/20  Iteration 3186/3560 Training loss: 1.5266 1.0016 sec/batch\n",
      "Epoch 18/20  Iteration 3187/3560 Training loss: 1.5268 1.1739 sec/batch\n",
      "Epoch 18/20  Iteration 3188/3560 Training loss: 1.5268 1.1485 sec/batch\n",
      "Epoch 18/20  Iteration 3189/3560 Training loss: 1.5269 0.9649 sec/batch\n",
      "Epoch 18/20  Iteration 3190/3560 Training loss: 1.5269 0.9889 sec/batch\n",
      "Epoch 18/20  Iteration 3191/3560 Training loss: 1.5270 1.0530 sec/batch\n",
      "Epoch 18/20  Iteration 3192/3560 Training loss: 1.5269 1.1779 sec/batch\n",
      "Epoch 18/20  Iteration 3193/3560 Training loss: 1.5271 1.0907 sec/batch\n",
      "Epoch 18/20  Iteration 3194/3560 Training loss: 1.5276 1.1083 sec/batch\n",
      "Epoch 18/20  Iteration 3195/3560 Training loss: 1.5275 1.0929 sec/batch\n",
      "Epoch 18/20  Iteration 3196/3560 Training loss: 1.5275 1.0147 sec/batch\n",
      "Epoch 18/20  Iteration 3197/3560 Training loss: 1.5275 0.9760 sec/batch\n",
      "Epoch 18/20  Iteration 3198/3560 Training loss: 1.5273 1.1593 sec/batch\n",
      "Epoch 18/20  Iteration 3199/3560 Training loss: 1.5274 1.0074 sec/batch\n",
      "Epoch 18/20  Iteration 3200/3560 Training loss: 1.5275 1.0349 sec/batch\n",
      "Epoch 18/20  Iteration 3201/3560 Training loss: 1.5276 1.1064 sec/batch\n",
      "Epoch 18/20  Iteration 3202/3560 Training loss: 1.5275 0.9749 sec/batch\n",
      "Epoch 18/20  Iteration 3203/3560 Training loss: 1.5274 1.0996 sec/batch\n",
      "Epoch 18/20  Iteration 3204/3560 Training loss: 1.5275 1.1086 sec/batch\n",
      "Epoch 19/20  Iteration 3205/3560 Training loss: 1.6027 1.0586 sec/batch\n",
      "Epoch 19/20  Iteration 3206/3560 Training loss: 1.5645 1.0327 sec/batch\n",
      "Epoch 19/20  Iteration 3207/3560 Training loss: 1.5511 0.9476 sec/batch\n",
      "Epoch 19/20  Iteration 3208/3560 Training loss: 1.5458 1.0834 sec/batch\n",
      "Epoch 19/20  Iteration 3209/3560 Training loss: 1.5375 1.0016 sec/batch\n",
      "Epoch 19/20  Iteration 3210/3560 Training loss: 1.5273 1.0512 sec/batch\n",
      "Epoch 19/20  Iteration 3211/3560 Training loss: 1.5270 1.0571 sec/batch\n",
      "Epoch 19/20  Iteration 3212/3560 Training loss: 1.5247 1.0842 sec/batch\n",
      "Epoch 19/20  Iteration 3213/3560 Training loss: 1.5261 1.1112 sec/batch\n",
      "Epoch 19/20  Iteration 3214/3560 Training loss: 1.5251 1.0599 sec/batch\n",
      "Epoch 19/20  Iteration 3215/3560 Training loss: 1.5218 1.1397 sec/batch\n",
      "Epoch 19/20  Iteration 3216/3560 Training loss: 1.5215 0.9928 sec/batch\n",
      "Epoch 19/20  Iteration 3217/3560 Training loss: 1.5216 0.9536 sec/batch\n",
      "Epoch 19/20  Iteration 3218/3560 Training loss: 1.5229 0.9958 sec/batch\n",
      "Epoch 19/20  Iteration 3219/3560 Training loss: 1.5224 1.1136 sec/batch\n",
      "Epoch 19/20  Iteration 3220/3560 Training loss: 1.5212 1.0194 sec/batch\n",
      "Epoch 19/20  Iteration 3221/3560 Training loss: 1.5215 1.0939 sec/batch\n",
      "Epoch 19/20  Iteration 3222/3560 Training loss: 1.5226 1.1259 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3223/3560 Training loss: 1.5226 1.0666 sec/batch\n",
      "Epoch 19/20  Iteration 3224/3560 Training loss: 1.5240 1.0181 sec/batch\n",
      "Epoch 19/20  Iteration 3225/3560 Training loss: 1.5231 1.0797 sec/batch\n",
      "Epoch 19/20  Iteration 3226/3560 Training loss: 1.5234 1.0118 sec/batch\n",
      "Epoch 19/20  Iteration 3227/3560 Training loss: 1.5229 1.1527 sec/batch\n",
      "Epoch 19/20  Iteration 3228/3560 Training loss: 1.5229 1.0702 sec/batch\n",
      "Epoch 19/20  Iteration 3229/3560 Training loss: 1.5231 0.9748 sec/batch\n",
      "Epoch 19/20  Iteration 3230/3560 Training loss: 1.5214 1.1707 sec/batch\n",
      "Epoch 19/20  Iteration 3231/3560 Training loss: 1.5202 1.0800 sec/batch\n",
      "Epoch 19/20  Iteration 3232/3560 Training loss: 1.5210 0.9884 sec/batch\n",
      "Epoch 19/20  Iteration 3233/3560 Training loss: 1.5217 1.0298 sec/batch\n",
      "Epoch 19/20  Iteration 3234/3560 Training loss: 1.5222 1.0290 sec/batch\n",
      "Epoch 19/20  Iteration 3235/3560 Training loss: 1.5218 1.0756 sec/batch\n",
      "Epoch 19/20  Iteration 3236/3560 Training loss: 1.5209 1.0424 sec/batch\n",
      "Epoch 19/20  Iteration 3237/3560 Training loss: 1.5213 1.0074 sec/batch\n",
      "Epoch 19/20  Iteration 3238/3560 Training loss: 1.5220 1.1153 sec/batch\n",
      "Epoch 19/20  Iteration 3239/3560 Training loss: 1.5216 1.1791 sec/batch\n",
      "Epoch 19/20  Iteration 3240/3560 Training loss: 1.5215 0.9809 sec/batch\n",
      "Epoch 19/20  Iteration 3241/3560 Training loss: 1.5208 1.1162 sec/batch\n",
      "Epoch 19/20  Iteration 3242/3560 Training loss: 1.5197 1.0672 sec/batch\n",
      "Epoch 19/20  Iteration 3243/3560 Training loss: 1.5184 1.0470 sec/batch\n",
      "Epoch 19/20  Iteration 3244/3560 Training loss: 1.5180 1.1098 sec/batch\n",
      "Epoch 19/20  Iteration 3245/3560 Training loss: 1.5175 0.9749 sec/batch\n",
      "Epoch 19/20  Iteration 3246/3560 Training loss: 1.5179 1.1637 sec/batch\n",
      "Epoch 19/20  Iteration 3247/3560 Training loss: 1.5174 1.0128 sec/batch\n",
      "Epoch 19/20  Iteration 3248/3560 Training loss: 1.5166 0.9869 sec/batch\n",
      "Epoch 19/20  Iteration 3249/3560 Training loss: 1.5169 0.9820 sec/batch\n",
      "Epoch 19/20  Iteration 3250/3560 Training loss: 1.5159 1.1337 sec/batch\n",
      "Epoch 19/20  Iteration 3251/3560 Training loss: 1.5154 1.0558 sec/batch\n",
      "Epoch 19/20  Iteration 3252/3560 Training loss: 1.5152 1.0300 sec/batch\n",
      "Epoch 19/20  Iteration 3253/3560 Training loss: 1.5152 1.1553 sec/batch\n",
      "Epoch 19/20  Iteration 3254/3560 Training loss: 1.5159 1.0542 sec/batch\n",
      "Epoch 19/20  Iteration 3255/3560 Training loss: 1.5153 1.0597 sec/batch\n",
      "Epoch 19/20  Iteration 3256/3560 Training loss: 1.5162 1.0333 sec/batch\n",
      "Epoch 19/20  Iteration 3257/3560 Training loss: 1.5162 1.0590 sec/batch\n",
      "Epoch 19/20  Iteration 3258/3560 Training loss: 1.5164 1.0495 sec/batch\n",
      "Epoch 19/20  Iteration 3259/3560 Training loss: 1.5162 1.1172 sec/batch\n",
      "Epoch 19/20  Iteration 3260/3560 Training loss: 1.5163 1.0989 sec/batch\n",
      "Epoch 19/20  Iteration 3261/3560 Training loss: 1.5168 0.9359 sec/batch\n",
      "Epoch 19/20  Iteration 3262/3560 Training loss: 1.5165 1.0850 sec/batch\n",
      "Epoch 19/20  Iteration 3263/3560 Training loss: 1.5161 1.0738 sec/batch\n",
      "Epoch 19/20  Iteration 3264/3560 Training loss: 1.5166 1.0134 sec/batch\n",
      "Epoch 19/20  Iteration 3265/3560 Training loss: 1.5166 1.0476 sec/batch\n",
      "Epoch 19/20  Iteration 3266/3560 Training loss: 1.5175 1.0445 sec/batch\n",
      "Epoch 19/20  Iteration 3267/3560 Training loss: 1.5181 0.9941 sec/batch\n",
      "Epoch 19/20  Iteration 3268/3560 Training loss: 1.5184 1.0616 sec/batch\n",
      "Epoch 19/20  Iteration 3269/3560 Training loss: 1.5183 1.0976 sec/batch\n",
      "Epoch 19/20  Iteration 3270/3560 Training loss: 1.5186 1.1567 sec/batch\n",
      "Epoch 19/20  Iteration 3271/3560 Training loss: 1.5188 1.0592 sec/batch\n",
      "Epoch 19/20  Iteration 3272/3560 Training loss: 1.5186 1.0768 sec/batch\n",
      "Epoch 19/20  Iteration 3273/3560 Training loss: 1.5185 0.9862 sec/batch\n",
      "Epoch 19/20  Iteration 3274/3560 Training loss: 1.5183 1.0078 sec/batch\n",
      "Epoch 19/20  Iteration 3275/3560 Training loss: 1.5192 1.0238 sec/batch\n",
      "Epoch 19/20  Iteration 3276/3560 Training loss: 1.5194 1.0136 sec/batch\n",
      "Epoch 19/20  Iteration 3277/3560 Training loss: 1.5199 1.0190 sec/batch\n",
      "Epoch 19/20  Iteration 3278/3560 Training loss: 1.5197 1.0503 sec/batch\n",
      "Epoch 19/20  Iteration 3279/3560 Training loss: 1.5198 1.1428 sec/batch\n",
      "Epoch 19/20  Iteration 3280/3560 Training loss: 1.5200 1.0589 sec/batch\n",
      "Epoch 19/20  Iteration 3281/3560 Training loss: 1.5200 0.9732 sec/batch\n",
      "Epoch 19/20  Iteration 3282/3560 Training loss: 1.5201 1.0744 sec/batch\n",
      "Epoch 19/20  Iteration 3283/3560 Training loss: 1.5196 1.1532 sec/batch\n",
      "Epoch 19/20  Iteration 3284/3560 Training loss: 1.5196 1.0543 sec/batch\n",
      "Epoch 19/20  Iteration 3285/3560 Training loss: 1.5191 0.9659 sec/batch\n",
      "Epoch 19/20  Iteration 3286/3560 Training loss: 1.5191 1.1093 sec/batch\n",
      "Epoch 19/20  Iteration 3287/3560 Training loss: 1.5187 1.0047 sec/batch\n",
      "Epoch 19/20  Iteration 3288/3560 Training loss: 1.5187 1.0907 sec/batch\n",
      "Epoch 19/20  Iteration 3289/3560 Training loss: 1.5184 0.9540 sec/batch\n",
      "Epoch 19/20  Iteration 3290/3560 Training loss: 1.5183 1.0393 sec/batch\n",
      "Epoch 19/20  Iteration 3291/3560 Training loss: 1.5181 1.0692 sec/batch\n",
      "Epoch 19/20  Iteration 3292/3560 Training loss: 1.5179 1.0697 sec/batch\n",
      "Epoch 19/20  Iteration 3293/3560 Training loss: 1.5175 0.9767 sec/batch\n",
      "Epoch 19/20  Iteration 3294/3560 Training loss: 1.5177 1.0532 sec/batch\n",
      "Epoch 19/20  Iteration 3295/3560 Training loss: 1.5174 1.1114 sec/batch\n",
      "Epoch 19/20  Iteration 3296/3560 Training loss: 1.5173 1.0272 sec/batch\n",
      "Epoch 19/20  Iteration 3297/3560 Training loss: 1.5169 1.0517 sec/batch\n",
      "Epoch 19/20  Iteration 3298/3560 Training loss: 1.5165 1.0826 sec/batch\n",
      "Epoch 19/20  Iteration 3299/3560 Training loss: 1.5163 1.0210 sec/batch\n",
      "Epoch 19/20  Iteration 3300/3560 Training loss: 1.5164 1.0900 sec/batch\n",
      "Epoch 19/20  Iteration 3301/3560 Training loss: 1.5164 1.0834 sec/batch\n",
      "Epoch 19/20  Iteration 3302/3560 Training loss: 1.5160 1.1036 sec/batch\n",
      "Epoch 19/20  Iteration 3303/3560 Training loss: 1.5157 0.9729 sec/batch\n",
      "Epoch 19/20  Iteration 3304/3560 Training loss: 1.5153 1.0561 sec/batch\n",
      "Epoch 19/20  Iteration 3305/3560 Training loss: 1.5153 0.9777 sec/batch\n",
      "Epoch 19/20  Iteration 3306/3560 Training loss: 1.5151 1.0171 sec/batch\n",
      "Epoch 19/20  Iteration 3307/3560 Training loss: 1.5151 1.2174 sec/batch\n",
      "Epoch 19/20  Iteration 3308/3560 Training loss: 1.5149 0.9676 sec/batch\n",
      "Epoch 19/20  Iteration 3309/3560 Training loss: 1.5147 1.0258 sec/batch\n",
      "Epoch 19/20  Iteration 3310/3560 Training loss: 1.5145 1.1598 sec/batch\n",
      "Epoch 19/20  Iteration 3311/3560 Training loss: 1.5146 1.0219 sec/batch\n",
      "Epoch 19/20  Iteration 3312/3560 Training loss: 1.5146 1.0263 sec/batch\n",
      "Epoch 19/20  Iteration 3313/3560 Training loss: 1.5144 0.9940 sec/batch\n",
      "Epoch 19/20  Iteration 3314/3560 Training loss: 1.5145 1.0114 sec/batch\n",
      "Epoch 19/20  Iteration 3315/3560 Training loss: 1.5143 1.1973 sec/batch\n",
      "Epoch 19/20  Iteration 3316/3560 Training loss: 1.5143 1.1109 sec/batch\n",
      "Epoch 19/20  Iteration 3317/3560 Training loss: 1.5141 1.0475 sec/batch\n",
      "Epoch 19/20  Iteration 3318/3560 Training loss: 1.5140 1.1017 sec/batch\n",
      "Epoch 19/20  Iteration 3319/3560 Training loss: 1.5137 1.0248 sec/batch\n",
      "Epoch 19/20  Iteration 3320/3560 Training loss: 1.5133 1.0227 sec/batch\n",
      "Epoch 19/20  Iteration 3321/3560 Training loss: 1.5133 1.0243 sec/batch\n",
      "Epoch 19/20  Iteration 3322/3560 Training loss: 1.5134 1.0332 sec/batch\n",
      "Epoch 19/20  Iteration 3323/3560 Training loss: 1.5132 1.1849 sec/batch\n",
      "Epoch 19/20  Iteration 3324/3560 Training loss: 1.5132 1.0750 sec/batch\n",
      "Epoch 19/20  Iteration 3325/3560 Training loss: 1.5130 0.9800 sec/batch\n",
      "Epoch 19/20  Iteration 3326/3560 Training loss: 1.5127 1.1857 sec/batch\n",
      "Epoch 19/20  Iteration 3327/3560 Training loss: 1.5123 1.0512 sec/batch\n",
      "Epoch 19/20  Iteration 3328/3560 Training loss: 1.5123 1.0376 sec/batch\n",
      "Epoch 19/20  Iteration 3329/3560 Training loss: 1.5123 0.9866 sec/batch\n",
      "Epoch 19/20  Iteration 3330/3560 Training loss: 1.5119 1.1284 sec/batch\n",
      "Epoch 19/20  Iteration 3331/3560 Training loss: 1.5120 1.0167 sec/batch\n",
      "Epoch 19/20  Iteration 3332/3560 Training loss: 1.5121 0.9988 sec/batch\n",
      "Epoch 19/20  Iteration 3333/3560 Training loss: 1.5119 1.1592 sec/batch\n",
      "Epoch 19/20  Iteration 3334/3560 Training loss: 1.5117 1.0951 sec/batch\n",
      "Epoch 19/20  Iteration 3335/3560 Training loss: 1.5113 1.1072 sec/batch\n",
      "Epoch 19/20  Iteration 3336/3560 Training loss: 1.5111 1.0761 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3337/3560 Training loss: 1.5113 1.0769 sec/batch\n",
      "Epoch 19/20  Iteration 3338/3560 Training loss: 1.5113 1.1472 sec/batch\n",
      "Epoch 19/20  Iteration 3339/3560 Training loss: 1.5113 1.1038 sec/batch\n",
      "Epoch 19/20  Iteration 3340/3560 Training loss: 1.5114 0.9899 sec/batch\n",
      "Epoch 19/20  Iteration 3341/3560 Training loss: 1.5115 1.1467 sec/batch\n",
      "Epoch 19/20  Iteration 3342/3560 Training loss: 1.5116 1.0353 sec/batch\n",
      "Epoch 19/20  Iteration 3343/3560 Training loss: 1.5116 0.9826 sec/batch\n",
      "Epoch 19/20  Iteration 3344/3560 Training loss: 1.5115 0.9830 sec/batch\n",
      "Epoch 19/20  Iteration 3345/3560 Training loss: 1.5119 0.9704 sec/batch\n",
      "Epoch 19/20  Iteration 3346/3560 Training loss: 1.5119 1.1984 sec/batch\n",
      "Epoch 19/20  Iteration 3347/3560 Training loss: 1.5119 1.0344 sec/batch\n",
      "Epoch 19/20  Iteration 3348/3560 Training loss: 1.5121 1.0359 sec/batch\n",
      "Epoch 19/20  Iteration 3349/3560 Training loss: 1.5119 1.1591 sec/batch\n",
      "Epoch 19/20  Iteration 3350/3560 Training loss: 1.5121 1.0373 sec/batch\n",
      "Epoch 19/20  Iteration 3351/3560 Training loss: 1.5120 1.0185 sec/batch\n",
      "Epoch 19/20  Iteration 3352/3560 Training loss: 1.5123 0.9929 sec/batch\n",
      "Epoch 19/20  Iteration 3353/3560 Training loss: 1.5124 1.0413 sec/batch\n",
      "Epoch 19/20  Iteration 3354/3560 Training loss: 1.5123 1.0300 sec/batch\n",
      "Epoch 19/20  Iteration 3355/3560 Training loss: 1.5120 1.0438 sec/batch\n",
      "Epoch 19/20  Iteration 3356/3560 Training loss: 1.5119 1.0750 sec/batch\n",
      "Epoch 19/20  Iteration 3357/3560 Training loss: 1.5120 1.1674 sec/batch\n",
      "Epoch 19/20  Iteration 3358/3560 Training loss: 1.5120 1.1246 sec/batch\n",
      "Epoch 19/20  Iteration 3359/3560 Training loss: 1.5121 1.0052 sec/batch\n",
      "Epoch 19/20  Iteration 3360/3560 Training loss: 1.5121 0.9459 sec/batch\n",
      "Epoch 19/20  Iteration 3361/3560 Training loss: 1.5122 1.0417 sec/batch\n",
      "Epoch 19/20  Iteration 3362/3560 Training loss: 1.5122 1.0593 sec/batch\n",
      "Epoch 19/20  Iteration 3363/3560 Training loss: 1.5120 1.0694 sec/batch\n",
      "Epoch 19/20  Iteration 3364/3560 Training loss: 1.5121 1.0827 sec/batch\n",
      "Epoch 19/20  Iteration 3365/3560 Training loss: 1.5124 1.1110 sec/batch\n",
      "Epoch 19/20  Iteration 3366/3560 Training loss: 1.5124 1.0155 sec/batch\n",
      "Epoch 19/20  Iteration 3367/3560 Training loss: 1.5124 1.1486 sec/batch\n",
      "Epoch 19/20  Iteration 3368/3560 Training loss: 1.5125 0.9812 sec/batch\n",
      "Epoch 19/20  Iteration 3369/3560 Training loss: 1.5124 1.0883 sec/batch\n",
      "Epoch 19/20  Iteration 3370/3560 Training loss: 1.5124 0.9311 sec/batch\n",
      "Epoch 19/20  Iteration 3371/3560 Training loss: 1.5127 1.1533 sec/batch\n",
      "Epoch 19/20  Iteration 3372/3560 Training loss: 1.5131 1.0579 sec/batch\n",
      "Epoch 19/20  Iteration 3373/3560 Training loss: 1.5130 1.1055 sec/batch\n",
      "Epoch 19/20  Iteration 3374/3560 Training loss: 1.5130 1.0623 sec/batch\n",
      "Epoch 19/20  Iteration 3375/3560 Training loss: 1.5129 1.0445 sec/batch\n",
      "Epoch 19/20  Iteration 3376/3560 Training loss: 1.5127 0.9407 sec/batch\n",
      "Epoch 19/20  Iteration 3377/3560 Training loss: 1.5128 1.0962 sec/batch\n",
      "Epoch 19/20  Iteration 3378/3560 Training loss: 1.5129 1.1458 sec/batch\n",
      "Epoch 19/20  Iteration 3379/3560 Training loss: 1.5129 1.2542 sec/batch\n",
      "Epoch 19/20  Iteration 3380/3560 Training loss: 1.5128 1.2434 sec/batch\n",
      "Epoch 19/20  Iteration 3381/3560 Training loss: 1.5127 1.0551 sec/batch\n",
      "Epoch 19/20  Iteration 3382/3560 Training loss: 1.5128 1.1865 sec/batch\n",
      "Epoch 20/20  Iteration 3383/3560 Training loss: 1.5892 1.1024 sec/batch\n",
      "Epoch 20/20  Iteration 3384/3560 Training loss: 1.5510 1.0184 sec/batch\n",
      "Epoch 20/20  Iteration 3385/3560 Training loss: 1.5389 1.0715 sec/batch\n",
      "Epoch 20/20  Iteration 3386/3560 Training loss: 1.5340 1.0791 sec/batch\n",
      "Epoch 20/20  Iteration 3387/3560 Training loss: 1.5251 1.0839 sec/batch\n",
      "Epoch 20/20  Iteration 3388/3560 Training loss: 1.5152 1.0951 sec/batch\n",
      "Epoch 20/20  Iteration 3389/3560 Training loss: 1.5149 1.1668 sec/batch\n",
      "Epoch 20/20  Iteration 3390/3560 Training loss: 1.5134 1.0566 sec/batch\n",
      "Epoch 20/20  Iteration 3391/3560 Training loss: 1.5140 0.9810 sec/batch\n",
      "Epoch 20/20  Iteration 3392/3560 Training loss: 1.5139 0.8985 sec/batch\n",
      "Epoch 20/20  Iteration 3393/3560 Training loss: 1.5106 1.0722 sec/batch\n",
      "Epoch 20/20  Iteration 3394/3560 Training loss: 1.5100 1.1388 sec/batch\n",
      "Epoch 20/20  Iteration 3395/3560 Training loss: 1.5096 1.2104 sec/batch\n",
      "Epoch 20/20  Iteration 3396/3560 Training loss: 1.5110 1.0152 sec/batch\n",
      "Epoch 20/20  Iteration 3397/3560 Training loss: 1.5100 0.9749 sec/batch\n",
      "Epoch 20/20  Iteration 3398/3560 Training loss: 1.5082 1.0455 sec/batch\n",
      "Epoch 20/20  Iteration 3399/3560 Training loss: 1.5087 1.0603 sec/batch\n",
      "Epoch 20/20  Iteration 3400/3560 Training loss: 1.5103 0.8630 sec/batch\n",
      "Epoch 20/20  Iteration 3401/3560 Training loss: 1.5105 1.0785 sec/batch\n",
      "Epoch 20/20  Iteration 3402/3560 Training loss: 1.5119 1.1621 sec/batch\n",
      "Epoch 20/20  Iteration 3403/3560 Training loss: 1.5113 1.0700 sec/batch\n",
      "Epoch 20/20  Iteration 3404/3560 Training loss: 1.5114 1.2932 sec/batch\n",
      "Epoch 20/20  Iteration 3405/3560 Training loss: 1.5105 1.1394 sec/batch\n",
      "Epoch 20/20  Iteration 3406/3560 Training loss: 1.5096 1.0215 sec/batch\n",
      "Epoch 20/20  Iteration 3407/3560 Training loss: 1.5096 0.9801 sec/batch\n",
      "Epoch 20/20  Iteration 3408/3560 Training loss: 1.5083 1.0581 sec/batch\n",
      "Epoch 20/20  Iteration 3409/3560 Training loss: 1.5070 0.9895 sec/batch\n",
      "Epoch 20/20  Iteration 3410/3560 Training loss: 1.5076 1.1084 sec/batch\n",
      "Epoch 20/20  Iteration 3411/3560 Training loss: 1.5081 1.1432 sec/batch\n",
      "Epoch 20/20  Iteration 3412/3560 Training loss: 1.5086 1.1748 sec/batch\n",
      "Epoch 20/20  Iteration 3413/3560 Training loss: 1.5085 1.0946 sec/batch\n",
      "Epoch 20/20  Iteration 3414/3560 Training loss: 1.5071 1.0042 sec/batch\n",
      "Epoch 20/20  Iteration 3415/3560 Training loss: 1.5074 1.0401 sec/batch\n",
      "Epoch 20/20  Iteration 3416/3560 Training loss: 1.5077 1.0060 sec/batch\n",
      "Epoch 20/20  Iteration 3417/3560 Training loss: 1.5075 0.9328 sec/batch\n",
      "Epoch 20/20  Iteration 3418/3560 Training loss: 1.5073 1.0247 sec/batch\n",
      "Epoch 20/20  Iteration 3419/3560 Training loss: 1.5068 1.1461 sec/batch\n",
      "Epoch 20/20  Iteration 3420/3560 Training loss: 1.5056 1.0376 sec/batch\n",
      "Epoch 20/20  Iteration 3421/3560 Training loss: 1.5041 1.0682 sec/batch\n",
      "Epoch 20/20  Iteration 3422/3560 Training loss: 1.5036 1.1177 sec/batch\n",
      "Epoch 20/20  Iteration 3423/3560 Training loss: 1.5030 1.0940 sec/batch\n",
      "Epoch 20/20  Iteration 3424/3560 Training loss: 1.5036 1.0391 sec/batch\n",
      "Epoch 20/20  Iteration 3425/3560 Training loss: 1.5032 1.0409 sec/batch\n",
      "Epoch 20/20  Iteration 3426/3560 Training loss: 1.5023 1.2000 sec/batch\n",
      "Epoch 20/20  Iteration 3427/3560 Training loss: 1.5024 1.0901 sec/batch\n",
      "Epoch 20/20  Iteration 3428/3560 Training loss: 1.5013 1.1353 sec/batch\n",
      "Epoch 20/20  Iteration 3429/3560 Training loss: 1.5009 1.1253 sec/batch\n",
      "Epoch 20/20  Iteration 3430/3560 Training loss: 1.5004 0.9044 sec/batch\n",
      "Epoch 20/20  Iteration 3431/3560 Training loss: 1.5004 1.1043 sec/batch\n",
      "Epoch 20/20  Iteration 3432/3560 Training loss: 1.5010 0.9642 sec/batch\n",
      "Epoch 20/20  Iteration 3433/3560 Training loss: 1.5007 1.1121 sec/batch\n",
      "Epoch 20/20  Iteration 3434/3560 Training loss: 1.5016 1.0598 sec/batch\n",
      "Epoch 20/20  Iteration 3435/3560 Training loss: 1.5015 1.0790 sec/batch\n",
      "Epoch 20/20  Iteration 3436/3560 Training loss: 1.5016 1.1193 sec/batch\n",
      "Epoch 20/20  Iteration 3437/3560 Training loss: 1.5015 1.1611 sec/batch\n",
      "Epoch 20/20  Iteration 3438/3560 Training loss: 1.5017 0.9947 sec/batch\n",
      "Epoch 20/20  Iteration 3439/3560 Training loss: 1.5022 0.9609 sec/batch\n",
      "Epoch 20/20  Iteration 3440/3560 Training loss: 1.5021 1.0923 sec/batch\n",
      "Epoch 20/20  Iteration 3441/3560 Training loss: 1.5017 1.1221 sec/batch\n",
      "Epoch 20/20  Iteration 3442/3560 Training loss: 1.5023 1.0918 sec/batch\n",
      "Epoch 20/20  Iteration 3443/3560 Training loss: 1.5022 1.1415 sec/batch\n",
      "Epoch 20/20  Iteration 3444/3560 Training loss: 1.5033 1.1089 sec/batch\n",
      "Epoch 20/20  Iteration 3445/3560 Training loss: 1.5036 1.0587 sec/batch\n",
      "Epoch 20/20  Iteration 3446/3560 Training loss: 1.5040 1.0215 sec/batch\n",
      "Epoch 20/20  Iteration 3447/3560 Training loss: 1.5041 1.0542 sec/batch\n",
      "Epoch 20/20  Iteration 3448/3560 Training loss: 1.5044 1.0468 sec/batch\n",
      "Epoch 20/20  Iteration 3449/3560 Training loss: 1.5046 1.0451 sec/batch\n",
      "Epoch 20/20  Iteration 3450/3560 Training loss: 1.5043 1.0466 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3451/3560 Training loss: 1.5044 1.1889 sec/batch\n",
      "Epoch 20/20  Iteration 3452/3560 Training loss: 1.5043 0.9917 sec/batch\n",
      "Epoch 20/20  Iteration 3453/3560 Training loss: 1.5051 1.1266 sec/batch\n",
      "Epoch 20/20  Iteration 3454/3560 Training loss: 1.5052 1.0343 sec/batch\n",
      "Epoch 20/20  Iteration 3455/3560 Training loss: 1.5058 1.0788 sec/batch\n",
      "Epoch 20/20  Iteration 3456/3560 Training loss: 1.5056 0.9779 sec/batch\n",
      "Epoch 20/20  Iteration 3457/3560 Training loss: 1.5056 1.0981 sec/batch\n",
      "Epoch 20/20  Iteration 3458/3560 Training loss: 1.5058 1.1223 sec/batch\n",
      "Epoch 20/20  Iteration 3459/3560 Training loss: 1.5058 1.1327 sec/batch\n",
      "Epoch 20/20  Iteration 3460/3560 Training loss: 1.5058 1.1251 sec/batch\n",
      "Epoch 20/20  Iteration 3461/3560 Training loss: 1.5053 1.0436 sec/batch\n",
      "Epoch 20/20  Iteration 3462/3560 Training loss: 1.5053 1.0604 sec/batch\n",
      "Epoch 20/20  Iteration 3463/3560 Training loss: 1.5049 1.0102 sec/batch\n",
      "Epoch 20/20  Iteration 3464/3560 Training loss: 1.5048 1.0418 sec/batch\n",
      "Epoch 20/20  Iteration 3465/3560 Training loss: 1.5043 1.0315 sec/batch\n",
      "Epoch 20/20  Iteration 3466/3560 Training loss: 1.5044 1.1251 sec/batch\n",
      "Epoch 20/20  Iteration 3467/3560 Training loss: 1.5040 1.1828 sec/batch\n",
      "Epoch 20/20  Iteration 3468/3560 Training loss: 1.5038 1.0580 sec/batch\n",
      "Epoch 20/20  Iteration 3469/3560 Training loss: 1.5036 1.0159 sec/batch\n",
      "Epoch 20/20  Iteration 3470/3560 Training loss: 1.5034 1.0932 sec/batch\n",
      "Epoch 20/20  Iteration 3471/3560 Training loss: 1.5029 1.0492 sec/batch\n",
      "Epoch 20/20  Iteration 3472/3560 Training loss: 1.5029 0.9926 sec/batch\n",
      "Epoch 20/20  Iteration 3473/3560 Training loss: 1.5027 0.9984 sec/batch\n",
      "Epoch 20/20  Iteration 3474/3560 Training loss: 1.5026 1.1587 sec/batch\n",
      "Epoch 20/20  Iteration 3475/3560 Training loss: 1.5023 1.2223 sec/batch\n",
      "Epoch 20/20  Iteration 3476/3560 Training loss: 1.5019 1.0188 sec/batch\n",
      "Epoch 20/20  Iteration 3477/3560 Training loss: 1.5017 1.1575 sec/batch\n",
      "Epoch 20/20  Iteration 3478/3560 Training loss: 1.5018 0.9531 sec/batch\n",
      "Epoch 20/20  Iteration 3479/3560 Training loss: 1.5018 1.2048 sec/batch\n",
      "Epoch 20/20  Iteration 3480/3560 Training loss: 1.5014 0.9430 sec/batch\n",
      "Epoch 20/20  Iteration 3481/3560 Training loss: 1.5012 1.2126 sec/batch\n",
      "Epoch 20/20  Iteration 3482/3560 Training loss: 1.5009 1.0305 sec/batch\n",
      "Epoch 20/20  Iteration 3483/3560 Training loss: 1.5009 1.1035 sec/batch\n",
      "Epoch 20/20  Iteration 3484/3560 Training loss: 1.5009 1.0555 sec/batch\n",
      "Epoch 20/20  Iteration 3485/3560 Training loss: 1.5008 1.0675 sec/batch\n",
      "Epoch 20/20  Iteration 3486/3560 Training loss: 1.5007 1.0189 sec/batch\n",
      "Epoch 20/20  Iteration 3487/3560 Training loss: 1.5006 1.0549 sec/batch\n",
      "Epoch 20/20  Iteration 3488/3560 Training loss: 1.5004 0.9979 sec/batch\n",
      "Epoch 20/20  Iteration 3489/3560 Training loss: 1.5005 1.0587 sec/batch\n",
      "Epoch 20/20  Iteration 3490/3560 Training loss: 1.5005 1.1320 sec/batch\n",
      "Epoch 20/20  Iteration 3491/3560 Training loss: 1.5004 1.0369 sec/batch\n",
      "Epoch 20/20  Iteration 3492/3560 Training loss: 1.5005 1.1111 sec/batch\n",
      "Epoch 20/20  Iteration 3493/3560 Training loss: 1.5004 1.0349 sec/batch\n",
      "Epoch 20/20  Iteration 3494/3560 Training loss: 1.5003 1.0934 sec/batch\n",
      "Epoch 20/20  Iteration 3495/3560 Training loss: 1.5001 0.9488 sec/batch\n",
      "Epoch 20/20  Iteration 3496/3560 Training loss: 1.4999 1.0709 sec/batch\n",
      "Epoch 20/20  Iteration 3497/3560 Training loss: 1.4997 1.0690 sec/batch\n",
      "Epoch 20/20  Iteration 3498/3560 Training loss: 1.4993 1.1527 sec/batch\n",
      "Epoch 20/20  Iteration 3499/3560 Training loss: 1.4992 1.1514 sec/batch\n",
      "Epoch 20/20  Iteration 3500/3560 Training loss: 1.4993 1.1126 sec/batch\n",
      "Epoch 20/20  Iteration 3501/3560 Training loss: 1.4992 1.1498 sec/batch\n",
      "Epoch 20/20  Iteration 3502/3560 Training loss: 1.4992 1.0004 sec/batch\n",
      "Epoch 20/20  Iteration 3503/3560 Training loss: 1.4991 1.0909 sec/batch\n",
      "Epoch 20/20  Iteration 3504/3560 Training loss: 1.4988 1.0200 sec/batch\n",
      "Epoch 20/20  Iteration 3505/3560 Training loss: 1.4984 1.0219 sec/batch\n",
      "Epoch 20/20  Iteration 3506/3560 Training loss: 1.4985 1.1190 sec/batch\n",
      "Epoch 20/20  Iteration 3507/3560 Training loss: 1.4985 1.0718 sec/batch\n",
      "Epoch 20/20  Iteration 3508/3560 Training loss: 1.4981 1.2336 sec/batch\n",
      "Epoch 20/20  Iteration 3509/3560 Training loss: 1.4982 1.0666 sec/batch\n",
      "Epoch 20/20  Iteration 3510/3560 Training loss: 1.4982 1.0214 sec/batch\n",
      "Epoch 20/20  Iteration 3511/3560 Training loss: 1.4981 1.0466 sec/batch\n",
      "Epoch 20/20  Iteration 3512/3560 Training loss: 1.4979 1.1011 sec/batch\n",
      "Epoch 20/20  Iteration 3513/3560 Training loss: 1.4974 1.0742 sec/batch\n",
      "Epoch 20/20  Iteration 3514/3560 Training loss: 1.4973 1.0738 sec/batch\n",
      "Epoch 20/20  Iteration 3515/3560 Training loss: 1.4974 1.1027 sec/batch\n",
      "Epoch 20/20  Iteration 3516/3560 Training loss: 1.4974 1.0940 sec/batch\n",
      "Epoch 20/20  Iteration 3517/3560 Training loss: 1.4974 1.0197 sec/batch\n",
      "Epoch 20/20  Iteration 3518/3560 Training loss: 1.4976 0.9872 sec/batch\n",
      "Epoch 20/20  Iteration 3519/3560 Training loss: 1.4977 1.0247 sec/batch\n",
      "Epoch 20/20  Iteration 3520/3560 Training loss: 1.4978 1.0605 sec/batch\n",
      "Epoch 20/20  Iteration 3521/3560 Training loss: 1.4979 1.0850 sec/batch\n",
      "Epoch 20/20  Iteration 3522/3560 Training loss: 1.4978 1.0592 sec/batch\n",
      "Epoch 20/20  Iteration 3523/3560 Training loss: 1.4981 1.0787 sec/batch\n",
      "Epoch 20/20  Iteration 3524/3560 Training loss: 1.4981 1.1659 sec/batch\n",
      "Epoch 20/20  Iteration 3525/3560 Training loss: 1.4980 1.0564 sec/batch\n",
      "Epoch 20/20  Iteration 3526/3560 Training loss: 1.4982 0.9933 sec/batch\n",
      "Epoch 20/20  Iteration 3527/3560 Training loss: 1.4981 1.0697 sec/batch\n",
      "Epoch 20/20  Iteration 3528/3560 Training loss: 1.4982 1.0967 sec/batch\n",
      "Epoch 20/20  Iteration 3529/3560 Training loss: 1.4982 1.0392 sec/batch\n",
      "Epoch 20/20  Iteration 3530/3560 Training loss: 1.4986 1.0391 sec/batch\n",
      "Epoch 20/20  Iteration 3531/3560 Training loss: 1.4987 1.0701 sec/batch\n",
      "Epoch 20/20  Iteration 3532/3560 Training loss: 1.4986 1.0370 sec/batch\n",
      "Epoch 20/20  Iteration 3533/3560 Training loss: 1.4982 0.9879 sec/batch\n",
      "Epoch 20/20  Iteration 3534/3560 Training loss: 1.4982 1.0870 sec/batch\n",
      "Epoch 20/20  Iteration 3535/3560 Training loss: 1.4982 0.9656 sec/batch\n",
      "Epoch 20/20  Iteration 3536/3560 Training loss: 1.4983 1.0967 sec/batch\n",
      "Epoch 20/20  Iteration 3537/3560 Training loss: 1.4983 1.1757 sec/batch\n",
      "Epoch 20/20  Iteration 3538/3560 Training loss: 1.4983 0.9912 sec/batch\n",
      "Epoch 20/20  Iteration 3539/3560 Training loss: 1.4984 1.1354 sec/batch\n",
      "Epoch 20/20  Iteration 3540/3560 Training loss: 1.4985 1.0858 sec/batch\n",
      "Epoch 20/20  Iteration 3541/3560 Training loss: 1.4983 0.9662 sec/batch\n",
      "Epoch 20/20  Iteration 3542/3560 Training loss: 1.4984 1.0067 sec/batch\n",
      "Epoch 20/20  Iteration 3543/3560 Training loss: 1.4986 1.0520 sec/batch\n",
      "Epoch 20/20  Iteration 3544/3560 Training loss: 1.4986 1.0490 sec/batch\n",
      "Epoch 20/20  Iteration 3545/3560 Training loss: 1.4986 1.2255 sec/batch\n",
      "Epoch 20/20  Iteration 3546/3560 Training loss: 1.4987 1.0391 sec/batch\n",
      "Epoch 20/20  Iteration 3547/3560 Training loss: 1.4987 1.0411 sec/batch\n",
      "Epoch 20/20  Iteration 3548/3560 Training loss: 1.4986 1.1112 sec/batch\n",
      "Epoch 20/20  Iteration 3549/3560 Training loss: 1.4989 1.0187 sec/batch\n",
      "Epoch 20/20  Iteration 3550/3560 Training loss: 1.4993 1.0055 sec/batch\n",
      "Epoch 20/20  Iteration 3551/3560 Training loss: 1.4993 1.1033 sec/batch\n",
      "Epoch 20/20  Iteration 3552/3560 Training loss: 1.4993 1.0292 sec/batch\n",
      "Epoch 20/20  Iteration 3553/3560 Training loss: 1.4992 1.2026 sec/batch\n",
      "Epoch 20/20  Iteration 3554/3560 Training loss: 1.4990 1.0996 sec/batch\n",
      "Epoch 20/20  Iteration 3555/3560 Training loss: 1.4992 0.9972 sec/batch\n",
      "Epoch 20/20  Iteration 3556/3560 Training loss: 1.4993 1.1779 sec/batch\n",
      "Epoch 20/20  Iteration 3557/3560 Training loss: 1.4994 1.0641 sec/batch\n",
      "Epoch 20/20  Iteration 3558/3560 Training loss: 1.4993 1.0436 sec/batch\n",
      "Epoch 20/20  Iteration 3559/3560 Training loss: 1.4992 1.0281 sec/batch\n",
      "Epoch 20/20  Iteration 3560/3560 Training loss: 1.4993 1.1650 sec/batch\n",
      "Epoch 1/20  Iteration 1/3560 Training loss: 4.4193 2.2075 sec/batch\n",
      "Epoch 1/20  Iteration 2/3560 Training loss: 4.3884 2.2863 sec/batch\n",
      "Epoch 1/20  Iteration 3/3560 Training loss: 4.2638 2.0014 sec/batch\n",
      "Epoch 1/20  Iteration 4/3560 Training loss: 4.1149 2.1242 sec/batch\n",
      "Epoch 1/20  Iteration 5/3560 Training loss: 4.0002 2.0186 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 6/3560 Training loss: 3.9136 2.1228 sec/batch\n",
      "Epoch 1/20  Iteration 7/3560 Training loss: 3.8441 1.9415 sec/batch\n",
      "Epoch 1/20  Iteration 8/3560 Training loss: 3.7852 1.9963 sec/batch\n",
      "Epoch 1/20  Iteration 9/3560 Training loss: 3.7349 2.0034 sec/batch\n",
      "Epoch 1/20  Iteration 10/3560 Training loss: 3.6929 1.9753 sec/batch\n",
      "Epoch 1/20  Iteration 11/3560 Training loss: 3.6534 2.1183 sec/batch\n",
      "Epoch 1/20  Iteration 12/3560 Training loss: 3.6220 2.1199 sec/batch\n",
      "Epoch 1/20  Iteration 13/3560 Training loss: 3.5936 1.9851 sec/batch\n",
      "Epoch 1/20  Iteration 14/3560 Training loss: 3.5698 2.0668 sec/batch\n",
      "Epoch 1/20  Iteration 15/3560 Training loss: 3.5480 2.0603 sec/batch\n",
      "Epoch 1/20  Iteration 16/3560 Training loss: 3.5287 2.2428 sec/batch\n",
      "Epoch 1/20  Iteration 17/3560 Training loss: 3.5100 1.9088 sec/batch\n",
      "Epoch 1/20  Iteration 18/3560 Training loss: 3.4950 2.2301 sec/batch\n",
      "Epoch 1/20  Iteration 19/3560 Training loss: 3.4807 2.3153 sec/batch\n",
      "Epoch 1/20  Iteration 20/3560 Training loss: 3.4657 2.3017 sec/batch\n",
      "Epoch 1/20  Iteration 21/3560 Training loss: 3.4528 2.3283 sec/batch\n",
      "Epoch 1/20  Iteration 22/3560 Training loss: 3.4410 1.9709 sec/batch\n",
      "Epoch 1/20  Iteration 23/3560 Training loss: 3.4301 2.1105 sec/batch\n",
      "Epoch 1/20  Iteration 24/3560 Training loss: 3.4200 2.1073 sec/batch\n",
      "Epoch 1/20  Iteration 25/3560 Training loss: 3.4105 2.1260 sec/batch\n",
      "Epoch 1/20  Iteration 26/3560 Training loss: 3.4023 2.0280 sec/batch\n",
      "Epoch 1/20  Iteration 27/3560 Training loss: 3.3946 1.9453 sec/batch\n",
      "Epoch 1/20  Iteration 28/3560 Training loss: 3.3864 2.1143 sec/batch\n",
      "Epoch 1/20  Iteration 29/3560 Training loss: 3.3788 2.1768 sec/batch\n",
      "Epoch 1/20  Iteration 30/3560 Training loss: 3.3722 1.9971 sec/batch\n",
      "Epoch 1/20  Iteration 31/3560 Training loss: 3.3667 1.9983 sec/batch\n",
      "Epoch 1/20  Iteration 32/3560 Training loss: 3.3603 2.0939 sec/batch\n",
      "Epoch 1/20  Iteration 33/3560 Training loss: 3.3541 2.2204 sec/batch\n",
      "Epoch 1/20  Iteration 34/3560 Training loss: 3.3489 2.0285 sec/batch\n",
      "Epoch 1/20  Iteration 35/3560 Training loss: 3.3435 1.9708 sec/batch\n",
      "Epoch 1/20  Iteration 36/3560 Training loss: 3.3388 2.0793 sec/batch\n",
      "Epoch 1/20  Iteration 37/3560 Training loss: 3.3334 2.1666 sec/batch\n",
      "Epoch 1/20  Iteration 38/3560 Training loss: 3.3286 2.0329 sec/batch\n",
      "Epoch 1/20  Iteration 39/3560 Training loss: 3.3238 2.0567 sec/batch\n",
      "Epoch 1/20  Iteration 40/3560 Training loss: 3.3194 1.9614 sec/batch\n",
      "Epoch 1/20  Iteration 41/3560 Training loss: 3.3150 2.1942 sec/batch\n",
      "Epoch 1/20  Iteration 42/3560 Training loss: 3.3110 2.1187 sec/batch\n",
      "Epoch 1/20  Iteration 43/3560 Training loss: 3.3070 2.0393 sec/batch\n",
      "Epoch 1/20  Iteration 44/3560 Training loss: 3.3031 1.9609 sec/batch\n",
      "Epoch 1/20  Iteration 45/3560 Training loss: 3.2993 2.1876 sec/batch\n",
      "Epoch 1/20  Iteration 46/3560 Training loss: 3.2960 2.1178 sec/batch\n",
      "Epoch 1/20  Iteration 47/3560 Training loss: 3.2929 2.0064 sec/batch\n",
      "Epoch 1/20  Iteration 48/3560 Training loss: 3.2900 1.9705 sec/batch\n",
      "Epoch 1/20  Iteration 49/3560 Training loss: 3.2872 2.1688 sec/batch\n",
      "Epoch 1/20  Iteration 50/3560 Training loss: 3.2844 2.1523 sec/batch\n",
      "Epoch 1/20  Iteration 51/3560 Training loss: 3.2815 1.9812 sec/batch\n",
      "Epoch 1/20  Iteration 52/3560 Training loss: 3.2786 2.0383 sec/batch\n",
      "Epoch 1/20  Iteration 53/3560 Training loss: 3.2761 2.0470 sec/batch\n",
      "Epoch 1/20  Iteration 54/3560 Training loss: 3.2732 2.0760 sec/batch\n",
      "Epoch 1/20  Iteration 55/3560 Training loss: 3.2708 2.1647 sec/batch\n",
      "Epoch 1/20  Iteration 56/3560 Training loss: 3.2681 1.9519 sec/batch\n",
      "Epoch 1/20  Iteration 57/3560 Training loss: 3.2657 2.1344 sec/batch\n",
      "Epoch 1/20  Iteration 58/3560 Training loss: 3.2634 2.0354 sec/batch\n",
      "Epoch 1/20  Iteration 59/3560 Training loss: 3.2609 2.0442 sec/batch\n",
      "Epoch 1/20  Iteration 60/3560 Training loss: 3.2587 2.0416 sec/batch\n",
      "Epoch 1/20  Iteration 61/3560 Training loss: 3.2566 2.0298 sec/batch\n",
      "Epoch 1/20  Iteration 62/3560 Training loss: 3.2548 2.1578 sec/batch\n",
      "Epoch 1/20  Iteration 63/3560 Training loss: 3.2532 2.1389 sec/batch\n",
      "Epoch 1/20  Iteration 64/3560 Training loss: 3.2510 2.0539 sec/batch\n",
      "Epoch 1/20  Iteration 65/3560 Training loss: 3.2489 2.1200 sec/batch\n",
      "Epoch 1/20  Iteration 66/3560 Training loss: 3.2472 2.1802 sec/batch\n",
      "Epoch 1/20  Iteration 67/3560 Training loss: 3.2454 2.1693 sec/batch\n",
      "Epoch 1/20  Iteration 68/3560 Training loss: 3.2430 2.0058 sec/batch\n",
      "Epoch 1/20  Iteration 69/3560 Training loss: 3.2409 2.0434 sec/batch\n",
      "Epoch 1/20  Iteration 70/3560 Training loss: 3.2392 2.1351 sec/batch\n",
      "Epoch 1/20  Iteration 71/3560 Training loss: 3.2374 2.1814 sec/batch\n",
      "Epoch 1/20  Iteration 72/3560 Training loss: 3.2359 2.0791 sec/batch\n",
      "Epoch 1/20  Iteration 73/3560 Training loss: 3.2341 2.0597 sec/batch\n",
      "Epoch 1/20  Iteration 74/3560 Training loss: 3.2324 2.1641 sec/batch\n",
      "Epoch 1/20  Iteration 75/3560 Training loss: 3.2309 2.2279 sec/batch\n",
      "Epoch 1/20  Iteration 76/3560 Training loss: 3.2294 2.0529 sec/batch\n",
      "Epoch 1/20  Iteration 77/3560 Training loss: 3.2278 1.9289 sec/batch\n",
      "Epoch 1/20  Iteration 78/3560 Training loss: 3.2262 2.1706 sec/batch\n",
      "Epoch 1/20  Iteration 79/3560 Training loss: 3.2245 2.1260 sec/batch\n",
      "Epoch 1/20  Iteration 80/3560 Training loss: 3.2226 2.0690 sec/batch\n",
      "Epoch 1/20  Iteration 81/3560 Training loss: 3.2209 1.9324 sec/batch\n",
      "Epoch 1/20  Iteration 82/3560 Training loss: 3.2193 2.0540 sec/batch\n",
      "Epoch 1/20  Iteration 83/3560 Training loss: 3.2177 2.2369 sec/batch\n",
      "Epoch 1/20  Iteration 84/3560 Training loss: 3.2160 2.1972 sec/batch\n",
      "Epoch 1/20  Iteration 85/3560 Training loss: 3.2140 2.1367 sec/batch\n",
      "Epoch 1/20  Iteration 86/3560 Training loss: 3.2122 1.9655 sec/batch\n",
      "Epoch 1/20  Iteration 87/3560 Training loss: 3.2103 2.1454 sec/batch\n",
      "Epoch 1/20  Iteration 88/3560 Training loss: 3.2084 2.1209 sec/batch\n",
      "Epoch 1/20  Iteration 89/3560 Training loss: 3.2067 2.0697 sec/batch\n",
      "Epoch 1/20  Iteration 90/3560 Training loss: 3.2049 1.9747 sec/batch\n",
      "Epoch 1/20  Iteration 91/3560 Training loss: 3.2031 2.0960 sec/batch\n",
      "Epoch 1/20  Iteration 92/3560 Training loss: 3.2012 2.0299 sec/batch\n",
      "Epoch 1/20  Iteration 93/3560 Training loss: 3.1991 2.0296 sec/batch\n",
      "Epoch 1/20  Iteration 94/3560 Training loss: 3.1971 1.9570 sec/batch\n",
      "Epoch 1/20  Iteration 95/3560 Training loss: 3.1949 2.1211 sec/batch\n",
      "Epoch 1/20  Iteration 96/3560 Training loss: 3.1926 2.1497 sec/batch\n",
      "Epoch 1/20  Iteration 97/3560 Training loss: 3.1906 2.1131 sec/batch\n",
      "Epoch 1/20  Iteration 98/3560 Training loss: 3.1883 2.0629 sec/batch\n",
      "Epoch 1/20  Iteration 99/3560 Training loss: 3.1860 2.3002 sec/batch\n",
      "Epoch 1/20  Iteration 100/3560 Training loss: 3.1836 2.0273 sec/batch\n",
      "Epoch 1/20  Iteration 101/3560 Training loss: 3.1813 2.1266 sec/batch\n",
      "Epoch 1/20  Iteration 102/3560 Training loss: 3.1788 2.0451 sec/batch\n",
      "Epoch 1/20  Iteration 103/3560 Training loss: 3.1762 2.0017 sec/batch\n",
      "Epoch 1/20  Iteration 104/3560 Training loss: 3.1735 2.0868 sec/batch\n",
      "Epoch 1/20  Iteration 105/3560 Training loss: 3.1711 2.1049 sec/batch\n",
      "Epoch 1/20  Iteration 106/3560 Training loss: 3.1690 2.0961 sec/batch\n",
      "Epoch 1/20  Iteration 107/3560 Training loss: 3.1660 1.9682 sec/batch\n",
      "Epoch 1/20  Iteration 108/3560 Training loss: 3.1635 1.9887 sec/batch\n",
      "Epoch 1/20  Iteration 109/3560 Training loss: 3.1608 2.1316 sec/batch\n",
      "Epoch 1/20  Iteration 110/3560 Training loss: 3.1579 2.0074 sec/batch\n",
      "Epoch 1/20  Iteration 111/3560 Training loss: 3.1552 1.8885 sec/batch\n",
      "Epoch 1/20  Iteration 112/3560 Training loss: 3.1524 2.0573 sec/batch\n",
      "Epoch 1/20  Iteration 113/3560 Training loss: 3.1494 2.0437 sec/batch\n",
      "Epoch 1/20  Iteration 114/3560 Training loss: 3.1463 2.1618 sec/batch\n",
      "Epoch 1/20  Iteration 115/3560 Training loss: 3.1432 2.0225 sec/batch\n",
      "Epoch 1/20  Iteration 116/3560 Training loss: 3.1400 1.9452 sec/batch\n",
      "Epoch 1/20  Iteration 117/3560 Training loss: 3.1368 2.1003 sec/batch\n",
      "Epoch 1/20  Iteration 118/3560 Training loss: 3.1338 2.0395 sec/batch\n",
      "Epoch 1/20  Iteration 119/3560 Training loss: 3.1308 2.1006 sec/batch\n",
      "Epoch 1/20  Iteration 120/3560 Training loss: 3.1276 2.0515 sec/batch\n",
      "Epoch 1/20  Iteration 121/3560 Training loss: 3.1247 2.0346 sec/batch\n",
      "Epoch 1/20  Iteration 122/3560 Training loss: 3.1216 2.1174 sec/batch\n",
      "Epoch 1/20  Iteration 123/3560 Training loss: 3.1184 1.9745 sec/batch\n",
      "Epoch 1/20  Iteration 124/3560 Training loss: 3.1153 2.0034 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 125/3560 Training loss: 3.1121 2.1253 sec/batch\n",
      "Epoch 1/20  Iteration 126/3560 Training loss: 3.1086 2.1022 sec/batch\n",
      "Epoch 1/20  Iteration 127/3560 Training loss: 3.1054 2.1701 sec/batch\n",
      "Epoch 1/20  Iteration 128/3560 Training loss: 3.1023 1.9870 sec/batch\n",
      "Epoch 1/20  Iteration 129/3560 Training loss: 3.0989 2.0834 sec/batch\n",
      "Epoch 1/20  Iteration 130/3560 Training loss: 3.0957 2.1661 sec/batch\n",
      "Epoch 1/20  Iteration 131/3560 Training loss: 3.0925 2.1858 sec/batch\n",
      "Epoch 1/20  Iteration 132/3560 Training loss: 3.0891 2.0203 sec/batch\n",
      "Epoch 1/20  Iteration 133/3560 Training loss: 3.0859 1.9566 sec/batch\n",
      "Epoch 1/20  Iteration 134/3560 Training loss: 3.0826 2.1848 sec/batch\n",
      "Epoch 1/20  Iteration 135/3560 Training loss: 3.0791 2.1939 sec/batch\n",
      "Epoch 1/20  Iteration 136/3560 Training loss: 3.0756 2.1176 sec/batch\n",
      "Epoch 1/20  Iteration 137/3560 Training loss: 3.0723 1.9981 sec/batch\n",
      "Epoch 1/20  Iteration 138/3560 Training loss: 3.0690 2.1257 sec/batch\n",
      "Epoch 1/20  Iteration 139/3560 Training loss: 3.0658 2.1643 sec/batch\n",
      "Epoch 1/20  Iteration 140/3560 Training loss: 3.0625 2.0158 sec/batch\n",
      "Epoch 1/20  Iteration 141/3560 Training loss: 3.0593 2.0049 sec/batch\n",
      "Epoch 1/20  Iteration 142/3560 Training loss: 3.0560 2.1727 sec/batch\n",
      "Epoch 1/20  Iteration 143/3560 Training loss: 3.0528 2.2792 sec/batch\n",
      "Epoch 1/20  Iteration 144/3560 Training loss: 3.0495 2.1824 sec/batch\n",
      "Epoch 1/20  Iteration 145/3560 Training loss: 3.0464 1.9501 sec/batch\n",
      "Epoch 1/20  Iteration 146/3560 Training loss: 3.0434 2.0374 sec/batch\n",
      "Epoch 1/20  Iteration 147/3560 Training loss: 3.0402 2.0935 sec/batch\n",
      "Epoch 1/20  Iteration 148/3560 Training loss: 3.0372 2.1782 sec/batch\n",
      "Epoch 1/20  Iteration 149/3560 Training loss: 3.0340 2.0906 sec/batch\n",
      "Epoch 1/20  Iteration 150/3560 Training loss: 3.0307 2.0230 sec/batch\n",
      "Epoch 1/20  Iteration 151/3560 Training loss: 3.0278 2.0217 sec/batch\n",
      "Epoch 1/20  Iteration 152/3560 Training loss: 3.0250 2.1091 sec/batch\n",
      "Epoch 1/20  Iteration 153/3560 Training loss: 3.0220 2.1281 sec/batch\n",
      "Epoch 1/20  Iteration 154/3560 Training loss: 3.0190 1.9333 sec/batch\n",
      "Epoch 1/20  Iteration 155/3560 Training loss: 3.0159 2.1326 sec/batch\n",
      "Epoch 1/20  Iteration 156/3560 Training loss: 3.0128 2.0691 sec/batch\n",
      "Epoch 1/20  Iteration 157/3560 Training loss: 3.0097 2.1731 sec/batch\n",
      "Epoch 1/20  Iteration 158/3560 Training loss: 3.0065 2.0052 sec/batch\n",
      "Epoch 1/20  Iteration 159/3560 Training loss: 3.0034 2.0386 sec/batch\n",
      "Epoch 1/20  Iteration 160/3560 Training loss: 3.0005 2.0556 sec/batch\n",
      "Epoch 1/20  Iteration 161/3560 Training loss: 2.9975 2.0859 sec/batch\n",
      "Epoch 1/20  Iteration 162/3560 Training loss: 2.9943 1.9319 sec/batch\n",
      "Epoch 1/20  Iteration 163/3560 Training loss: 2.9912 2.0886 sec/batch\n",
      "Epoch 1/20  Iteration 164/3560 Training loss: 2.9882 2.1129 sec/batch\n",
      "Epoch 1/20  Iteration 165/3560 Training loss: 2.9853 2.1072 sec/batch\n",
      "Epoch 1/20  Iteration 166/3560 Training loss: 2.9824 1.9888 sec/batch\n",
      "Epoch 1/20  Iteration 167/3560 Training loss: 2.9795 2.0224 sec/batch\n",
      "Epoch 1/20  Iteration 168/3560 Training loss: 2.9766 2.0446 sec/batch\n",
      "Epoch 1/20  Iteration 169/3560 Training loss: 2.9738 2.1492 sec/batch\n",
      "Epoch 1/20  Iteration 170/3560 Training loss: 2.9708 2.0337 sec/batch\n",
      "Epoch 1/20  Iteration 171/3560 Training loss: 2.9680 1.9942 sec/batch\n",
      "Epoch 1/20  Iteration 172/3560 Training loss: 2.9653 2.1762 sec/batch\n",
      "Epoch 1/20  Iteration 173/3560 Training loss: 2.9628 2.1558 sec/batch\n",
      "Epoch 1/20  Iteration 174/3560 Training loss: 2.9602 1.9776 sec/batch\n",
      "Epoch 1/20  Iteration 175/3560 Training loss: 2.9577 1.9327 sec/batch\n",
      "Epoch 1/20  Iteration 176/3560 Training loss: 2.9549 2.0283 sec/batch\n",
      "Epoch 1/20  Iteration 177/3560 Training loss: 2.9521 2.0795 sec/batch\n",
      "Epoch 1/20  Iteration 178/3560 Training loss: 2.9492 2.0852 sec/batch\n",
      "Epoch 2/20  Iteration 179/3560 Training loss: 2.4979 1.9004 sec/batch\n",
      "Epoch 2/20  Iteration 180/3560 Training loss: 2.4598 1.9888 sec/batch\n",
      "Epoch 2/20  Iteration 181/3560 Training loss: 2.4499 2.0755 sec/batch\n",
      "Epoch 2/20  Iteration 182/3560 Training loss: 2.4467 2.1326 sec/batch\n",
      "Epoch 2/20  Iteration 183/3560 Training loss: 2.4457 2.0661 sec/batch\n",
      "Epoch 2/20  Iteration 184/3560 Training loss: 2.4425 1.9512 sec/batch\n",
      "Epoch 2/20  Iteration 185/3560 Training loss: 2.4422 2.1408 sec/batch\n",
      "Epoch 2/20  Iteration 186/3560 Training loss: 2.4425 2.0505 sec/batch\n",
      "Epoch 2/20  Iteration 187/3560 Training loss: 2.4418 2.0624 sec/batch\n",
      "Epoch 2/20  Iteration 188/3560 Training loss: 2.4404 2.0069 sec/batch\n",
      "Epoch 2/20  Iteration 189/3560 Training loss: 2.4384 2.1063 sec/batch\n",
      "Epoch 2/20  Iteration 190/3560 Training loss: 2.4382 2.0788 sec/batch\n",
      "Epoch 2/20  Iteration 191/3560 Training loss: 2.4368 2.1054 sec/batch\n",
      "Epoch 2/20  Iteration 192/3560 Training loss: 2.4384 1.9966 sec/batch\n",
      "Epoch 2/20  Iteration 193/3560 Training loss: 2.4395 2.0630 sec/batch\n",
      "Epoch 2/20  Iteration 194/3560 Training loss: 2.4391 1.9969 sec/batch\n",
      "Epoch 2/20  Iteration 195/3560 Training loss: 2.4378 2.1225 sec/batch\n",
      "Epoch 2/20  Iteration 196/3560 Training loss: 2.4386 2.0127 sec/batch\n",
      "Epoch 2/20  Iteration 197/3560 Training loss: 2.4380 2.0346 sec/batch\n",
      "Epoch 2/20  Iteration 198/3560 Training loss: 2.4351 2.0424 sec/batch\n",
      "Epoch 2/20  Iteration 199/3560 Training loss: 2.4334 2.1360 sec/batch\n",
      "Epoch 2/20  Iteration 200/3560 Training loss: 2.4329 1.9029 sec/batch\n",
      "Epoch 2/20  Iteration 201/3560 Training loss: 2.4314 2.0352 sec/batch\n",
      "Epoch 2/20  Iteration 202/3560 Training loss: 2.4296 2.1232 sec/batch\n",
      "Epoch 2/20  Iteration 203/3560 Training loss: 2.4277 2.1226 sec/batch\n",
      "Epoch 2/20  Iteration 204/3560 Training loss: 2.4265 2.0632 sec/batch\n",
      "Epoch 2/20  Iteration 205/3560 Training loss: 2.4246 1.9787 sec/batch\n",
      "Epoch 2/20  Iteration 206/3560 Training loss: 2.4230 2.0225 sec/batch\n",
      "Epoch 2/20  Iteration 207/3560 Training loss: 2.4221 2.0757 sec/batch\n",
      "Epoch 2/20  Iteration 208/3560 Training loss: 2.4209 2.1061 sec/batch\n",
      "Epoch 2/20  Iteration 209/3560 Training loss: 2.4200 2.1105 sec/batch\n",
      "Epoch 2/20  Iteration 210/3560 Training loss: 2.4183 2.1103 sec/batch\n",
      "Epoch 2/20  Iteration 211/3560 Training loss: 2.4166 1.8895 sec/batch\n",
      "Epoch 2/20  Iteration 212/3560 Training loss: 2.4153 2.1507 sec/batch\n",
      "Epoch 2/20  Iteration 213/3560 Training loss: 2.4134 2.0304 sec/batch\n",
      "Epoch 2/20  Iteration 214/3560 Training loss: 2.4121 1.9583 sec/batch\n",
      "Epoch 2/20  Iteration 215/3560 Training loss: 2.4104 2.1043 sec/batch\n",
      "Epoch 2/20  Iteration 216/3560 Training loss: 2.4082 2.0810 sec/batch\n",
      "Epoch 2/20  Iteration 217/3560 Training loss: 2.4063 2.0610 sec/batch\n",
      "Epoch 2/20  Iteration 218/3560 Training loss: 2.4044 2.0932 sec/batch\n",
      "Epoch 2/20  Iteration 219/3560 Training loss: 2.4029 1.9722 sec/batch\n",
      "Epoch 2/20  Iteration 220/3560 Training loss: 2.4009 2.1249 sec/batch\n",
      "Epoch 2/20  Iteration 221/3560 Training loss: 2.3989 2.2134 sec/batch\n",
      "Epoch 2/20  Iteration 222/3560 Training loss: 2.3974 2.0126 sec/batch\n",
      "Epoch 2/20  Iteration 223/3560 Training loss: 2.3956 2.0335 sec/batch\n",
      "Epoch 2/20  Iteration 224/3560 Training loss: 2.3934 2.0515 sec/batch\n",
      "Epoch 2/20  Iteration 225/3560 Training loss: 2.3924 2.0916 sec/batch\n",
      "Epoch 2/20  Iteration 226/3560 Training loss: 2.3907 2.0109 sec/batch\n",
      "Epoch 2/20  Iteration 227/3560 Training loss: 2.3891 2.3739 sec/batch\n",
      "Epoch 2/20  Iteration 228/3560 Training loss: 2.3882 2.0742 sec/batch\n",
      "Epoch 2/20  Iteration 229/3560 Training loss: 2.3864 2.1392 sec/batch\n",
      "Epoch 2/20  Iteration 230/3560 Training loss: 2.3853 2.0912 sec/batch\n",
      "Epoch 2/20  Iteration 231/3560 Training loss: 2.3838 1.9319 sec/batch\n",
      "Epoch 2/20  Iteration 232/3560 Training loss: 2.3823 2.0867 sec/batch\n",
      "Epoch 2/20  Iteration 233/3560 Training loss: 2.3808 2.3335 sec/batch\n",
      "Epoch 2/20  Iteration 234/3560 Training loss: 2.3797 2.2765 sec/batch\n",
      "Epoch 2/20  Iteration 235/3560 Training loss: 2.3784 2.0317 sec/batch\n",
      "Epoch 2/20  Iteration 236/3560 Training loss: 2.3769 2.0494 sec/batch\n",
      "Epoch 2/20  Iteration 237/3560 Training loss: 2.3754 2.1329 sec/batch\n",
      "Epoch 2/20  Iteration 238/3560 Training loss: 2.3744 2.2023 sec/batch\n",
      "Epoch 2/20  Iteration 239/3560 Training loss: 2.3730 2.0021 sec/batch\n",
      "Epoch 2/20  Iteration 240/3560 Training loss: 2.3719 2.0021 sec/batch\n",
      "Epoch 2/20  Iteration 241/3560 Training loss: 2.3710 1.9995 sec/batch\n",
      "Epoch 2/20  Iteration 242/3560 Training loss: 2.3695 2.2457 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 243/3560 Training loss: 2.3680 2.1221 sec/batch\n",
      "Epoch 2/20  Iteration 244/3560 Training loss: 2.3670 1.9616 sec/batch\n",
      "Epoch 2/20  Iteration 245/3560 Training loss: 2.3657 2.3133 sec/batch\n",
      "Epoch 2/20  Iteration 246/3560 Training loss: 2.3640 2.2296 sec/batch\n",
      "Epoch 2/20  Iteration 247/3560 Training loss: 2.3625 2.0881 sec/batch\n",
      "Epoch 2/20  Iteration 248/3560 Training loss: 2.3613 2.0497 sec/batch\n",
      "Epoch 2/20  Iteration 249/3560 Training loss: 2.3602 1.9550 sec/batch\n",
      "Epoch 2/20  Iteration 250/3560 Training loss: 2.3591 2.1826 sec/batch\n",
      "Epoch 2/20  Iteration 251/3560 Training loss: 2.3580 2.0868 sec/batch\n",
      "Epoch 2/20  Iteration 252/3560 Training loss: 2.3566 1.8980 sec/batch\n",
      "Epoch 2/20  Iteration 253/3560 Training loss: 2.3552 1.9991 sec/batch\n",
      "Epoch 2/20  Iteration 254/3560 Training loss: 2.3545 2.1902 sec/batch\n",
      "Epoch 2/20  Iteration 255/3560 Training loss: 2.3533 2.3048 sec/batch\n",
      "Epoch 2/20  Iteration 256/3560 Training loss: 2.3523 2.2385 sec/batch\n",
      "Epoch 2/20  Iteration 257/3560 Training loss: 2.3508 2.2465 sec/batch\n",
      "Epoch 2/20  Iteration 258/3560 Training loss: 2.3494 2.2896 sec/batch\n",
      "Epoch 2/20  Iteration 259/3560 Training loss: 2.3481 2.2552 sec/batch\n",
      "Epoch 2/20  Iteration 260/3560 Training loss: 2.3470 2.2356 sec/batch\n",
      "Epoch 2/20  Iteration 261/3560 Training loss: 2.3456 1.9919 sec/batch\n",
      "Epoch 2/20  Iteration 262/3560 Training loss: 2.3442 2.2402 sec/batch\n",
      "Epoch 2/20  Iteration 263/3560 Training loss: 2.3424 2.1036 sec/batch\n",
      "Epoch 2/20  Iteration 264/3560 Training loss: 2.3410 2.0040 sec/batch\n",
      "Epoch 2/20  Iteration 265/3560 Training loss: 2.3398 2.0653 sec/batch\n",
      "Epoch 2/20  Iteration 266/3560 Training loss: 2.3382 2.0822 sec/batch\n",
      "Epoch 2/20  Iteration 267/3560 Training loss: 2.3368 2.1129 sec/batch\n",
      "Epoch 2/20  Iteration 268/3560 Training loss: 2.3357 2.0207 sec/batch\n",
      "Epoch 2/20  Iteration 269/3560 Training loss: 2.3344 2.1300 sec/batch\n",
      "Epoch 2/20  Iteration 270/3560 Training loss: 2.3333 2.0862 sec/batch\n",
      "Epoch 2/20  Iteration 271/3560 Training loss: 2.3319 2.0925 sec/batch\n",
      "Epoch 2/20  Iteration 272/3560 Training loss: 2.3305 1.6773 sec/batch\n",
      "Epoch 2/20  Iteration 273/3560 Training loss: 2.3290 1.6068 sec/batch\n",
      "Epoch 2/20  Iteration 274/3560 Training loss: 2.3277 1.4219 sec/batch\n",
      "Epoch 2/20  Iteration 275/3560 Training loss: 2.3265 1.4482 sec/batch\n",
      "Epoch 2/20  Iteration 276/3560 Training loss: 2.3252 1.3990 sec/batch\n",
      "Epoch 2/20  Iteration 277/3560 Training loss: 2.3238 1.4375 sec/batch\n",
      "Epoch 2/20  Iteration 278/3560 Training loss: 2.3226 1.4250 sec/batch\n",
      "Epoch 2/20  Iteration 279/3560 Training loss: 2.3215 1.3617 sec/batch\n",
      "Epoch 2/20  Iteration 280/3560 Training loss: 2.3204 1.5504 sec/batch\n",
      "Epoch 2/20  Iteration 281/3560 Training loss: 2.3191 1.4247 sec/batch\n",
      "Epoch 2/20  Iteration 282/3560 Training loss: 2.3179 1.3741 sec/batch\n",
      "Epoch 2/20  Iteration 283/3560 Training loss: 2.3165 1.3630 sec/batch\n",
      "Epoch 2/20  Iteration 284/3560 Training loss: 2.3153 1.4846 sec/batch\n",
      "Epoch 2/20  Iteration 285/3560 Training loss: 2.3142 1.3356 sec/batch\n",
      "Epoch 2/20  Iteration 286/3560 Training loss: 2.3132 1.4521 sec/batch\n",
      "Epoch 2/20  Iteration 287/3560 Training loss: 2.3122 1.3673 sec/batch\n",
      "Epoch 2/20  Iteration 288/3560 Training loss: 2.3109 1.5315 sec/batch\n",
      "Epoch 2/20  Iteration 289/3560 Training loss: 2.3098 1.4062 sec/batch\n",
      "Epoch 2/20  Iteration 290/3560 Training loss: 2.3088 1.4029 sec/batch\n",
      "Epoch 2/20  Iteration 291/3560 Training loss: 2.3076 1.3274 sec/batch\n",
      "Epoch 2/20  Iteration 292/3560 Training loss: 2.3064 1.5706 sec/batch\n",
      "Epoch 2/20  Iteration 293/3560 Training loss: 2.3053 1.4600 sec/batch\n",
      "Epoch 2/20  Iteration 294/3560 Training loss: 2.3038 2.5479 sec/batch\n",
      "Epoch 2/20  Iteration 295/3560 Training loss: 2.3028 1.6925 sec/batch\n",
      "Epoch 2/20  Iteration 296/3560 Training loss: 2.3016 1.8241 sec/batch\n",
      "Epoch 2/20  Iteration 297/3560 Training loss: 2.3007 1.8188 sec/batch\n",
      "Epoch 2/20  Iteration 298/3560 Training loss: 2.2996 1.4897 sec/batch\n",
      "Epoch 2/20  Iteration 299/3560 Training loss: 2.2986 1.5245 sec/batch\n",
      "Epoch 2/20  Iteration 300/3560 Training loss: 2.2974 1.5406 sec/batch\n",
      "Epoch 2/20  Iteration 301/3560 Training loss: 2.2962 1.5966 sec/batch\n",
      "Epoch 2/20  Iteration 302/3560 Training loss: 2.2952 1.6324 sec/batch\n",
      "Epoch 2/20  Iteration 303/3560 Training loss: 2.2942 1.4576 sec/batch\n",
      "Epoch 2/20  Iteration 304/3560 Training loss: 2.2929 1.4433 sec/batch\n",
      "Epoch 2/20  Iteration 305/3560 Training loss: 2.2919 1.4435 sec/batch\n",
      "Epoch 2/20  Iteration 306/3560 Training loss: 2.2909 1.4469 sec/batch\n",
      "Epoch 2/20  Iteration 307/3560 Training loss: 2.2899 1.3945 sec/batch\n",
      "Epoch 2/20  Iteration 308/3560 Training loss: 2.2889 1.3130 sec/batch\n",
      "Epoch 2/20  Iteration 309/3560 Training loss: 2.2878 1.5555 sec/batch\n",
      "Epoch 2/20  Iteration 310/3560 Training loss: 2.2864 1.4532 sec/batch\n",
      "Epoch 2/20  Iteration 311/3560 Training loss: 2.2854 1.3704 sec/batch\n",
      "Epoch 2/20  Iteration 312/3560 Training loss: 2.2845 1.3502 sec/batch\n",
      "Epoch 2/20  Iteration 313/3560 Training loss: 2.2835 1.4838 sec/batch\n",
      "Epoch 2/20  Iteration 314/3560 Training loss: 2.2825 1.4230 sec/batch\n",
      "Epoch 2/20  Iteration 315/3560 Training loss: 2.2815 1.4903 sec/batch\n",
      "Epoch 2/20  Iteration 316/3560 Training loss: 2.2805 1.4060 sec/batch\n",
      "Epoch 2/20  Iteration 317/3560 Training loss: 2.2797 1.5688 sec/batch\n",
      "Epoch 2/20  Iteration 318/3560 Training loss: 2.2787 1.5124 sec/batch\n",
      "Epoch 2/20  Iteration 319/3560 Training loss: 2.2778 1.3336 sec/batch\n",
      "Epoch 2/20  Iteration 320/3560 Training loss: 2.2767 1.3249 sec/batch\n",
      "Epoch 2/20  Iteration 321/3560 Training loss: 2.2757 1.4819 sec/batch\n",
      "Epoch 2/20  Iteration 322/3560 Training loss: 2.2746 1.4424 sec/batch\n",
      "Epoch 2/20  Iteration 323/3560 Training loss: 2.2736 1.3630 sec/batch\n",
      "Epoch 2/20  Iteration 324/3560 Training loss: 2.2728 1.3173 sec/batch\n",
      "Epoch 2/20  Iteration 325/3560 Training loss: 2.2718 1.4762 sec/batch\n",
      "Epoch 2/20  Iteration 326/3560 Training loss: 2.2710 1.4258 sec/batch\n",
      "Epoch 2/20  Iteration 327/3560 Training loss: 2.2700 1.3995 sec/batch\n",
      "Epoch 2/20  Iteration 328/3560 Training loss: 2.2690 1.4346 sec/batch\n",
      "Epoch 2/20  Iteration 329/3560 Training loss: 2.2681 1.4813 sec/batch\n",
      "Epoch 2/20  Iteration 330/3560 Training loss: 2.2679 1.4685 sec/batch\n",
      "Epoch 2/20  Iteration 331/3560 Training loss: 2.2675 1.3995 sec/batch\n",
      "Epoch 2/20  Iteration 332/3560 Training loss: 2.2670 1.3901 sec/batch\n",
      "Epoch 2/20  Iteration 333/3560 Training loss: 2.2663 1.4144 sec/batch\n",
      "Epoch 2/20  Iteration 334/3560 Training loss: 2.2655 1.4401 sec/batch\n",
      "Epoch 2/20  Iteration 335/3560 Training loss: 2.2647 1.4295 sec/batch\n",
      "Epoch 2/20  Iteration 336/3560 Training loss: 2.2639 1.3388 sec/batch\n",
      "Epoch 2/20  Iteration 337/3560 Training loss: 2.2630 1.4723 sec/batch\n",
      "Epoch 2/20  Iteration 338/3560 Training loss: 2.2623 1.5565 sec/batch\n",
      "Epoch 2/20  Iteration 339/3560 Training loss: 2.2615 1.3383 sec/batch\n",
      "Epoch 2/20  Iteration 340/3560 Training loss: 2.2606 1.3972 sec/batch\n",
      "Epoch 2/20  Iteration 341/3560 Training loss: 2.2598 1.3893 sec/batch\n",
      "Epoch 2/20  Iteration 342/3560 Training loss: 2.2589 1.4840 sec/batch\n",
      "Epoch 2/20  Iteration 343/3560 Training loss: 2.2581 1.4092 sec/batch\n",
      "Epoch 2/20  Iteration 344/3560 Training loss: 2.2573 1.3949 sec/batch\n",
      "Epoch 2/20  Iteration 345/3560 Training loss: 2.2565 1.4680 sec/batch\n",
      "Epoch 2/20  Iteration 346/3560 Training loss: 2.2558 1.4848 sec/batch\n",
      "Epoch 2/20  Iteration 347/3560 Training loss: 2.2550 1.4296 sec/batch\n",
      "Epoch 2/20  Iteration 348/3560 Training loss: 2.2541 1.4211 sec/batch\n",
      "Epoch 2/20  Iteration 349/3560 Training loss: 2.2532 1.3165 sec/batch\n",
      "Epoch 2/20  Iteration 350/3560 Training loss: 2.2523 1.5348 sec/batch\n",
      "Epoch 2/20  Iteration 351/3560 Training loss: 2.2517 1.4799 sec/batch\n",
      "Epoch 2/20  Iteration 352/3560 Training loss: 2.2510 1.3510 sec/batch\n",
      "Epoch 2/20  Iteration 353/3560 Training loss: 2.2503 1.3513 sec/batch\n",
      "Epoch 2/20  Iteration 354/3560 Training loss: 2.2495 1.5150 sec/batch\n",
      "Epoch 2/20  Iteration 355/3560 Training loss: 2.2485 1.4112 sec/batch\n",
      "Epoch 2/20  Iteration 356/3560 Training loss: 2.2476 1.4393 sec/batch\n",
      "Epoch 3/20  Iteration 357/3560 Training loss: 2.1302 1.3250 sec/batch\n",
      "Epoch 3/20  Iteration 358/3560 Training loss: 2.0933 1.5144 sec/batch\n",
      "Epoch 3/20  Iteration 359/3560 Training loss: 2.0816 1.5522 sec/batch\n",
      "Epoch 3/20  Iteration 360/3560 Training loss: 2.0773 1.4019 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 361/3560 Training loss: 2.0759 1.3946 sec/batch\n",
      "Epoch 3/20  Iteration 362/3560 Training loss: 2.0689 1.5462 sec/batch\n",
      "Epoch 3/20  Iteration 363/3560 Training loss: 2.0686 1.4421 sec/batch\n",
      "Epoch 3/20  Iteration 364/3560 Training loss: 2.0689 1.3663 sec/batch\n",
      "Epoch 3/20  Iteration 365/3560 Training loss: 2.0721 1.4032 sec/batch\n",
      "Epoch 3/20  Iteration 366/3560 Training loss: 2.0712 1.4651 sec/batch\n",
      "Epoch 3/20  Iteration 367/3560 Training loss: 2.0692 1.4967 sec/batch\n",
      "Epoch 3/20  Iteration 368/3560 Training loss: 2.0675 1.4279 sec/batch\n",
      "Epoch 3/20  Iteration 369/3560 Training loss: 2.0670 1.4127 sec/batch\n",
      "Epoch 3/20  Iteration 370/3560 Training loss: 2.0699 1.3632 sec/batch\n",
      "Epoch 3/20  Iteration 371/3560 Training loss: 2.0688 1.4116 sec/batch\n",
      "Epoch 3/20  Iteration 372/3560 Training loss: 2.0670 1.3590 sec/batch\n",
      "Epoch 3/20  Iteration 373/3560 Training loss: 2.0668 1.4044 sec/batch\n",
      "Epoch 3/20  Iteration 374/3560 Training loss: 2.0681 1.3719 sec/batch\n",
      "Epoch 3/20  Iteration 375/3560 Training loss: 2.0675 1.4887 sec/batch\n",
      "Epoch 3/20  Iteration 376/3560 Training loss: 2.0667 1.3724 sec/batch\n",
      "Epoch 3/20  Iteration 377/3560 Training loss: 2.0652 1.4240 sec/batch\n",
      "Epoch 3/20  Iteration 378/3560 Training loss: 2.0662 1.3724 sec/batch\n",
      "Epoch 3/20  Iteration 379/3560 Training loss: 2.0645 1.5488 sec/batch\n",
      "Epoch 3/20  Iteration 380/3560 Training loss: 2.0633 1.3991 sec/batch\n",
      "Epoch 3/20  Iteration 381/3560 Training loss: 2.0622 1.4730 sec/batch\n",
      "Epoch 3/20  Iteration 382/3560 Training loss: 2.0606 1.3638 sec/batch\n",
      "Epoch 3/20  Iteration 383/3560 Training loss: 2.0593 1.4616 sec/batch\n",
      "Epoch 3/20  Iteration 384/3560 Training loss: 2.0589 1.3288 sec/batch\n",
      "Epoch 3/20  Iteration 385/3560 Training loss: 2.0593 1.4365 sec/batch\n",
      "Epoch 3/20  Iteration 386/3560 Training loss: 2.0589 1.3715 sec/batch\n",
      "Epoch 3/20  Iteration 387/3560 Training loss: 2.0582 1.4907 sec/batch\n",
      "Epoch 3/20  Iteration 388/3560 Training loss: 2.0570 1.4122 sec/batch\n",
      "Epoch 3/20  Iteration 389/3560 Training loss: 2.0562 1.3700 sec/batch\n",
      "Epoch 3/20  Iteration 390/3560 Training loss: 2.0564 1.3584 sec/batch\n",
      "Epoch 3/20  Iteration 391/3560 Training loss: 2.0551 1.4125 sec/batch\n",
      "Epoch 3/20  Iteration 392/3560 Training loss: 2.0544 1.3830 sec/batch\n",
      "Epoch 3/20  Iteration 393/3560 Training loss: 2.0534 1.4006 sec/batch\n",
      "Epoch 3/20  Iteration 394/3560 Training loss: 2.0518 1.3402 sec/batch\n",
      "Epoch 3/20  Iteration 395/3560 Training loss: 2.0500 1.5165 sec/batch\n",
      "Epoch 3/20  Iteration 396/3560 Training loss: 2.0485 1.4702 sec/batch\n",
      "Epoch 3/20  Iteration 397/3560 Training loss: 2.0475 1.3632 sec/batch\n",
      "Epoch 3/20  Iteration 398/3560 Training loss: 2.0466 1.3111 sec/batch\n",
      "Epoch 3/20  Iteration 399/3560 Training loss: 2.0452 1.4716 sec/batch\n",
      "Epoch 3/20  Iteration 400/3560 Training loss: 2.0436 1.4704 sec/batch\n",
      "Epoch 3/20  Iteration 401/3560 Training loss: 2.0428 1.4053 sec/batch\n",
      "Epoch 3/20  Iteration 402/3560 Training loss: 2.0408 1.4036 sec/batch\n",
      "Epoch 3/20  Iteration 403/3560 Training loss: 2.0402 1.4167 sec/batch\n",
      "Epoch 3/20  Iteration 404/3560 Training loss: 2.0390 1.4597 sec/batch\n",
      "Epoch 3/20  Iteration 405/3560 Training loss: 2.0382 1.4632 sec/batch\n",
      "Epoch 3/20  Iteration 406/3560 Training loss: 2.0382 1.4096 sec/batch\n",
      "Epoch 3/20  Iteration 407/3560 Training loss: 2.0369 1.3705 sec/batch\n",
      "Epoch 3/20  Iteration 408/3560 Training loss: 2.0370 1.4942 sec/batch\n",
      "Epoch 3/20  Iteration 409/3560 Training loss: 2.0361 1.4772 sec/batch\n",
      "Epoch 3/20  Iteration 410/3560 Training loss: 2.0355 1.3945 sec/batch\n",
      "Epoch 3/20  Iteration 411/3560 Training loss: 2.0347 1.4070 sec/batch\n",
      "Epoch 3/20  Iteration 412/3560 Training loss: 2.0343 1.4731 sec/batch\n",
      "Epoch 3/20  Iteration 413/3560 Training loss: 2.0340 1.3908 sec/batch\n",
      "Epoch 3/20  Iteration 414/3560 Training loss: 2.0330 1.3480 sec/batch\n",
      "Epoch 3/20  Iteration 415/3560 Training loss: 2.0320 1.3665 sec/batch\n",
      "Epoch 3/20  Iteration 416/3560 Training loss: 2.0319 1.5030 sec/batch\n",
      "Epoch 3/20  Iteration 417/3560 Training loss: 2.0313 1.4320 sec/batch\n",
      "Epoch 3/20  Iteration 418/3560 Training loss: 2.0313 1.2969 sec/batch\n",
      "Epoch 3/20  Iteration 419/3560 Training loss: 2.0309 1.3237 sec/batch\n",
      "Epoch 3/20  Iteration 420/3560 Training loss: 2.0303 1.4253 sec/batch\n",
      "Epoch 3/20  Iteration 421/3560 Training loss: 2.0297 1.4249 sec/batch\n",
      "Epoch 3/20  Iteration 422/3560 Training loss: 2.0296 1.4078 sec/batch\n",
      "Epoch 3/20  Iteration 423/3560 Training loss: 2.0292 1.4218 sec/batch\n",
      "Epoch 3/20  Iteration 424/3560 Training loss: 2.0282 1.4865 sec/batch\n",
      "Epoch 3/20  Iteration 425/3560 Training loss: 2.0273 1.4163 sec/batch\n",
      "Epoch 3/20  Iteration 426/3560 Training loss: 2.0268 1.3438 sec/batch\n",
      "Epoch 3/20  Iteration 427/3560 Training loss: 2.0266 1.4015 sec/batch\n",
      "Epoch 3/20  Iteration 428/3560 Training loss: 2.0261 1.4558 sec/batch\n",
      "Epoch 3/20  Iteration 429/3560 Training loss: 2.0260 1.4213 sec/batch\n",
      "Epoch 3/20  Iteration 430/3560 Training loss: 2.0251 1.3872 sec/batch\n",
      "Epoch 3/20  Iteration 431/3560 Training loss: 2.0245 1.4501 sec/batch\n",
      "Epoch 3/20  Iteration 432/3560 Training loss: 2.0244 1.4514 sec/batch\n",
      "Epoch 3/20  Iteration 433/3560 Training loss: 2.0237 1.4736 sec/batch\n",
      "Epoch 3/20  Iteration 434/3560 Training loss: 2.0233 1.4203 sec/batch\n",
      "Epoch 3/20  Iteration 435/3560 Training loss: 2.0223 1.3734 sec/batch\n",
      "Epoch 3/20  Iteration 436/3560 Training loss: 2.0216 1.4171 sec/batch\n",
      "Epoch 3/20  Iteration 437/3560 Training loss: 2.0205 1.5250 sec/batch\n",
      "Epoch 3/20  Iteration 438/3560 Training loss: 2.0203 1.3391 sec/batch\n",
      "Epoch 3/20  Iteration 439/3560 Training loss: 2.0191 1.4072 sec/batch\n",
      "Epoch 3/20  Iteration 440/3560 Training loss: 2.0184 1.4593 sec/batch\n",
      "Epoch 3/20  Iteration 441/3560 Training loss: 2.0174 1.4226 sec/batch\n",
      "Epoch 3/20  Iteration 442/3560 Training loss: 2.0166 1.4046 sec/batch\n",
      "Epoch 3/20  Iteration 443/3560 Training loss: 2.0158 1.4120 sec/batch\n",
      "Epoch 3/20  Iteration 444/3560 Training loss: 2.0150 1.3871 sec/batch\n",
      "Epoch 3/20  Iteration 445/3560 Training loss: 2.0141 1.4848 sec/batch\n",
      "Epoch 3/20  Iteration 446/3560 Training loss: 2.0135 1.4536 sec/batch\n",
      "Epoch 3/20  Iteration 447/3560 Training loss: 2.0128 1.3950 sec/batch\n",
      "Epoch 3/20  Iteration 448/3560 Training loss: 2.0121 1.3036 sec/batch\n",
      "Epoch 3/20  Iteration 449/3560 Training loss: 2.0112 1.4837 sec/batch\n",
      "Epoch 3/20  Iteration 450/3560 Training loss: 2.0103 1.3430 sec/batch\n",
      "Epoch 3/20  Iteration 451/3560 Training loss: 2.0094 1.3760 sec/batch\n",
      "Epoch 3/20  Iteration 452/3560 Training loss: 2.0088 1.4816 sec/batch\n",
      "Epoch 3/20  Iteration 453/3560 Training loss: 2.0082 1.5388 sec/batch\n",
      "Epoch 3/20  Iteration 454/3560 Training loss: 2.0074 1.5184 sec/batch\n",
      "Epoch 3/20  Iteration 455/3560 Training loss: 2.0065 1.3703 sec/batch\n",
      "Epoch 3/20  Iteration 456/3560 Training loss: 2.0055 1.4121 sec/batch\n",
      "Epoch 3/20  Iteration 457/3560 Training loss: 2.0049 1.4792 sec/batch\n",
      "Epoch 3/20  Iteration 458/3560 Training loss: 2.0043 1.4544 sec/batch\n",
      "Epoch 3/20  Iteration 459/3560 Training loss: 2.0035 1.4018 sec/batch\n",
      "Epoch 3/20  Iteration 460/3560 Training loss: 2.0029 1.3442 sec/batch\n",
      "Epoch 3/20  Iteration 461/3560 Training loss: 2.0021 1.4798 sec/batch\n",
      "Epoch 3/20  Iteration 462/3560 Training loss: 2.0015 1.4168 sec/batch\n",
      "Epoch 3/20  Iteration 463/3560 Training loss: 2.0009 1.3664 sec/batch\n",
      "Epoch 3/20  Iteration 464/3560 Training loss: 2.0004 1.3284 sec/batch\n",
      "Epoch 3/20  Iteration 465/3560 Training loss: 1.9999 1.4244 sec/batch\n",
      "Epoch 3/20  Iteration 466/3560 Training loss: 1.9993 1.3849 sec/batch\n",
      "Epoch 3/20  Iteration 467/3560 Training loss: 1.9988 1.4128 sec/batch\n",
      "Epoch 3/20  Iteration 468/3560 Training loss: 1.9982 1.3559 sec/batch\n",
      "Epoch 3/20  Iteration 469/3560 Training loss: 1.9975 1.4168 sec/batch\n",
      "Epoch 3/20  Iteration 470/3560 Training loss: 1.9968 1.4763 sec/batch\n",
      "Epoch 3/20  Iteration 471/3560 Training loss: 1.9960 1.4206 sec/batch\n",
      "Epoch 3/20  Iteration 472/3560 Training loss: 1.9952 1.3750 sec/batch\n",
      "Epoch 3/20  Iteration 473/3560 Training loss: 1.9946 1.4618 sec/batch\n",
      "Epoch 3/20  Iteration 474/3560 Training loss: 1.9940 1.4822 sec/batch\n",
      "Epoch 3/20  Iteration 475/3560 Training loss: 1.9935 1.4390 sec/batch\n",
      "Epoch 3/20  Iteration 476/3560 Training loss: 1.9929 1.4252 sec/batch\n",
      "Epoch 3/20  Iteration 477/3560 Training loss: 1.9925 1.3105 sec/batch\n",
      "Epoch 3/20  Iteration 478/3560 Training loss: 1.9916 1.5404 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 479/3560 Training loss: 1.9909 1.4447 sec/batch\n",
      "Epoch 3/20  Iteration 480/3560 Training loss: 1.9905 1.4409 sec/batch\n",
      "Epoch 3/20  Iteration 481/3560 Training loss: 1.9900 1.4103 sec/batch\n",
      "Epoch 3/20  Iteration 482/3560 Training loss: 1.9891 1.5520 sec/batch\n",
      "Epoch 3/20  Iteration 483/3560 Training loss: 1.9887 1.3656 sec/batch\n",
      "Epoch 3/20  Iteration 484/3560 Training loss: 1.9882 1.3503 sec/batch\n",
      "Epoch 3/20  Iteration 485/3560 Training loss: 1.9876 1.3540 sec/batch\n",
      "Epoch 3/20  Iteration 486/3560 Training loss: 1.9871 1.4889 sec/batch\n",
      "Epoch 3/20  Iteration 487/3560 Training loss: 1.9864 1.4762 sec/batch\n",
      "Epoch 3/20  Iteration 488/3560 Training loss: 1.9857 1.3359 sec/batch\n",
      "Epoch 3/20  Iteration 489/3560 Training loss: 1.9852 1.4506 sec/batch\n",
      "Epoch 3/20  Iteration 490/3560 Training loss: 1.9847 1.4321 sec/batch\n",
      "Epoch 3/20  Iteration 491/3560 Training loss: 1.9842 1.4557 sec/batch\n",
      "Epoch 3/20  Iteration 492/3560 Training loss: 1.9838 1.3656 sec/batch\n",
      "Epoch 3/20  Iteration 493/3560 Training loss: 1.9833 1.4426 sec/batch\n",
      "Epoch 3/20  Iteration 494/3560 Training loss: 1.9828 1.5258 sec/batch\n",
      "Epoch 3/20  Iteration 495/3560 Training loss: 1.9825 1.5042 sec/batch\n",
      "Epoch 3/20  Iteration 496/3560 Training loss: 1.9819 1.4443 sec/batch\n",
      "Epoch 3/20  Iteration 497/3560 Training loss: 1.9816 1.3972 sec/batch\n",
      "Epoch 3/20  Iteration 498/3560 Training loss: 1.9810 1.4631 sec/batch\n",
      "Epoch 3/20  Iteration 499/3560 Training loss: 1.9805 1.4163 sec/batch\n",
      "Epoch 3/20  Iteration 500/3560 Training loss: 1.9800 1.4772 sec/batch\n",
      "Epoch 3/20  Iteration 501/3560 Training loss: 1.9794 1.3071 sec/batch\n",
      "Epoch 3/20  Iteration 502/3560 Training loss: 1.9791 1.4144 sec/batch\n",
      "Epoch 3/20  Iteration 503/3560 Training loss: 1.9787 1.4562 sec/batch\n",
      "Epoch 3/20  Iteration 504/3560 Training loss: 1.9783 1.4481 sec/batch\n",
      "Epoch 3/20  Iteration 505/3560 Training loss: 1.9779 1.3909 sec/batch\n",
      "Epoch 3/20  Iteration 506/3560 Training loss: 1.9773 1.3929 sec/batch\n",
      "Epoch 3/20  Iteration 507/3560 Training loss: 1.9767 1.4713 sec/batch\n",
      "Epoch 3/20  Iteration 508/3560 Training loss: 1.9764 1.3873 sec/batch\n",
      "Epoch 3/20  Iteration 509/3560 Training loss: 1.9760 1.4463 sec/batch\n",
      "Epoch 3/20  Iteration 510/3560 Training loss: 1.9755 1.3869 sec/batch\n",
      "Epoch 3/20  Iteration 511/3560 Training loss: 1.9750 1.4683 sec/batch\n",
      "Epoch 3/20  Iteration 512/3560 Training loss: 1.9744 1.3587 sec/batch\n",
      "Epoch 3/20  Iteration 513/3560 Training loss: 1.9740 1.3851 sec/batch\n",
      "Epoch 3/20  Iteration 514/3560 Training loss: 1.9735 1.4100 sec/batch\n",
      "Epoch 3/20  Iteration 515/3560 Training loss: 1.9728 1.5141 sec/batch\n",
      "Epoch 3/20  Iteration 516/3560 Training loss: 1.9725 1.3921 sec/batch\n",
      "Epoch 3/20  Iteration 517/3560 Training loss: 1.9722 1.4087 sec/batch\n",
      "Epoch 3/20  Iteration 518/3560 Training loss: 1.9716 1.3934 sec/batch\n",
      "Epoch 3/20  Iteration 519/3560 Training loss: 1.9712 1.4468 sec/batch\n",
      "Epoch 3/20  Iteration 520/3560 Training loss: 1.9707 1.4411 sec/batch\n",
      "Epoch 3/20  Iteration 521/3560 Training loss: 1.9703 1.3906 sec/batch\n",
      "Epoch 3/20  Iteration 522/3560 Training loss: 1.9698 1.2985 sec/batch\n",
      "Epoch 3/20  Iteration 523/3560 Training loss: 1.9694 1.5615 sec/batch\n",
      "Epoch 3/20  Iteration 524/3560 Training loss: 1.9692 1.4004 sec/batch\n",
      "Epoch 3/20  Iteration 525/3560 Training loss: 1.9687 1.3499 sec/batch\n",
      "Epoch 3/20  Iteration 526/3560 Training loss: 1.9682 1.4265 sec/batch\n",
      "Epoch 3/20  Iteration 527/3560 Training loss: 1.9676 1.4688 sec/batch\n",
      "Epoch 3/20  Iteration 528/3560 Training loss: 1.9671 1.4224 sec/batch\n",
      "Epoch 3/20  Iteration 529/3560 Training loss: 1.9668 1.4212 sec/batch\n",
      "Epoch 3/20  Iteration 530/3560 Training loss: 1.9664 1.3611 sec/batch\n",
      "Epoch 3/20  Iteration 531/3560 Training loss: 1.9661 1.4388 sec/batch\n",
      "Epoch 3/20  Iteration 532/3560 Training loss: 1.9656 1.4732 sec/batch\n",
      "Epoch 3/20  Iteration 533/3560 Training loss: 1.9650 1.4424 sec/batch\n",
      "Epoch 3/20  Iteration 534/3560 Training loss: 1.9645 1.3976 sec/batch\n",
      "Epoch 4/20  Iteration 535/3560 Training loss: 1.9456 1.4056 sec/batch\n",
      "Epoch 4/20  Iteration 536/3560 Training loss: 1.9057 1.4110 sec/batch\n",
      "Epoch 4/20  Iteration 537/3560 Training loss: 1.8971 1.5043 sec/batch\n",
      "Epoch 4/20  Iteration 538/3560 Training loss: 1.8906 1.3444 sec/batch\n",
      "Epoch 4/20  Iteration 539/3560 Training loss: 1.8871 1.4478 sec/batch\n",
      "Epoch 4/20  Iteration 540/3560 Training loss: 1.8764 1.5132 sec/batch\n",
      "Epoch 4/20  Iteration 541/3560 Training loss: 1.8778 1.4506 sec/batch\n",
      "Epoch 4/20  Iteration 542/3560 Training loss: 1.8773 1.4118 sec/batch\n",
      "Epoch 4/20  Iteration 543/3560 Training loss: 1.8797 1.4957 sec/batch\n",
      "Epoch 4/20  Iteration 544/3560 Training loss: 1.8799 1.4401 sec/batch\n",
      "Epoch 4/20  Iteration 545/3560 Training loss: 1.8767 1.4121 sec/batch\n",
      "Epoch 4/20  Iteration 546/3560 Training loss: 1.8746 1.3441 sec/batch\n",
      "Epoch 4/20  Iteration 547/3560 Training loss: 1.8740 1.4984 sec/batch\n",
      "Epoch 4/20  Iteration 548/3560 Training loss: 1.8760 1.3932 sec/batch\n",
      "Epoch 4/20  Iteration 549/3560 Training loss: 1.8743 1.3909 sec/batch\n",
      "Epoch 4/20  Iteration 550/3560 Training loss: 1.8727 1.4022 sec/batch\n",
      "Epoch 4/20  Iteration 551/3560 Training loss: 1.8716 1.3874 sec/batch\n",
      "Epoch 4/20  Iteration 552/3560 Training loss: 1.8737 1.5583 sec/batch\n",
      "Epoch 4/20  Iteration 553/3560 Training loss: 1.8732 1.4551 sec/batch\n",
      "Epoch 4/20  Iteration 554/3560 Training loss: 1.8731 1.3705 sec/batch\n",
      "Epoch 4/20  Iteration 555/3560 Training loss: 1.8718 1.3914 sec/batch\n",
      "Epoch 4/20  Iteration 556/3560 Training loss: 1.8731 1.4328 sec/batch\n",
      "Epoch 4/20  Iteration 557/3560 Training loss: 1.8720 1.3781 sec/batch\n",
      "Epoch 4/20  Iteration 558/3560 Training loss: 1.8711 1.3428 sec/batch\n",
      "Epoch 4/20  Iteration 559/3560 Training loss: 1.8700 1.4613 sec/batch\n",
      "Epoch 4/20  Iteration 560/3560 Training loss: 1.8687 1.4069 sec/batch\n",
      "Epoch 4/20  Iteration 561/3560 Training loss: 1.8673 1.3877 sec/batch\n",
      "Epoch 4/20  Iteration 562/3560 Training loss: 1.8672 1.4994 sec/batch\n",
      "Epoch 4/20  Iteration 563/3560 Training loss: 1.8677 1.4111 sec/batch\n",
      "Epoch 4/20  Iteration 564/3560 Training loss: 1.8677 1.3384 sec/batch\n",
      "Epoch 4/20  Iteration 565/3560 Training loss: 1.8672 1.4800 sec/batch\n",
      "Epoch 4/20  Iteration 566/3560 Training loss: 1.8661 1.4338 sec/batch\n",
      "Epoch 4/20  Iteration 567/3560 Training loss: 1.8657 1.4153 sec/batch\n",
      "Epoch 4/20  Iteration 568/3560 Training loss: 1.8662 1.4982 sec/batch\n",
      "Epoch 4/20  Iteration 569/3560 Training loss: 1.8656 1.3983 sec/batch\n",
      "Epoch 4/20  Iteration 570/3560 Training loss: 1.8651 1.4634 sec/batch\n",
      "Epoch 4/20  Iteration 571/3560 Training loss: 1.8644 1.3940 sec/batch\n",
      "Epoch 4/20  Iteration 572/3560 Training loss: 1.8628 1.4230 sec/batch\n",
      "Epoch 4/20  Iteration 573/3560 Training loss: 1.8611 1.3759 sec/batch\n",
      "Epoch 4/20  Iteration 574/3560 Training loss: 1.8600 1.3734 sec/batch\n",
      "Epoch 4/20  Iteration 575/3560 Training loss: 1.8589 1.4259 sec/batch\n",
      "Epoch 4/20  Iteration 576/3560 Training loss: 1.8588 1.3998 sec/batch\n",
      "Epoch 4/20  Iteration 577/3560 Training loss: 1.8579 1.4011 sec/batch\n",
      "Epoch 4/20  Iteration 578/3560 Training loss: 1.8566 1.3812 sec/batch\n",
      "Epoch 4/20  Iteration 579/3560 Training loss: 1.8565 1.5224 sec/batch\n",
      "Epoch 4/20  Iteration 580/3560 Training loss: 1.8552 1.4328 sec/batch\n",
      "Epoch 4/20  Iteration 581/3560 Training loss: 1.8549 1.3642 sec/batch\n",
      "Epoch 4/20  Iteration 582/3560 Training loss: 1.8543 1.4153 sec/batch\n",
      "Epoch 4/20  Iteration 583/3560 Training loss: 1.8537 1.3765 sec/batch\n",
      "Epoch 4/20  Iteration 584/3560 Training loss: 1.8542 1.4073 sec/batch\n",
      "Epoch 4/20  Iteration 585/3560 Training loss: 1.8535 1.4150 sec/batch\n",
      "Epoch 4/20  Iteration 586/3560 Training loss: 1.8542 1.4378 sec/batch\n",
      "Epoch 4/20  Iteration 587/3560 Training loss: 1.8536 1.4672 sec/batch\n",
      "Epoch 4/20  Iteration 588/3560 Training loss: 1.8533 1.4112 sec/batch\n",
      "Epoch 4/20  Iteration 589/3560 Training loss: 1.8527 1.2856 sec/batch\n",
      "Epoch 4/20  Iteration 590/3560 Training loss: 1.8526 0.8962 sec/batch\n",
      "Epoch 4/20  Iteration 591/3560 Training loss: 1.8524 0.9076 sec/batch\n",
      "Epoch 4/20  Iteration 592/3560 Training loss: 1.8518 0.7913 sec/batch\n",
      "Epoch 4/20  Iteration 593/3560 Training loss: 1.8511 0.8060 sec/batch\n",
      "Epoch 4/20  Iteration 594/3560 Training loss: 1.8513 0.8320 sec/batch\n",
      "Epoch 4/20  Iteration 595/3560 Training loss: 1.8511 0.8882 sec/batch\n",
      "Epoch 4/20  Iteration 596/3560 Training loss: 1.8515 0.7816 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 597/3560 Training loss: 1.8516 0.8037 sec/batch\n",
      "Epoch 4/20  Iteration 598/3560 Training loss: 1.8515 0.8170 sec/batch\n",
      "Epoch 4/20  Iteration 599/3560 Training loss: 1.8511 0.8159 sec/batch\n",
      "Epoch 4/20  Iteration 600/3560 Training loss: 1.8511 0.7761 sec/batch\n",
      "Epoch 4/20  Iteration 601/3560 Training loss: 1.8511 0.8470 sec/batch\n",
      "Epoch 4/20  Iteration 602/3560 Training loss: 1.8504 0.7931 sec/batch\n",
      "Epoch 4/20  Iteration 603/3560 Training loss: 1.8500 0.7795 sec/batch\n",
      "Epoch 4/20  Iteration 604/3560 Training loss: 1.8495 0.7821 sec/batch\n",
      "Epoch 4/20  Iteration 605/3560 Training loss: 1.8496 0.8015 sec/batch\n",
      "Epoch 4/20  Iteration 606/3560 Training loss: 1.8494 0.8008 sec/batch\n",
      "Epoch 4/20  Iteration 607/3560 Training loss: 1.8495 0.7701 sec/batch\n",
      "Epoch 4/20  Iteration 608/3560 Training loss: 1.8490 0.8132 sec/batch\n",
      "Epoch 4/20  Iteration 609/3560 Training loss: 1.8484 0.7904 sec/batch\n",
      "Epoch 4/20  Iteration 610/3560 Training loss: 1.8484 0.8441 sec/batch\n",
      "Epoch 4/20  Iteration 611/3560 Training loss: 1.8480 0.8251 sec/batch\n",
      "Epoch 4/20  Iteration 612/3560 Training loss: 1.8477 0.7990 sec/batch\n",
      "Epoch 4/20  Iteration 613/3560 Training loss: 1.8470 0.8164 sec/batch\n",
      "Epoch 4/20  Iteration 614/3560 Training loss: 1.8467 0.7723 sec/batch\n",
      "Epoch 4/20  Iteration 615/3560 Training loss: 1.8458 0.7866 sec/batch\n",
      "Epoch 4/20  Iteration 616/3560 Training loss: 1.8456 0.7995 sec/batch\n",
      "Epoch 4/20  Iteration 617/3560 Training loss: 1.8449 0.8070 sec/batch\n",
      "Epoch 4/20  Iteration 618/3560 Training loss: 1.8446 0.7801 sec/batch\n",
      "Epoch 4/20  Iteration 619/3560 Training loss: 1.8438 0.8156 sec/batch\n",
      "Epoch 4/20  Iteration 620/3560 Training loss: 1.8432 0.8520 sec/batch\n",
      "Epoch 4/20  Iteration 621/3560 Training loss: 1.8429 0.8011 sec/batch\n",
      "Epoch 4/20  Iteration 622/3560 Training loss: 1.8423 0.7803 sec/batch\n",
      "Epoch 4/20  Iteration 623/3560 Training loss: 1.8415 0.8274 sec/batch\n",
      "Epoch 4/20  Iteration 624/3560 Training loss: 1.8414 0.7868 sec/batch\n",
      "Epoch 4/20  Iteration 625/3560 Training loss: 1.8408 0.7915 sec/batch\n",
      "Epoch 4/20  Iteration 626/3560 Training loss: 1.8404 0.8142 sec/batch\n",
      "Epoch 4/20  Iteration 627/3560 Training loss: 1.8397 0.8203 sec/batch\n",
      "Epoch 4/20  Iteration 628/3560 Training loss: 1.8393 0.8208 sec/batch\n",
      "Epoch 4/20  Iteration 629/3560 Training loss: 1.8387 0.8273 sec/batch\n",
      "Epoch 4/20  Iteration 630/3560 Training loss: 1.8384 0.8124 sec/batch\n",
      "Epoch 4/20  Iteration 631/3560 Training loss: 1.8381 0.7888 sec/batch\n",
      "Epoch 4/20  Iteration 632/3560 Training loss: 1.8375 0.7748 sec/batch\n",
      "Epoch 4/20  Iteration 633/3560 Training loss: 1.8367 0.8361 sec/batch\n",
      "Epoch 4/20  Iteration 634/3560 Training loss: 1.8360 0.7856 sec/batch\n",
      "Epoch 4/20  Iteration 635/3560 Training loss: 1.8356 0.7813 sec/batch\n",
      "Epoch 4/20  Iteration 636/3560 Training loss: 1.8353 0.7944 sec/batch\n",
      "Epoch 4/20  Iteration 637/3560 Training loss: 1.8348 0.7803 sec/batch\n",
      "Epoch 4/20  Iteration 638/3560 Training loss: 1.8344 0.8202 sec/batch\n",
      "Epoch 4/20  Iteration 639/3560 Training loss: 1.8339 0.7985 sec/batch\n",
      "Epoch 4/20  Iteration 640/3560 Training loss: 1.8334 0.8363 sec/batch\n",
      "Epoch 4/20  Iteration 641/3560 Training loss: 1.8330 0.7964 sec/batch\n",
      "Epoch 4/20  Iteration 642/3560 Training loss: 1.8327 0.8056 sec/batch\n",
      "Epoch 4/20  Iteration 643/3560 Training loss: 1.8325 0.7921 sec/batch\n",
      "Epoch 4/20  Iteration 644/3560 Training loss: 1.8322 0.7734 sec/batch\n",
      "Epoch 4/20  Iteration 645/3560 Training loss: 1.8320 0.7989 sec/batch\n",
      "Epoch 4/20  Iteration 646/3560 Training loss: 1.8316 0.8495 sec/batch\n",
      "Epoch 4/20  Iteration 647/3560 Training loss: 1.8312 0.7854 sec/batch\n",
      "Epoch 4/20  Iteration 648/3560 Training loss: 1.8307 0.8139 sec/batch\n",
      "Epoch 4/20  Iteration 649/3560 Training loss: 1.8303 0.7987 sec/batch\n",
      "Epoch 4/20  Iteration 650/3560 Training loss: 1.8296 0.8061 sec/batch\n",
      "Epoch 4/20  Iteration 651/3560 Training loss: 1.8292 0.7913 sec/batch\n",
      "Epoch 4/20  Iteration 652/3560 Training loss: 1.8288 0.8118 sec/batch\n",
      "Epoch 4/20  Iteration 653/3560 Training loss: 1.8285 0.7787 sec/batch\n",
      "Epoch 4/20  Iteration 654/3560 Training loss: 1.8282 0.8221 sec/batch\n",
      "Epoch 4/20  Iteration 655/3560 Training loss: 1.8279 0.8205 sec/batch\n",
      "Epoch 4/20  Iteration 656/3560 Training loss: 1.8274 0.8125 sec/batch\n",
      "Epoch 4/20  Iteration 657/3560 Training loss: 1.8268 0.7979 sec/batch\n",
      "Epoch 4/20  Iteration 658/3560 Training loss: 1.8266 0.7960 sec/batch\n",
      "Epoch 4/20  Iteration 659/3560 Training loss: 1.8264 0.8222 sec/batch\n",
      "Epoch 4/20  Iteration 660/3560 Training loss: 1.8257 0.8121 sec/batch\n",
      "Epoch 4/20  Iteration 661/3560 Training loss: 1.8255 0.7968 sec/batch\n",
      "Epoch 4/20  Iteration 662/3560 Training loss: 1.8252 0.8093 sec/batch\n",
      "Epoch 4/20  Iteration 663/3560 Training loss: 1.8249 0.7804 sec/batch\n",
      "Epoch 4/20  Iteration 664/3560 Training loss: 1.8245 0.8257 sec/batch\n",
      "Epoch 4/20  Iteration 665/3560 Training loss: 1.8240 0.8143 sec/batch\n",
      "Epoch 4/20  Iteration 666/3560 Training loss: 1.8235 0.8094 sec/batch\n",
      "Epoch 4/20  Iteration 667/3560 Training loss: 1.8232 0.8090 sec/batch\n",
      "Epoch 4/20  Iteration 668/3560 Training loss: 1.8230 0.8221 sec/batch\n",
      "Epoch 4/20  Iteration 669/3560 Training loss: 1.8228 0.8103 sec/batch\n",
      "Epoch 4/20  Iteration 670/3560 Training loss: 1.8225 0.7819 sec/batch\n",
      "Epoch 4/20  Iteration 671/3560 Training loss: 1.8222 0.8879 sec/batch\n",
      "Epoch 4/20  Iteration 672/3560 Training loss: 1.8220 0.8063 sec/batch\n",
      "Epoch 4/20  Iteration 673/3560 Training loss: 1.8219 0.7906 sec/batch\n",
      "Epoch 4/20  Iteration 674/3560 Training loss: 1.8216 0.7762 sec/batch\n",
      "Epoch 4/20  Iteration 675/3560 Training loss: 1.8215 0.8457 sec/batch\n",
      "Epoch 4/20  Iteration 676/3560 Training loss: 1.8212 0.7882 sec/batch\n",
      "Epoch 4/20  Iteration 677/3560 Training loss: 1.8209 0.8093 sec/batch\n",
      "Epoch 4/20  Iteration 678/3560 Training loss: 1.8207 0.8169 sec/batch\n",
      "Epoch 4/20  Iteration 679/3560 Training loss: 1.8203 0.7947 sec/batch\n",
      "Epoch 4/20  Iteration 680/3560 Training loss: 1.8201 0.7850 sec/batch\n",
      "Epoch 4/20  Iteration 681/3560 Training loss: 1.8199 0.7741 sec/batch\n",
      "Epoch 4/20  Iteration 682/3560 Training loss: 1.8198 0.7738 sec/batch\n",
      "Epoch 4/20  Iteration 683/3560 Training loss: 1.8196 0.8178 sec/batch\n",
      "Epoch 4/20  Iteration 684/3560 Training loss: 1.8193 0.8114 sec/batch\n",
      "Epoch 4/20  Iteration 685/3560 Training loss: 1.8188 0.7918 sec/batch\n",
      "Epoch 4/20  Iteration 686/3560 Training loss: 1.8186 0.8022 sec/batch\n",
      "Epoch 4/20  Iteration 687/3560 Training loss: 1.8184 0.7743 sec/batch\n",
      "Epoch 4/20  Iteration 688/3560 Training loss: 1.8182 0.8126 sec/batch\n",
      "Epoch 4/20  Iteration 689/3560 Training loss: 1.8180 0.8103 sec/batch\n",
      "Epoch 4/20  Iteration 690/3560 Training loss: 1.8176 0.8199 sec/batch\n",
      "Epoch 4/20  Iteration 691/3560 Training loss: 1.8174 0.8453 sec/batch\n",
      "Epoch 4/20  Iteration 692/3560 Training loss: 1.8171 0.7846 sec/batch\n",
      "Epoch 4/20  Iteration 693/3560 Training loss: 1.8167 0.8002 sec/batch\n",
      "Epoch 4/20  Iteration 694/3560 Training loss: 1.8165 0.7782 sec/batch\n",
      "Epoch 4/20  Iteration 695/3560 Training loss: 1.8164 0.8218 sec/batch\n",
      "Epoch 4/20  Iteration 696/3560 Training loss: 1.8160 0.8621 sec/batch\n",
      "Epoch 4/20  Iteration 697/3560 Training loss: 1.8159 0.8020 sec/batch\n",
      "Epoch 4/20  Iteration 698/3560 Training loss: 1.8157 0.8195 sec/batch\n",
      "Epoch 4/20  Iteration 699/3560 Training loss: 1.8154 0.8924 sec/batch\n",
      "Epoch 4/20  Iteration 700/3560 Training loss: 1.8151 0.7980 sec/batch\n",
      "Epoch 4/20  Iteration 701/3560 Training loss: 1.8150 0.8616 sec/batch\n",
      "Epoch 4/20  Iteration 702/3560 Training loss: 1.8151 0.8282 sec/batch\n",
      "Epoch 4/20  Iteration 703/3560 Training loss: 1.8148 0.7892 sec/batch\n",
      "Epoch 4/20  Iteration 704/3560 Training loss: 1.8145 0.7909 sec/batch\n",
      "Epoch 4/20  Iteration 705/3560 Training loss: 1.8141 0.8239 sec/batch\n",
      "Epoch 4/20  Iteration 706/3560 Training loss: 1.8138 0.7727 sec/batch\n",
      "Epoch 4/20  Iteration 707/3560 Training loss: 1.8136 0.8116 sec/batch\n",
      "Epoch 4/20  Iteration 708/3560 Training loss: 1.8134 0.8006 sec/batch\n",
      "Epoch 4/20  Iteration 709/3560 Training loss: 1.8133 0.8381 sec/batch\n",
      "Epoch 4/20  Iteration 710/3560 Training loss: 1.8131 0.8215 sec/batch\n",
      "Epoch 4/20  Iteration 711/3560 Training loss: 1.8127 0.7793 sec/batch\n",
      "Epoch 4/20  Iteration 712/3560 Training loss: 1.8124 0.7985 sec/batch\n",
      "Epoch 5/20  Iteration 713/3560 Training loss: 1.8387 0.8404 sec/batch\n",
      "Epoch 5/20  Iteration 714/3560 Training loss: 1.7998 0.7724 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 715/3560 Training loss: 1.7847 0.8127 sec/batch\n",
      "Epoch 5/20  Iteration 716/3560 Training loss: 1.7781 0.7699 sec/batch\n",
      "Epoch 5/20  Iteration 717/3560 Training loss: 1.7738 0.7699 sec/batch\n",
      "Epoch 5/20  Iteration 718/3560 Training loss: 1.7629 0.8004 sec/batch\n",
      "Epoch 5/20  Iteration 719/3560 Training loss: 1.7637 0.8361 sec/batch\n",
      "Epoch 5/20  Iteration 720/3560 Training loss: 1.7626 0.7850 sec/batch\n",
      "Epoch 5/20  Iteration 721/3560 Training loss: 1.7644 0.8724 sec/batch\n",
      "Epoch 5/20  Iteration 722/3560 Training loss: 1.7642 0.7979 sec/batch\n",
      "Epoch 5/20  Iteration 723/3560 Training loss: 1.7610 0.8045 sec/batch\n",
      "Epoch 5/20  Iteration 724/3560 Training loss: 1.7593 0.8103 sec/batch\n",
      "Epoch 5/20  Iteration 725/3560 Training loss: 1.7593 0.7917 sec/batch\n",
      "Epoch 5/20  Iteration 726/3560 Training loss: 1.7621 0.8226 sec/batch\n",
      "Epoch 5/20  Iteration 727/3560 Training loss: 1.7609 0.7904 sec/batch\n",
      "Epoch 5/20  Iteration 728/3560 Training loss: 1.7589 0.8359 sec/batch\n",
      "Epoch 5/20  Iteration 729/3560 Training loss: 1.7583 0.7712 sec/batch\n",
      "Epoch 5/20  Iteration 730/3560 Training loss: 1.7603 0.8101 sec/batch\n",
      "Epoch 5/20  Iteration 731/3560 Training loss: 1.7598 0.8190 sec/batch\n",
      "Epoch 5/20  Iteration 732/3560 Training loss: 1.7598 0.8086 sec/batch\n",
      "Epoch 5/20  Iteration 733/3560 Training loss: 1.7592 0.8187 sec/batch\n",
      "Epoch 5/20  Iteration 734/3560 Training loss: 1.7595 0.8638 sec/batch\n",
      "Epoch 5/20  Iteration 735/3560 Training loss: 1.7584 0.8217 sec/batch\n",
      "Epoch 5/20  Iteration 736/3560 Training loss: 1.7583 0.8002 sec/batch\n",
      "Epoch 5/20  Iteration 737/3560 Training loss: 1.7576 0.7833 sec/batch\n",
      "Epoch 5/20  Iteration 738/3560 Training loss: 1.7560 0.8078 sec/batch\n",
      "Epoch 5/20  Iteration 739/3560 Training loss: 1.7545 0.7995 sec/batch\n",
      "Epoch 5/20  Iteration 740/3560 Training loss: 1.7547 0.7981 sec/batch\n",
      "Epoch 5/20  Iteration 741/3560 Training loss: 1.7553 0.7803 sec/batch\n",
      "Epoch 5/20  Iteration 742/3560 Training loss: 1.7554 0.7863 sec/batch\n",
      "Epoch 5/20  Iteration 743/3560 Training loss: 1.7549 0.8203 sec/batch\n",
      "Epoch 5/20  Iteration 744/3560 Training loss: 1.7541 0.8009 sec/batch\n",
      "Epoch 5/20  Iteration 745/3560 Training loss: 1.7544 0.8109 sec/batch\n",
      "Epoch 5/20  Iteration 746/3560 Training loss: 1.7548 0.8991 sec/batch\n",
      "Epoch 5/20  Iteration 747/3560 Training loss: 1.7541 0.7895 sec/batch\n",
      "Epoch 5/20  Iteration 748/3560 Training loss: 1.7536 0.8132 sec/batch\n",
      "Epoch 5/20  Iteration 749/3560 Training loss: 1.7529 0.8004 sec/batch\n",
      "Epoch 5/20  Iteration 750/3560 Training loss: 1.7515 0.8174 sec/batch\n",
      "Epoch 5/20  Iteration 751/3560 Training loss: 1.7502 0.7665 sec/batch\n",
      "Epoch 5/20  Iteration 752/3560 Training loss: 1.7495 0.7825 sec/batch\n",
      "Epoch 5/20  Iteration 753/3560 Training loss: 1.7487 0.8060 sec/batch\n",
      "Epoch 5/20  Iteration 754/3560 Training loss: 1.7488 0.7738 sec/batch\n",
      "Epoch 5/20  Iteration 755/3560 Training loss: 1.7480 0.8116 sec/batch\n",
      "Epoch 5/20  Iteration 756/3560 Training loss: 1.7472 0.8104 sec/batch\n",
      "Epoch 5/20  Iteration 757/3560 Training loss: 1.7471 0.7754 sec/batch\n",
      "Epoch 5/20  Iteration 758/3560 Training loss: 1.7461 0.8324 sec/batch\n",
      "Epoch 5/20  Iteration 759/3560 Training loss: 1.7457 0.8653 sec/batch\n",
      "Epoch 5/20  Iteration 760/3560 Training loss: 1.7451 0.8064 sec/batch\n",
      "Epoch 5/20  Iteration 761/3560 Training loss: 1.7446 0.8095 sec/batch\n",
      "Epoch 5/20  Iteration 762/3560 Training loss: 1.7450 0.8068 sec/batch\n",
      "Epoch 5/20  Iteration 763/3560 Training loss: 1.7444 0.8488 sec/batch\n",
      "Epoch 5/20  Iteration 764/3560 Training loss: 1.7454 0.8157 sec/batch\n",
      "Epoch 5/20  Iteration 765/3560 Training loss: 1.7452 0.8406 sec/batch\n",
      "Epoch 5/20  Iteration 766/3560 Training loss: 1.7450 0.8334 sec/batch\n",
      "Epoch 5/20  Iteration 767/3560 Training loss: 1.7445 0.7861 sec/batch\n",
      "Epoch 5/20  Iteration 768/3560 Training loss: 1.7446 0.7962 sec/batch\n",
      "Epoch 5/20  Iteration 769/3560 Training loss: 1.7448 0.7816 sec/batch\n",
      "Epoch 5/20  Iteration 770/3560 Training loss: 1.7444 0.7905 sec/batch\n",
      "Epoch 5/20  Iteration 771/3560 Training loss: 1.7438 0.8298 sec/batch\n",
      "Epoch 5/20  Iteration 772/3560 Training loss: 1.7441 0.8319 sec/batch\n",
      "Epoch 5/20  Iteration 773/3560 Training loss: 1.7439 0.8115 sec/batch\n",
      "Epoch 5/20  Iteration 774/3560 Training loss: 1.7444 0.8145 sec/batch\n",
      "Epoch 5/20  Iteration 775/3560 Training loss: 1.7447 0.7948 sec/batch\n",
      "Epoch 5/20  Iteration 776/3560 Training loss: 1.7448 0.8144 sec/batch\n",
      "Epoch 5/20  Iteration 777/3560 Training loss: 1.7445 0.7924 sec/batch\n",
      "Epoch 5/20  Iteration 778/3560 Training loss: 1.7446 0.8318 sec/batch\n",
      "Epoch 5/20  Iteration 779/3560 Training loss: 1.7446 0.8013 sec/batch\n",
      "Epoch 5/20  Iteration 780/3560 Training loss: 1.7441 0.8574 sec/batch\n",
      "Epoch 5/20  Iteration 781/3560 Training loss: 1.7438 0.7762 sec/batch\n",
      "Epoch 5/20  Iteration 782/3560 Training loss: 1.7436 0.8216 sec/batch\n",
      "Epoch 5/20  Iteration 783/3560 Training loss: 1.7439 0.8128 sec/batch\n",
      "Epoch 5/20  Iteration 784/3560 Training loss: 1.7439 0.8368 sec/batch\n",
      "Epoch 5/20  Iteration 785/3560 Training loss: 1.7441 0.8292 sec/batch\n",
      "Epoch 5/20  Iteration 786/3560 Training loss: 1.7436 0.7989 sec/batch\n",
      "Epoch 5/20  Iteration 787/3560 Training loss: 1.7434 0.7776 sec/batch\n",
      "Epoch 5/20  Iteration 788/3560 Training loss: 1.7434 0.7788 sec/batch\n",
      "Epoch 5/20  Iteration 789/3560 Training loss: 1.7431 0.8075 sec/batch\n",
      "Epoch 5/20  Iteration 790/3560 Training loss: 1.7430 0.8029 sec/batch\n",
      "Epoch 5/20  Iteration 791/3560 Training loss: 1.7423 0.8099 sec/batch\n",
      "Epoch 5/20  Iteration 792/3560 Training loss: 1.7420 0.7817 sec/batch\n",
      "Epoch 5/20  Iteration 793/3560 Training loss: 1.7414 0.7813 sec/batch\n",
      "Epoch 5/20  Iteration 794/3560 Training loss: 1.7413 0.8215 sec/batch\n",
      "Epoch 5/20  Iteration 795/3560 Training loss: 1.7407 0.7949 sec/batch\n",
      "Epoch 5/20  Iteration 796/3560 Training loss: 1.7405 0.8412 sec/batch\n",
      "Epoch 5/20  Iteration 797/3560 Training loss: 1.7398 0.8052 sec/batch\n",
      "Epoch 5/20  Iteration 798/3560 Training loss: 1.7393 0.7951 sec/batch\n",
      "Epoch 5/20  Iteration 799/3560 Training loss: 1.7391 0.8125 sec/batch\n",
      "Epoch 5/20  Iteration 800/3560 Training loss: 1.7386 0.7849 sec/batch\n",
      "Epoch 5/20  Iteration 801/3560 Training loss: 1.7380 0.7822 sec/batch\n",
      "Epoch 5/20  Iteration 802/3560 Training loss: 1.7380 0.7812 sec/batch\n",
      "Epoch 5/20  Iteration 803/3560 Training loss: 1.7377 0.8109 sec/batch\n",
      "Epoch 5/20  Iteration 804/3560 Training loss: 1.7374 0.7744 sec/batch\n",
      "Epoch 5/20  Iteration 805/3560 Training loss: 1.7368 0.8311 sec/batch\n",
      "Epoch 5/20  Iteration 806/3560 Training loss: 1.7364 0.7867 sec/batch\n",
      "Epoch 5/20  Iteration 807/3560 Training loss: 1.7358 0.7806 sec/batch\n",
      "Epoch 5/20  Iteration 808/3560 Training loss: 1.7357 0.8141 sec/batch\n",
      "Epoch 5/20  Iteration 809/3560 Training loss: 1.7354 0.8303 sec/batch\n",
      "Epoch 5/20  Iteration 810/3560 Training loss: 1.7348 0.7952 sec/batch\n",
      "Epoch 5/20  Iteration 811/3560 Training loss: 1.7343 0.8126 sec/batch\n",
      "Epoch 5/20  Iteration 812/3560 Training loss: 1.7338 0.7838 sec/batch\n",
      "Epoch 5/20  Iteration 813/3560 Training loss: 1.7336 0.7909 sec/batch\n",
      "Epoch 5/20  Iteration 814/3560 Training loss: 1.7335 0.7787 sec/batch\n",
      "Epoch 5/20  Iteration 815/3560 Training loss: 1.7331 0.7943 sec/batch\n",
      "Epoch 5/20  Iteration 816/3560 Training loss: 1.7328 0.8014 sec/batch\n",
      "Epoch 5/20  Iteration 817/3560 Training loss: 1.7324 0.8124 sec/batch\n",
      "Epoch 5/20  Iteration 818/3560 Training loss: 1.7321 0.8222 sec/batch\n",
      "Epoch 5/20  Iteration 819/3560 Training loss: 1.7319 0.7767 sec/batch\n",
      "Epoch 5/20  Iteration 820/3560 Training loss: 1.7317 0.7890 sec/batch\n",
      "Epoch 5/20  Iteration 821/3560 Training loss: 1.7316 0.9065 sec/batch\n",
      "Epoch 5/20  Iteration 822/3560 Training loss: 1.7314 0.8169 sec/batch\n",
      "Epoch 5/20  Iteration 823/3560 Training loss: 1.7313 0.8218 sec/batch\n",
      "Epoch 5/20  Iteration 824/3560 Training loss: 1.7309 0.7977 sec/batch\n",
      "Epoch 5/20  Iteration 825/3560 Training loss: 1.7306 0.8172 sec/batch\n",
      "Epoch 5/20  Iteration 826/3560 Training loss: 1.7303 0.8006 sec/batch\n",
      "Epoch 5/20  Iteration 827/3560 Training loss: 1.7299 0.8197 sec/batch\n",
      "Epoch 5/20  Iteration 828/3560 Training loss: 1.7293 0.8030 sec/batch\n",
      "Epoch 5/20  Iteration 829/3560 Training loss: 1.7291 0.7956 sec/batch\n",
      "Epoch 5/20  Iteration 830/3560 Training loss: 1.7289 0.7874 sec/batch\n",
      "Epoch 5/20  Iteration 831/3560 Training loss: 1.7286 0.7668 sec/batch\n",
      "Epoch 5/20  Iteration 832/3560 Training loss: 1.7284 0.7809 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 833/3560 Training loss: 1.7283 0.8376 sec/batch\n",
      "Epoch 5/20  Iteration 834/3560 Training loss: 1.7278 0.8048 sec/batch\n",
      "Epoch 5/20  Iteration 835/3560 Training loss: 1.7273 0.7896 sec/batch\n",
      "Epoch 5/20  Iteration 836/3560 Training loss: 1.7273 0.8166 sec/batch\n",
      "Epoch 5/20  Iteration 837/3560 Training loss: 1.7270 0.8459 sec/batch\n",
      "Epoch 5/20  Iteration 838/3560 Training loss: 1.7264 0.7841 sec/batch\n",
      "Epoch 5/20  Iteration 839/3560 Training loss: 1.7264 0.8069 sec/batch\n",
      "Epoch 5/20  Iteration 840/3560 Training loss: 1.7262 0.8361 sec/batch\n",
      "Epoch 5/20  Iteration 841/3560 Training loss: 1.7259 0.8009 sec/batch\n",
      "Epoch 5/20  Iteration 842/3560 Training loss: 1.7256 0.7981 sec/batch\n",
      "Epoch 5/20  Iteration 843/3560 Training loss: 1.7252 0.7859 sec/batch\n",
      "Epoch 5/20  Iteration 844/3560 Training loss: 1.7248 0.7959 sec/batch\n",
      "Epoch 5/20  Iteration 845/3560 Training loss: 1.7246 0.8977 sec/batch\n",
      "Epoch 5/20  Iteration 846/3560 Training loss: 1.7245 0.8227 sec/batch\n",
      "Epoch 5/20  Iteration 847/3560 Training loss: 1.7243 0.8178 sec/batch\n",
      "Epoch 5/20  Iteration 848/3560 Training loss: 1.7242 0.8105 sec/batch\n",
      "Epoch 5/20  Iteration 849/3560 Training loss: 1.7241 0.8134 sec/batch\n",
      "Epoch 5/20  Iteration 850/3560 Training loss: 1.7240 0.7746 sec/batch\n",
      "Epoch 5/20  Iteration 851/3560 Training loss: 1.7240 0.7880 sec/batch\n",
      "Epoch 5/20  Iteration 852/3560 Training loss: 1.7237 0.8228 sec/batch\n",
      "Epoch 5/20  Iteration 853/3560 Training loss: 1.7239 0.8264 sec/batch\n",
      "Epoch 5/20  Iteration 854/3560 Training loss: 1.7236 0.8583 sec/batch\n",
      "Epoch 5/20  Iteration 855/3560 Training loss: 1.7234 0.8070 sec/batch\n",
      "Epoch 5/20  Iteration 856/3560 Training loss: 1.7233 0.8031 sec/batch\n",
      "Epoch 5/20  Iteration 857/3560 Training loss: 1.7230 0.7841 sec/batch\n",
      "Epoch 5/20  Iteration 858/3560 Training loss: 1.7229 0.8720 sec/batch\n",
      "Epoch 5/20  Iteration 859/3560 Training loss: 1.7227 0.8275 sec/batch\n",
      "Epoch 5/20  Iteration 860/3560 Training loss: 1.7228 0.8184 sec/batch\n",
      "Epoch 5/20  Iteration 861/3560 Training loss: 1.7226 0.7883 sec/batch\n",
      "Epoch 5/20  Iteration 862/3560 Training loss: 1.7224 0.8097 sec/batch\n",
      "Epoch 5/20  Iteration 863/3560 Training loss: 1.7219 0.8008 sec/batch\n",
      "Epoch 5/20  Iteration 864/3560 Training loss: 1.7219 0.7981 sec/batch\n",
      "Epoch 5/20  Iteration 865/3560 Training loss: 1.7218 0.8005 sec/batch\n",
      "Epoch 5/20  Iteration 866/3560 Training loss: 1.7216 0.8359 sec/batch\n",
      "Epoch 5/20  Iteration 867/3560 Training loss: 1.7215 0.7943 sec/batch\n",
      "Epoch 5/20  Iteration 868/3560 Training loss: 1.7213 0.8293 sec/batch\n",
      "Epoch 5/20  Iteration 869/3560 Training loss: 1.7212 0.8236 sec/batch\n",
      "Epoch 5/20  Iteration 870/3560 Training loss: 1.7210 0.8913 sec/batch\n",
      "Epoch 5/20  Iteration 871/3560 Training loss: 1.7206 0.8003 sec/batch\n",
      "Epoch 5/20  Iteration 872/3560 Training loss: 1.7205 0.8362 sec/batch\n",
      "Epoch 5/20  Iteration 873/3560 Training loss: 1.7205 0.8058 sec/batch\n",
      "Epoch 5/20  Iteration 874/3560 Training loss: 1.7203 0.8072 sec/batch\n",
      "Epoch 5/20  Iteration 875/3560 Training loss: 1.7202 0.7956 sec/batch\n",
      "Epoch 5/20  Iteration 876/3560 Training loss: 1.7200 0.8065 sec/batch\n",
      "Epoch 5/20  Iteration 877/3560 Training loss: 1.7198 0.8191 sec/batch\n",
      "Epoch 5/20  Iteration 878/3560 Training loss: 1.7196 0.7850 sec/batch\n",
      "Epoch 5/20  Iteration 879/3560 Training loss: 1.7196 0.7986 sec/batch\n",
      "Epoch 5/20  Iteration 880/3560 Training loss: 1.7199 0.7869 sec/batch\n",
      "Epoch 5/20  Iteration 881/3560 Training loss: 1.7197 0.8063 sec/batch\n",
      "Epoch 5/20  Iteration 882/3560 Training loss: 1.7195 0.7727 sec/batch\n",
      "Epoch 5/20  Iteration 883/3560 Training loss: 1.7192 0.9029 sec/batch\n",
      "Epoch 5/20  Iteration 884/3560 Training loss: 1.7189 0.7904 sec/batch\n",
      "Epoch 5/20  Iteration 885/3560 Training loss: 1.7189 0.7838 sec/batch\n",
      "Epoch 5/20  Iteration 886/3560 Training loss: 1.7189 0.8059 sec/batch\n",
      "Epoch 5/20  Iteration 887/3560 Training loss: 1.7188 0.7884 sec/batch\n",
      "Epoch 5/20  Iteration 888/3560 Training loss: 1.7186 0.7756 sec/batch\n",
      "Epoch 5/20  Iteration 889/3560 Training loss: 1.7182 0.8509 sec/batch\n",
      "Epoch 5/20  Iteration 890/3560 Training loss: 1.7181 0.8253 sec/batch\n",
      "Epoch 6/20  Iteration 891/3560 Training loss: 1.7699 0.7783 sec/batch\n",
      "Epoch 6/20  Iteration 892/3560 Training loss: 1.7297 0.7881 sec/batch\n",
      "Epoch 6/20  Iteration 893/3560 Training loss: 1.7117 0.7827 sec/batch\n",
      "Epoch 6/20  Iteration 894/3560 Training loss: 1.7061 0.8030 sec/batch\n",
      "Epoch 6/20  Iteration 895/3560 Training loss: 1.6981 0.8501 sec/batch\n",
      "Epoch 6/20  Iteration 896/3560 Training loss: 1.6890 0.8485 sec/batch\n",
      "Epoch 6/20  Iteration 897/3560 Training loss: 1.6883 0.8538 sec/batch\n",
      "Epoch 6/20  Iteration 898/3560 Training loss: 1.6872 0.8041 sec/batch\n",
      "Epoch 6/20  Iteration 899/3560 Training loss: 1.6891 0.8531 sec/batch\n",
      "Epoch 6/20  Iteration 900/3560 Training loss: 1.6879 0.8153 sec/batch\n",
      "Epoch 6/20  Iteration 901/3560 Training loss: 1.6850 0.8307 sec/batch\n",
      "Epoch 6/20  Iteration 902/3560 Training loss: 1.6834 0.8169 sec/batch\n",
      "Epoch 6/20  Iteration 903/3560 Training loss: 1.6829 0.7990 sec/batch\n",
      "Epoch 6/20  Iteration 904/3560 Training loss: 1.6847 0.8046 sec/batch\n",
      "Epoch 6/20  Iteration 905/3560 Training loss: 1.6832 0.8102 sec/batch\n",
      "Epoch 6/20  Iteration 906/3560 Training loss: 1.6821 0.8071 sec/batch\n",
      "Epoch 6/20  Iteration 907/3560 Training loss: 1.6816 0.8131 sec/batch\n",
      "Epoch 6/20  Iteration 908/3560 Training loss: 1.6831 0.8569 sec/batch\n",
      "Epoch 6/20  Iteration 909/3560 Training loss: 1.6826 0.8057 sec/batch\n",
      "Epoch 6/20  Iteration 910/3560 Training loss: 1.6828 0.7854 sec/batch\n",
      "Epoch 6/20  Iteration 911/3560 Training loss: 1.6816 0.8060 sec/batch\n",
      "Epoch 6/20  Iteration 912/3560 Training loss: 1.6826 0.8054 sec/batch\n",
      "Epoch 6/20  Iteration 913/3560 Training loss: 1.6821 0.8231 sec/batch\n",
      "Epoch 6/20  Iteration 914/3560 Training loss: 1.6818 0.7836 sec/batch\n",
      "Epoch 6/20  Iteration 915/3560 Training loss: 1.6813 0.8006 sec/batch\n",
      "Epoch 6/20  Iteration 916/3560 Training loss: 1.6797 0.7914 sec/batch\n",
      "Epoch 6/20  Iteration 917/3560 Training loss: 1.6784 0.8173 sec/batch\n",
      "Epoch 6/20  Iteration 918/3560 Training loss: 1.6783 0.8244 sec/batch\n",
      "Epoch 6/20  Iteration 919/3560 Training loss: 1.6789 0.7713 sec/batch\n",
      "Epoch 6/20  Iteration 920/3560 Training loss: 1.6792 0.8108 sec/batch\n",
      "Epoch 6/20  Iteration 921/3560 Training loss: 1.6792 0.8230 sec/batch\n",
      "Epoch 6/20  Iteration 922/3560 Training loss: 1.6785 0.8147 sec/batch\n",
      "Epoch 6/20  Iteration 923/3560 Training loss: 1.6786 0.8081 sec/batch\n",
      "Epoch 6/20  Iteration 924/3560 Training loss: 1.6787 0.8075 sec/batch\n",
      "Epoch 6/20  Iteration 925/3560 Training loss: 1.6783 0.7878 sec/batch\n",
      "Epoch 6/20  Iteration 926/3560 Training loss: 1.6779 0.8616 sec/batch\n",
      "Epoch 6/20  Iteration 927/3560 Training loss: 1.6772 0.8125 sec/batch\n",
      "Epoch 6/20  Iteration 928/3560 Training loss: 1.6762 0.7861 sec/batch\n",
      "Epoch 6/20  Iteration 929/3560 Training loss: 1.6749 0.7810 sec/batch\n",
      "Epoch 6/20  Iteration 930/3560 Training loss: 1.6742 0.8152 sec/batch\n",
      "Epoch 6/20  Iteration 931/3560 Training loss: 1.6736 0.8005 sec/batch\n",
      "Epoch 6/20  Iteration 932/3560 Training loss: 1.6739 0.8077 sec/batch\n",
      "Epoch 6/20  Iteration 933/3560 Training loss: 1.6733 0.8434 sec/batch\n",
      "Epoch 6/20  Iteration 934/3560 Training loss: 1.6723 0.8259 sec/batch\n",
      "Epoch 6/20  Iteration 935/3560 Training loss: 1.6726 0.7714 sec/batch\n",
      "Epoch 6/20  Iteration 936/3560 Training loss: 1.6713 0.7841 sec/batch\n",
      "Epoch 6/20  Iteration 937/3560 Training loss: 1.6709 0.7932 sec/batch\n",
      "Epoch 6/20  Iteration 938/3560 Training loss: 1.6704 0.8122 sec/batch\n",
      "Epoch 6/20  Iteration 939/3560 Training loss: 1.6701 0.7669 sec/batch\n",
      "Epoch 6/20  Iteration 940/3560 Training loss: 1.6708 0.7784 sec/batch\n",
      "Epoch 6/20  Iteration 941/3560 Training loss: 1.6703 0.7805 sec/batch\n",
      "Epoch 6/20  Iteration 942/3560 Training loss: 1.6713 0.7794 sec/batch\n",
      "Epoch 6/20  Iteration 943/3560 Training loss: 1.6710 0.8196 sec/batch\n",
      "Epoch 6/20  Iteration 944/3560 Training loss: 1.6709 0.8400 sec/batch\n",
      "Epoch 6/20  Iteration 945/3560 Training loss: 1.6706 0.8535 sec/batch\n",
      "Epoch 6/20  Iteration 946/3560 Training loss: 1.6707 0.8410 sec/batch\n",
      "Epoch 6/20  Iteration 947/3560 Training loss: 1.6711 0.7838 sec/batch\n",
      "Epoch 6/20  Iteration 948/3560 Training loss: 1.6707 0.8139 sec/batch\n",
      "Epoch 6/20  Iteration 949/3560 Training loss: 1.6701 0.8036 sec/batch\n",
      "Epoch 6/20  Iteration 950/3560 Training loss: 1.6705 0.7775 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 951/3560 Training loss: 1.6703 0.8125 sec/batch\n",
      "Epoch 6/20  Iteration 952/3560 Training loss: 1.6712 0.7786 sec/batch\n",
      "Epoch 6/20  Iteration 953/3560 Training loss: 1.6715 0.7736 sec/batch\n",
      "Epoch 6/20  Iteration 954/3560 Training loss: 1.6716 0.8141 sec/batch\n",
      "Epoch 6/20  Iteration 955/3560 Training loss: 1.6715 0.8281 sec/batch\n",
      "Epoch 6/20  Iteration 956/3560 Training loss: 1.6717 0.8121 sec/batch\n",
      "Epoch 6/20  Iteration 957/3560 Training loss: 1.6720 0.8050 sec/batch\n",
      "Epoch 6/20  Iteration 958/3560 Training loss: 1.6715 0.8347 sec/batch\n",
      "Epoch 6/20  Iteration 959/3560 Training loss: 1.6713 0.8439 sec/batch\n",
      "Epoch 6/20  Iteration 960/3560 Training loss: 1.6711 0.8023 sec/batch\n",
      "Epoch 6/20  Iteration 961/3560 Training loss: 1.6715 0.8220 sec/batch\n",
      "Epoch 6/20  Iteration 962/3560 Training loss: 1.6716 0.7638 sec/batch\n",
      "Epoch 6/20  Iteration 963/3560 Training loss: 1.6719 0.8022 sec/batch\n",
      "Epoch 6/20  Iteration 964/3560 Training loss: 1.6716 0.7987 sec/batch\n",
      "Epoch 6/20  Iteration 965/3560 Training loss: 1.6713 0.7818 sec/batch\n",
      "Epoch 6/20  Iteration 966/3560 Training loss: 1.6715 0.7859 sec/batch\n",
      "Epoch 6/20  Iteration 967/3560 Training loss: 1.6713 0.8122 sec/batch\n",
      "Epoch 6/20  Iteration 968/3560 Training loss: 1.6714 0.8196 sec/batch\n",
      "Epoch 6/20  Iteration 969/3560 Training loss: 1.6708 0.8072 sec/batch\n",
      "Epoch 6/20  Iteration 970/3560 Training loss: 1.6706 0.8349 sec/batch\n",
      "Epoch 6/20  Iteration 971/3560 Training loss: 1.6700 0.8926 sec/batch\n",
      "Epoch 6/20  Iteration 972/3560 Training loss: 1.6700 0.7952 sec/batch\n",
      "Epoch 6/20  Iteration 973/3560 Training loss: 1.6694 0.8090 sec/batch\n",
      "Epoch 6/20  Iteration 974/3560 Training loss: 1.6692 0.7977 sec/batch\n",
      "Epoch 6/20  Iteration 975/3560 Training loss: 1.6686 0.7786 sec/batch\n",
      "Epoch 6/20  Iteration 976/3560 Training loss: 1.6682 0.7916 sec/batch\n",
      "Epoch 6/20  Iteration 977/3560 Training loss: 1.6678 0.7920 sec/batch\n",
      "Epoch 6/20  Iteration 978/3560 Training loss: 1.6673 0.7896 sec/batch\n",
      "Epoch 6/20  Iteration 979/3560 Training loss: 1.6667 0.7844 sec/batch\n",
      "Epoch 6/20  Iteration 980/3560 Training loss: 1.6667 0.8223 sec/batch\n",
      "Epoch 6/20  Iteration 981/3560 Training loss: 1.6663 0.8015 sec/batch\n",
      "Epoch 6/20  Iteration 982/3560 Training loss: 1.6660 0.7936 sec/batch\n",
      "Epoch 6/20  Iteration 983/3560 Training loss: 1.6655 0.8576 sec/batch\n",
      "Epoch 6/20  Iteration 984/3560 Training loss: 1.6650 0.8049 sec/batch\n",
      "Epoch 6/20  Iteration 985/3560 Training loss: 1.6646 0.8329 sec/batch\n",
      "Epoch 6/20  Iteration 986/3560 Training loss: 1.6645 0.7910 sec/batch\n",
      "Epoch 6/20  Iteration 987/3560 Training loss: 1.6643 0.7749 sec/batch\n",
      "Epoch 6/20  Iteration 988/3560 Training loss: 1.6638 0.8330 sec/batch\n",
      "Epoch 6/20  Iteration 989/3560 Training loss: 1.6633 0.7984 sec/batch\n",
      "Epoch 6/20  Iteration 990/3560 Training loss: 1.6627 0.7850 sec/batch\n",
      "Epoch 6/20  Iteration 991/3560 Training loss: 1.6626 0.8145 sec/batch\n",
      "Epoch 6/20  Iteration 992/3560 Training loss: 1.6625 0.7768 sec/batch\n",
      "Epoch 6/20  Iteration 993/3560 Training loss: 1.6622 0.7941 sec/batch\n",
      "Epoch 6/20  Iteration 994/3560 Training loss: 1.6619 0.8021 sec/batch\n",
      "Epoch 6/20  Iteration 995/3560 Training loss: 1.6616 0.8012 sec/batch\n",
      "Epoch 6/20  Iteration 996/3560 Training loss: 1.6614 0.8521 sec/batch\n",
      "Epoch 6/20  Iteration 997/3560 Training loss: 1.6613 0.7673 sec/batch\n",
      "Epoch 6/20  Iteration 998/3560 Training loss: 1.6611 0.7889 sec/batch\n",
      "Epoch 6/20  Iteration 999/3560 Training loss: 1.6611 0.7772 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3560 Training loss: 1.6610 0.8133 sec/batch\n",
      "Epoch 6/20  Iteration 1001/3560 Training loss: 1.6608 0.8266 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3560 Training loss: 1.6606 0.7802 sec/batch\n",
      "Epoch 6/20  Iteration 1003/3560 Training loss: 1.6603 0.8088 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3560 Training loss: 1.6601 0.8006 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3560 Training loss: 1.6597 0.7984 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3560 Training loss: 1.6592 0.8149 sec/batch\n",
      "Epoch 6/20  Iteration 1007/3560 Training loss: 1.6590 0.8219 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3560 Training loss: 1.6588 0.8370 sec/batch\n",
      "Epoch 6/20  Iteration 1009/3560 Training loss: 1.6587 0.8473 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3560 Training loss: 1.6585 0.7990 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3560 Training loss: 1.6584 0.8122 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3560 Training loss: 1.6579 0.7879 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3560 Training loss: 1.6574 0.8098 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3560 Training loss: 1.6574 0.7989 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3560 Training loss: 1.6572 0.7913 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3560 Training loss: 1.6567 0.7849 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3560 Training loss: 1.6567 0.8136 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3560 Training loss: 1.6567 0.8377 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3560 Training loss: 1.6564 0.8220 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3560 Training loss: 1.6561 0.7814 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3560 Training loss: 1.6557 0.8578 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3560 Training loss: 1.6553 0.7846 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3560 Training loss: 1.6552 0.8242 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3560 Training loss: 1.6550 0.8157 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3560 Training loss: 1.6549 0.8253 sec/batch\n",
      "Epoch 6/20  Iteration 1026/3560 Training loss: 1.6549 0.7866 sec/batch\n",
      "Epoch 6/20  Iteration 1027/3560 Training loss: 1.6548 0.8182 sec/batch\n",
      "Epoch 6/20  Iteration 1028/3560 Training loss: 1.6547 0.8050 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3560 Training loss: 1.6547 0.8069 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3560 Training loss: 1.6545 0.7778 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3560 Training loss: 1.6546 0.8404 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3560 Training loss: 1.6545 0.7783 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3560 Training loss: 1.6543 0.8514 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3560 Training loss: 1.6542 0.8338 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3560 Training loss: 1.6540 0.7971 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3560 Training loss: 1.6540 0.8079 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3560 Training loss: 1.6539 0.8007 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3560 Training loss: 1.6540 0.7903 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3560 Training loss: 1.6539 0.7972 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3560 Training loss: 1.6537 0.7851 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3560 Training loss: 1.6533 0.7974 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3560 Training loss: 1.6532 0.7699 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3560 Training loss: 1.6531 0.8179 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3560 Training loss: 1.6530 0.8211 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3560 Training loss: 1.6529 0.8250 sec/batch\n",
      "Epoch 6/20  Iteration 1046/3560 Training loss: 1.6528 0.9202 sec/batch\n",
      "Epoch 6/20  Iteration 1047/3560 Training loss: 1.6527 0.7871 sec/batch\n",
      "Epoch 6/20  Iteration 1048/3560 Training loss: 1.6526 0.8127 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3560 Training loss: 1.6523 0.7828 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3560 Training loss: 1.6523 0.8184 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3560 Training loss: 1.6524 0.8034 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3560 Training loss: 1.6523 0.8167 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3560 Training loss: 1.6522 0.8116 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3560 Training loss: 1.6521 0.7795 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3560 Training loss: 1.6520 0.7894 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3560 Training loss: 1.6518 0.8456 sec/batch\n",
      "Epoch 6/20  Iteration 1057/3560 Training loss: 1.6518 0.8101 sec/batch\n",
      "Epoch 6/20  Iteration 1058/3560 Training loss: 1.6521 0.8518 sec/batch\n",
      "Epoch 6/20  Iteration 1059/3560 Training loss: 1.6519 0.8039 sec/batch\n",
      "Epoch 6/20  Iteration 1060/3560 Training loss: 1.6518 0.7766 sec/batch\n",
      "Epoch 6/20  Iteration 1061/3560 Training loss: 1.6517 0.7849 sec/batch\n",
      "Epoch 6/20  Iteration 1062/3560 Training loss: 1.6515 0.8045 sec/batch\n",
      "Epoch 6/20  Iteration 1063/3560 Training loss: 1.6515 0.7823 sec/batch\n",
      "Epoch 6/20  Iteration 1064/3560 Training loss: 1.6515 0.8003 sec/batch\n",
      "Epoch 6/20  Iteration 1065/3560 Training loss: 1.6515 0.8321 sec/batch\n",
      "Epoch 6/20  Iteration 1066/3560 Training loss: 1.6514 0.8063 sec/batch\n",
      "Epoch 6/20  Iteration 1067/3560 Training loss: 1.6512 0.7895 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 1068/3560 Training loss: 1.6512 0.8501 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3560 Training loss: 1.7265 0.7797 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3560 Training loss: 1.6812 0.8198 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3560 Training loss: 1.6598 0.8333 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3560 Training loss: 1.6538 0.8330 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3560 Training loss: 1.6460 0.7976 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3560 Training loss: 1.6360 0.8077 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3560 Training loss: 1.6350 0.7975 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3560 Training loss: 1.6322 0.8093 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3560 Training loss: 1.6343 0.8057 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3560 Training loss: 1.6332 0.8284 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3560 Training loss: 1.6304 0.7984 sec/batch\n",
      "Epoch 7/20  Iteration 1080/3560 Training loss: 1.6298 0.7959 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3560 Training loss: 1.6292 0.8424 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3560 Training loss: 1.6314 0.8242 sec/batch\n",
      "Epoch 7/20  Iteration 1083/3560 Training loss: 1.6299 0.8895 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3560 Training loss: 1.6277 0.7941 sec/batch\n",
      "Epoch 7/20  Iteration 1085/3560 Training loss: 1.6279 0.8078 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3560 Training loss: 1.6293 0.7792 sec/batch\n",
      "Epoch 7/20  Iteration 1087/3560 Training loss: 1.6289 0.8136 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3560 Training loss: 1.6296 0.8197 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3560 Training loss: 1.6281 0.8212 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3560 Training loss: 1.6284 0.8181 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3560 Training loss: 1.6271 0.7786 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3560 Training loss: 1.6267 0.7853 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3560 Training loss: 1.6263 0.8225 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3560 Training loss: 1.6248 0.8745 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3560 Training loss: 1.6236 0.7914 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3560 Training loss: 1.6240 0.8779 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3560 Training loss: 1.6244 0.7863 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3560 Training loss: 1.6247 0.7842 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3560 Training loss: 1.6243 0.7902 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3560 Training loss: 1.6233 0.8215 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3560 Training loss: 1.6238 0.8129 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3560 Training loss: 1.6241 0.8055 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3560 Training loss: 1.6237 0.7911 sec/batch\n",
      "Epoch 7/20  Iteration 1104/3560 Training loss: 1.6235 0.8033 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3560 Training loss: 1.6228 0.8434 sec/batch\n",
      "Epoch 7/20  Iteration 1106/3560 Training loss: 1.6215 0.8036 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3560 Training loss: 1.6200 0.8127 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3560 Training loss: 1.6194 0.8558 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3560 Training loss: 1.6186 0.7984 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3560 Training loss: 1.6187 0.7988 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3560 Training loss: 1.6181 0.8201 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3560 Training loss: 1.6173 0.8231 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3560 Training loss: 1.6172 0.7675 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3560 Training loss: 1.6161 0.7879 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3560 Training loss: 1.6158 0.8198 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3560 Training loss: 1.6153 0.7968 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3560 Training loss: 1.6148 0.8025 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3560 Training loss: 1.6154 0.8084 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3560 Training loss: 1.6149 0.8024 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3560 Training loss: 1.6159 0.7999 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3560 Training loss: 1.6157 0.8682 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3560 Training loss: 1.6156 0.8118 sec/batch\n",
      "Epoch 7/20  Iteration 1123/3560 Training loss: 1.6154 0.8000 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3560 Training loss: 1.6154 0.7883 sec/batch\n",
      "Epoch 7/20  Iteration 1125/3560 Training loss: 1.6157 0.8094 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3560 Training loss: 1.6153 1.0034 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3560 Training loss: 1.6146 1.0111 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3560 Training loss: 1.6150 0.8866 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3560 Training loss: 1.6149 0.8929 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3560 Training loss: 1.6157 0.8368 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3560 Training loss: 1.6160 0.7827 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3560 Training loss: 1.6161 0.7916 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3560 Training loss: 1.6161 0.8803 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3560 Training loss: 1.6162 0.8382 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3560 Training loss: 1.6164 0.7872 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3560 Training loss: 1.6159 0.8121 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3560 Training loss: 1.6158 0.7826 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3560 Training loss: 1.6156 0.7986 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3560 Training loss: 1.6159 0.8340 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3560 Training loss: 1.6161 0.8146 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3560 Training loss: 1.6166 0.7992 sec/batch\n",
      "Epoch 7/20  Iteration 1142/3560 Training loss: 1.6162 0.8196 sec/batch\n",
      "Epoch 7/20  Iteration 1143/3560 Training loss: 1.6160 0.7795 sec/batch\n",
      "Epoch 7/20  Iteration 1144/3560 Training loss: 1.6162 0.7978 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3560 Training loss: 1.6161 0.8481 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3560 Training loss: 1.6162 0.8955 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3560 Training loss: 1.6156 0.8929 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3560 Training loss: 1.6153 0.8079 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3560 Training loss: 1.6148 0.7865 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3560 Training loss: 1.6147 0.7730 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3560 Training loss: 1.6140 0.8036 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3560 Training loss: 1.6139 0.8124 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3560 Training loss: 1.6135 0.7880 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3560 Training loss: 1.6132 0.8024 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3560 Training loss: 1.6129 0.8129 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3560 Training loss: 1.6124 0.8070 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3560 Training loss: 1.6119 0.8498 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3560 Training loss: 1.6121 0.8582 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3560 Training loss: 1.6117 0.7847 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3560 Training loss: 1.6115 0.7981 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3560 Training loss: 1.6110 0.8083 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3560 Training loss: 1.6107 0.7823 sec/batch\n",
      "Epoch 7/20  Iteration 1163/3560 Training loss: 1.6102 0.8579 sec/batch\n",
      "Epoch 7/20  Iteration 1164/3560 Training loss: 1.6103 0.7978 sec/batch\n",
      "Epoch 7/20  Iteration 1165/3560 Training loss: 1.6102 0.8200 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3560 Training loss: 1.6098 0.7969 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3560 Training loss: 1.6093 0.8162 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3560 Training loss: 1.6088 0.7950 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3560 Training loss: 1.6087 0.8218 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3560 Training loss: 1.6085 0.8749 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3560 Training loss: 1.6083 0.8406 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3560 Training loss: 1.6081 0.8062 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3560 Training loss: 1.6079 0.7827 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3560 Training loss: 1.6078 0.7771 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3560 Training loss: 1.6077 0.8164 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3560 Training loss: 1.6075 0.8430 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3560 Training loss: 1.6075 0.8388 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3560 Training loss: 1.6074 0.8236 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3560 Training loss: 1.6072 0.8059 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3560 Training loss: 1.6070 0.7887 sec/batch\n",
      "Epoch 7/20  Iteration 1181/3560 Training loss: 1.6067 0.8058 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3560 Training loss: 1.6065 0.8966 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3560 Training loss: 1.6061 0.7925 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1184/3560 Training loss: 1.6056 0.8454 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3560 Training loss: 1.6055 0.7687 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3560 Training loss: 1.6054 0.7992 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3560 Training loss: 1.6051 0.8121 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3560 Training loss: 1.6050 0.8049 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3560 Training loss: 1.6048 0.8256 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3560 Training loss: 1.6044 0.7834 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3560 Training loss: 1.6041 0.7981 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3560 Training loss: 1.6041 0.8150 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3560 Training loss: 1.6039 0.8017 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3560 Training loss: 1.6034 0.8402 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3560 Training loss: 1.6034 0.8740 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3560 Training loss: 1.6033 0.7812 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3560 Training loss: 1.6031 0.7900 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3560 Training loss: 1.6028 0.7842 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3560 Training loss: 1.6025 0.8006 sec/batch\n",
      "Epoch 7/20  Iteration 1200/3560 Training loss: 1.6022 0.8343 sec/batch\n",
      "Epoch 7/20  Iteration 1201/3560 Training loss: 1.6022 0.7994 sec/batch\n",
      "Epoch 7/20  Iteration 1202/3560 Training loss: 1.6022 0.7839 sec/batch\n",
      "Epoch 7/20  Iteration 1203/3560 Training loss: 1.6021 0.8454 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3560 Training loss: 1.6020 0.8272 sec/batch\n",
      "Epoch 7/20  Iteration 1205/3560 Training loss: 1.6020 0.7893 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3560 Training loss: 1.6021 0.8283 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3560 Training loss: 1.6021 0.8654 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3560 Training loss: 1.6019 0.8280 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3560 Training loss: 1.6022 0.8051 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3560 Training loss: 1.6020 0.8171 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3560 Training loss: 1.6018 0.7833 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3560 Training loss: 1.6018 0.8724 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3560 Training loss: 1.6016 0.8159 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3560 Training loss: 1.6017 0.8046 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3560 Training loss: 1.6016 0.8075 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3560 Training loss: 1.6018 0.7831 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3560 Training loss: 1.6017 0.8265 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3560 Training loss: 1.6015 0.8305 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3560 Training loss: 1.6012 0.8590 sec/batch\n",
      "Epoch 7/20  Iteration 1220/3560 Training loss: 1.6011 0.8243 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3560 Training loss: 1.6010 0.8056 sec/batch\n",
      "Epoch 7/20  Iteration 1222/3560 Training loss: 1.6010 0.7733 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3560 Training loss: 1.6010 0.7844 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3560 Training loss: 1.6009 0.7965 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3560 Training loss: 1.6010 0.8550 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3560 Training loss: 1.6009 0.7886 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3560 Training loss: 1.6005 0.8035 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3560 Training loss: 1.6005 0.8090 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3560 Training loss: 1.6006 0.7829 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3560 Training loss: 1.6005 0.8277 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3560 Training loss: 1.6005 0.8235 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3560 Training loss: 1.6004 0.8633 sec/batch\n",
      "Epoch 7/20  Iteration 1233/3560 Training loss: 1.6003 0.8163 sec/batch\n",
      "Epoch 7/20  Iteration 1234/3560 Training loss: 1.6002 0.8134 sec/batch\n",
      "Epoch 7/20  Iteration 1235/3560 Training loss: 1.6002 0.7780 sec/batch\n",
      "Epoch 7/20  Iteration 1236/3560 Training loss: 1.6006 0.8180 sec/batch\n",
      "Epoch 7/20  Iteration 1237/3560 Training loss: 1.6004 0.8523 sec/batch\n",
      "Epoch 7/20  Iteration 1238/3560 Training loss: 1.6003 0.8236 sec/batch\n",
      "Epoch 7/20  Iteration 1239/3560 Training loss: 1.6001 0.7853 sec/batch\n",
      "Epoch 7/20  Iteration 1240/3560 Training loss: 1.5998 0.7889 sec/batch\n",
      "Epoch 7/20  Iteration 1241/3560 Training loss: 1.5999 0.8076 sec/batch\n",
      "Epoch 7/20  Iteration 1242/3560 Training loss: 1.5999 0.8086 sec/batch\n",
      "Epoch 7/20  Iteration 1243/3560 Training loss: 1.5999 0.8522 sec/batch\n",
      "Epoch 7/20  Iteration 1244/3560 Training loss: 1.5997 0.8253 sec/batch\n",
      "Epoch 7/20  Iteration 1245/3560 Training loss: 1.5995 0.8272 sec/batch\n",
      "Epoch 7/20  Iteration 1246/3560 Training loss: 1.5995 0.8003 sec/batch\n",
      "Epoch 8/20  Iteration 1247/3560 Training loss: 1.6660 0.7969 sec/batch\n",
      "Epoch 8/20  Iteration 1248/3560 Training loss: 1.6238 0.8019 sec/batch\n",
      "Epoch 8/20  Iteration 1249/3560 Training loss: 1.6080 0.8307 sec/batch\n",
      "Epoch 8/20  Iteration 1250/3560 Training loss: 1.6018 0.8001 sec/batch\n",
      "Epoch 8/20  Iteration 1251/3560 Training loss: 1.5950 0.8276 sec/batch\n",
      "Epoch 8/20  Iteration 1252/3560 Training loss: 1.5837 0.7907 sec/batch\n",
      "Epoch 8/20  Iteration 1253/3560 Training loss: 1.5837 0.7995 sec/batch\n",
      "Epoch 8/20  Iteration 1254/3560 Training loss: 1.5820 0.8465 sec/batch\n",
      "Epoch 8/20  Iteration 1255/3560 Training loss: 1.5838 0.8295 sec/batch\n",
      "Epoch 8/20  Iteration 1256/3560 Training loss: 1.5834 0.8097 sec/batch\n",
      "Epoch 8/20  Iteration 1257/3560 Training loss: 1.5796 0.8413 sec/batch\n",
      "Epoch 8/20  Iteration 1258/3560 Training loss: 1.5782 0.7725 sec/batch\n",
      "Epoch 8/20  Iteration 1259/3560 Training loss: 1.5786 0.8409 sec/batch\n",
      "Epoch 8/20  Iteration 1260/3560 Training loss: 1.5806 0.8084 sec/batch\n",
      "Epoch 8/20  Iteration 1261/3560 Training loss: 1.5796 0.7917 sec/batch\n",
      "Epoch 8/20  Iteration 1262/3560 Training loss: 1.5770 0.8572 sec/batch\n",
      "Epoch 8/20  Iteration 1263/3560 Training loss: 1.5773 0.8255 sec/batch\n",
      "Epoch 8/20  Iteration 1264/3560 Training loss: 1.5791 0.8000 sec/batch\n",
      "Epoch 8/20  Iteration 1265/3560 Training loss: 1.5793 0.8194 sec/batch\n",
      "Epoch 8/20  Iteration 1266/3560 Training loss: 1.5795 0.8484 sec/batch\n",
      "Epoch 8/20  Iteration 1267/3560 Training loss: 1.5787 0.8132 sec/batch\n",
      "Epoch 8/20  Iteration 1268/3560 Training loss: 1.5791 0.8118 sec/batch\n",
      "Epoch 8/20  Iteration 1269/3560 Training loss: 1.5788 0.8587 sec/batch\n",
      "Epoch 8/20  Iteration 1270/3560 Training loss: 1.5783 0.7918 sec/batch\n",
      "Epoch 8/20  Iteration 1271/3560 Training loss: 1.5782 0.8050 sec/batch\n",
      "Epoch 8/20  Iteration 1272/3560 Training loss: 1.5769 0.8176 sec/batch\n",
      "Epoch 8/20  Iteration 1273/3560 Training loss: 1.5758 0.7898 sec/batch\n",
      "Epoch 8/20  Iteration 1274/3560 Training loss: 1.5763 0.8453 sec/batch\n",
      "Epoch 8/20  Iteration 1275/3560 Training loss: 1.5772 0.8170 sec/batch\n",
      "Epoch 8/20  Iteration 1276/3560 Training loss: 1.5775 0.8229 sec/batch\n",
      "Epoch 8/20  Iteration 1277/3560 Training loss: 1.5771 0.7799 sec/batch\n",
      "Epoch 8/20  Iteration 1278/3560 Training loss: 1.5761 0.8369 sec/batch\n",
      "Epoch 8/20  Iteration 1279/3560 Training loss: 1.5763 0.8212 sec/batch\n",
      "Epoch 8/20  Iteration 1280/3560 Training loss: 1.5769 0.8167 sec/batch\n",
      "Epoch 8/20  Iteration 1281/3560 Training loss: 1.5767 0.8075 sec/batch\n",
      "Epoch 8/20  Iteration 1282/3560 Training loss: 1.5764 0.8449 sec/batch\n",
      "Epoch 8/20  Iteration 1283/3560 Training loss: 1.5759 0.7934 sec/batch\n",
      "Epoch 8/20  Iteration 1284/3560 Training loss: 1.5748 0.8159 sec/batch\n",
      "Epoch 8/20  Iteration 1285/3560 Training loss: 1.5733 0.7781 sec/batch\n",
      "Epoch 8/20  Iteration 1286/3560 Training loss: 1.5726 0.7970 sec/batch\n",
      "Epoch 8/20  Iteration 1287/3560 Training loss: 1.5721 0.8405 sec/batch\n",
      "Epoch 8/20  Iteration 1288/3560 Training loss: 1.5724 0.7896 sec/batch\n",
      "Epoch 8/20  Iteration 1289/3560 Training loss: 1.5720 0.8066 sec/batch\n",
      "Epoch 8/20  Iteration 1290/3560 Training loss: 1.5714 0.8677 sec/batch\n",
      "Epoch 8/20  Iteration 1291/3560 Training loss: 1.5713 0.8581 sec/batch\n",
      "Epoch 8/20  Iteration 1292/3560 Training loss: 1.5702 0.8704 sec/batch\n",
      "Epoch 8/20  Iteration 1293/3560 Training loss: 1.5701 0.8367 sec/batch\n",
      "Epoch 8/20  Iteration 1294/3560 Training loss: 1.5696 0.8494 sec/batch\n",
      "Epoch 8/20  Iteration 1295/3560 Training loss: 1.5693 0.8019 sec/batch\n",
      "Epoch 8/20  Iteration 1296/3560 Training loss: 1.5698 0.8418 sec/batch\n",
      "Epoch 8/20  Iteration 1297/3560 Training loss: 1.5693 0.7891 sec/batch\n",
      "Epoch 8/20  Iteration 1298/3560 Training loss: 1.5701 0.8128 sec/batch\n",
      "Epoch 8/20  Iteration 1299/3560 Training loss: 1.5701 0.7990 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1300/3560 Training loss: 1.5700 0.7830 sec/batch\n",
      "Epoch 8/20  Iteration 1301/3560 Training loss: 1.5696 0.7965 sec/batch\n",
      "Epoch 8/20  Iteration 1302/3560 Training loss: 1.5698 0.8039 sec/batch\n",
      "Epoch 8/20  Iteration 1303/3560 Training loss: 1.5704 0.8565 sec/batch\n",
      "Epoch 8/20  Iteration 1304/3560 Training loss: 1.5701 0.8071 sec/batch\n",
      "Epoch 8/20  Iteration 1305/3560 Training loss: 1.5696 0.8026 sec/batch\n",
      "Epoch 8/20  Iteration 1306/3560 Training loss: 1.5700 0.8569 sec/batch\n",
      "Epoch 8/20  Iteration 1307/3560 Training loss: 1.5700 0.7963 sec/batch\n",
      "Epoch 8/20  Iteration 1308/3560 Training loss: 1.5707 0.7689 sec/batch\n",
      "Epoch 8/20  Iteration 1309/3560 Training loss: 1.5713 0.8179 sec/batch\n",
      "Epoch 8/20  Iteration 1310/3560 Training loss: 1.5715 0.8042 sec/batch\n",
      "Epoch 8/20  Iteration 1311/3560 Training loss: 1.5715 0.8264 sec/batch\n",
      "Epoch 8/20  Iteration 1312/3560 Training loss: 1.5717 0.7963 sec/batch\n",
      "Epoch 8/20  Iteration 1313/3560 Training loss: 1.5718 0.7885 sec/batch\n",
      "Epoch 8/20  Iteration 1314/3560 Training loss: 1.5715 0.8004 sec/batch\n",
      "Epoch 8/20  Iteration 1315/3560 Training loss: 1.5715 0.8135 sec/batch\n",
      "Epoch 8/20  Iteration 1316/3560 Training loss: 1.5715 0.8186 sec/batch\n",
      "Epoch 8/20  Iteration 1317/3560 Training loss: 1.5719 0.8403 sec/batch\n",
      "Epoch 8/20  Iteration 1318/3560 Training loss: 1.5720 0.7862 sec/batch\n",
      "Epoch 8/20  Iteration 1319/3560 Training loss: 1.5724 0.8692 sec/batch\n",
      "Epoch 8/20  Iteration 1320/3560 Training loss: 1.5720 0.8063 sec/batch\n",
      "Epoch 8/20  Iteration 1321/3560 Training loss: 1.5719 0.7954 sec/batch\n",
      "Epoch 8/20  Iteration 1322/3560 Training loss: 1.5719 0.7771 sec/batch\n",
      "Epoch 8/20  Iteration 1323/3560 Training loss: 1.5718 0.7963 sec/batch\n",
      "Epoch 8/20  Iteration 1324/3560 Training loss: 1.5717 0.8150 sec/batch\n",
      "Epoch 8/20  Iteration 1325/3560 Training loss: 1.5712 0.8186 sec/batch\n",
      "Epoch 8/20  Iteration 1326/3560 Training loss: 1.5710 0.7865 sec/batch\n",
      "Epoch 8/20  Iteration 1327/3560 Training loss: 1.5704 0.7983 sec/batch\n",
      "Epoch 8/20  Iteration 1328/3560 Training loss: 1.5702 0.8386 sec/batch\n",
      "Epoch 8/20  Iteration 1329/3560 Training loss: 1.5697 0.8334 sec/batch\n",
      "Epoch 8/20  Iteration 1330/3560 Training loss: 1.5697 0.8137 sec/batch\n",
      "Epoch 8/20  Iteration 1331/3560 Training loss: 1.5692 0.8536 sec/batch\n",
      "Epoch 8/20  Iteration 1332/3560 Training loss: 1.5688 0.8201 sec/batch\n",
      "Epoch 8/20  Iteration 1333/3560 Training loss: 1.5685 0.8155 sec/batch\n",
      "Epoch 8/20  Iteration 1334/3560 Training loss: 1.5682 0.7820 sec/batch\n",
      "Epoch 8/20  Iteration 1335/3560 Training loss: 1.5677 0.8036 sec/batch\n",
      "Epoch 8/20  Iteration 1336/3560 Training loss: 1.5678 0.8004 sec/batch\n",
      "Epoch 8/20  Iteration 1337/3560 Training loss: 1.5675 0.7677 sec/batch\n",
      "Epoch 8/20  Iteration 1338/3560 Training loss: 1.5673 0.8341 sec/batch\n",
      "Epoch 8/20  Iteration 1339/3560 Training loss: 1.5667 0.7827 sec/batch\n",
      "Epoch 8/20  Iteration 1340/3560 Training loss: 1.5665 0.8027 sec/batch\n",
      "Epoch 8/20  Iteration 1341/3560 Training loss: 1.5661 0.8234 sec/batch\n",
      "Epoch 8/20  Iteration 1342/3560 Training loss: 1.5661 0.8216 sec/batch\n",
      "Epoch 8/20  Iteration 1343/3560 Training loss: 1.5660 0.8118 sec/batch\n",
      "Epoch 8/20  Iteration 1344/3560 Training loss: 1.5655 0.8899 sec/batch\n",
      "Epoch 8/20  Iteration 1345/3560 Training loss: 1.5652 0.8582 sec/batch\n",
      "Epoch 8/20  Iteration 1346/3560 Training loss: 1.5647 0.8194 sec/batch\n",
      "Epoch 8/20  Iteration 1347/3560 Training loss: 1.5647 0.8013 sec/batch\n",
      "Epoch 8/20  Iteration 1348/3560 Training loss: 1.5645 0.8087 sec/batch\n",
      "Epoch 8/20  Iteration 1349/3560 Training loss: 1.5642 0.8096 sec/batch\n",
      "Epoch 8/20  Iteration 1350/3560 Training loss: 1.5639 0.7718 sec/batch\n",
      "Epoch 8/20  Iteration 1351/3560 Training loss: 1.5637 0.8301 sec/batch\n",
      "Epoch 8/20  Iteration 1352/3560 Training loss: 1.5636 0.8120 sec/batch\n",
      "Epoch 8/20  Iteration 1353/3560 Training loss: 1.5633 0.8090 sec/batch\n",
      "Epoch 8/20  Iteration 1354/3560 Training loss: 1.5632 0.7976 sec/batch\n",
      "Epoch 8/20  Iteration 1355/3560 Training loss: 1.5632 0.8204 sec/batch\n",
      "Epoch 8/20  Iteration 1356/3560 Training loss: 1.5631 0.8282 sec/batch\n",
      "Epoch 8/20  Iteration 1357/3560 Training loss: 1.5629 0.8374 sec/batch\n",
      "Epoch 8/20  Iteration 1358/3560 Training loss: 1.5626 0.7826 sec/batch\n",
      "Epoch 8/20  Iteration 1359/3560 Training loss: 1.5625 0.8133 sec/batch\n",
      "Epoch 8/20  Iteration 1360/3560 Training loss: 1.5622 0.7937 sec/batch\n",
      "Epoch 8/20  Iteration 1361/3560 Training loss: 1.5620 0.7914 sec/batch\n",
      "Epoch 8/20  Iteration 1362/3560 Training loss: 1.5616 0.8053 sec/batch\n",
      "Epoch 8/20  Iteration 1363/3560 Training loss: 1.5615 0.7783 sec/batch\n",
      "Epoch 8/20  Iteration 1364/3560 Training loss: 1.5614 0.8072 sec/batch\n",
      "Epoch 8/20  Iteration 1365/3560 Training loss: 1.5613 0.7928 sec/batch\n",
      "Epoch 8/20  Iteration 1366/3560 Training loss: 1.5612 0.8194 sec/batch\n",
      "Epoch 8/20  Iteration 1367/3560 Training loss: 1.5611 0.7810 sec/batch\n",
      "Epoch 8/20  Iteration 1368/3560 Training loss: 1.5607 0.8037 sec/batch\n",
      "Epoch 8/20  Iteration 1369/3560 Training loss: 1.5602 0.8729 sec/batch\n",
      "Epoch 8/20  Iteration 1370/3560 Training loss: 1.5602 0.8469 sec/batch\n",
      "Epoch 8/20  Iteration 1371/3560 Training loss: 1.5600 0.7936 sec/batch\n",
      "Epoch 8/20  Iteration 1372/3560 Training loss: 1.5595 0.8033 sec/batch\n",
      "Epoch 8/20  Iteration 1373/3560 Training loss: 1.5595 0.8046 sec/batch\n",
      "Epoch 8/20  Iteration 1374/3560 Training loss: 1.5595 0.8035 sec/batch\n",
      "Epoch 8/20  Iteration 1375/3560 Training loss: 1.5594 0.8307 sec/batch\n",
      "Epoch 8/20  Iteration 1376/3560 Training loss: 1.5591 0.8354 sec/batch\n",
      "Epoch 8/20  Iteration 1377/3560 Training loss: 1.5587 0.8252 sec/batch\n",
      "Epoch 8/20  Iteration 1378/3560 Training loss: 1.5584 0.8521 sec/batch\n",
      "Epoch 8/20  Iteration 1379/3560 Training loss: 1.5584 0.8180 sec/batch\n",
      "Epoch 8/20  Iteration 1380/3560 Training loss: 1.5584 0.8166 sec/batch\n",
      "Epoch 8/20  Iteration 1381/3560 Training loss: 1.5584 0.7958 sec/batch\n",
      "Epoch 8/20  Iteration 1382/3560 Training loss: 1.5584 0.8790 sec/batch\n",
      "Epoch 8/20  Iteration 1383/3560 Training loss: 1.5585 0.8209 sec/batch\n",
      "Epoch 8/20  Iteration 1384/3560 Training loss: 1.5585 0.8120 sec/batch\n",
      "Epoch 8/20  Iteration 1385/3560 Training loss: 1.5585 0.7880 sec/batch\n",
      "Epoch 8/20  Iteration 1386/3560 Training loss: 1.5585 0.8239 sec/batch\n",
      "Epoch 8/20  Iteration 1387/3560 Training loss: 1.5587 0.7886 sec/batch\n",
      "Epoch 8/20  Iteration 1388/3560 Training loss: 1.5587 0.7867 sec/batch\n",
      "Epoch 8/20  Iteration 1389/3560 Training loss: 1.5586 0.7979 sec/batch\n",
      "Epoch 8/20  Iteration 1390/3560 Training loss: 1.5586 0.8066 sec/batch\n",
      "Epoch 8/20  Iteration 1391/3560 Training loss: 1.5584 0.8118 sec/batch\n",
      "Epoch 8/20  Iteration 1392/3560 Training loss: 1.5585 0.8017 sec/batch\n",
      "Epoch 8/20  Iteration 1393/3560 Training loss: 1.5585 0.8188 sec/batch\n",
      "Epoch 8/20  Iteration 1394/3560 Training loss: 1.5587 0.8359 sec/batch\n",
      "Epoch 8/20  Iteration 1395/3560 Training loss: 1.5588 0.7836 sec/batch\n",
      "Epoch 8/20  Iteration 1396/3560 Training loss: 1.5586 0.8316 sec/batch\n",
      "Epoch 8/20  Iteration 1397/3560 Training loss: 1.5582 0.8073 sec/batch\n",
      "Epoch 8/20  Iteration 1398/3560 Training loss: 1.5582 0.8499 sec/batch\n",
      "Epoch 8/20  Iteration 1399/3560 Training loss: 1.5581 0.8106 sec/batch\n",
      "Epoch 8/20  Iteration 1400/3560 Training loss: 1.5581 0.8061 sec/batch\n",
      "Epoch 8/20  Iteration 1401/3560 Training loss: 1.5580 0.8182 sec/batch\n",
      "Epoch 8/20  Iteration 1402/3560 Training loss: 1.5579 0.8550 sec/batch\n",
      "Epoch 8/20  Iteration 1403/3560 Training loss: 1.5579 0.9605 sec/batch\n",
      "Epoch 8/20  Iteration 1404/3560 Training loss: 1.5578 1.0757 sec/batch\n",
      "Epoch 8/20  Iteration 1405/3560 Training loss: 1.5575 0.9696 sec/batch\n",
      "Epoch 8/20  Iteration 1406/3560 Training loss: 1.5576 0.9519 sec/batch\n",
      "Epoch 8/20  Iteration 1407/3560 Training loss: 1.5577 0.9610 sec/batch\n",
      "Epoch 8/20  Iteration 1408/3560 Training loss: 1.5576 0.8778 sec/batch\n",
      "Epoch 8/20  Iteration 1409/3560 Training loss: 1.5575 0.8781 sec/batch\n",
      "Epoch 8/20  Iteration 1410/3560 Training loss: 1.5574 0.8881 sec/batch\n",
      "Epoch 8/20  Iteration 1411/3560 Training loss: 1.5573 0.9774 sec/batch\n",
      "Epoch 8/20  Iteration 1412/3560 Training loss: 1.5571 0.8907 sec/batch\n",
      "Epoch 8/20  Iteration 1413/3560 Training loss: 1.5572 1.0407 sec/batch\n",
      "Epoch 8/20  Iteration 1414/3560 Training loss: 1.5576 0.9248 sec/batch\n",
      "Epoch 8/20  Iteration 1415/3560 Training loss: 1.5575 0.9778 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1416/3560 Training loss: 1.5574 1.1922 sec/batch\n",
      "Epoch 8/20  Iteration 1417/3560 Training loss: 1.5573 0.9180 sec/batch\n",
      "Epoch 8/20  Iteration 1418/3560 Training loss: 1.5571 0.8914 sec/batch\n",
      "Epoch 8/20  Iteration 1419/3560 Training loss: 1.5572 0.8694 sec/batch\n",
      "Epoch 8/20  Iteration 1420/3560 Training loss: 1.5572 0.9599 sec/batch\n",
      "Epoch 8/20  Iteration 1421/3560 Training loss: 1.5573 1.1396 sec/batch\n",
      "Epoch 8/20  Iteration 1422/3560 Training loss: 1.5571 0.9696 sec/batch\n",
      "Epoch 8/20  Iteration 1423/3560 Training loss: 1.5570 1.2036 sec/batch\n",
      "Epoch 8/20  Iteration 1424/3560 Training loss: 1.5570 0.9314 sec/batch\n",
      "Epoch 9/20  Iteration 1425/3560 Training loss: 1.6390 0.9182 sec/batch\n",
      "Epoch 9/20  Iteration 1426/3560 Training loss: 1.5982 0.9152 sec/batch\n",
      "Epoch 9/20  Iteration 1427/3560 Training loss: 1.5794 0.8786 sec/batch\n",
      "Epoch 9/20  Iteration 1428/3560 Training loss: 1.5740 0.7766 sec/batch\n",
      "Epoch 9/20  Iteration 1429/3560 Training loss: 1.5639 0.8323 sec/batch\n",
      "Epoch 9/20  Iteration 1430/3560 Training loss: 1.5541 0.8139 sec/batch\n",
      "Epoch 9/20  Iteration 1431/3560 Training loss: 1.5552 0.8200 sec/batch\n",
      "Epoch 9/20  Iteration 1432/3560 Training loss: 1.5530 0.8381 sec/batch\n",
      "Epoch 9/20  Iteration 1433/3560 Training loss: 1.5542 0.8751 sec/batch\n",
      "Epoch 9/20  Iteration 1434/3560 Training loss: 1.5530 0.9755 sec/batch\n",
      "Epoch 9/20  Iteration 1435/3560 Training loss: 1.5485 1.2177 sec/batch\n",
      "Epoch 9/20  Iteration 1436/3560 Training loss: 1.5475 1.0921 sec/batch\n",
      "Epoch 9/20  Iteration 1437/3560 Training loss: 1.5470 1.0284 sec/batch\n",
      "Epoch 9/20  Iteration 1438/3560 Training loss: 1.5493 0.8877 sec/batch\n",
      "Epoch 9/20  Iteration 1439/3560 Training loss: 1.5481 0.8557 sec/batch\n",
      "Epoch 9/20  Iteration 1440/3560 Training loss: 1.5462 0.8851 sec/batch\n",
      "Epoch 9/20  Iteration 1441/3560 Training loss: 1.5466 1.0480 sec/batch\n",
      "Epoch 9/20  Iteration 1442/3560 Training loss: 1.5480 0.9820 sec/batch\n",
      "Epoch 9/20  Iteration 1443/3560 Training loss: 1.5477 1.1145 sec/batch\n",
      "Epoch 9/20  Iteration 1444/3560 Training loss: 1.5485 0.9952 sec/batch\n",
      "Epoch 9/20  Iteration 1445/3560 Training loss: 1.5475 0.9657 sec/batch\n",
      "Epoch 9/20  Iteration 1446/3560 Training loss: 1.5476 0.9138 sec/batch\n",
      "Epoch 9/20  Iteration 1447/3560 Training loss: 1.5466 0.8540 sec/batch\n",
      "Epoch 9/20  Iteration 1448/3560 Training loss: 1.5459 1.0946 sec/batch\n",
      "Epoch 9/20  Iteration 1449/3560 Training loss: 1.5455 0.9384 sec/batch\n",
      "Epoch 9/20  Iteration 1450/3560 Training loss: 1.5438 1.0981 sec/batch\n",
      "Epoch 9/20  Iteration 1451/3560 Training loss: 1.5424 1.0193 sec/batch\n",
      "Epoch 9/20  Iteration 1452/3560 Training loss: 1.5425 0.9640 sec/batch\n",
      "Epoch 9/20  Iteration 1453/3560 Training loss: 1.5430 0.9379 sec/batch\n",
      "Epoch 9/20  Iteration 1454/3560 Training loss: 1.5432 0.8518 sec/batch\n",
      "Epoch 9/20  Iteration 1455/3560 Training loss: 1.5428 0.9319 sec/batch\n",
      "Epoch 9/20  Iteration 1456/3560 Training loss: 1.5418 0.9159 sec/batch\n",
      "Epoch 9/20  Iteration 1457/3560 Training loss: 1.5423 0.9038 sec/batch\n",
      "Epoch 9/20  Iteration 1458/3560 Training loss: 1.5427 0.8754 sec/batch\n",
      "Epoch 9/20  Iteration 1459/3560 Training loss: 1.5423 0.8853 sec/batch\n",
      "Epoch 9/20  Iteration 1460/3560 Training loss: 1.5423 0.8401 sec/batch\n",
      "Epoch 9/20  Iteration 1461/3560 Training loss: 1.5417 1.0354 sec/batch\n",
      "Epoch 9/20  Iteration 1462/3560 Training loss: 1.5408 0.9858 sec/batch\n",
      "Epoch 9/20  Iteration 1463/3560 Training loss: 1.5394 1.7350 sec/batch\n",
      "Epoch 9/20  Iteration 1464/3560 Training loss: 1.5388 1.3732 sec/batch\n",
      "Epoch 9/20  Iteration 1465/3560 Training loss: 1.5383 1.4080 sec/batch\n",
      "Epoch 9/20  Iteration 1466/3560 Training loss: 1.5388 1.4319 sec/batch\n",
      "Epoch 9/20  Iteration 1467/3560 Training loss: 1.5381 0.9905 sec/batch\n",
      "Epoch 9/20  Iteration 1468/3560 Training loss: 1.5374 0.9489 sec/batch\n",
      "Epoch 9/20  Iteration 1469/3560 Training loss: 1.5375 0.9071 sec/batch\n",
      "Epoch 9/20  Iteration 1470/3560 Training loss: 1.5364 0.8713 sec/batch\n",
      "Epoch 9/20  Iteration 1471/3560 Training loss: 1.5360 0.8879 sec/batch\n",
      "Epoch 9/20  Iteration 1472/3560 Training loss: 1.5354 0.8301 sec/batch\n",
      "Epoch 9/20  Iteration 1473/3560 Training loss: 1.5351 0.9180 sec/batch\n",
      "Epoch 9/20  Iteration 1474/3560 Training loss: 1.5355 0.9105 sec/batch\n",
      "Epoch 9/20  Iteration 1475/3560 Training loss: 1.5349 0.8965 sec/batch\n",
      "Epoch 9/20  Iteration 1476/3560 Training loss: 1.5358 1.1776 sec/batch\n",
      "Epoch 9/20  Iteration 1477/3560 Training loss: 1.5358 1.2866 sec/batch\n",
      "Epoch 9/20  Iteration 1478/3560 Training loss: 1.5359 1.3853 sec/batch\n",
      "Epoch 9/20  Iteration 1479/3560 Training loss: 1.5356 1.4513 sec/batch\n",
      "Epoch 9/20  Iteration 1480/3560 Training loss: 1.5357 1.0780 sec/batch\n",
      "Epoch 9/20  Iteration 1481/3560 Training loss: 1.5362 0.9212 sec/batch\n",
      "Epoch 9/20  Iteration 1482/3560 Training loss: 1.5357 0.9425 sec/batch\n",
      "Epoch 9/20  Iteration 1483/3560 Training loss: 1.5351 0.8560 sec/batch\n",
      "Epoch 9/20  Iteration 1484/3560 Training loss: 1.5354 0.8420 sec/batch\n",
      "Epoch 9/20  Iteration 1485/3560 Training loss: 1.5353 0.8994 sec/batch\n",
      "Epoch 9/20  Iteration 1486/3560 Training loss: 1.5362 0.9395 sec/batch\n",
      "Epoch 9/20  Iteration 1487/3560 Training loss: 1.5366 1.3187 sec/batch\n",
      "Epoch 9/20  Iteration 1488/3560 Training loss: 1.5367 1.1038 sec/batch\n",
      "Epoch 9/20  Iteration 1489/3560 Training loss: 1.5365 1.2907 sec/batch\n",
      "Epoch 9/20  Iteration 1490/3560 Training loss: 1.5366 1.2917 sec/batch\n",
      "Epoch 9/20  Iteration 1491/3560 Training loss: 1.5368 0.9429 sec/batch\n",
      "Epoch 9/20  Iteration 1492/3560 Training loss: 1.5364 1.0508 sec/batch\n",
      "Epoch 9/20  Iteration 1493/3560 Training loss: 1.5364 0.9333 sec/batch\n",
      "Epoch 9/20  Iteration 1494/3560 Training loss: 1.5363 1.0164 sec/batch\n",
      "Epoch 9/20  Iteration 1495/3560 Training loss: 1.5367 0.8730 sec/batch\n",
      "Epoch 9/20  Iteration 1496/3560 Training loss: 1.5369 0.9238 sec/batch\n",
      "Epoch 9/20  Iteration 1497/3560 Training loss: 1.5374 0.9002 sec/batch\n",
      "Epoch 9/20  Iteration 1498/3560 Training loss: 1.5371 0.9248 sec/batch\n",
      "Epoch 9/20  Iteration 1499/3560 Training loss: 1.5370 0.9352 sec/batch\n",
      "Epoch 9/20  Iteration 1500/3560 Training loss: 1.5371 0.9250 sec/batch\n",
      "Epoch 9/20  Iteration 1501/3560 Training loss: 1.5371 1.0871 sec/batch\n",
      "Epoch 9/20  Iteration 1502/3560 Training loss: 1.5370 1.0577 sec/batch\n",
      "Epoch 9/20  Iteration 1503/3560 Training loss: 1.5364 1.3401 sec/batch\n",
      "Epoch 9/20  Iteration 1504/3560 Training loss: 1.5362 1.3957 sec/batch\n",
      "Epoch 9/20  Iteration 1505/3560 Training loss: 1.5356 0.8653 sec/batch\n",
      "Epoch 9/20  Iteration 1506/3560 Training loss: 1.5357 0.8782 sec/batch\n",
      "Epoch 9/20  Iteration 1507/3560 Training loss: 1.5351 0.8041 sec/batch\n",
      "Epoch 9/20  Iteration 1508/3560 Training loss: 1.5351 0.8340 sec/batch\n",
      "Epoch 9/20  Iteration 1509/3560 Training loss: 1.5348 0.8109 sec/batch\n",
      "Epoch 9/20  Iteration 1510/3560 Training loss: 1.5345 0.8436 sec/batch\n",
      "Epoch 9/20  Iteration 1511/3560 Training loss: 1.5342 0.8047 sec/batch\n",
      "Epoch 9/20  Iteration 1512/3560 Training loss: 1.5339 0.8108 sec/batch\n",
      "Epoch 9/20  Iteration 1513/3560 Training loss: 1.5335 0.8381 sec/batch\n",
      "Epoch 9/20  Iteration 1514/3560 Training loss: 1.5334 0.8710 sec/batch\n",
      "Epoch 9/20  Iteration 1515/3560 Training loss: 1.5331 0.7852 sec/batch\n",
      "Epoch 9/20  Iteration 1516/3560 Training loss: 1.5329 0.8087 sec/batch\n",
      "Epoch 9/20  Iteration 1517/3560 Training loss: 1.5324 0.8012 sec/batch\n",
      "Epoch 9/20  Iteration 1518/3560 Training loss: 1.5321 0.8655 sec/batch\n",
      "Epoch 9/20  Iteration 1519/3560 Training loss: 1.5318 0.7899 sec/batch\n",
      "Epoch 9/20  Iteration 1520/3560 Training loss: 1.5318 0.8370 sec/batch\n",
      "Epoch 9/20  Iteration 1521/3560 Training loss: 1.5319 0.8329 sec/batch\n",
      "Epoch 9/20  Iteration 1522/3560 Training loss: 1.5316 0.7982 sec/batch\n",
      "Epoch 9/20  Iteration 1523/3560 Training loss: 1.5312 0.8098 sec/batch\n",
      "Epoch 9/20  Iteration 1524/3560 Training loss: 1.5307 0.8296 sec/batch\n",
      "Epoch 9/20  Iteration 1525/3560 Training loss: 1.5307 0.8261 sec/batch\n",
      "Epoch 9/20  Iteration 1526/3560 Training loss: 1.5305 0.8401 sec/batch\n",
      "Epoch 9/20  Iteration 1527/3560 Training loss: 1.5302 0.8035 sec/batch\n",
      "Epoch 9/20  Iteration 1528/3560 Training loss: 1.5300 0.8142 sec/batch\n",
      "Epoch 9/20  Iteration 1529/3560 Training loss: 1.5299 0.8139 sec/batch\n",
      "Epoch 9/20  Iteration 1530/3560 Training loss: 1.5299 0.8221 sec/batch\n",
      "Epoch 9/20  Iteration 1531/3560 Training loss: 1.5297 0.8696 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1532/3560 Training loss: 1.5296 0.8398 sec/batch\n",
      "Epoch 9/20  Iteration 1533/3560 Training loss: 1.5295 0.7947 sec/batch\n",
      "Epoch 9/20  Iteration 1534/3560 Training loss: 1.5295 0.7951 sec/batch\n",
      "Epoch 9/20  Iteration 1535/3560 Training loss: 1.5293 0.8384 sec/batch\n",
      "Epoch 9/20  Iteration 1536/3560 Training loss: 1.5291 0.8033 sec/batch\n",
      "Epoch 9/20  Iteration 1537/3560 Training loss: 1.5290 0.8325 sec/batch\n",
      "Epoch 9/20  Iteration 1538/3560 Training loss: 1.5288 0.8499 sec/batch\n",
      "Epoch 9/20  Iteration 1539/3560 Training loss: 1.5283 0.8221 sec/batch\n",
      "Epoch 9/20  Iteration 1540/3560 Training loss: 1.5280 0.8643 sec/batch\n",
      "Epoch 9/20  Iteration 1541/3560 Training loss: 1.5278 0.8091 sec/batch\n",
      "Epoch 9/20  Iteration 1542/3560 Training loss: 1.5277 0.8277 sec/batch\n",
      "Epoch 9/20  Iteration 1543/3560 Training loss: 1.5276 0.8785 sec/batch\n",
      "Epoch 9/20  Iteration 1544/3560 Training loss: 1.5275 0.8242 sec/batch\n",
      "Epoch 9/20  Iteration 1545/3560 Training loss: 1.5274 0.8143 sec/batch\n",
      "Epoch 9/20  Iteration 1546/3560 Training loss: 1.5270 0.8488 sec/batch\n",
      "Epoch 9/20  Iteration 1547/3560 Training loss: 1.5265 0.8276 sec/batch\n",
      "Epoch 9/20  Iteration 1548/3560 Training loss: 1.5267 0.8214 sec/batch\n",
      "Epoch 9/20  Iteration 1549/3560 Training loss: 1.5265 0.8264 sec/batch\n",
      "Epoch 9/20  Iteration 1550/3560 Training loss: 1.5261 0.8377 sec/batch\n",
      "Epoch 9/20  Iteration 1551/3560 Training loss: 1.5261 0.8300 sec/batch\n",
      "Epoch 9/20  Iteration 1552/3560 Training loss: 1.5261 0.8412 sec/batch\n",
      "Epoch 9/20  Iteration 1553/3560 Training loss: 1.5259 0.8094 sec/batch\n",
      "Epoch 9/20  Iteration 1554/3560 Training loss: 1.5256 0.8145 sec/batch\n",
      "Epoch 9/20  Iteration 1555/3560 Training loss: 1.5252 0.8380 sec/batch\n",
      "Epoch 9/20  Iteration 1556/3560 Training loss: 1.5249 0.8768 sec/batch\n",
      "Epoch 9/20  Iteration 1557/3560 Training loss: 1.5249 0.7905 sec/batch\n",
      "Epoch 9/20  Iteration 1558/3560 Training loss: 1.5250 0.7833 sec/batch\n",
      "Epoch 9/20  Iteration 1559/3560 Training loss: 1.5249 0.8036 sec/batch\n",
      "Epoch 9/20  Iteration 1560/3560 Training loss: 1.5250 0.8852 sec/batch\n",
      "Epoch 9/20  Iteration 1561/3560 Training loss: 1.5251 0.8467 sec/batch\n",
      "Epoch 9/20  Iteration 1562/3560 Training loss: 1.5252 0.9213 sec/batch\n",
      "Epoch 9/20  Iteration 1563/3560 Training loss: 1.5252 0.8323 sec/batch\n",
      "Epoch 9/20  Iteration 1564/3560 Training loss: 1.5252 0.8469 sec/batch\n",
      "Epoch 9/20  Iteration 1565/3560 Training loss: 1.5255 0.8368 sec/batch\n",
      "Epoch 9/20  Iteration 1566/3560 Training loss: 1.5255 0.8216 sec/batch\n",
      "Epoch 9/20  Iteration 1567/3560 Training loss: 1.5253 0.8280 sec/batch\n",
      "Epoch 9/20  Iteration 1568/3560 Training loss: 1.5254 0.8683 sec/batch\n",
      "Epoch 9/20  Iteration 1569/3560 Training loss: 1.5253 0.7997 sec/batch\n",
      "Epoch 9/20  Iteration 1570/3560 Training loss: 1.5254 0.7913 sec/batch\n",
      "Epoch 9/20  Iteration 1571/3560 Training loss: 1.5254 0.8820 sec/batch\n",
      "Epoch 9/20  Iteration 1572/3560 Training loss: 1.5255 0.8253 sec/batch\n",
      "Epoch 9/20  Iteration 1573/3560 Training loss: 1.5256 0.8335 sec/batch\n",
      "Epoch 9/20  Iteration 1574/3560 Training loss: 1.5255 0.8226 sec/batch\n",
      "Epoch 9/20  Iteration 1575/3560 Training loss: 1.5253 0.7984 sec/batch\n",
      "Epoch 9/20  Iteration 1576/3560 Training loss: 1.5252 0.7986 sec/batch\n",
      "Epoch 9/20  Iteration 1577/3560 Training loss: 1.5253 0.8415 sec/batch\n",
      "Epoch 9/20  Iteration 1578/3560 Training loss: 1.5253 0.7801 sec/batch\n",
      "Epoch 9/20  Iteration 1579/3560 Training loss: 1.5253 0.8086 sec/batch\n",
      "Epoch 9/20  Iteration 1580/3560 Training loss: 1.5252 0.8455 sec/batch\n",
      "Epoch 9/20  Iteration 1581/3560 Training loss: 1.5252 0.8153 sec/batch\n",
      "Epoch 9/20  Iteration 1582/3560 Training loss: 1.5252 0.7951 sec/batch\n",
      "Epoch 9/20  Iteration 1583/3560 Training loss: 1.5249 0.8666 sec/batch\n",
      "Epoch 9/20  Iteration 1584/3560 Training loss: 1.5249 0.8201 sec/batch\n",
      "Epoch 9/20  Iteration 1585/3560 Training loss: 1.5251 0.7923 sec/batch\n",
      "Epoch 9/20  Iteration 1586/3560 Training loss: 1.5250 0.8280 sec/batch\n",
      "Epoch 9/20  Iteration 1587/3560 Training loss: 1.5249 0.8368 sec/batch\n",
      "Epoch 9/20  Iteration 1588/3560 Training loss: 1.5250 0.8295 sec/batch\n",
      "Epoch 9/20  Iteration 1589/3560 Training loss: 1.5249 0.8146 sec/batch\n",
      "Epoch 9/20  Iteration 1590/3560 Training loss: 1.5248 0.8275 sec/batch\n",
      "Epoch 9/20  Iteration 1591/3560 Training loss: 1.5249 0.8003 sec/batch\n",
      "Epoch 9/20  Iteration 1592/3560 Training loss: 1.5253 0.8595 sec/batch\n",
      "Epoch 9/20  Iteration 1593/3560 Training loss: 1.5252 0.8436 sec/batch\n",
      "Epoch 9/20  Iteration 1594/3560 Training loss: 1.5252 0.7977 sec/batch\n",
      "Epoch 9/20  Iteration 1595/3560 Training loss: 1.5250 0.7916 sec/batch\n",
      "Epoch 9/20  Iteration 1596/3560 Training loss: 1.5249 0.8025 sec/batch\n",
      "Epoch 9/20  Iteration 1597/3560 Training loss: 1.5249 0.8270 sec/batch\n",
      "Epoch 9/20  Iteration 1598/3560 Training loss: 1.5249 0.8119 sec/batch\n",
      "Epoch 9/20  Iteration 1599/3560 Training loss: 1.5250 0.8133 sec/batch\n",
      "Epoch 9/20  Iteration 1600/3560 Training loss: 1.5248 0.8100 sec/batch\n",
      "Epoch 9/20  Iteration 1601/3560 Training loss: 1.5247 0.9846 sec/batch\n",
      "Epoch 9/20  Iteration 1602/3560 Training loss: 1.5248 0.8000 sec/batch\n",
      "Epoch 10/20  Iteration 1603/3560 Training loss: 1.6005 1.0058 sec/batch\n",
      "Epoch 10/20  Iteration 1604/3560 Training loss: 1.5607 1.1188 sec/batch\n",
      "Epoch 10/20  Iteration 1605/3560 Training loss: 1.5453 0.8767 sec/batch\n",
      "Epoch 10/20  Iteration 1606/3560 Training loss: 1.5385 0.7896 sec/batch\n",
      "Epoch 10/20  Iteration 1607/3560 Training loss: 1.5321 0.8167 sec/batch\n",
      "Epoch 10/20  Iteration 1608/3560 Training loss: 1.5212 0.8141 sec/batch\n",
      "Epoch 10/20  Iteration 1609/3560 Training loss: 1.5204 0.8063 sec/batch\n",
      "Epoch 10/20  Iteration 1610/3560 Training loss: 1.5177 0.8484 sec/batch\n",
      "Epoch 10/20  Iteration 1611/3560 Training loss: 1.5188 0.8612 sec/batch\n",
      "Epoch 10/20  Iteration 1612/3560 Training loss: 1.5186 0.8222 sec/batch\n",
      "Epoch 10/20  Iteration 1613/3560 Training loss: 1.5147 0.8426 sec/batch\n",
      "Epoch 10/20  Iteration 1614/3560 Training loss: 1.5135 0.8283 sec/batch\n",
      "Epoch 10/20  Iteration 1615/3560 Training loss: 1.5128 0.8238 sec/batch\n",
      "Epoch 10/20  Iteration 1616/3560 Training loss: 1.5155 0.8932 sec/batch\n",
      "Epoch 10/20  Iteration 1617/3560 Training loss: 1.5143 0.8167 sec/batch\n",
      "Epoch 10/20  Iteration 1618/3560 Training loss: 1.5127 0.8077 sec/batch\n",
      "Epoch 10/20  Iteration 1619/3560 Training loss: 1.5133 0.8374 sec/batch\n",
      "Epoch 10/20  Iteration 1620/3560 Training loss: 1.5145 0.7867 sec/batch\n",
      "Epoch 10/20  Iteration 1621/3560 Training loss: 1.5148 0.8171 sec/batch\n",
      "Epoch 10/20  Iteration 1622/3560 Training loss: 1.5153 0.8215 sec/batch\n",
      "Epoch 10/20  Iteration 1623/3560 Training loss: 1.5142 0.7903 sec/batch\n",
      "Epoch 10/20  Iteration 1624/3560 Training loss: 1.5143 0.8223 sec/batch\n",
      "Epoch 10/20  Iteration 1625/3560 Training loss: 1.5135 0.8294 sec/batch\n",
      "Epoch 10/20  Iteration 1626/3560 Training loss: 1.5131 0.7981 sec/batch\n",
      "Epoch 10/20  Iteration 1627/3560 Training loss: 1.5129 0.7968 sec/batch\n",
      "Epoch 10/20  Iteration 1628/3560 Training loss: 1.5119 0.8953 sec/batch\n",
      "Epoch 10/20  Iteration 1629/3560 Training loss: 1.5107 0.8332 sec/batch\n",
      "Epoch 10/20  Iteration 1630/3560 Training loss: 1.5111 0.8172 sec/batch\n",
      "Epoch 10/20  Iteration 1631/3560 Training loss: 1.5116 0.8054 sec/batch\n",
      "Epoch 10/20  Iteration 1632/3560 Training loss: 1.5115 0.8018 sec/batch\n",
      "Epoch 10/20  Iteration 1633/3560 Training loss: 1.5112 0.7768 sec/batch\n",
      "Epoch 10/20  Iteration 1634/3560 Training loss: 1.5100 0.8096 sec/batch\n",
      "Epoch 10/20  Iteration 1635/3560 Training loss: 1.5104 0.7884 sec/batch\n",
      "Epoch 10/20  Iteration 1636/3560 Training loss: 1.5108 0.8282 sec/batch\n",
      "Epoch 10/20  Iteration 1637/3560 Training loss: 1.5107 0.8583 sec/batch\n",
      "Epoch 10/20  Iteration 1638/3560 Training loss: 1.5107 0.8327 sec/batch\n",
      "Epoch 10/20  Iteration 1639/3560 Training loss: 1.5098 0.8023 sec/batch\n",
      "Epoch 10/20  Iteration 1640/3560 Training loss: 1.5088 0.8351 sec/batch\n",
      "Epoch 10/20  Iteration 1641/3560 Training loss: 1.5077 0.8383 sec/batch\n",
      "Epoch 10/20  Iteration 1642/3560 Training loss: 1.5071 0.8145 sec/batch\n",
      "Epoch 10/20  Iteration 1643/3560 Training loss: 1.5067 0.7920 sec/batch\n",
      "Epoch 10/20  Iteration 1644/3560 Training loss: 1.5073 0.8043 sec/batch\n",
      "Epoch 10/20  Iteration 1645/3560 Training loss: 1.5067 0.8051 sec/batch\n",
      "Epoch 10/20  Iteration 1646/3560 Training loss: 1.5061 0.8089 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1647/3560 Training loss: 1.5064 0.8192 sec/batch\n",
      "Epoch 10/20  Iteration 1648/3560 Training loss: 1.5054 0.8007 sec/batch\n",
      "Epoch 10/20  Iteration 1649/3560 Training loss: 1.5052 0.8230 sec/batch\n",
      "Epoch 10/20  Iteration 1650/3560 Training loss: 1.5047 0.8590 sec/batch\n",
      "Epoch 10/20  Iteration 1651/3560 Training loss: 1.5044 0.8327 sec/batch\n",
      "Epoch 10/20  Iteration 1652/3560 Training loss: 1.5046 0.7951 sec/batch\n",
      "Epoch 10/20  Iteration 1653/3560 Training loss: 1.5042 0.8943 sec/batch\n",
      "Epoch 10/20  Iteration 1654/3560 Training loss: 1.5049 0.8126 sec/batch\n",
      "Epoch 10/20  Iteration 1655/3560 Training loss: 1.5048 0.8051 sec/batch\n",
      "Epoch 10/20  Iteration 1656/3560 Training loss: 1.5048 0.8029 sec/batch\n",
      "Epoch 10/20  Iteration 1657/3560 Training loss: 1.5047 0.8032 sec/batch\n",
      "Epoch 10/20  Iteration 1658/3560 Training loss: 1.5047 0.8394 sec/batch\n",
      "Epoch 10/20  Iteration 1659/3560 Training loss: 1.5052 0.8264 sec/batch\n",
      "Epoch 10/20  Iteration 1660/3560 Training loss: 1.5048 0.8003 sec/batch\n",
      "Epoch 10/20  Iteration 1661/3560 Training loss: 1.5044 0.7980 sec/batch\n",
      "Epoch 10/20  Iteration 1662/3560 Training loss: 1.5049 0.7826 sec/batch\n",
      "Epoch 10/20  Iteration 1663/3560 Training loss: 1.5049 0.8291 sec/batch\n",
      "Epoch 10/20  Iteration 1664/3560 Training loss: 1.5058 0.8460 sec/batch\n",
      "Epoch 10/20  Iteration 1665/3560 Training loss: 1.5062 0.8312 sec/batch\n",
      "Epoch 10/20  Iteration 1666/3560 Training loss: 1.5063 0.8246 sec/batch\n",
      "Epoch 10/20  Iteration 1667/3560 Training loss: 1.5063 0.8118 sec/batch\n",
      "Epoch 10/20  Iteration 1668/3560 Training loss: 1.5065 0.8244 sec/batch\n",
      "Epoch 10/20  Iteration 1669/3560 Training loss: 1.5065 0.8118 sec/batch\n",
      "Epoch 10/20  Iteration 1670/3560 Training loss: 1.5062 0.8232 sec/batch\n",
      "Epoch 10/20  Iteration 1671/3560 Training loss: 1.5062 0.8139 sec/batch\n",
      "Epoch 10/20  Iteration 1672/3560 Training loss: 1.5062 0.8282 sec/batch\n",
      "Epoch 10/20  Iteration 1673/3560 Training loss: 1.5066 0.8310 sec/batch\n",
      "Epoch 10/20  Iteration 1674/3560 Training loss: 1.5068 0.7806 sec/batch\n",
      "Epoch 10/20  Iteration 1675/3560 Training loss: 1.5072 0.7957 sec/batch\n",
      "Epoch 10/20  Iteration 1676/3560 Training loss: 1.5068 0.8049 sec/batch\n",
      "Epoch 10/20  Iteration 1677/3560 Training loss: 1.5067 0.8119 sec/batch\n",
      "Epoch 10/20  Iteration 1678/3560 Training loss: 1.5067 0.8622 sec/batch\n",
      "Epoch 10/20  Iteration 1679/3560 Training loss: 1.5066 0.8390 sec/batch\n",
      "Epoch 10/20  Iteration 1680/3560 Training loss: 1.5066 0.7734 sec/batch\n",
      "Epoch 10/20  Iteration 1681/3560 Training loss: 1.5062 0.8354 sec/batch\n",
      "Epoch 10/20  Iteration 1682/3560 Training loss: 1.5060 0.8249 sec/batch\n",
      "Epoch 10/20  Iteration 1683/3560 Training loss: 1.5054 0.8197 sec/batch\n",
      "Epoch 10/20  Iteration 1684/3560 Training loss: 1.5052 0.8172 sec/batch\n",
      "Epoch 10/20  Iteration 1685/3560 Training loss: 1.5047 0.8122 sec/batch\n",
      "Epoch 10/20  Iteration 1686/3560 Training loss: 1.5048 0.8220 sec/batch\n",
      "Epoch 10/20  Iteration 1687/3560 Training loss: 1.5045 0.8493 sec/batch\n",
      "Epoch 10/20  Iteration 1688/3560 Training loss: 1.5043 0.7849 sec/batch\n",
      "Epoch 10/20  Iteration 1689/3560 Training loss: 1.5041 0.8095 sec/batch\n",
      "Epoch 10/20  Iteration 1690/3560 Training loss: 1.5039 0.8945 sec/batch\n",
      "Epoch 10/20  Iteration 1691/3560 Training loss: 1.5034 0.7989 sec/batch\n",
      "Epoch 10/20  Iteration 1692/3560 Training loss: 1.5035 0.8230 sec/batch\n",
      "Epoch 10/20  Iteration 1693/3560 Training loss: 1.5031 0.8348 sec/batch\n",
      "Epoch 10/20  Iteration 1694/3560 Training loss: 1.5029 0.7978 sec/batch\n",
      "Epoch 10/20  Iteration 1695/3560 Training loss: 1.5025 0.8506 sec/batch\n",
      "Epoch 10/20  Iteration 1696/3560 Training loss: 1.5022 0.7970 sec/batch\n",
      "Epoch 10/20  Iteration 1697/3560 Training loss: 1.5019 0.8048 sec/batch\n",
      "Epoch 10/20  Iteration 1698/3560 Training loss: 1.5020 0.8591 sec/batch\n",
      "Epoch 10/20  Iteration 1699/3560 Training loss: 1.5019 0.8003 sec/batch\n",
      "Epoch 10/20  Iteration 1700/3560 Training loss: 1.5015 0.8055 sec/batch\n",
      "Epoch 10/20  Iteration 1701/3560 Training loss: 1.5012 0.8042 sec/batch\n",
      "Epoch 10/20  Iteration 1702/3560 Training loss: 1.5007 0.8606 sec/batch\n",
      "Epoch 10/20  Iteration 1703/3560 Training loss: 1.5007 0.8464 sec/batch\n",
      "Epoch 10/20  Iteration 1704/3560 Training loss: 1.5006 0.8150 sec/batch\n",
      "Epoch 10/20  Iteration 1705/3560 Training loss: 1.5004 0.8153 sec/batch\n",
      "Epoch 10/20  Iteration 1706/3560 Training loss: 1.5003 0.8127 sec/batch\n",
      "Epoch 10/20  Iteration 1707/3560 Training loss: 1.5001 0.8219 sec/batch\n",
      "Epoch 10/20  Iteration 1708/3560 Training loss: 1.4999 0.8658 sec/batch\n",
      "Epoch 10/20  Iteration 1709/3560 Training loss: 1.4997 0.8606 sec/batch\n",
      "Epoch 10/20  Iteration 1710/3560 Training loss: 1.4996 0.8504 sec/batch\n",
      "Epoch 10/20  Iteration 1711/3560 Training loss: 1.4996 0.8433 sec/batch\n",
      "Epoch 10/20  Iteration 1712/3560 Training loss: 1.4995 0.8493 sec/batch\n",
      "Epoch 10/20  Iteration 1713/3560 Training loss: 1.4993 0.7901 sec/batch\n",
      "Epoch 10/20  Iteration 1714/3560 Training loss: 1.4992 0.8212 sec/batch\n",
      "Epoch 10/20  Iteration 1715/3560 Training loss: 1.4990 0.8556 sec/batch\n",
      "Epoch 10/20  Iteration 1716/3560 Training loss: 1.4988 0.8221 sec/batch\n",
      "Epoch 10/20  Iteration 1717/3560 Training loss: 1.4985 0.8215 sec/batch\n",
      "Epoch 10/20  Iteration 1718/3560 Training loss: 1.4981 0.8655 sec/batch\n",
      "Epoch 10/20  Iteration 1719/3560 Training loss: 1.4980 0.7982 sec/batch\n",
      "Epoch 10/20  Iteration 1720/3560 Training loss: 1.4980 0.7781 sec/batch\n",
      "Epoch 10/20  Iteration 1721/3560 Training loss: 1.4979 0.8182 sec/batch\n",
      "Epoch 10/20  Iteration 1722/3560 Training loss: 1.4978 0.8305 sec/batch\n",
      "Epoch 10/20  Iteration 1723/3560 Training loss: 1.4977 0.8264 sec/batch\n",
      "Epoch 10/20  Iteration 1724/3560 Training loss: 1.4974 0.8346 sec/batch\n",
      "Epoch 10/20  Iteration 1725/3560 Training loss: 1.4970 0.8212 sec/batch\n",
      "Epoch 10/20  Iteration 1726/3560 Training loss: 1.4971 0.8079 sec/batch\n",
      "Epoch 10/20  Iteration 1727/3560 Training loss: 1.4970 0.8685 sec/batch\n",
      "Epoch 10/20  Iteration 1728/3560 Training loss: 1.4966 0.8129 sec/batch\n",
      "Epoch 10/20  Iteration 1729/3560 Training loss: 1.4967 0.8307 sec/batch\n",
      "Epoch 10/20  Iteration 1730/3560 Training loss: 1.4968 0.8390 sec/batch\n",
      "Epoch 10/20  Iteration 1731/3560 Training loss: 1.4966 0.8241 sec/batch\n",
      "Epoch 10/20  Iteration 1732/3560 Training loss: 1.4963 0.8362 sec/batch\n",
      "Epoch 10/20  Iteration 1733/3560 Training loss: 1.4958 0.8059 sec/batch\n",
      "Epoch 10/20  Iteration 1734/3560 Training loss: 1.4956 0.7938 sec/batch\n",
      "Epoch 10/20  Iteration 1735/3560 Training loss: 1.4956 0.8235 sec/batch\n",
      "Epoch 10/20  Iteration 1736/3560 Training loss: 1.4956 0.8117 sec/batch\n",
      "Epoch 10/20  Iteration 1737/3560 Training loss: 1.4955 0.8183 sec/batch\n",
      "Epoch 10/20  Iteration 1738/3560 Training loss: 1.4955 0.8162 sec/batch\n",
      "Epoch 10/20  Iteration 1739/3560 Training loss: 1.4957 0.8830 sec/batch\n",
      "Epoch 10/20  Iteration 1740/3560 Training loss: 1.4958 0.8238 sec/batch\n",
      "Epoch 10/20  Iteration 1741/3560 Training loss: 1.4958 0.8533 sec/batch\n",
      "Epoch 10/20  Iteration 1742/3560 Training loss: 1.4958 0.8062 sec/batch\n",
      "Epoch 10/20  Iteration 1743/3560 Training loss: 1.4961 0.7774 sec/batch\n",
      "Epoch 10/20  Iteration 1744/3560 Training loss: 1.4960 0.8184 sec/batch\n",
      "Epoch 10/20  Iteration 1745/3560 Training loss: 1.4958 0.8214 sec/batch\n",
      "Epoch 10/20  Iteration 1746/3560 Training loss: 1.4960 0.8338 sec/batch\n",
      "Epoch 10/20  Iteration 1747/3560 Training loss: 1.4958 0.8197 sec/batch\n",
      "Epoch 10/20  Iteration 1748/3560 Training loss: 1.4959 0.8031 sec/batch\n",
      "Epoch 10/20  Iteration 1749/3560 Training loss: 1.4960 0.8500 sec/batch\n",
      "Epoch 10/20  Iteration 1750/3560 Training loss: 1.4962 0.8681 sec/batch\n",
      "Epoch 10/20  Iteration 1751/3560 Training loss: 1.4962 0.8472 sec/batch\n",
      "Epoch 10/20  Iteration 1752/3560 Training loss: 1.4961 0.8311 sec/batch\n",
      "Epoch 10/20  Iteration 1753/3560 Training loss: 1.4959 0.7998 sec/batch\n",
      "Epoch 10/20  Iteration 1754/3560 Training loss: 1.4958 0.8313 sec/batch\n",
      "Epoch 10/20  Iteration 1755/3560 Training loss: 1.4958 0.7927 sec/batch\n",
      "Epoch 10/20  Iteration 1756/3560 Training loss: 1.4959 0.8345 sec/batch\n",
      "Epoch 10/20  Iteration 1757/3560 Training loss: 1.4958 0.8231 sec/batch\n",
      "Epoch 10/20  Iteration 1758/3560 Training loss: 1.4958 0.8357 sec/batch\n",
      "Epoch 10/20  Iteration 1759/3560 Training loss: 1.4958 0.8095 sec/batch\n",
      "Epoch 10/20  Iteration 1760/3560 Training loss: 1.4958 0.7978 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1761/3560 Training loss: 1.4956 0.8078 sec/batch\n",
      "Epoch 10/20  Iteration 1762/3560 Training loss: 1.4957 0.8089 sec/batch\n",
      "Epoch 10/20  Iteration 1763/3560 Training loss: 1.4959 0.8397 sec/batch\n",
      "Epoch 10/20  Iteration 1764/3560 Training loss: 1.4958 0.8927 sec/batch\n",
      "Epoch 10/20  Iteration 1765/3560 Training loss: 1.4958 0.8221 sec/batch\n",
      "Epoch 10/20  Iteration 1766/3560 Training loss: 1.4958 0.8042 sec/batch\n",
      "Epoch 10/20  Iteration 1767/3560 Training loss: 1.4958 0.8294 sec/batch\n",
      "Epoch 10/20  Iteration 1768/3560 Training loss: 1.4957 0.8325 sec/batch\n",
      "Epoch 10/20  Iteration 1769/3560 Training loss: 1.4958 0.8184 sec/batch\n",
      "Epoch 10/20  Iteration 1770/3560 Training loss: 1.4962 0.8327 sec/batch\n",
      "Epoch 10/20  Iteration 1771/3560 Training loss: 1.4961 0.8110 sec/batch\n",
      "Epoch 10/20  Iteration 1772/3560 Training loss: 1.4961 0.7813 sec/batch\n",
      "Epoch 10/20  Iteration 1773/3560 Training loss: 1.4960 0.8378 sec/batch\n",
      "Epoch 10/20  Iteration 1774/3560 Training loss: 1.4958 0.8539 sec/batch\n",
      "Epoch 10/20  Iteration 1775/3560 Training loss: 1.4959 0.8309 sec/batch\n",
      "Epoch 10/20  Iteration 1776/3560 Training loss: 1.4960 0.8909 sec/batch\n",
      "Epoch 10/20  Iteration 1777/3560 Training loss: 1.4961 0.7975 sec/batch\n",
      "Epoch 10/20  Iteration 1778/3560 Training loss: 1.4959 0.8189 sec/batch\n",
      "Epoch 10/20  Iteration 1779/3560 Training loss: 1.4957 0.8269 sec/batch\n",
      "Epoch 10/20  Iteration 1780/3560 Training loss: 1.4958 0.8500 sec/batch\n",
      "Epoch 11/20  Iteration 1781/3560 Training loss: 1.5826 0.8027 sec/batch\n",
      "Epoch 11/20  Iteration 1782/3560 Training loss: 1.5382 0.8021 sec/batch\n",
      "Epoch 11/20  Iteration 1783/3560 Training loss: 1.5238 0.8048 sec/batch\n",
      "Epoch 11/20  Iteration 1784/3560 Training loss: 1.5169 0.8172 sec/batch\n",
      "Epoch 11/20  Iteration 1785/3560 Training loss: 1.5084 0.8328 sec/batch\n",
      "Epoch 11/20  Iteration 1786/3560 Training loss: 1.4982 0.8161 sec/batch\n",
      "Epoch 11/20  Iteration 1787/3560 Training loss: 1.4983 0.7861 sec/batch\n",
      "Epoch 11/20  Iteration 1788/3560 Training loss: 1.4956 0.8232 sec/batch\n",
      "Epoch 11/20  Iteration 1789/3560 Training loss: 1.4961 0.8498 sec/batch\n",
      "Epoch 11/20  Iteration 1790/3560 Training loss: 1.4956 0.7997 sec/batch\n",
      "Epoch 11/20  Iteration 1791/3560 Training loss: 1.4920 0.8318 sec/batch\n",
      "Epoch 11/20  Iteration 1792/3560 Training loss: 1.4914 0.8242 sec/batch\n",
      "Epoch 11/20  Iteration 1793/3560 Training loss: 1.4907 0.7917 sec/batch\n",
      "Epoch 11/20  Iteration 1794/3560 Training loss: 1.4920 0.8244 sec/batch\n",
      "Epoch 11/20  Iteration 1795/3560 Training loss: 1.4908 0.8213 sec/batch\n",
      "Epoch 11/20  Iteration 1796/3560 Training loss: 1.4887 0.8135 sec/batch\n",
      "Epoch 11/20  Iteration 1797/3560 Training loss: 1.4885 0.8370 sec/batch\n",
      "Epoch 11/20  Iteration 1798/3560 Training loss: 1.4894 0.8044 sec/batch\n",
      "Epoch 11/20  Iteration 1799/3560 Training loss: 1.4891 0.9164 sec/batch\n",
      "Epoch 11/20  Iteration 1800/3560 Training loss: 1.4897 0.8695 sec/batch\n",
      "Epoch 11/20  Iteration 1801/3560 Training loss: 1.4888 0.8852 sec/batch\n",
      "Epoch 11/20  Iteration 1802/3560 Training loss: 1.4893 0.8288 sec/batch\n",
      "Epoch 11/20  Iteration 1803/3560 Training loss: 1.4884 0.8227 sec/batch\n",
      "Epoch 11/20  Iteration 1804/3560 Training loss: 1.4882 0.8565 sec/batch\n",
      "Epoch 11/20  Iteration 1805/3560 Training loss: 1.4882 0.8213 sec/batch\n",
      "Epoch 11/20  Iteration 1806/3560 Training loss: 1.4869 0.8214 sec/batch\n",
      "Epoch 11/20  Iteration 1807/3560 Training loss: 1.4858 0.7942 sec/batch\n",
      "Epoch 11/20  Iteration 1808/3560 Training loss: 1.4862 0.8008 sec/batch\n",
      "Epoch 11/20  Iteration 1809/3560 Training loss: 1.4865 0.8053 sec/batch\n",
      "Epoch 11/20  Iteration 1810/3560 Training loss: 1.4862 0.8545 sec/batch\n",
      "Epoch 11/20  Iteration 1811/3560 Training loss: 1.4858 0.8335 sec/batch\n",
      "Epoch 11/20  Iteration 1812/3560 Training loss: 1.4847 0.8107 sec/batch\n",
      "Epoch 11/20  Iteration 1813/3560 Training loss: 1.4852 0.8593 sec/batch\n",
      "Epoch 11/20  Iteration 1814/3560 Training loss: 1.4855 0.8288 sec/batch\n",
      "Epoch 11/20  Iteration 1815/3560 Training loss: 1.4855 0.8485 sec/batch\n",
      "Epoch 11/20  Iteration 1816/3560 Training loss: 1.4855 0.8256 sec/batch\n",
      "Epoch 11/20  Iteration 1817/3560 Training loss: 1.4848 0.8151 sec/batch\n",
      "Epoch 11/20  Iteration 1818/3560 Training loss: 1.4838 0.8251 sec/batch\n",
      "Epoch 11/20  Iteration 1819/3560 Training loss: 1.4823 0.8275 sec/batch\n",
      "Epoch 11/20  Iteration 1820/3560 Training loss: 1.4820 0.8107 sec/batch\n",
      "Epoch 11/20  Iteration 1821/3560 Training loss: 1.4815 0.8216 sec/batch\n",
      "Epoch 11/20  Iteration 1822/3560 Training loss: 1.4821 0.8468 sec/batch\n",
      "Epoch 11/20  Iteration 1823/3560 Training loss: 1.4816 0.7863 sec/batch\n",
      "Epoch 11/20  Iteration 1824/3560 Training loss: 1.4811 0.7889 sec/batch\n",
      "Epoch 11/20  Iteration 1825/3560 Training loss: 1.4811 0.8733 sec/batch\n",
      "Epoch 11/20  Iteration 1826/3560 Training loss: 1.4802 0.8029 sec/batch\n",
      "Epoch 11/20  Iteration 1827/3560 Training loss: 1.4798 0.8453 sec/batch\n",
      "Epoch 11/20  Iteration 1828/3560 Training loss: 1.4794 0.8145 sec/batch\n",
      "Epoch 11/20  Iteration 1829/3560 Training loss: 1.4791 0.8213 sec/batch\n",
      "Epoch 11/20  Iteration 1830/3560 Training loss: 1.4795 0.8046 sec/batch\n",
      "Epoch 11/20  Iteration 1831/3560 Training loss: 1.4789 0.8155 sec/batch\n",
      "Epoch 11/20  Iteration 1832/3560 Training loss: 1.4799 0.8138 sec/batch\n",
      "Epoch 11/20  Iteration 1833/3560 Training loss: 1.4798 0.8244 sec/batch\n",
      "Epoch 11/20  Iteration 1834/3560 Training loss: 1.4799 0.8587 sec/batch\n",
      "Epoch 11/20  Iteration 1835/3560 Training loss: 1.4796 0.8145 sec/batch\n",
      "Epoch 11/20  Iteration 1836/3560 Training loss: 1.4798 0.8410 sec/batch\n",
      "Epoch 11/20  Iteration 1837/3560 Training loss: 1.4801 0.8148 sec/batch\n",
      "Epoch 11/20  Iteration 1838/3560 Training loss: 1.4798 0.8815 sec/batch\n",
      "Epoch 11/20  Iteration 1839/3560 Training loss: 1.4793 0.7833 sec/batch\n",
      "Epoch 11/20  Iteration 1840/3560 Training loss: 1.4797 0.8740 sec/batch\n",
      "Epoch 11/20  Iteration 1841/3560 Training loss: 1.4796 0.8149 sec/batch\n",
      "Epoch 11/20  Iteration 1842/3560 Training loss: 1.4804 0.8840 sec/batch\n",
      "Epoch 11/20  Iteration 1843/3560 Training loss: 1.4809 0.8504 sec/batch\n",
      "Epoch 11/20  Iteration 1844/3560 Training loss: 1.4810 0.8699 sec/batch\n",
      "Epoch 11/20  Iteration 1845/3560 Training loss: 1.4811 0.8982 sec/batch\n",
      "Epoch 11/20  Iteration 1846/3560 Training loss: 1.4812 0.8357 sec/batch\n",
      "Epoch 11/20  Iteration 1847/3560 Training loss: 1.4814 0.8327 sec/batch\n",
      "Epoch 11/20  Iteration 1848/3560 Training loss: 1.4810 0.8132 sec/batch\n",
      "Epoch 11/20  Iteration 1849/3560 Training loss: 1.4810 0.8737 sec/batch\n",
      "Epoch 11/20  Iteration 1850/3560 Training loss: 1.4809 0.8223 sec/batch\n",
      "Epoch 11/20  Iteration 1851/3560 Training loss: 1.4813 0.7911 sec/batch\n",
      "Epoch 11/20  Iteration 1852/3560 Training loss: 1.4815 0.8591 sec/batch\n",
      "Epoch 11/20  Iteration 1853/3560 Training loss: 1.4820 0.8254 sec/batch\n",
      "Epoch 11/20  Iteration 1854/3560 Training loss: 1.4817 0.7843 sec/batch\n",
      "Epoch 11/20  Iteration 1855/3560 Training loss: 1.4816 0.8101 sec/batch\n",
      "Epoch 11/20  Iteration 1856/3560 Training loss: 1.4817 0.8182 sec/batch\n",
      "Epoch 11/20  Iteration 1857/3560 Training loss: 1.4815 0.8267 sec/batch\n",
      "Epoch 11/20  Iteration 1858/3560 Training loss: 1.4816 0.8167 sec/batch\n",
      "Epoch 11/20  Iteration 1859/3560 Training loss: 1.4809 0.8376 sec/batch\n",
      "Epoch 11/20  Iteration 1860/3560 Training loss: 1.4807 0.8294 sec/batch\n",
      "Epoch 11/20  Iteration 1861/3560 Training loss: 1.4802 0.8131 sec/batch\n",
      "Epoch 11/20  Iteration 1862/3560 Training loss: 1.4801 0.8362 sec/batch\n",
      "Epoch 11/20  Iteration 1863/3560 Training loss: 1.4797 0.8308 sec/batch\n",
      "Epoch 11/20  Iteration 1864/3560 Training loss: 1.4797 0.7994 sec/batch\n",
      "Epoch 11/20  Iteration 1865/3560 Training loss: 1.4794 0.8692 sec/batch\n",
      "Epoch 11/20  Iteration 1866/3560 Training loss: 1.4790 0.8113 sec/batch\n",
      "Epoch 11/20  Iteration 1867/3560 Training loss: 1.4787 0.8310 sec/batch\n",
      "Epoch 11/20  Iteration 1868/3560 Training loss: 1.4786 0.7797 sec/batch\n",
      "Epoch 11/20  Iteration 1869/3560 Training loss: 1.4782 0.8216 sec/batch\n",
      "Epoch 11/20  Iteration 1870/3560 Training loss: 1.4783 0.8117 sec/batch\n",
      "Epoch 11/20  Iteration 1871/3560 Training loss: 1.4780 0.7947 sec/batch\n",
      "Epoch 11/20  Iteration 1872/3560 Training loss: 1.4779 0.8402 sec/batch\n",
      "Epoch 11/20  Iteration 1873/3560 Training loss: 1.4775 0.8084 sec/batch\n",
      "Epoch 11/20  Iteration 1874/3560 Training loss: 1.4772 0.8619 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1875/3560 Training loss: 1.4769 0.8131 sec/batch\n",
      "Epoch 11/20  Iteration 1876/3560 Training loss: 1.4769 0.8003 sec/batch\n",
      "Epoch 11/20  Iteration 1877/3560 Training loss: 1.4768 0.8191 sec/batch\n",
      "Epoch 11/20  Iteration 1878/3560 Training loss: 1.4764 0.8449 sec/batch\n",
      "Epoch 11/20  Iteration 1879/3560 Training loss: 1.4758 0.7979 sec/batch\n",
      "Epoch 11/20  Iteration 1880/3560 Training loss: 1.4754 0.8256 sec/batch\n",
      "Epoch 11/20  Iteration 1881/3560 Training loss: 1.4754 0.7954 sec/batch\n",
      "Epoch 11/20  Iteration 1882/3560 Training loss: 1.4753 0.8501 sec/batch\n",
      "Epoch 11/20  Iteration 1883/3560 Training loss: 1.4751 0.8170 sec/batch\n",
      "Epoch 11/20  Iteration 1884/3560 Training loss: 1.4750 0.8271 sec/batch\n",
      "Epoch 11/20  Iteration 1885/3560 Training loss: 1.4749 0.8200 sec/batch\n",
      "Epoch 11/20  Iteration 1886/3560 Training loss: 1.4747 0.8254 sec/batch\n",
      "Epoch 11/20  Iteration 1887/3560 Training loss: 1.4746 0.8282 sec/batch\n",
      "Epoch 11/20  Iteration 1888/3560 Training loss: 1.4745 0.8021 sec/batch\n",
      "Epoch 11/20  Iteration 1889/3560 Training loss: 1.4745 0.8598 sec/batch\n",
      "Epoch 11/20  Iteration 1890/3560 Training loss: 1.4745 0.8307 sec/batch\n",
      "Epoch 11/20  Iteration 1891/3560 Training loss: 1.4743 0.8253 sec/batch\n",
      "Epoch 11/20  Iteration 1892/3560 Training loss: 1.4742 0.7715 sec/batch\n",
      "Epoch 11/20  Iteration 1893/3560 Training loss: 1.4740 0.7925 sec/batch\n",
      "Epoch 11/20  Iteration 1894/3560 Training loss: 1.4738 0.8149 sec/batch\n",
      "Epoch 11/20  Iteration 1895/3560 Training loss: 1.4736 0.8224 sec/batch\n",
      "Epoch 11/20  Iteration 1896/3560 Training loss: 1.4732 0.8242 sec/batch\n",
      "Epoch 11/20  Iteration 1897/3560 Training loss: 1.4731 0.8189 sec/batch\n",
      "Epoch 11/20  Iteration 1898/3560 Training loss: 1.4731 0.8354 sec/batch\n",
      "Epoch 11/20  Iteration 1899/3560 Training loss: 1.4729 0.8341 sec/batch\n",
      "Epoch 11/20  Iteration 1900/3560 Training loss: 1.4729 0.7983 sec/batch\n",
      "Epoch 11/20  Iteration 1901/3560 Training loss: 1.4729 0.8114 sec/batch\n",
      "Epoch 11/20  Iteration 1902/3560 Training loss: 1.4725 0.8374 sec/batch\n",
      "Epoch 11/20  Iteration 1903/3560 Training loss: 1.4721 0.8254 sec/batch\n",
      "Epoch 11/20  Iteration 1904/3560 Training loss: 1.4721 0.8211 sec/batch\n",
      "Epoch 11/20  Iteration 1905/3560 Training loss: 1.4720 0.8269 sec/batch\n",
      "Epoch 11/20  Iteration 1906/3560 Training loss: 1.4716 0.7937 sec/batch\n",
      "Epoch 11/20  Iteration 1907/3560 Training loss: 1.4717 0.8089 sec/batch\n",
      "Epoch 11/20  Iteration 1908/3560 Training loss: 1.4716 0.8236 sec/batch\n",
      "Epoch 11/20  Iteration 1909/3560 Training loss: 1.4715 0.7954 sec/batch\n",
      "Epoch 11/20  Iteration 1910/3560 Training loss: 1.4711 0.8209 sec/batch\n",
      "Epoch 11/20  Iteration 1911/3560 Training loss: 1.4708 0.8219 sec/batch\n",
      "Epoch 11/20  Iteration 1912/3560 Training loss: 1.4706 0.8641 sec/batch\n",
      "Epoch 11/20  Iteration 1913/3560 Training loss: 1.4706 0.8107 sec/batch\n",
      "Epoch 11/20  Iteration 1914/3560 Training loss: 1.4706 0.8366 sec/batch\n",
      "Epoch 11/20  Iteration 1915/3560 Training loss: 1.4706 0.8351 sec/batch\n",
      "Epoch 11/20  Iteration 1916/3560 Training loss: 1.4706 0.8336 sec/batch\n",
      "Epoch 11/20  Iteration 1917/3560 Training loss: 1.4708 0.8120 sec/batch\n",
      "Epoch 11/20  Iteration 1918/3560 Training loss: 1.4708 0.8249 sec/batch\n",
      "Epoch 11/20  Iteration 1919/3560 Training loss: 1.4709 0.8095 sec/batch\n",
      "Epoch 11/20  Iteration 1920/3560 Training loss: 1.4709 0.8487 sec/batch\n",
      "Epoch 11/20  Iteration 1921/3560 Training loss: 1.4712 0.8820 sec/batch\n",
      "Epoch 11/20  Iteration 1922/3560 Training loss: 1.4711 0.8973 sec/batch\n",
      "Epoch 11/20  Iteration 1923/3560 Training loss: 1.4710 0.8942 sec/batch\n",
      "Epoch 11/20  Iteration 1924/3560 Training loss: 1.4712 0.8888 sec/batch\n",
      "Epoch 11/20  Iteration 1925/3560 Training loss: 1.4712 0.8487 sec/batch\n",
      "Epoch 11/20  Iteration 1926/3560 Training loss: 1.4713 0.8176 sec/batch\n",
      "Epoch 11/20  Iteration 1927/3560 Training loss: 1.4713 0.9112 sec/batch\n",
      "Epoch 11/20  Iteration 1928/3560 Training loss: 1.4716 1.1125 sec/batch\n",
      "Epoch 11/20  Iteration 1929/3560 Training loss: 1.4717 0.9407 sec/batch\n",
      "Epoch 11/20  Iteration 1930/3560 Training loss: 1.4716 1.1581 sec/batch\n",
      "Epoch 11/20  Iteration 1931/3560 Training loss: 1.4713 1.0448 sec/batch\n",
      "Epoch 11/20  Iteration 1932/3560 Training loss: 1.4712 0.9257 sec/batch\n",
      "Epoch 11/20  Iteration 1933/3560 Training loss: 1.4713 0.8672 sec/batch\n",
      "Epoch 11/20  Iteration 1934/3560 Training loss: 1.4713 0.9556 sec/batch\n",
      "Epoch 11/20  Iteration 1935/3560 Training loss: 1.4713 0.8495 sec/batch\n",
      "Epoch 11/20  Iteration 1936/3560 Training loss: 1.4713 0.8960 sec/batch\n",
      "Epoch 11/20  Iteration 1937/3560 Training loss: 1.4713 0.9324 sec/batch\n",
      "Epoch 11/20  Iteration 1938/3560 Training loss: 1.4712 1.0323 sec/batch\n",
      "Epoch 11/20  Iteration 1939/3560 Training loss: 1.4709 1.0899 sec/batch\n",
      "Epoch 11/20  Iteration 1940/3560 Training loss: 1.4710 1.0027 sec/batch\n",
      "Epoch 11/20  Iteration 1941/3560 Training loss: 1.4711 0.9077 sec/batch\n",
      "Epoch 11/20  Iteration 1942/3560 Training loss: 1.4711 0.8796 sec/batch\n",
      "Epoch 11/20  Iteration 1943/3560 Training loss: 1.4711 0.8194 sec/batch\n",
      "Epoch 11/20  Iteration 1944/3560 Training loss: 1.4711 0.8168 sec/batch\n",
      "Epoch 11/20  Iteration 1945/3560 Training loss: 1.4711 0.8497 sec/batch\n",
      "Epoch 11/20  Iteration 1946/3560 Training loss: 1.4710 0.8401 sec/batch\n",
      "Epoch 11/20  Iteration 1947/3560 Training loss: 1.4711 0.8024 sec/batch\n",
      "Epoch 11/20  Iteration 1948/3560 Training loss: 1.4715 0.8019 sec/batch\n",
      "Epoch 11/20  Iteration 1949/3560 Training loss: 1.4714 0.9976 sec/batch\n",
      "Epoch 11/20  Iteration 1950/3560 Training loss: 1.4714 1.0005 sec/batch\n",
      "Epoch 11/20  Iteration 1951/3560 Training loss: 1.4713 0.8507 sec/batch\n",
      "Epoch 11/20  Iteration 1952/3560 Training loss: 1.4711 0.8992 sec/batch\n",
      "Epoch 11/20  Iteration 1953/3560 Training loss: 1.4712 0.9519 sec/batch\n",
      "Epoch 11/20  Iteration 1954/3560 Training loss: 1.4711 0.8670 sec/batch\n",
      "Epoch 11/20  Iteration 1955/3560 Training loss: 1.4713 0.8454 sec/batch\n",
      "Epoch 11/20  Iteration 1956/3560 Training loss: 1.4711 0.8744 sec/batch\n",
      "Epoch 11/20  Iteration 1957/3560 Training loss: 1.4710 0.8920 sec/batch\n",
      "Epoch 11/20  Iteration 1958/3560 Training loss: 1.4711 0.9547 sec/batch\n",
      "Epoch 12/20  Iteration 1959/3560 Training loss: 1.5591 1.0039 sec/batch\n",
      "Epoch 12/20  Iteration 1960/3560 Training loss: 1.5123 0.9205 sec/batch\n",
      "Epoch 12/20  Iteration 1961/3560 Training loss: 1.4953 0.9017 sec/batch\n",
      "Epoch 12/20  Iteration 1962/3560 Training loss: 1.4903 1.0072 sec/batch\n",
      "Epoch 12/20  Iteration 1963/3560 Training loss: 1.4822 1.2154 sec/batch\n",
      "Epoch 12/20  Iteration 1964/3560 Training loss: 1.4716 1.3000 sec/batch\n",
      "Epoch 12/20  Iteration 1965/3560 Training loss: 1.4731 1.2914 sec/batch\n",
      "Epoch 12/20  Iteration 1966/3560 Training loss: 1.4722 1.3520 sec/batch\n",
      "Epoch 12/20  Iteration 1967/3560 Training loss: 1.4734 1.3755 sec/batch\n",
      "Epoch 12/20  Iteration 1968/3560 Training loss: 1.4717 1.4392 sec/batch\n",
      "Epoch 12/20  Iteration 1969/3560 Training loss: 1.4682 1.0790 sec/batch\n",
      "Epoch 12/20  Iteration 1970/3560 Training loss: 1.4678 0.9778 sec/batch\n",
      "Epoch 12/20  Iteration 1971/3560 Training loss: 1.4667 0.9521 sec/batch\n",
      "Epoch 12/20  Iteration 1972/3560 Training loss: 1.4680 0.9642 sec/batch\n",
      "Epoch 12/20  Iteration 1973/3560 Training loss: 1.4667 0.8783 sec/batch\n",
      "Epoch 12/20  Iteration 1974/3560 Training loss: 1.4648 0.9078 sec/batch\n",
      "Epoch 12/20  Iteration 1975/3560 Training loss: 1.4649 0.9326 sec/batch\n",
      "Epoch 12/20  Iteration 1976/3560 Training loss: 1.4655 1.1625 sec/batch\n",
      "Epoch 12/20  Iteration 1977/3560 Training loss: 1.4656 1.3278 sec/batch\n",
      "Epoch 12/20  Iteration 1978/3560 Training loss: 1.4665 1.4278 sec/batch\n",
      "Epoch 12/20  Iteration 1979/3560 Training loss: 1.4657 1.2947 sec/batch\n",
      "Epoch 12/20  Iteration 1980/3560 Training loss: 1.4659 0.9149 sec/batch\n",
      "Epoch 12/20  Iteration 1981/3560 Training loss: 1.4650 1.0092 sec/batch\n",
      "Epoch 12/20  Iteration 1982/3560 Training loss: 1.4645 0.9600 sec/batch\n",
      "Epoch 12/20  Iteration 1983/3560 Training loss: 1.4643 0.9196 sec/batch\n",
      "Epoch 12/20  Iteration 1984/3560 Training loss: 1.4627 0.9177 sec/batch\n",
      "Epoch 12/20  Iteration 1985/3560 Training loss: 1.4621 0.9515 sec/batch\n",
      "Epoch 12/20  Iteration 1986/3560 Training loss: 1.4628 0.9772 sec/batch\n",
      "Epoch 12/20  Iteration 1987/3560 Training loss: 1.4630 1.2041 sec/batch\n",
      "Epoch 12/20  Iteration 1988/3560 Training loss: 1.4634 1.0881 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 1989/3560 Training loss: 1.4629 0.9343 sec/batch\n",
      "Epoch 12/20  Iteration 1990/3560 Training loss: 1.4620 0.9465 sec/batch\n",
      "Epoch 12/20  Iteration 1991/3560 Training loss: 1.4622 1.0741 sec/batch\n",
      "Epoch 12/20  Iteration 1992/3560 Training loss: 1.4626 1.0636 sec/batch\n",
      "Epoch 12/20  Iteration 1993/3560 Training loss: 1.4622 1.1035 sec/batch\n",
      "Epoch 12/20  Iteration 1994/3560 Training loss: 1.4620 1.0114 sec/batch\n",
      "Epoch 12/20  Iteration 1995/3560 Training loss: 1.4614 0.9357 sec/batch\n",
      "Epoch 12/20  Iteration 1996/3560 Training loss: 1.4603 1.2101 sec/batch\n",
      "Epoch 12/20  Iteration 1997/3560 Training loss: 1.4590 0.9671 sec/batch\n",
      "Epoch 12/20  Iteration 1998/3560 Training loss: 1.4586 0.9594 sec/batch\n",
      "Epoch 12/20  Iteration 1999/3560 Training loss: 1.4581 0.8655 sec/batch\n",
      "Epoch 12/20  Iteration 2000/3560 Training loss: 1.4586 1.0247 sec/batch\n",
      "Epoch 12/20  Iteration 2001/3560 Training loss: 1.4584 1.0395 sec/batch\n",
      "Epoch 12/20  Iteration 2002/3560 Training loss: 1.4579 1.3488 sec/batch\n",
      "Epoch 12/20  Iteration 2003/3560 Training loss: 1.4583 1.2249 sec/batch\n",
      "Epoch 12/20  Iteration 2004/3560 Training loss: 1.4571 0.9248 sec/batch\n",
      "Epoch 12/20  Iteration 2005/3560 Training loss: 1.4568 0.9983 sec/batch\n",
      "Epoch 12/20  Iteration 2006/3560 Training loss: 1.4564 0.9441 sec/batch\n",
      "Epoch 12/20  Iteration 2007/3560 Training loss: 1.4564 0.9462 sec/batch\n",
      "Epoch 12/20  Iteration 2008/3560 Training loss: 1.4567 0.9111 sec/batch\n",
      "Epoch 12/20  Iteration 2009/3560 Training loss: 1.4562 0.8562 sec/batch\n",
      "Epoch 12/20  Iteration 2010/3560 Training loss: 1.4571 0.8965 sec/batch\n",
      "Epoch 12/20  Iteration 2011/3560 Training loss: 1.4571 0.9593 sec/batch\n",
      "Epoch 12/20  Iteration 2012/3560 Training loss: 1.4575 0.9861 sec/batch\n",
      "Epoch 12/20  Iteration 2013/3560 Training loss: 1.4572 1.0862 sec/batch\n",
      "Epoch 12/20  Iteration 2014/3560 Training loss: 1.4573 1.2171 sec/batch\n",
      "Epoch 12/20  Iteration 2015/3560 Training loss: 1.4579 1.1697 sec/batch\n",
      "Epoch 12/20  Iteration 2016/3560 Training loss: 1.4575 0.9474 sec/batch\n",
      "Epoch 12/20  Iteration 2017/3560 Training loss: 1.4572 0.8674 sec/batch\n",
      "Epoch 12/20  Iteration 2018/3560 Training loss: 1.4576 0.8168 sec/batch\n",
      "Epoch 12/20  Iteration 2019/3560 Training loss: 1.4576 0.8098 sec/batch\n",
      "Epoch 12/20  Iteration 2020/3560 Training loss: 1.4584 0.8206 sec/batch\n",
      "Epoch 12/20  Iteration 2021/3560 Training loss: 1.4587 0.8462 sec/batch\n",
      "Epoch 12/20  Iteration 2022/3560 Training loss: 1.4589 0.8535 sec/batch\n",
      "Epoch 12/20  Iteration 2023/3560 Training loss: 1.4589 0.8304 sec/batch\n",
      "Epoch 12/20  Iteration 2024/3560 Training loss: 1.4590 0.7989 sec/batch\n",
      "Epoch 12/20  Iteration 2025/3560 Training loss: 1.4591 1.0375 sec/batch\n",
      "Epoch 12/20  Iteration 2026/3560 Training loss: 1.4588 1.1773 sec/batch\n",
      "Epoch 12/20  Iteration 2027/3560 Training loss: 1.4587 0.9186 sec/batch\n",
      "Epoch 12/20  Iteration 2028/3560 Training loss: 1.4586 0.9503 sec/batch\n",
      "Epoch 12/20  Iteration 2029/3560 Training loss: 1.4593 0.8432 sec/batch\n",
      "Epoch 12/20  Iteration 2030/3560 Training loss: 1.4594 0.8412 sec/batch\n",
      "Epoch 12/20  Iteration 2031/3560 Training loss: 1.4597 0.8407 sec/batch\n",
      "Epoch 12/20  Iteration 2032/3560 Training loss: 1.4593 0.8726 sec/batch\n",
      "Epoch 12/20  Iteration 2033/3560 Training loss: 1.4592 0.8172 sec/batch\n",
      "Epoch 12/20  Iteration 2034/3560 Training loss: 1.4594 0.8187 sec/batch\n",
      "Epoch 12/20  Iteration 2035/3560 Training loss: 1.4594 0.8480 sec/batch\n",
      "Epoch 12/20  Iteration 2036/3560 Training loss: 1.4595 0.8394 sec/batch\n",
      "Epoch 12/20  Iteration 2037/3560 Training loss: 1.4590 0.9180 sec/batch\n",
      "Epoch 12/20  Iteration 2038/3560 Training loss: 1.4588 0.8533 sec/batch\n",
      "Epoch 12/20  Iteration 2039/3560 Training loss: 1.4583 0.8362 sec/batch\n",
      "Epoch 12/20  Iteration 2040/3560 Training loss: 1.4583 0.8157 sec/batch\n",
      "Epoch 12/20  Iteration 2041/3560 Training loss: 1.4579 0.8337 sec/batch\n",
      "Epoch 12/20  Iteration 2042/3560 Training loss: 1.4579 0.8074 sec/batch\n",
      "Epoch 12/20  Iteration 2043/3560 Training loss: 1.4576 0.8436 sec/batch\n",
      "Epoch 12/20  Iteration 2044/3560 Training loss: 1.4574 0.8544 sec/batch\n",
      "Epoch 12/20  Iteration 2045/3560 Training loss: 1.4571 0.9049 sec/batch\n",
      "Epoch 12/20  Iteration 2046/3560 Training loss: 1.4567 0.8601 sec/batch\n",
      "Epoch 12/20  Iteration 2047/3560 Training loss: 1.4563 0.8492 sec/batch\n",
      "Epoch 12/20  Iteration 2048/3560 Training loss: 1.4564 0.8072 sec/batch\n",
      "Epoch 12/20  Iteration 2049/3560 Training loss: 1.4561 0.8673 sec/batch\n",
      "Epoch 12/20  Iteration 2050/3560 Training loss: 1.4559 0.8513 sec/batch\n",
      "Epoch 12/20  Iteration 2051/3560 Training loss: 1.4556 0.8292 sec/batch\n",
      "Epoch 12/20  Iteration 2052/3560 Training loss: 1.4553 0.8692 sec/batch\n",
      "Epoch 12/20  Iteration 2053/3560 Training loss: 1.4550 0.8694 sec/batch\n",
      "Epoch 12/20  Iteration 2054/3560 Training loss: 1.4550 1.0402 sec/batch\n",
      "Epoch 12/20  Iteration 2055/3560 Training loss: 1.4551 1.1786 sec/batch\n",
      "Epoch 12/20  Iteration 2056/3560 Training loss: 1.4548 1.1297 sec/batch\n",
      "Epoch 12/20  Iteration 2057/3560 Training loss: 1.4544 1.4430 sec/batch\n",
      "Epoch 12/20  Iteration 2058/3560 Training loss: 1.4540 1.1082 sec/batch\n",
      "Epoch 12/20  Iteration 2059/3560 Training loss: 1.4540 1.2273 sec/batch\n",
      "Epoch 12/20  Iteration 2060/3560 Training loss: 1.4540 1.3245 sec/batch\n",
      "Epoch 12/20  Iteration 2061/3560 Training loss: 1.4538 1.0514 sec/batch\n",
      "Epoch 12/20  Iteration 2062/3560 Training loss: 1.4536 0.9153 sec/batch\n",
      "Epoch 12/20  Iteration 2063/3560 Training loss: 1.4535 0.9446 sec/batch\n",
      "Epoch 12/20  Iteration 2064/3560 Training loss: 1.4534 1.0528 sec/batch\n",
      "Epoch 12/20  Iteration 2065/3560 Training loss: 1.4533 1.0330 sec/batch\n",
      "Epoch 12/20  Iteration 2066/3560 Training loss: 1.4532 1.0231 sec/batch\n",
      "Epoch 12/20  Iteration 2067/3560 Training loss: 1.4531 0.8926 sec/batch\n",
      "Epoch 12/20  Iteration 2068/3560 Training loss: 1.4532 0.9477 sec/batch\n",
      "Epoch 12/20  Iteration 2069/3560 Training loss: 1.4530 1.1648 sec/batch\n",
      "Epoch 12/20  Iteration 2070/3560 Training loss: 1.4530 1.0025 sec/batch\n",
      "Epoch 12/20  Iteration 2071/3560 Training loss: 1.4529 0.8512 sec/batch\n",
      "Epoch 12/20  Iteration 2072/3560 Training loss: 1.4527 0.7962 sec/batch\n",
      "Epoch 12/20  Iteration 2073/3560 Training loss: 1.4524 1.2301 sec/batch\n",
      "Epoch 12/20  Iteration 2074/3560 Training loss: 1.4520 1.2590 sec/batch\n",
      "Epoch 12/20  Iteration 2075/3560 Training loss: 1.4519 0.9680 sec/batch\n",
      "Epoch 12/20  Iteration 2076/3560 Training loss: 1.4519 0.8636 sec/batch\n",
      "Epoch 12/20  Iteration 2077/3560 Training loss: 1.4517 0.9246 sec/batch\n",
      "Epoch 12/20  Iteration 2078/3560 Training loss: 1.4517 0.8087 sec/batch\n",
      "Epoch 12/20  Iteration 2079/3560 Training loss: 1.4516 0.9392 sec/batch\n",
      "Epoch 12/20  Iteration 2080/3560 Training loss: 1.4513 0.8353 sec/batch\n",
      "Epoch 12/20  Iteration 2081/3560 Training loss: 1.4508 0.8336 sec/batch\n",
      "Epoch 12/20  Iteration 2082/3560 Training loss: 1.4508 0.8581 sec/batch\n",
      "Epoch 12/20  Iteration 2083/3560 Training loss: 1.4508 0.8532 sec/batch\n",
      "Epoch 12/20  Iteration 2084/3560 Training loss: 1.4504 0.8056 sec/batch\n",
      "Epoch 12/20  Iteration 2085/3560 Training loss: 1.4505 0.8143 sec/batch\n",
      "Epoch 12/20  Iteration 2086/3560 Training loss: 1.4504 0.8209 sec/batch\n",
      "Epoch 12/20  Iteration 2087/3560 Training loss: 1.4502 0.8308 sec/batch\n",
      "Epoch 12/20  Iteration 2088/3560 Training loss: 1.4499 1.2029 sec/batch\n",
      "Epoch 12/20  Iteration 2089/3560 Training loss: 1.4496 1.0971 sec/batch\n",
      "Epoch 12/20  Iteration 2090/3560 Training loss: 1.4494 0.9881 sec/batch\n",
      "Epoch 12/20  Iteration 2091/3560 Training loss: 1.4494 1.0022 sec/batch\n",
      "Epoch 12/20  Iteration 2092/3560 Training loss: 1.4494 0.8733 sec/batch\n",
      "Epoch 12/20  Iteration 2093/3560 Training loss: 1.4494 0.8145 sec/batch\n",
      "Epoch 12/20  Iteration 2094/3560 Training loss: 1.4494 0.8276 sec/batch\n",
      "Epoch 12/20  Iteration 2095/3560 Training loss: 1.4496 0.8503 sec/batch\n",
      "Epoch 12/20  Iteration 2096/3560 Training loss: 1.4498 0.8311 sec/batch\n",
      "Epoch 12/20  Iteration 2097/3560 Training loss: 1.4498 0.8342 sec/batch\n",
      "Epoch 12/20  Iteration 2098/3560 Training loss: 1.4498 0.8256 sec/batch\n",
      "Epoch 12/20  Iteration 2099/3560 Training loss: 1.4502 0.8364 sec/batch\n",
      "Epoch 12/20  Iteration 2100/3560 Training loss: 1.4502 0.8323 sec/batch\n",
      "Epoch 12/20  Iteration 2101/3560 Training loss: 1.4500 0.7959 sec/batch\n",
      "Epoch 12/20  Iteration 2102/3560 Training loss: 1.4502 0.9208 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 2103/3560 Training loss: 1.4501 0.8868 sec/batch\n",
      "Epoch 12/20  Iteration 2104/3560 Training loss: 1.4503 0.8158 sec/batch\n",
      "Epoch 12/20  Iteration 2105/3560 Training loss: 1.4503 0.8154 sec/batch\n",
      "Epoch 12/20  Iteration 2106/3560 Training loss: 1.4505 0.8105 sec/batch\n",
      "Epoch 12/20  Iteration 2107/3560 Training loss: 1.4506 0.8272 sec/batch\n",
      "Epoch 12/20  Iteration 2108/3560 Training loss: 1.4506 0.8307 sec/batch\n",
      "Epoch 12/20  Iteration 2109/3560 Training loss: 1.4503 0.8157 sec/batch\n",
      "Epoch 12/20  Iteration 2110/3560 Training loss: 1.4502 0.8060 sec/batch\n",
      "Epoch 12/20  Iteration 2111/3560 Training loss: 1.4502 0.8307 sec/batch\n",
      "Epoch 12/20  Iteration 2112/3560 Training loss: 1.4503 0.8437 sec/batch\n",
      "Epoch 12/20  Iteration 2113/3560 Training loss: 1.4503 0.8444 sec/batch\n",
      "Epoch 12/20  Iteration 2114/3560 Training loss: 1.4503 0.8984 sec/batch\n",
      "Epoch 12/20  Iteration 2115/3560 Training loss: 1.4504 0.8265 sec/batch\n",
      "Epoch 12/20  Iteration 2116/3560 Training loss: 1.4503 0.9386 sec/batch\n",
      "Epoch 12/20  Iteration 2117/3560 Training loss: 1.4501 0.8456 sec/batch\n",
      "Epoch 12/20  Iteration 2118/3560 Training loss: 1.4503 0.7953 sec/batch\n",
      "Epoch 12/20  Iteration 2119/3560 Training loss: 1.4505 0.8107 sec/batch\n",
      "Epoch 12/20  Iteration 2120/3560 Training loss: 1.4505 0.8080 sec/batch\n",
      "Epoch 12/20  Iteration 2121/3560 Training loss: 1.4505 0.9853 sec/batch\n",
      "Epoch 12/20  Iteration 2122/3560 Training loss: 1.4505 0.8943 sec/batch\n",
      "Epoch 12/20  Iteration 2123/3560 Training loss: 1.4505 0.8602 sec/batch\n",
      "Epoch 12/20  Iteration 2124/3560 Training loss: 1.4504 0.7925 sec/batch\n",
      "Epoch 12/20  Iteration 2125/3560 Training loss: 1.4505 0.8027 sec/batch\n",
      "Epoch 12/20  Iteration 2126/3560 Training loss: 1.4510 0.8426 sec/batch\n",
      "Epoch 12/20  Iteration 2127/3560 Training loss: 1.4509 0.7961 sec/batch\n",
      "Epoch 12/20  Iteration 2128/3560 Training loss: 1.4509 0.7849 sec/batch\n",
      "Epoch 12/20  Iteration 2129/3560 Training loss: 1.4509 0.8066 sec/batch\n",
      "Epoch 12/20  Iteration 2130/3560 Training loss: 1.4508 0.8075 sec/batch\n",
      "Epoch 12/20  Iteration 2131/3560 Training loss: 1.4508 0.9382 sec/batch\n",
      "Epoch 12/20  Iteration 2132/3560 Training loss: 1.4508 1.2112 sec/batch\n",
      "Epoch 12/20  Iteration 2133/3560 Training loss: 1.4509 1.0794 sec/batch\n",
      "Epoch 12/20  Iteration 2134/3560 Training loss: 1.4508 0.8934 sec/batch\n",
      "Epoch 12/20  Iteration 2135/3560 Training loss: 1.4507 0.8135 sec/batch\n",
      "Epoch 12/20  Iteration 2136/3560 Training loss: 1.4507 0.7952 sec/batch\n",
      "Epoch 13/20  Iteration 2137/3560 Training loss: 1.5369 0.8558 sec/batch\n",
      "Epoch 13/20  Iteration 2138/3560 Training loss: 1.4955 0.8291 sec/batch\n",
      "Epoch 13/20  Iteration 2139/3560 Training loss: 1.4847 0.8162 sec/batch\n",
      "Epoch 13/20  Iteration 2140/3560 Training loss: 1.4792 0.8200 sec/batch\n",
      "Epoch 13/20  Iteration 2141/3560 Training loss: 1.4683 0.8169 sec/batch\n",
      "Epoch 13/20  Iteration 2142/3560 Training loss: 1.4583 0.7988 sec/batch\n",
      "Epoch 13/20  Iteration 2143/3560 Training loss: 1.4569 0.8380 sec/batch\n",
      "Epoch 13/20  Iteration 2144/3560 Training loss: 1.4551 0.8579 sec/batch\n",
      "Epoch 13/20  Iteration 2145/3560 Training loss: 1.4546 0.9086 sec/batch\n",
      "Epoch 13/20  Iteration 2146/3560 Training loss: 1.4536 0.9132 sec/batch\n",
      "Epoch 13/20  Iteration 2147/3560 Training loss: 1.4496 0.8789 sec/batch\n",
      "Epoch 13/20  Iteration 2148/3560 Training loss: 1.4491 0.8714 sec/batch\n",
      "Epoch 13/20  Iteration 2149/3560 Training loss: 1.4485 0.9105 sec/batch\n",
      "Epoch 13/20  Iteration 2150/3560 Training loss: 1.4501 0.9543 sec/batch\n",
      "Epoch 13/20  Iteration 2151/3560 Training loss: 1.4486 0.8856 sec/batch\n",
      "Epoch 13/20  Iteration 2152/3560 Training loss: 1.4466 0.8655 sec/batch\n",
      "Epoch 13/20  Iteration 2153/3560 Training loss: 1.4471 1.2078 sec/batch\n",
      "Epoch 13/20  Iteration 2154/3560 Training loss: 1.4485 1.0806 sec/batch\n",
      "Epoch 13/20  Iteration 2155/3560 Training loss: 1.4479 0.9926 sec/batch\n",
      "Epoch 13/20  Iteration 2156/3560 Training loss: 1.4486 0.9124 sec/batch\n",
      "Epoch 13/20  Iteration 2157/3560 Training loss: 1.4480 1.0881 sec/batch\n",
      "Epoch 13/20  Iteration 2158/3560 Training loss: 1.4488 1.1465 sec/batch\n",
      "Epoch 13/20  Iteration 2159/3560 Training loss: 1.4476 0.8895 sec/batch\n",
      "Epoch 13/20  Iteration 2160/3560 Training loss: 1.4476 1.0527 sec/batch\n",
      "Epoch 13/20  Iteration 2161/3560 Training loss: 1.4470 0.9670 sec/batch\n",
      "Epoch 13/20  Iteration 2162/3560 Training loss: 1.4455 0.8984 sec/batch\n",
      "Epoch 13/20  Iteration 2163/3560 Training loss: 1.4445 0.8316 sec/batch\n",
      "Epoch 13/20  Iteration 2164/3560 Training loss: 1.4450 0.8724 sec/batch\n",
      "Epoch 13/20  Iteration 2165/3560 Training loss: 1.4453 0.8571 sec/batch\n",
      "Epoch 13/20  Iteration 2166/3560 Training loss: 1.4456 0.8321 sec/batch\n",
      "Epoch 13/20  Iteration 2167/3560 Training loss: 1.4451 0.8732 sec/batch\n",
      "Epoch 13/20  Iteration 2168/3560 Training loss: 1.4438 0.8551 sec/batch\n",
      "Epoch 13/20  Iteration 2169/3560 Training loss: 1.4439 0.8602 sec/batch\n",
      "Epoch 13/20  Iteration 2170/3560 Training loss: 1.4442 0.9908 sec/batch\n",
      "Epoch 13/20  Iteration 2171/3560 Training loss: 1.4440 0.9806 sec/batch\n",
      "Epoch 13/20  Iteration 2172/3560 Training loss: 1.4441 1.2479 sec/batch\n",
      "Epoch 13/20  Iteration 2173/3560 Training loss: 1.4432 1.2240 sec/batch\n",
      "Epoch 13/20  Iteration 2174/3560 Training loss: 1.4422 1.3166 sec/batch\n",
      "Epoch 13/20  Iteration 2175/3560 Training loss: 1.4408 0.8476 sec/batch\n",
      "Epoch 13/20  Iteration 2176/3560 Training loss: 1.4403 0.8362 sec/batch\n",
      "Epoch 13/20  Iteration 2177/3560 Training loss: 1.4397 0.9945 sec/batch\n",
      "Epoch 13/20  Iteration 2178/3560 Training loss: 1.4407 0.8712 sec/batch\n",
      "Epoch 13/20  Iteration 2179/3560 Training loss: 1.4404 1.0714 sec/batch\n",
      "Epoch 13/20  Iteration 2180/3560 Training loss: 1.4396 0.9443 sec/batch\n",
      "Epoch 13/20  Iteration 2181/3560 Training loss: 1.4397 0.9948 sec/batch\n",
      "Epoch 13/20  Iteration 2182/3560 Training loss: 1.4387 1.1716 sec/batch\n",
      "Epoch 13/20  Iteration 2183/3560 Training loss: 1.4384 1.2174 sec/batch\n",
      "Epoch 13/20  Iteration 2184/3560 Training loss: 1.4380 1.0310 sec/batch\n",
      "Epoch 13/20  Iteration 2185/3560 Training loss: 1.4378 1.0040 sec/batch\n",
      "Epoch 13/20  Iteration 2186/3560 Training loss: 1.4382 0.8245 sec/batch\n",
      "Epoch 13/20  Iteration 2187/3560 Training loss: 1.4377 0.8264 sec/batch\n",
      "Epoch 13/20  Iteration 2188/3560 Training loss: 1.4384 0.8144 sec/batch\n",
      "Epoch 13/20  Iteration 2189/3560 Training loss: 1.4386 0.8005 sec/batch\n",
      "Epoch 13/20  Iteration 2190/3560 Training loss: 1.4387 0.8481 sec/batch\n",
      "Epoch 13/20  Iteration 2191/3560 Training loss: 1.4386 0.8708 sec/batch\n",
      "Epoch 13/20  Iteration 2192/3560 Training loss: 1.4386 0.8355 sec/batch\n",
      "Epoch 13/20  Iteration 2193/3560 Training loss: 1.4392 0.9950 sec/batch\n",
      "Epoch 13/20  Iteration 2194/3560 Training loss: 1.4389 0.8566 sec/batch\n",
      "Epoch 13/20  Iteration 2195/3560 Training loss: 1.4382 0.9645 sec/batch\n",
      "Epoch 13/20  Iteration 2196/3560 Training loss: 1.4387 1.0769 sec/batch\n",
      "Epoch 13/20  Iteration 2197/3560 Training loss: 1.4387 1.1574 sec/batch\n",
      "Epoch 13/20  Iteration 2198/3560 Training loss: 1.4397 1.1530 sec/batch\n",
      "Epoch 13/20  Iteration 2199/3560 Training loss: 1.4403 0.9344 sec/batch\n",
      "Epoch 13/20  Iteration 2200/3560 Training loss: 1.4405 0.8848 sec/batch\n",
      "Epoch 13/20  Iteration 2201/3560 Training loss: 1.4404 0.8334 sec/batch\n",
      "Epoch 13/20  Iteration 2202/3560 Training loss: 1.4406 1.2874 sec/batch\n",
      "Epoch 13/20  Iteration 2203/3560 Training loss: 1.4407 0.8728 sec/batch\n",
      "Epoch 13/20  Iteration 2204/3560 Training loss: 1.4404 0.8236 sec/batch\n",
      "Epoch 13/20  Iteration 2205/3560 Training loss: 1.4404 0.8466 sec/batch\n",
      "Epoch 13/20  Iteration 2206/3560 Training loss: 1.4402 0.9232 sec/batch\n",
      "Epoch 13/20  Iteration 2207/3560 Training loss: 1.4408 0.8519 sec/batch\n",
      "Epoch 13/20  Iteration 2208/3560 Training loss: 1.4411 0.8297 sec/batch\n",
      "Epoch 13/20  Iteration 2209/3560 Training loss: 1.4415 0.8553 sec/batch\n",
      "Epoch 13/20  Iteration 2210/3560 Training loss: 1.4410 0.9759 sec/batch\n",
      "Epoch 13/20  Iteration 2211/3560 Training loss: 1.4409 0.8570 sec/batch\n",
      "Epoch 13/20  Iteration 2212/3560 Training loss: 1.4412 0.8702 sec/batch\n",
      "Epoch 13/20  Iteration 2213/3560 Training loss: 1.4410 1.2778 sec/batch\n",
      "Epoch 13/20  Iteration 2214/3560 Training loss: 1.4409 1.0117 sec/batch\n",
      "Epoch 13/20  Iteration 2215/3560 Training loss: 1.4404 0.9222 sec/batch\n",
      "Epoch 13/20  Iteration 2216/3560 Training loss: 1.4403 0.8307 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2217/3560 Training loss: 1.4398 0.8576 sec/batch\n",
      "Epoch 13/20  Iteration 2218/3560 Training loss: 1.4398 0.8705 sec/batch\n",
      "Epoch 13/20  Iteration 2219/3560 Training loss: 1.4394 0.8039 sec/batch\n",
      "Epoch 13/20  Iteration 2220/3560 Training loss: 1.4394 0.8215 sec/batch\n",
      "Epoch 13/20  Iteration 2221/3560 Training loss: 1.4392 0.9218 sec/batch\n",
      "Epoch 13/20  Iteration 2222/3560 Training loss: 1.4390 0.8945 sec/batch\n",
      "Epoch 13/20  Iteration 2223/3560 Training loss: 1.4387 0.8640 sec/batch\n",
      "Epoch 13/20  Iteration 2224/3560 Training loss: 1.4386 0.9170 sec/batch\n",
      "Epoch 13/20  Iteration 2225/3560 Training loss: 1.4382 0.8263 sec/batch\n",
      "Epoch 13/20  Iteration 2226/3560 Training loss: 1.4383 0.8092 sec/batch\n",
      "Epoch 13/20  Iteration 2227/3560 Training loss: 1.4379 0.8174 sec/batch\n",
      "Epoch 13/20  Iteration 2228/3560 Training loss: 1.4379 0.8702 sec/batch\n",
      "Epoch 13/20  Iteration 2229/3560 Training loss: 1.4375 0.8608 sec/batch\n",
      "Epoch 13/20  Iteration 2230/3560 Training loss: 1.4372 0.8138 sec/batch\n",
      "Epoch 13/20  Iteration 2231/3560 Training loss: 1.4368 0.8451 sec/batch\n",
      "Epoch 13/20  Iteration 2232/3560 Training loss: 1.4369 0.8097 sec/batch\n",
      "Epoch 13/20  Iteration 2233/3560 Training loss: 1.4369 0.8131 sec/batch\n",
      "Epoch 13/20  Iteration 2234/3560 Training loss: 1.4367 0.8125 sec/batch\n",
      "Epoch 13/20  Iteration 2235/3560 Training loss: 1.4362 0.8175 sec/batch\n",
      "Epoch 13/20  Iteration 2236/3560 Training loss: 1.4358 0.8696 sec/batch\n",
      "Epoch 13/20  Iteration 2237/3560 Training loss: 1.4358 0.9589 sec/batch\n",
      "Epoch 13/20  Iteration 2238/3560 Training loss: 1.4356 0.8052 sec/batch\n",
      "Epoch 13/20  Iteration 2239/3560 Training loss: 1.4354 0.7960 sec/batch\n",
      "Epoch 13/20  Iteration 2240/3560 Training loss: 1.4352 0.8635 sec/batch\n",
      "Epoch 13/20  Iteration 2241/3560 Training loss: 1.4352 0.8039 sec/batch\n",
      "Epoch 13/20  Iteration 2242/3560 Training loss: 1.4350 0.8300 sec/batch\n",
      "Epoch 13/20  Iteration 2243/3560 Training loss: 1.4350 0.8468 sec/batch\n",
      "Epoch 13/20  Iteration 2244/3560 Training loss: 1.4350 0.8139 sec/batch\n",
      "Epoch 13/20  Iteration 2245/3560 Training loss: 1.4348 0.8786 sec/batch\n",
      "Epoch 13/20  Iteration 2246/3560 Training loss: 1.4349 0.8616 sec/batch\n",
      "Epoch 13/20  Iteration 2247/3560 Training loss: 1.4347 0.8230 sec/batch\n",
      "Epoch 13/20  Iteration 2248/3560 Training loss: 1.4346 0.9167 sec/batch\n",
      "Epoch 13/20  Iteration 2249/3560 Training loss: 1.4344 0.8939 sec/batch\n",
      "Epoch 13/20  Iteration 2250/3560 Training loss: 1.4343 0.8235 sec/batch\n",
      "Epoch 13/20  Iteration 2251/3560 Training loss: 1.4340 0.8583 sec/batch\n",
      "Epoch 13/20  Iteration 2252/3560 Training loss: 1.4337 0.8778 sec/batch\n",
      "Epoch 13/20  Iteration 2253/3560 Training loss: 1.4336 0.8351 sec/batch\n",
      "Epoch 13/20  Iteration 2254/3560 Training loss: 1.4336 0.9418 sec/batch\n",
      "Epoch 13/20  Iteration 2255/3560 Training loss: 1.4335 1.0533 sec/batch\n",
      "Epoch 13/20  Iteration 2256/3560 Training loss: 1.4335 0.9685 sec/batch\n",
      "Epoch 13/20  Iteration 2257/3560 Training loss: 1.4334 0.8769 sec/batch\n",
      "Epoch 13/20  Iteration 2258/3560 Training loss: 1.4330 0.8253 sec/batch\n",
      "Epoch 13/20  Iteration 2259/3560 Training loss: 1.4326 0.8388 sec/batch\n",
      "Epoch 13/20  Iteration 2260/3560 Training loss: 1.4327 0.8707 sec/batch\n",
      "Epoch 13/20  Iteration 2261/3560 Training loss: 1.4326 0.8469 sec/batch\n",
      "Epoch 13/20  Iteration 2262/3560 Training loss: 1.4321 0.8210 sec/batch\n",
      "Epoch 13/20  Iteration 2263/3560 Training loss: 1.4322 0.8834 sec/batch\n",
      "Epoch 13/20  Iteration 2264/3560 Training loss: 1.4324 0.8180 sec/batch\n",
      "Epoch 13/20  Iteration 2265/3560 Training loss: 1.4322 0.8238 sec/batch\n",
      "Epoch 13/20  Iteration 2266/3560 Training loss: 1.4318 0.8396 sec/batch\n",
      "Epoch 13/20  Iteration 2267/3560 Training loss: 1.4314 0.8494 sec/batch\n",
      "Epoch 13/20  Iteration 2268/3560 Training loss: 1.4312 0.8514 sec/batch\n",
      "Epoch 13/20  Iteration 2269/3560 Training loss: 1.4312 0.8616 sec/batch\n",
      "Epoch 13/20  Iteration 2270/3560 Training loss: 1.4312 0.8602 sec/batch\n",
      "Epoch 13/20  Iteration 2271/3560 Training loss: 1.4312 0.9713 sec/batch\n",
      "Epoch 13/20  Iteration 2272/3560 Training loss: 1.4312 0.8876 sec/batch\n",
      "Epoch 13/20  Iteration 2273/3560 Training loss: 1.4313 0.8922 sec/batch\n",
      "Epoch 13/20  Iteration 2274/3560 Training loss: 1.4314 0.8900 sec/batch\n",
      "Epoch 13/20  Iteration 2275/3560 Training loss: 1.4315 0.8588 sec/batch\n",
      "Epoch 13/20  Iteration 2276/3560 Training loss: 1.4315 0.8857 sec/batch\n",
      "Epoch 13/20  Iteration 2277/3560 Training loss: 1.4318 0.8534 sec/batch\n",
      "Epoch 13/20  Iteration 2278/3560 Training loss: 1.4318 0.8971 sec/batch\n",
      "Epoch 13/20  Iteration 2279/3560 Training loss: 1.4318 0.9580 sec/batch\n",
      "Epoch 13/20  Iteration 2280/3560 Training loss: 1.4320 0.9069 sec/batch\n",
      "Epoch 13/20  Iteration 2281/3560 Training loss: 1.4318 0.8910 sec/batch\n",
      "Epoch 13/20  Iteration 2282/3560 Training loss: 1.4321 0.8569 sec/batch\n",
      "Epoch 13/20  Iteration 2283/3560 Training loss: 1.4321 0.9100 sec/batch\n",
      "Epoch 13/20  Iteration 2284/3560 Training loss: 1.4323 0.8880 sec/batch\n",
      "Epoch 13/20  Iteration 2285/3560 Training loss: 1.4324 0.8554 sec/batch\n",
      "Epoch 13/20  Iteration 2286/3560 Training loss: 1.4322 0.8052 sec/batch\n",
      "Epoch 13/20  Iteration 2287/3560 Training loss: 1.4320 0.8264 sec/batch\n",
      "Epoch 13/20  Iteration 2288/3560 Training loss: 1.4320 0.8333 sec/batch\n",
      "Epoch 13/20  Iteration 2289/3560 Training loss: 1.4321 0.8330 sec/batch\n",
      "Epoch 13/20  Iteration 2290/3560 Training loss: 1.4321 0.8297 sec/batch\n",
      "Epoch 13/20  Iteration 2291/3560 Training loss: 1.4321 0.9988 sec/batch\n",
      "Epoch 13/20  Iteration 2292/3560 Training loss: 1.4321 0.8336 sec/batch\n",
      "Epoch 13/20  Iteration 2293/3560 Training loss: 1.4323 0.8610 sec/batch\n",
      "Epoch 13/20  Iteration 2294/3560 Training loss: 1.4323 0.8536 sec/batch\n",
      "Epoch 13/20  Iteration 2295/3560 Training loss: 1.4320 0.8962 sec/batch\n",
      "Epoch 13/20  Iteration 2296/3560 Training loss: 1.4321 0.8740 sec/batch\n",
      "Epoch 13/20  Iteration 2297/3560 Training loss: 1.4323 0.8725 sec/batch\n",
      "Epoch 13/20  Iteration 2298/3560 Training loss: 1.4323 0.8612 sec/batch\n",
      "Epoch 13/20  Iteration 2299/3560 Training loss: 1.4323 0.8369 sec/batch\n",
      "Epoch 13/20  Iteration 2300/3560 Training loss: 1.4324 0.8158 sec/batch\n",
      "Epoch 13/20  Iteration 2301/3560 Training loss: 1.4324 0.8488 sec/batch\n",
      "Epoch 13/20  Iteration 2302/3560 Training loss: 1.4323 0.8472 sec/batch\n",
      "Epoch 13/20  Iteration 2303/3560 Training loss: 1.4325 0.8817 sec/batch\n",
      "Epoch 13/20  Iteration 2304/3560 Training loss: 1.4329 0.8602 sec/batch\n",
      "Epoch 13/20  Iteration 2305/3560 Training loss: 1.4329 0.9155 sec/batch\n",
      "Epoch 13/20  Iteration 2306/3560 Training loss: 1.4329 1.2122 sec/batch\n",
      "Epoch 13/20  Iteration 2307/3560 Training loss: 1.4328 1.0462 sec/batch\n",
      "Epoch 13/20  Iteration 2308/3560 Training loss: 1.4327 0.9405 sec/batch\n",
      "Epoch 13/20  Iteration 2309/3560 Training loss: 1.4328 0.8709 sec/batch\n",
      "Epoch 13/20  Iteration 2310/3560 Training loss: 1.4329 0.8294 sec/batch\n",
      "Epoch 13/20  Iteration 2311/3560 Training loss: 1.4330 0.9265 sec/batch\n",
      "Epoch 13/20  Iteration 2312/3560 Training loss: 1.4329 0.9790 sec/batch\n",
      "Epoch 13/20  Iteration 2313/3560 Training loss: 1.4328 0.8698 sec/batch\n",
      "Epoch 13/20  Iteration 2314/3560 Training loss: 1.4329 0.8521 sec/batch\n",
      "Epoch 14/20  Iteration 2315/3560 Training loss: 1.5195 0.8429 sec/batch\n",
      "Epoch 14/20  Iteration 2316/3560 Training loss: 1.4807 0.8168 sec/batch\n",
      "Epoch 14/20  Iteration 2317/3560 Training loss: 1.4635 1.1884 sec/batch\n",
      "Epoch 14/20  Iteration 2318/3560 Training loss: 1.4577 1.1281 sec/batch\n",
      "Epoch 14/20  Iteration 2319/3560 Training loss: 1.4494 1.0922 sec/batch\n",
      "Epoch 14/20  Iteration 2320/3560 Training loss: 1.4376 0.8796 sec/batch\n",
      "Epoch 14/20  Iteration 2321/3560 Training loss: 1.4377 0.7929 sec/batch\n",
      "Epoch 14/20  Iteration 2322/3560 Training loss: 1.4357 0.8398 sec/batch\n",
      "Epoch 14/20  Iteration 2323/3560 Training loss: 1.4362 0.8181 sec/batch\n",
      "Epoch 14/20  Iteration 2324/3560 Training loss: 1.4349 0.8515 sec/batch\n",
      "Epoch 14/20  Iteration 2325/3560 Training loss: 1.4310 0.8377 sec/batch\n",
      "Epoch 14/20  Iteration 2326/3560 Training loss: 1.4299 0.8154 sec/batch\n",
      "Epoch 14/20  Iteration 2327/3560 Training loss: 1.4293 0.8166 sec/batch\n",
      "Epoch 14/20  Iteration 2328/3560 Training loss: 1.4306 0.8490 sec/batch\n",
      "Epoch 14/20  Iteration 2329/3560 Training loss: 1.4297 0.8215 sec/batch\n",
      "Epoch 14/20  Iteration 2330/3560 Training loss: 1.4274 0.9033 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2331/3560 Training loss: 1.4282 1.1270 sec/batch\n",
      "Epoch 14/20  Iteration 2332/3560 Training loss: 1.4297 1.0040 sec/batch\n",
      "Epoch 14/20  Iteration 2333/3560 Training loss: 1.4297 0.8587 sec/batch\n",
      "Epoch 14/20  Iteration 2334/3560 Training loss: 1.4304 0.8803 sec/batch\n",
      "Epoch 14/20  Iteration 2335/3560 Training loss: 1.4301 0.9294 sec/batch\n",
      "Epoch 14/20  Iteration 2336/3560 Training loss: 1.4305 0.9420 sec/batch\n",
      "Epoch 14/20  Iteration 2337/3560 Training loss: 1.4294 0.9080 sec/batch\n",
      "Epoch 14/20  Iteration 2338/3560 Training loss: 1.4292 0.8947 sec/batch\n",
      "Epoch 14/20  Iteration 2339/3560 Training loss: 1.4292 0.9681 sec/batch\n",
      "Epoch 14/20  Iteration 2340/3560 Training loss: 1.4278 1.0533 sec/batch\n",
      "Epoch 14/20  Iteration 2341/3560 Training loss: 1.4269 0.9835 sec/batch\n",
      "Epoch 14/20  Iteration 2342/3560 Training loss: 1.4276 0.9581 sec/batch\n",
      "Epoch 14/20  Iteration 2343/3560 Training loss: 1.4277 0.8722 sec/batch\n",
      "Epoch 14/20  Iteration 2344/3560 Training loss: 1.4281 0.8403 sec/batch\n",
      "Epoch 14/20  Iteration 2345/3560 Training loss: 1.4274 0.8132 sec/batch\n",
      "Epoch 14/20  Iteration 2346/3560 Training loss: 1.4264 0.8378 sec/batch\n",
      "Epoch 14/20  Iteration 2347/3560 Training loss: 1.4264 0.8358 sec/batch\n",
      "Epoch 14/20  Iteration 2348/3560 Training loss: 1.4266 0.8500 sec/batch\n",
      "Epoch 14/20  Iteration 2349/3560 Training loss: 1.4265 0.8296 sec/batch\n",
      "Epoch 14/20  Iteration 2350/3560 Training loss: 1.4263 0.8426 sec/batch\n",
      "Epoch 14/20  Iteration 2351/3560 Training loss: 1.4253 0.8651 sec/batch\n",
      "Epoch 14/20  Iteration 2352/3560 Training loss: 1.4242 0.7994 sec/batch\n",
      "Epoch 14/20  Iteration 2353/3560 Training loss: 1.4230 0.9601 sec/batch\n",
      "Epoch 14/20  Iteration 2354/3560 Training loss: 1.4226 0.8538 sec/batch\n",
      "Epoch 14/20  Iteration 2355/3560 Training loss: 1.4221 0.9456 sec/batch\n",
      "Epoch 14/20  Iteration 2356/3560 Training loss: 1.4230 1.0776 sec/batch\n",
      "Epoch 14/20  Iteration 2357/3560 Training loss: 1.4228 1.0594 sec/batch\n",
      "Epoch 14/20  Iteration 2358/3560 Training loss: 1.4221 1.1222 sec/batch\n",
      "Epoch 14/20  Iteration 2359/3560 Training loss: 1.4223 0.8637 sec/batch\n",
      "Epoch 14/20  Iteration 2360/3560 Training loss: 1.4214 0.8348 sec/batch\n",
      "Epoch 14/20  Iteration 2361/3560 Training loss: 1.4211 0.8913 sec/batch\n",
      "Epoch 14/20  Iteration 2362/3560 Training loss: 1.4206 0.8791 sec/batch\n",
      "Epoch 14/20  Iteration 2363/3560 Training loss: 1.4205 0.8552 sec/batch\n",
      "Epoch 14/20  Iteration 2364/3560 Training loss: 1.4206 0.8486 sec/batch\n",
      "Epoch 14/20  Iteration 2365/3560 Training loss: 1.4201 0.8251 sec/batch\n",
      "Epoch 14/20  Iteration 2366/3560 Training loss: 1.4210 0.7971 sec/batch\n",
      "Epoch 14/20  Iteration 2367/3560 Training loss: 1.4210 0.9356 sec/batch\n",
      "Epoch 14/20  Iteration 2368/3560 Training loss: 1.4213 0.9587 sec/batch\n",
      "Epoch 14/20  Iteration 2369/3560 Training loss: 1.4211 0.8540 sec/batch\n",
      "Epoch 14/20  Iteration 2370/3560 Training loss: 1.4213 0.8730 sec/batch\n",
      "Epoch 14/20  Iteration 2371/3560 Training loss: 1.4218 0.8352 sec/batch\n",
      "Epoch 14/20  Iteration 2372/3560 Training loss: 1.4215 0.8034 sec/batch\n",
      "Epoch 14/20  Iteration 2373/3560 Training loss: 1.4209 0.8791 sec/batch\n",
      "Epoch 14/20  Iteration 2374/3560 Training loss: 1.4216 0.9109 sec/batch\n",
      "Epoch 14/20  Iteration 2375/3560 Training loss: 1.4217 0.8164 sec/batch\n",
      "Epoch 14/20  Iteration 2376/3560 Training loss: 1.4226 1.0586 sec/batch\n",
      "Epoch 14/20  Iteration 2377/3560 Training loss: 1.4231 0.9746 sec/batch\n",
      "Epoch 14/20  Iteration 2378/3560 Training loss: 1.4233 0.8679 sec/batch\n",
      "Epoch 14/20  Iteration 2379/3560 Training loss: 1.4233 0.8340 sec/batch\n",
      "Epoch 14/20  Iteration 2380/3560 Training loss: 1.4234 0.8232 sec/batch\n",
      "Epoch 14/20  Iteration 2381/3560 Training loss: 1.4235 0.8307 sec/batch\n",
      "Epoch 14/20  Iteration 2382/3560 Training loss: 1.4233 1.0367 sec/batch\n",
      "Epoch 14/20  Iteration 2383/3560 Training loss: 1.4234 1.0734 sec/batch\n",
      "Epoch 14/20  Iteration 2384/3560 Training loss: 1.4232 1.2739 sec/batch\n",
      "Epoch 14/20  Iteration 2385/3560 Training loss: 1.4238 0.9153 sec/batch\n",
      "Epoch 14/20  Iteration 2386/3560 Training loss: 1.4241 0.8344 sec/batch\n",
      "Epoch 14/20  Iteration 2387/3560 Training loss: 1.4246 0.8252 sec/batch\n",
      "Epoch 14/20  Iteration 2388/3560 Training loss: 1.4242 0.8482 sec/batch\n",
      "Epoch 14/20  Iteration 2389/3560 Training loss: 1.4241 0.8357 sec/batch\n",
      "Epoch 14/20  Iteration 2390/3560 Training loss: 1.4243 1.0205 sec/batch\n",
      "Epoch 14/20  Iteration 2391/3560 Training loss: 1.4243 0.9537 sec/batch\n",
      "Epoch 14/20  Iteration 2392/3560 Training loss: 1.4243 1.0562 sec/batch\n",
      "Epoch 14/20  Iteration 2393/3560 Training loss: 1.4237 0.8622 sec/batch\n",
      "Epoch 14/20  Iteration 2394/3560 Training loss: 1.4235 0.8109 sec/batch\n",
      "Epoch 14/20  Iteration 2395/3560 Training loss: 1.4231 0.8931 sec/batch\n",
      "Epoch 14/20  Iteration 2396/3560 Training loss: 1.4229 0.9469 sec/batch\n",
      "Epoch 14/20  Iteration 2397/3560 Training loss: 1.4225 0.8032 sec/batch\n",
      "Epoch 14/20  Iteration 2398/3560 Training loss: 1.4224 0.8563 sec/batch\n",
      "Epoch 14/20  Iteration 2399/3560 Training loss: 1.4220 0.9164 sec/batch\n",
      "Epoch 14/20  Iteration 2400/3560 Training loss: 1.4219 0.8922 sec/batch\n",
      "Epoch 14/20  Iteration 2401/3560 Training loss: 1.4216 0.8310 sec/batch\n",
      "Epoch 14/20  Iteration 2402/3560 Training loss: 1.4214 0.8708 sec/batch\n",
      "Epoch 14/20  Iteration 2403/3560 Training loss: 1.4211 0.9670 sec/batch\n",
      "Epoch 14/20  Iteration 2404/3560 Training loss: 1.4210 1.1493 sec/batch\n",
      "Epoch 14/20  Iteration 2405/3560 Training loss: 1.4210 1.0451 sec/batch\n",
      "Epoch 14/20  Iteration 2406/3560 Training loss: 1.4208 1.0064 sec/batch\n",
      "Epoch 14/20  Iteration 2407/3560 Training loss: 1.4203 1.1284 sec/batch\n",
      "Epoch 14/20  Iteration 2408/3560 Training loss: 1.4200 0.8961 sec/batch\n",
      "Epoch 14/20  Iteration 2409/3560 Training loss: 1.4197 0.8388 sec/batch\n",
      "Epoch 14/20  Iteration 2410/3560 Training loss: 1.4198 0.8090 sec/batch\n",
      "Epoch 14/20  Iteration 2411/3560 Training loss: 1.4199 0.8442 sec/batch\n",
      "Epoch 14/20  Iteration 2412/3560 Training loss: 1.4195 1.0584 sec/batch\n",
      "Epoch 14/20  Iteration 2413/3560 Training loss: 1.4190 0.9593 sec/batch\n",
      "Epoch 14/20  Iteration 2414/3560 Training loss: 1.4186 0.8712 sec/batch\n",
      "Epoch 14/20  Iteration 2415/3560 Training loss: 1.4187 0.8299 sec/batch\n",
      "Epoch 14/20  Iteration 2416/3560 Training loss: 1.4186 0.8037 sec/batch\n",
      "Epoch 14/20  Iteration 2417/3560 Training loss: 1.4185 0.8514 sec/batch\n",
      "Epoch 14/20  Iteration 2418/3560 Training loss: 1.4183 0.8509 sec/batch\n",
      "Epoch 14/20  Iteration 2419/3560 Training loss: 1.4182 0.7969 sec/batch\n",
      "Epoch 14/20  Iteration 2420/3560 Training loss: 1.4180 0.8491 sec/batch\n",
      "Epoch 14/20  Iteration 2421/3560 Training loss: 1.4180 0.8123 sec/batch\n",
      "Epoch 14/20  Iteration 2422/3560 Training loss: 1.4180 0.8186 sec/batch\n",
      "Epoch 14/20  Iteration 2423/3560 Training loss: 1.4179 0.8329 sec/batch\n",
      "Epoch 14/20  Iteration 2424/3560 Training loss: 1.4180 0.8520 sec/batch\n",
      "Epoch 14/20  Iteration 2425/3560 Training loss: 1.4178 0.8921 sec/batch\n",
      "Epoch 14/20  Iteration 2426/3560 Training loss: 1.4178 0.9993 sec/batch\n",
      "Epoch 14/20  Iteration 2427/3560 Training loss: 1.4177 0.9272 sec/batch\n",
      "Epoch 14/20  Iteration 2428/3560 Training loss: 1.4176 0.9193 sec/batch\n",
      "Epoch 14/20  Iteration 2429/3560 Training loss: 1.4174 0.8621 sec/batch\n",
      "Epoch 14/20  Iteration 2430/3560 Training loss: 1.4171 0.8618 sec/batch\n",
      "Epoch 14/20  Iteration 2431/3560 Training loss: 1.4170 0.8255 sec/batch\n",
      "Epoch 14/20  Iteration 2432/3560 Training loss: 1.4171 0.8918 sec/batch\n",
      "Epoch 14/20  Iteration 2433/3560 Training loss: 1.4170 1.1816 sec/batch\n",
      "Epoch 14/20  Iteration 2434/3560 Training loss: 1.4169 1.0537 sec/batch\n",
      "Epoch 14/20  Iteration 2435/3560 Training loss: 1.4169 0.8601 sec/batch\n",
      "Epoch 14/20  Iteration 2436/3560 Training loss: 1.4166 0.9565 sec/batch\n",
      "Epoch 14/20  Iteration 2437/3560 Training loss: 1.4162 1.1277 sec/batch\n",
      "Epoch 14/20  Iteration 2438/3560 Training loss: 1.4163 1.0228 sec/batch\n",
      "Epoch 14/20  Iteration 2439/3560 Training loss: 1.4162 1.0985 sec/batch\n",
      "Epoch 14/20  Iteration 2440/3560 Training loss: 1.4158 1.1042 sec/batch\n",
      "Epoch 14/20  Iteration 2441/3560 Training loss: 1.4159 0.9743 sec/batch\n",
      "Epoch 14/20  Iteration 2442/3560 Training loss: 1.4159 1.2088 sec/batch\n",
      "Epoch 14/20  Iteration 2443/3560 Training loss: 1.4157 1.0536 sec/batch\n",
      "Epoch 14/20  Iteration 2444/3560 Training loss: 1.4154 0.9986 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2445/3560 Training loss: 1.4150 0.9029 sec/batch\n",
      "Epoch 14/20  Iteration 2446/3560 Training loss: 1.4148 0.9424 sec/batch\n",
      "Epoch 14/20  Iteration 2447/3560 Training loss: 1.4149 0.8448 sec/batch\n",
      "Epoch 14/20  Iteration 2448/3560 Training loss: 1.4149 0.8433 sec/batch\n",
      "Epoch 14/20  Iteration 2449/3560 Training loss: 1.4150 0.9775 sec/batch\n",
      "Epoch 14/20  Iteration 2450/3560 Training loss: 1.4150 0.8780 sec/batch\n",
      "Epoch 14/20  Iteration 2451/3560 Training loss: 1.4152 0.9096 sec/batch\n",
      "Epoch 14/20  Iteration 2452/3560 Training loss: 1.4152 0.8233 sec/batch\n",
      "Epoch 14/20  Iteration 2453/3560 Training loss: 1.4152 0.8207 sec/batch\n",
      "Epoch 14/20  Iteration 2454/3560 Training loss: 1.4152 0.8507 sec/batch\n",
      "Epoch 14/20  Iteration 2455/3560 Training loss: 1.4155 0.7974 sec/batch\n",
      "Epoch 14/20  Iteration 2456/3560 Training loss: 1.4155 0.8415 sec/batch\n",
      "Epoch 14/20  Iteration 2457/3560 Training loss: 1.4154 0.8533 sec/batch\n",
      "Epoch 14/20  Iteration 2458/3560 Training loss: 1.4156 0.8493 sec/batch\n",
      "Epoch 14/20  Iteration 2459/3560 Training loss: 1.4154 0.8369 sec/batch\n",
      "Epoch 14/20  Iteration 2460/3560 Training loss: 1.4156 0.8192 sec/batch\n",
      "Epoch 14/20  Iteration 2461/3560 Training loss: 1.4157 0.8707 sec/batch\n",
      "Epoch 14/20  Iteration 2462/3560 Training loss: 1.4159 0.9100 sec/batch\n",
      "Epoch 14/20  Iteration 2463/3560 Training loss: 1.4161 0.8319 sec/batch\n",
      "Epoch 14/20  Iteration 2464/3560 Training loss: 1.4160 0.8631 sec/batch\n",
      "Epoch 14/20  Iteration 2465/3560 Training loss: 1.4158 0.8395 sec/batch\n",
      "Epoch 14/20  Iteration 2466/3560 Training loss: 1.4157 0.9342 sec/batch\n",
      "Epoch 14/20  Iteration 2467/3560 Training loss: 1.4158 0.9252 sec/batch\n",
      "Epoch 14/20  Iteration 2468/3560 Training loss: 1.4159 0.8498 sec/batch\n",
      "Epoch 14/20  Iteration 2469/3560 Training loss: 1.4159 0.8642 sec/batch\n",
      "Epoch 14/20  Iteration 2470/3560 Training loss: 1.4159 1.1256 sec/batch\n",
      "Epoch 14/20  Iteration 2471/3560 Training loss: 1.4159 0.8686 sec/batch\n",
      "Epoch 14/20  Iteration 2472/3560 Training loss: 1.4159 0.8855 sec/batch\n",
      "Epoch 14/20  Iteration 2473/3560 Training loss: 1.4158 0.8866 sec/batch\n",
      "Epoch 14/20  Iteration 2474/3560 Training loss: 1.4159 0.8117 sec/batch\n",
      "Epoch 14/20  Iteration 2475/3560 Training loss: 1.4161 0.8565 sec/batch\n",
      "Epoch 14/20  Iteration 2476/3560 Training loss: 1.4161 0.8437 sec/batch\n",
      "Epoch 14/20  Iteration 2477/3560 Training loss: 1.4161 1.1224 sec/batch\n",
      "Epoch 14/20  Iteration 2478/3560 Training loss: 1.4162 0.9213 sec/batch\n",
      "Epoch 14/20  Iteration 2479/3560 Training loss: 1.4161 0.8538 sec/batch\n",
      "Epoch 14/20  Iteration 2480/3560 Training loss: 1.4161 0.8446 sec/batch\n",
      "Epoch 14/20  Iteration 2481/3560 Training loss: 1.4163 0.8906 sec/batch\n",
      "Epoch 14/20  Iteration 2482/3560 Training loss: 1.4167 0.8433 sec/batch\n",
      "Epoch 14/20  Iteration 2483/3560 Training loss: 1.4168 0.8569 sec/batch\n",
      "Epoch 14/20  Iteration 2484/3560 Training loss: 1.4168 1.0229 sec/batch\n",
      "Epoch 14/20  Iteration 2485/3560 Training loss: 1.4167 0.8466 sec/batch\n",
      "Epoch 14/20  Iteration 2486/3560 Training loss: 1.4166 0.8980 sec/batch\n",
      "Epoch 14/20  Iteration 2487/3560 Training loss: 1.4167 0.8267 sec/batch\n",
      "Epoch 14/20  Iteration 2488/3560 Training loss: 1.4167 0.8350 sec/batch\n",
      "Epoch 14/20  Iteration 2489/3560 Training loss: 1.4168 0.8388 sec/batch\n",
      "Epoch 14/20  Iteration 2490/3560 Training loss: 1.4167 0.8642 sec/batch\n",
      "Epoch 14/20  Iteration 2491/3560 Training loss: 1.4165 0.8964 sec/batch\n",
      "Epoch 14/20  Iteration 2492/3560 Training loss: 1.4167 1.0183 sec/batch\n",
      "Epoch 15/20  Iteration 2493/3560 Training loss: 1.5055 0.9441 sec/batch\n",
      "Epoch 15/20  Iteration 2494/3560 Training loss: 1.4673 0.8640 sec/batch\n",
      "Epoch 15/20  Iteration 2495/3560 Training loss: 1.4529 0.9319 sec/batch\n",
      "Epoch 15/20  Iteration 2496/3560 Training loss: 1.4435 0.8447 sec/batch\n",
      "Epoch 15/20  Iteration 2497/3560 Training loss: 1.4347 1.0033 sec/batch\n",
      "Epoch 15/20  Iteration 2498/3560 Training loss: 1.4232 0.8236 sec/batch\n",
      "Epoch 15/20  Iteration 2499/3560 Training loss: 1.4234 0.8280 sec/batch\n",
      "Epoch 15/20  Iteration 2500/3560 Training loss: 1.4204 0.8451 sec/batch\n",
      "Epoch 15/20  Iteration 2501/3560 Training loss: 1.4217 1.0311 sec/batch\n",
      "Epoch 15/20  Iteration 2502/3560 Training loss: 1.4211 0.9067 sec/batch\n",
      "Epoch 15/20  Iteration 2503/3560 Training loss: 1.4180 0.8401 sec/batch\n",
      "Epoch 15/20  Iteration 2504/3560 Training loss: 1.4178 0.8491 sec/batch\n",
      "Epoch 15/20  Iteration 2505/3560 Training loss: 1.4165 0.8336 sec/batch\n",
      "Epoch 15/20  Iteration 2506/3560 Training loss: 1.4175 0.8733 sec/batch\n",
      "Epoch 15/20  Iteration 2507/3560 Training loss: 1.4166 0.8904 sec/batch\n",
      "Epoch 15/20  Iteration 2508/3560 Training loss: 1.4146 0.8169 sec/batch\n",
      "Epoch 15/20  Iteration 2509/3560 Training loss: 1.4153 1.0582 sec/batch\n",
      "Epoch 15/20  Iteration 2510/3560 Training loss: 1.4172 1.1249 sec/batch\n",
      "Epoch 15/20  Iteration 2511/3560 Training loss: 1.4165 1.0045 sec/batch\n",
      "Epoch 15/20  Iteration 2512/3560 Training loss: 1.4175 1.1179 sec/batch\n",
      "Epoch 15/20  Iteration 2513/3560 Training loss: 1.4170 0.8814 sec/batch\n",
      "Epoch 15/20  Iteration 2514/3560 Training loss: 1.4173 0.8189 sec/batch\n",
      "Epoch 15/20  Iteration 2515/3560 Training loss: 1.4164 0.8484 sec/batch\n",
      "Epoch 15/20  Iteration 2516/3560 Training loss: 1.4163 0.8366 sec/batch\n",
      "Epoch 15/20  Iteration 2517/3560 Training loss: 1.4161 0.8869 sec/batch\n",
      "Epoch 15/20  Iteration 2518/3560 Training loss: 1.4148 0.8314 sec/batch\n",
      "Epoch 15/20  Iteration 2519/3560 Training loss: 1.4137 0.8315 sec/batch\n",
      "Epoch 15/20  Iteration 2520/3560 Training loss: 1.4141 0.8457 sec/batch\n",
      "Epoch 15/20  Iteration 2521/3560 Training loss: 1.4144 0.8456 sec/batch\n",
      "Epoch 15/20  Iteration 2522/3560 Training loss: 1.4145 0.8190 sec/batch\n",
      "Epoch 15/20  Iteration 2523/3560 Training loss: 1.4139 0.8423 sec/batch\n",
      "Epoch 15/20  Iteration 2524/3560 Training loss: 1.4131 0.8214 sec/batch\n",
      "Epoch 15/20  Iteration 2525/3560 Training loss: 1.4132 0.8616 sec/batch\n",
      "Epoch 15/20  Iteration 2526/3560 Training loss: 1.4134 0.8433 sec/batch\n",
      "Epoch 15/20  Iteration 2527/3560 Training loss: 1.4132 0.8119 sec/batch\n",
      "Epoch 15/20  Iteration 2528/3560 Training loss: 1.4129 0.8194 sec/batch\n",
      "Epoch 15/20  Iteration 2529/3560 Training loss: 1.4121 1.0196 sec/batch\n",
      "Epoch 15/20  Iteration 2530/3560 Training loss: 1.4110 0.8320 sec/batch\n",
      "Epoch 15/20  Iteration 2531/3560 Training loss: 1.4098 0.8270 sec/batch\n",
      "Epoch 15/20  Iteration 2532/3560 Training loss: 1.4097 0.8528 sec/batch\n",
      "Epoch 15/20  Iteration 2533/3560 Training loss: 1.4091 0.8351 sec/batch\n",
      "Epoch 15/20  Iteration 2534/3560 Training loss: 1.4100 1.1041 sec/batch\n",
      "Epoch 15/20  Iteration 2535/3560 Training loss: 1.4096 1.0893 sec/batch\n",
      "Epoch 15/20  Iteration 2536/3560 Training loss: 1.4090 0.9948 sec/batch\n",
      "Epoch 15/20  Iteration 2537/3560 Training loss: 1.4090 0.9736 sec/batch\n",
      "Epoch 15/20  Iteration 2538/3560 Training loss: 1.4080 0.8763 sec/batch\n",
      "Epoch 15/20  Iteration 2539/3560 Training loss: 1.4076 0.8670 sec/batch\n",
      "Epoch 15/20  Iteration 2540/3560 Training loss: 1.4072 0.9168 sec/batch\n",
      "Epoch 15/20  Iteration 2541/3560 Training loss: 1.4070 0.9438 sec/batch\n",
      "Epoch 15/20  Iteration 2542/3560 Training loss: 1.4073 0.8779 sec/batch\n",
      "Epoch 15/20  Iteration 2543/3560 Training loss: 1.4068 1.1627 sec/batch\n",
      "Epoch 15/20  Iteration 2544/3560 Training loss: 1.4076 0.9773 sec/batch\n",
      "Epoch 15/20  Iteration 2545/3560 Training loss: 1.4074 0.9302 sec/batch\n",
      "Epoch 15/20  Iteration 2546/3560 Training loss: 1.4075 0.9042 sec/batch\n",
      "Epoch 15/20  Iteration 2547/3560 Training loss: 1.4074 0.8918 sec/batch\n",
      "Epoch 15/20  Iteration 2548/3560 Training loss: 1.4075 0.8434 sec/batch\n",
      "Epoch 15/20  Iteration 2549/3560 Training loss: 1.4079 0.8173 sec/batch\n",
      "Epoch 15/20  Iteration 2550/3560 Training loss: 1.4076 0.8308 sec/batch\n",
      "Epoch 15/20  Iteration 2551/3560 Training loss: 1.4071 0.8726 sec/batch\n",
      "Epoch 15/20  Iteration 2552/3560 Training loss: 1.4076 0.8908 sec/batch\n",
      "Epoch 15/20  Iteration 2553/3560 Training loss: 1.4076 0.9254 sec/batch\n",
      "Epoch 15/20  Iteration 2554/3560 Training loss: 1.4084 0.8995 sec/batch\n",
      "Epoch 15/20  Iteration 2555/3560 Training loss: 1.4088 0.9051 sec/batch\n",
      "Epoch 15/20  Iteration 2556/3560 Training loss: 1.4089 0.8285 sec/batch\n",
      "Epoch 15/20  Iteration 2557/3560 Training loss: 1.4089 0.9598 sec/batch\n",
      "Epoch 15/20  Iteration 2558/3560 Training loss: 1.4091 0.9116 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2559/3560 Training loss: 1.4093 0.8724 sec/batch\n",
      "Epoch 15/20  Iteration 2560/3560 Training loss: 1.4090 1.0609 sec/batch\n",
      "Epoch 15/20  Iteration 2561/3560 Training loss: 1.4089 1.0969 sec/batch\n",
      "Epoch 15/20  Iteration 2562/3560 Training loss: 1.4088 1.0386 sec/batch\n",
      "Epoch 15/20  Iteration 2563/3560 Training loss: 1.4093 1.2374 sec/batch\n",
      "Epoch 15/20  Iteration 2564/3560 Training loss: 1.4095 1.1383 sec/batch\n",
      "Epoch 15/20  Iteration 2565/3560 Training loss: 1.4100 0.9915 sec/batch\n",
      "Epoch 15/20  Iteration 2566/3560 Training loss: 1.4097 0.8701 sec/batch\n",
      "Epoch 15/20  Iteration 2567/3560 Training loss: 1.4096 1.1764 sec/batch\n",
      "Epoch 15/20  Iteration 2568/3560 Training loss: 1.4096 1.3535 sec/batch\n",
      "Epoch 15/20  Iteration 2569/3560 Training loss: 1.4095 1.3912 sec/batch\n",
      "Epoch 15/20  Iteration 2570/3560 Training loss: 1.4095 1.1805 sec/batch\n",
      "Epoch 15/20  Iteration 2571/3560 Training loss: 1.4091 1.5488 sec/batch\n",
      "Epoch 15/20  Iteration 2572/3560 Training loss: 1.4090 0.9981 sec/batch\n",
      "Epoch 15/20  Iteration 2573/3560 Training loss: 1.4084 0.8579 sec/batch\n",
      "Epoch 15/20  Iteration 2574/3560 Training loss: 1.4083 0.8026 sec/batch\n",
      "Epoch 15/20  Iteration 2575/3560 Training loss: 1.4078 0.8510 sec/batch\n",
      "Epoch 15/20  Iteration 2576/3560 Training loss: 1.4079 0.8360 sec/batch\n",
      "Epoch 15/20  Iteration 2577/3560 Training loss: 1.4075 0.8005 sec/batch\n",
      "Epoch 15/20  Iteration 2578/3560 Training loss: 1.4073 0.8256 sec/batch\n",
      "Epoch 15/20  Iteration 2579/3560 Training loss: 1.4071 0.8409 sec/batch\n",
      "Epoch 15/20  Iteration 2580/3560 Training loss: 1.4069 0.8784 sec/batch\n",
      "Epoch 15/20  Iteration 2581/3560 Training loss: 1.4066 0.9047 sec/batch\n",
      "Epoch 15/20  Iteration 2582/3560 Training loss: 1.4068 0.8499 sec/batch\n",
      "Epoch 15/20  Iteration 2583/3560 Training loss: 1.4066 0.9245 sec/batch\n",
      "Epoch 15/20  Iteration 2584/3560 Training loss: 1.4065 0.9658 sec/batch\n",
      "Epoch 15/20  Iteration 2585/3560 Training loss: 1.4061 0.8833 sec/batch\n",
      "Epoch 15/20  Iteration 2586/3560 Training loss: 1.4059 0.8417 sec/batch\n",
      "Epoch 15/20  Iteration 2587/3560 Training loss: 1.4056 0.8125 sec/batch\n",
      "Epoch 15/20  Iteration 2588/3560 Training loss: 1.4057 0.7975 sec/batch\n",
      "Epoch 15/20  Iteration 2589/3560 Training loss: 1.4058 0.8321 sec/batch\n",
      "Epoch 15/20  Iteration 2590/3560 Training loss: 1.4055 0.8244 sec/batch\n",
      "Epoch 15/20  Iteration 2591/3560 Training loss: 1.4051 0.8205 sec/batch\n",
      "Epoch 15/20  Iteration 2592/3560 Training loss: 1.4048 0.9401 sec/batch\n",
      "Epoch 15/20  Iteration 2593/3560 Training loss: 1.4048 1.3308 sec/batch\n",
      "Epoch 15/20  Iteration 2594/3560 Training loss: 1.4048 1.2944 sec/batch\n",
      "Epoch 15/20  Iteration 2595/3560 Training loss: 1.4047 1.1140 sec/batch\n",
      "Epoch 15/20  Iteration 2596/3560 Training loss: 1.4045 1.0911 sec/batch\n",
      "Epoch 15/20  Iteration 2597/3560 Training loss: 1.4043 1.2195 sec/batch\n",
      "Epoch 15/20  Iteration 2598/3560 Training loss: 1.4042 1.2553 sec/batch\n",
      "Epoch 15/20  Iteration 2599/3560 Training loss: 1.4042 1.3945 sec/batch\n",
      "Epoch 15/20  Iteration 2600/3560 Training loss: 1.4042 1.0135 sec/batch\n",
      "Epoch 15/20  Iteration 2601/3560 Training loss: 1.4041 1.2631 sec/batch\n",
      "Epoch 15/20  Iteration 2602/3560 Training loss: 1.4042 1.2384 sec/batch\n",
      "Epoch 15/20  Iteration 2603/3560 Training loss: 1.4041 1.1872 sec/batch\n",
      "Epoch 15/20  Iteration 2604/3560 Training loss: 1.4041 1.1837 sec/batch\n",
      "Epoch 15/20  Iteration 2605/3560 Training loss: 1.4039 1.5494 sec/batch\n",
      "Epoch 15/20  Iteration 2606/3560 Training loss: 1.4038 1.1666 sec/batch\n",
      "Epoch 15/20  Iteration 2607/3560 Training loss: 1.4036 1.0748 sec/batch\n",
      "Epoch 15/20  Iteration 2608/3560 Training loss: 1.4033 1.0060 sec/batch\n",
      "Epoch 15/20  Iteration 2609/3560 Training loss: 1.4032 0.8494 sec/batch\n",
      "Epoch 15/20  Iteration 2610/3560 Training loss: 1.4032 0.7589 sec/batch\n",
      "Epoch 15/20  Iteration 2611/3560 Training loss: 1.4032 0.8040 sec/batch\n",
      "Epoch 15/20  Iteration 2612/3560 Training loss: 1.4032 0.8016 sec/batch\n",
      "Epoch 15/20  Iteration 2613/3560 Training loss: 1.4031 0.7784 sec/batch\n",
      "Epoch 15/20  Iteration 2614/3560 Training loss: 1.4027 0.7849 sec/batch\n",
      "Epoch 15/20  Iteration 2615/3560 Training loss: 1.4023 0.8206 sec/batch\n",
      "Epoch 15/20  Iteration 2616/3560 Training loss: 1.4023 0.7793 sec/batch\n",
      "Epoch 15/20  Iteration 2617/3560 Training loss: 1.4022 0.7635 sec/batch\n",
      "Epoch 15/20  Iteration 2618/3560 Training loss: 1.4019 0.7623 sec/batch\n",
      "Epoch 15/20  Iteration 2619/3560 Training loss: 1.4019 0.7999 sec/batch\n",
      "Epoch 15/20  Iteration 2620/3560 Training loss: 1.4019 0.8154 sec/batch\n",
      "Epoch 15/20  Iteration 2621/3560 Training loss: 1.4017 0.7912 sec/batch\n",
      "Epoch 15/20  Iteration 2622/3560 Training loss: 1.4015 0.7981 sec/batch\n",
      "Epoch 15/20  Iteration 2623/3560 Training loss: 1.4011 1.0008 sec/batch\n",
      "Epoch 15/20  Iteration 2624/3560 Training loss: 1.4009 1.0527 sec/batch\n",
      "Epoch 15/20  Iteration 2625/3560 Training loss: 1.4010 1.0532 sec/batch\n",
      "Epoch 15/20  Iteration 2626/3560 Training loss: 1.4010 0.7198 sec/batch\n",
      "Epoch 15/20  Iteration 2627/3560 Training loss: 1.4010 0.7284 sec/batch\n",
      "Epoch 15/20  Iteration 2628/3560 Training loss: 1.4010 0.7537 sec/batch\n",
      "Epoch 15/20  Iteration 2629/3560 Training loss: 1.4011 0.7855 sec/batch\n",
      "Epoch 15/20  Iteration 2630/3560 Training loss: 1.4012 0.7243 sec/batch\n",
      "Epoch 15/20  Iteration 2631/3560 Training loss: 1.4012 0.7585 sec/batch\n",
      "Epoch 15/20  Iteration 2632/3560 Training loss: 1.4012 0.7604 sec/batch\n",
      "Epoch 15/20  Iteration 2633/3560 Training loss: 1.4016 0.7586 sec/batch\n",
      "Epoch 15/20  Iteration 2634/3560 Training loss: 1.4015 0.7513 sec/batch\n",
      "Epoch 15/20  Iteration 2635/3560 Training loss: 1.4014 0.7765 sec/batch\n",
      "Epoch 15/20  Iteration 2636/3560 Training loss: 1.4016 0.9212 sec/batch\n",
      "Epoch 15/20  Iteration 2637/3560 Training loss: 1.4015 0.8000 sec/batch\n",
      "Epoch 15/20  Iteration 2638/3560 Training loss: 1.4017 0.7372 sec/batch\n",
      "Epoch 15/20  Iteration 2639/3560 Training loss: 1.4017 0.7599 sec/batch\n",
      "Epoch 15/20  Iteration 2640/3560 Training loss: 1.4020 0.7490 sec/batch\n",
      "Epoch 15/20  Iteration 2641/3560 Training loss: 1.4022 0.7271 sec/batch\n",
      "Epoch 15/20  Iteration 2642/3560 Training loss: 1.4021 0.7646 sec/batch\n",
      "Epoch 15/20  Iteration 2643/3560 Training loss: 1.4018 0.7589 sec/batch\n",
      "Epoch 15/20  Iteration 2644/3560 Training loss: 1.4017 0.7711 sec/batch\n",
      "Epoch 15/20  Iteration 2645/3560 Training loss: 1.4018 0.7645 sec/batch\n",
      "Epoch 15/20  Iteration 2646/3560 Training loss: 1.4018 0.7811 sec/batch\n",
      "Epoch 15/20  Iteration 2647/3560 Training loss: 1.4018 0.7584 sec/batch\n",
      "Epoch 15/20  Iteration 2648/3560 Training loss: 1.4018 0.7675 sec/batch\n",
      "Epoch 15/20  Iteration 2649/3560 Training loss: 1.4019 0.7915 sec/batch\n",
      "Epoch 15/20  Iteration 2650/3560 Training loss: 1.4019 0.7584 sec/batch\n",
      "Epoch 15/20  Iteration 2651/3560 Training loss: 1.4016 0.7860 sec/batch\n",
      "Epoch 15/20  Iteration 2652/3560 Training loss: 1.4018 0.7328 sec/batch\n",
      "Epoch 15/20  Iteration 2653/3560 Training loss: 1.4020 0.7784 sec/batch\n",
      "Epoch 15/20  Iteration 2654/3560 Training loss: 1.4020 0.7465 sec/batch\n",
      "Epoch 15/20  Iteration 2655/3560 Training loss: 1.4020 0.7636 sec/batch\n",
      "Epoch 15/20  Iteration 2656/3560 Training loss: 1.4020 0.7498 sec/batch\n",
      "Epoch 15/20  Iteration 2657/3560 Training loss: 1.4020 0.7484 sec/batch\n",
      "Epoch 15/20  Iteration 2658/3560 Training loss: 1.4020 0.7735 sec/batch\n",
      "Epoch 15/20  Iteration 2659/3560 Training loss: 1.4022 0.7697 sec/batch\n",
      "Epoch 15/20  Iteration 2660/3560 Training loss: 1.4026 0.7574 sec/batch\n",
      "Epoch 15/20  Iteration 2661/3560 Training loss: 1.4027 0.7368 sec/batch\n",
      "Epoch 15/20  Iteration 2662/3560 Training loss: 1.4027 0.7898 sec/batch\n",
      "Epoch 15/20  Iteration 2663/3560 Training loss: 1.4027 0.7498 sec/batch\n",
      "Epoch 15/20  Iteration 2664/3560 Training loss: 1.4025 0.7436 sec/batch\n",
      "Epoch 15/20  Iteration 2665/3560 Training loss: 1.4027 0.7532 sec/batch\n",
      "Epoch 15/20  Iteration 2666/3560 Training loss: 1.4027 0.7428 sec/batch\n",
      "Epoch 15/20  Iteration 2667/3560 Training loss: 1.4028 0.7279 sec/batch\n",
      "Epoch 15/20  Iteration 2668/3560 Training loss: 1.4027 0.7396 sec/batch\n",
      "Epoch 15/20  Iteration 2669/3560 Training loss: 1.4026 0.7509 sec/batch\n",
      "Epoch 15/20  Iteration 2670/3560 Training loss: 1.4027 0.7369 sec/batch\n",
      "Epoch 16/20  Iteration 2671/3560 Training loss: 1.4860 0.7712 sec/batch\n",
      "Epoch 16/20  Iteration 2672/3560 Training loss: 1.4472 0.7546 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2673/3560 Training loss: 1.4302 0.7818 sec/batch\n",
      "Epoch 16/20  Iteration 2674/3560 Training loss: 1.4289 0.7708 sec/batch\n",
      "Epoch 16/20  Iteration 2675/3560 Training loss: 1.4191 0.7391 sec/batch\n",
      "Epoch 16/20  Iteration 2676/3560 Training loss: 1.4093 0.8118 sec/batch\n",
      "Epoch 16/20  Iteration 2677/3560 Training loss: 1.4105 0.7414 sec/batch\n",
      "Epoch 16/20  Iteration 2678/3560 Training loss: 1.4083 0.7614 sec/batch\n",
      "Epoch 16/20  Iteration 2679/3560 Training loss: 1.4085 0.7486 sec/batch\n",
      "Epoch 16/20  Iteration 2680/3560 Training loss: 1.4072 0.7636 sec/batch\n",
      "Epoch 16/20  Iteration 2681/3560 Training loss: 1.4040 0.9034 sec/batch\n",
      "Epoch 16/20  Iteration 2682/3560 Training loss: 1.4041 1.0196 sec/batch\n",
      "Epoch 16/20  Iteration 2683/3560 Training loss: 1.4034 1.0371 sec/batch\n",
      "Epoch 16/20  Iteration 2684/3560 Training loss: 1.4049 0.9956 sec/batch\n",
      "Epoch 16/20  Iteration 2685/3560 Training loss: 1.4036 0.7458 sec/batch\n",
      "Epoch 16/20  Iteration 2686/3560 Training loss: 1.4017 0.7601 sec/batch\n",
      "Epoch 16/20  Iteration 2687/3560 Training loss: 1.4021 0.7342 sec/batch\n",
      "Epoch 16/20  Iteration 2688/3560 Training loss: 1.4033 0.8024 sec/batch\n",
      "Epoch 16/20  Iteration 2689/3560 Training loss: 1.4030 0.7558 sec/batch\n",
      "Epoch 16/20  Iteration 2690/3560 Training loss: 1.4038 0.7816 sec/batch\n",
      "Epoch 16/20  Iteration 2691/3560 Training loss: 1.4030 0.7679 sec/batch\n",
      "Epoch 16/20  Iteration 2692/3560 Training loss: 1.4035 0.7343 sec/batch\n",
      "Epoch 16/20  Iteration 2693/3560 Training loss: 1.4028 0.7577 sec/batch\n",
      "Epoch 16/20  Iteration 2694/3560 Training loss: 1.4026 0.7387 sec/batch\n",
      "Epoch 16/20  Iteration 2695/3560 Training loss: 1.4023 0.7303 sec/batch\n",
      "Epoch 16/20  Iteration 2696/3560 Training loss: 1.4009 0.7640 sec/batch\n",
      "Epoch 16/20  Iteration 2697/3560 Training loss: 1.4001 0.7661 sec/batch\n",
      "Epoch 16/20  Iteration 2698/3560 Training loss: 1.4007 0.7550 sec/batch\n",
      "Epoch 16/20  Iteration 2699/3560 Training loss: 1.4009 0.7398 sec/batch\n",
      "Epoch 16/20  Iteration 2700/3560 Training loss: 1.4013 0.7384 sec/batch\n",
      "Epoch 16/20  Iteration 2701/3560 Training loss: 1.4007 0.8027 sec/batch\n",
      "Epoch 16/20  Iteration 2702/3560 Training loss: 1.3997 0.7707 sec/batch\n",
      "Epoch 16/20  Iteration 2703/3560 Training loss: 1.3998 0.7656 sec/batch\n",
      "Epoch 16/20  Iteration 2704/3560 Training loss: 1.3999 0.7653 sec/batch\n",
      "Epoch 16/20  Iteration 2705/3560 Training loss: 1.3997 0.7534 sec/batch\n",
      "Epoch 16/20  Iteration 2706/3560 Training loss: 1.3996 0.7367 sec/batch\n",
      "Epoch 16/20  Iteration 2707/3560 Training loss: 1.3989 0.7790 sec/batch\n",
      "Epoch 16/20  Iteration 2708/3560 Training loss: 1.3979 0.8237 sec/batch\n",
      "Epoch 16/20  Iteration 2709/3560 Training loss: 1.3966 0.7746 sec/batch\n",
      "Epoch 16/20  Iteration 2710/3560 Training loss: 1.3962 0.7790 sec/batch\n",
      "Epoch 16/20  Iteration 2711/3560 Training loss: 1.3956 0.7427 sec/batch\n",
      "Epoch 16/20  Iteration 2712/3560 Training loss: 1.3963 0.7657 sec/batch\n",
      "Epoch 16/20  Iteration 2713/3560 Training loss: 1.3960 0.7626 sec/batch\n",
      "Epoch 16/20  Iteration 2714/3560 Training loss: 1.3952 0.7952 sec/batch\n",
      "Epoch 16/20  Iteration 2715/3560 Training loss: 1.3955 0.7962 sec/batch\n",
      "Epoch 16/20  Iteration 2716/3560 Training loss: 1.3945 0.7446 sec/batch\n",
      "Epoch 16/20  Iteration 2717/3560 Training loss: 1.3943 0.7816 sec/batch\n",
      "Epoch 16/20  Iteration 2718/3560 Training loss: 1.3939 0.7489 sec/batch\n",
      "Epoch 16/20  Iteration 2719/3560 Training loss: 1.3937 0.7454 sec/batch\n",
      "Epoch 16/20  Iteration 2720/3560 Training loss: 1.3938 0.7623 sec/batch\n",
      "Epoch 16/20  Iteration 2721/3560 Training loss: 1.3934 0.7689 sec/batch\n",
      "Epoch 16/20  Iteration 2722/3560 Training loss: 1.3939 0.8016 sec/batch\n",
      "Epoch 16/20  Iteration 2723/3560 Training loss: 1.3939 0.7765 sec/batch\n",
      "Epoch 16/20  Iteration 2724/3560 Training loss: 1.3941 0.7795 sec/batch\n",
      "Epoch 16/20  Iteration 2725/3560 Training loss: 1.3938 0.7421 sec/batch\n",
      "Epoch 16/20  Iteration 2726/3560 Training loss: 1.3938 0.7413 sec/batch\n",
      "Epoch 16/20  Iteration 2727/3560 Training loss: 1.3941 0.7621 sec/batch\n",
      "Epoch 16/20  Iteration 2728/3560 Training loss: 1.3939 0.7903 sec/batch\n",
      "Epoch 16/20  Iteration 2729/3560 Training loss: 1.3934 0.7493 sec/batch\n",
      "Epoch 16/20  Iteration 2730/3560 Training loss: 1.3939 0.7751 sec/batch\n",
      "Epoch 16/20  Iteration 2731/3560 Training loss: 1.3940 0.7602 sec/batch\n",
      "Epoch 16/20  Iteration 2732/3560 Training loss: 1.3949 0.7682 sec/batch\n",
      "Epoch 16/20  Iteration 2733/3560 Training loss: 1.3953 0.7443 sec/batch\n",
      "Epoch 16/20  Iteration 2734/3560 Training loss: 1.3954 0.7264 sec/batch\n",
      "Epoch 16/20  Iteration 2735/3560 Training loss: 1.3954 0.8474 sec/batch\n",
      "Epoch 16/20  Iteration 2736/3560 Training loss: 1.3954 0.9726 sec/batch\n",
      "Epoch 16/20  Iteration 2737/3560 Training loss: 1.3956 1.0019 sec/batch\n",
      "Epoch 16/20  Iteration 2738/3560 Training loss: 1.3954 0.9914 sec/batch\n",
      "Epoch 16/20  Iteration 2739/3560 Training loss: 1.3954 0.8295 sec/batch\n",
      "Epoch 16/20  Iteration 2740/3560 Training loss: 1.3953 0.8485 sec/batch\n",
      "Epoch 16/20  Iteration 2741/3560 Training loss: 1.3957 0.7427 sec/batch\n",
      "Epoch 16/20  Iteration 2742/3560 Training loss: 1.3958 0.9015 sec/batch\n",
      "Epoch 16/20  Iteration 2743/3560 Training loss: 1.3963 0.7937 sec/batch\n",
      "Epoch 16/20  Iteration 2744/3560 Training loss: 1.3958 0.7470 sec/batch\n",
      "Epoch 16/20  Iteration 2745/3560 Training loss: 1.3957 0.8608 sec/batch\n",
      "Epoch 16/20  Iteration 2746/3560 Training loss: 1.3958 0.8438 sec/batch\n",
      "Epoch 16/20  Iteration 2747/3560 Training loss: 1.3957 0.8922 sec/batch\n",
      "Epoch 16/20  Iteration 2748/3560 Training loss: 1.3957 0.7937 sec/batch\n",
      "Epoch 16/20  Iteration 2749/3560 Training loss: 1.3951 0.7393 sec/batch\n",
      "Epoch 16/20  Iteration 2750/3560 Training loss: 1.3950 0.7154 sec/batch\n",
      "Epoch 16/20  Iteration 2751/3560 Training loss: 1.3945 0.7127 sec/batch\n",
      "Epoch 16/20  Iteration 2752/3560 Training loss: 1.3945 0.7412 sec/batch\n",
      "Epoch 16/20  Iteration 2753/3560 Training loss: 1.3940 0.7345 sec/batch\n",
      "Epoch 16/20  Iteration 2754/3560 Training loss: 1.3939 0.7346 sec/batch\n",
      "Epoch 16/20  Iteration 2755/3560 Training loss: 1.3936 0.7074 sec/batch\n",
      "Epoch 16/20  Iteration 2756/3560 Training loss: 1.3935 0.7415 sec/batch\n",
      "Epoch 16/20  Iteration 2757/3560 Training loss: 1.3933 0.7244 sec/batch\n",
      "Epoch 16/20  Iteration 2758/3560 Training loss: 1.3931 0.7560 sec/batch\n",
      "Epoch 16/20  Iteration 2759/3560 Training loss: 1.3927 0.7159 sec/batch\n",
      "Epoch 16/20  Iteration 2760/3560 Training loss: 1.3928 0.7175 sec/batch\n",
      "Epoch 16/20  Iteration 2761/3560 Training loss: 1.3926 0.7367 sec/batch\n",
      "Epoch 16/20  Iteration 2762/3560 Training loss: 1.3925 0.7123 sec/batch\n",
      "Epoch 16/20  Iteration 2763/3560 Training loss: 1.3921 0.7329 sec/batch\n",
      "Epoch 16/20  Iteration 2764/3560 Training loss: 1.3918 0.7123 sec/batch\n",
      "Epoch 16/20  Iteration 2765/3560 Training loss: 1.3915 0.7359 sec/batch\n",
      "Epoch 16/20  Iteration 2766/3560 Training loss: 1.3916 0.7529 sec/batch\n",
      "Epoch 16/20  Iteration 2767/3560 Training loss: 1.3916 0.7228 sec/batch\n",
      "Epoch 16/20  Iteration 2768/3560 Training loss: 1.3911 0.7102 sec/batch\n",
      "Epoch 16/20  Iteration 2769/3560 Training loss: 1.3908 0.7903 sec/batch\n",
      "Epoch 16/20  Iteration 2770/3560 Training loss: 1.3905 0.8071 sec/batch\n",
      "Epoch 16/20  Iteration 2771/3560 Training loss: 1.3904 0.7602 sec/batch\n",
      "Epoch 16/20  Iteration 2772/3560 Training loss: 1.3904 0.7862 sec/batch\n",
      "Epoch 16/20  Iteration 2773/3560 Training loss: 1.3903 0.8006 sec/batch\n",
      "Epoch 16/20  Iteration 2774/3560 Training loss: 1.3902 0.7757 sec/batch\n",
      "Epoch 16/20  Iteration 2775/3560 Training loss: 1.3900 0.7675 sec/batch\n",
      "Epoch 16/20  Iteration 2776/3560 Training loss: 1.3898 0.8027 sec/batch\n",
      "Epoch 16/20  Iteration 2777/3560 Training loss: 1.3898 0.8582 sec/batch\n",
      "Epoch 16/20  Iteration 2778/3560 Training loss: 1.3898 0.8218 sec/batch\n",
      "Epoch 16/20  Iteration 2779/3560 Training loss: 1.3897 0.8922 sec/batch\n",
      "Epoch 16/20  Iteration 2780/3560 Training loss: 1.3898 0.7188 sec/batch\n",
      "Epoch 16/20  Iteration 2781/3560 Training loss: 1.3896 0.7300 sec/batch\n",
      "Epoch 16/20  Iteration 2782/3560 Training loss: 1.3897 0.7516 sec/batch\n",
      "Epoch 16/20  Iteration 2783/3560 Training loss: 1.3897 0.8210 sec/batch\n",
      "Epoch 16/20  Iteration 2784/3560 Training loss: 1.3896 0.8089 sec/batch\n",
      "Epoch 16/20  Iteration 2785/3560 Training loss: 1.3893 0.7770 sec/batch\n",
      "Epoch 16/20  Iteration 2786/3560 Training loss: 1.3890 0.7717 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2787/3560 Training loss: 1.3890 0.7461 sec/batch\n",
      "Epoch 16/20  Iteration 2788/3560 Training loss: 1.3889 0.7715 sec/batch\n",
      "Epoch 16/20  Iteration 2789/3560 Training loss: 1.3888 0.7723 sec/batch\n",
      "Epoch 16/20  Iteration 2790/3560 Training loss: 1.3887 0.7357 sec/batch\n",
      "Epoch 16/20  Iteration 2791/3560 Training loss: 1.3887 0.7370 sec/batch\n",
      "Epoch 16/20  Iteration 2792/3560 Training loss: 1.3884 0.8373 sec/batch\n",
      "Epoch 16/20  Iteration 2793/3560 Training loss: 1.3881 0.7301 sec/batch\n",
      "Epoch 16/20  Iteration 2794/3560 Training loss: 1.3882 0.7598 sec/batch\n",
      "Epoch 16/20  Iteration 2795/3560 Training loss: 1.3881 0.7530 sec/batch\n",
      "Epoch 16/20  Iteration 2796/3560 Training loss: 1.3878 0.7776 sec/batch\n",
      "Epoch 16/20  Iteration 2797/3560 Training loss: 1.3879 0.7788 sec/batch\n",
      "Epoch 16/20  Iteration 2798/3560 Training loss: 1.3879 0.7625 sec/batch\n",
      "Epoch 16/20  Iteration 2799/3560 Training loss: 1.3877 0.7715 sec/batch\n",
      "Epoch 16/20  Iteration 2800/3560 Training loss: 1.3875 0.7589 sec/batch\n",
      "Epoch 16/20  Iteration 2801/3560 Training loss: 1.3871 0.7761 sec/batch\n",
      "Epoch 16/20  Iteration 2802/3560 Training loss: 1.3869 0.7769 sec/batch\n",
      "Epoch 16/20  Iteration 2803/3560 Training loss: 1.3869 0.7657 sec/batch\n",
      "Epoch 16/20  Iteration 2804/3560 Training loss: 1.3869 0.7799 sec/batch\n",
      "Epoch 16/20  Iteration 2805/3560 Training loss: 1.3869 0.7992 sec/batch\n",
      "Epoch 16/20  Iteration 2806/3560 Training loss: 1.3870 0.7491 sec/batch\n",
      "Epoch 16/20  Iteration 2807/3560 Training loss: 1.3872 0.7711 sec/batch\n",
      "Epoch 16/20  Iteration 2808/3560 Training loss: 1.3873 0.7641 sec/batch\n",
      "Epoch 16/20  Iteration 2809/3560 Training loss: 1.3873 0.7706 sec/batch\n",
      "Epoch 16/20  Iteration 2810/3560 Training loss: 1.3874 0.7409 sec/batch\n",
      "Epoch 16/20  Iteration 2811/3560 Training loss: 1.3878 0.7209 sec/batch\n",
      "Epoch 16/20  Iteration 2812/3560 Training loss: 1.3879 0.7474 sec/batch\n",
      "Epoch 16/20  Iteration 2813/3560 Training loss: 1.3878 0.7525 sec/batch\n",
      "Epoch 16/20  Iteration 2814/3560 Training loss: 1.3880 0.7573 sec/batch\n",
      "Epoch 16/20  Iteration 2815/3560 Training loss: 1.3879 0.7653 sec/batch\n",
      "Epoch 16/20  Iteration 2816/3560 Training loss: 1.3880 0.7792 sec/batch\n",
      "Epoch 16/20  Iteration 2817/3560 Training loss: 1.3880 0.8019 sec/batch\n",
      "Epoch 16/20  Iteration 2818/3560 Training loss: 1.3883 0.8317 sec/batch\n",
      "Epoch 16/20  Iteration 2819/3560 Training loss: 1.3884 0.7278 sec/batch\n",
      "Epoch 16/20  Iteration 2820/3560 Training loss: 1.3884 0.7519 sec/batch\n",
      "Epoch 16/20  Iteration 2821/3560 Training loss: 1.3881 0.7299 sec/batch\n",
      "Epoch 16/20  Iteration 2822/3560 Training loss: 1.3879 0.7683 sec/batch\n",
      "Epoch 16/20  Iteration 2823/3560 Training loss: 1.3880 0.7363 sec/batch\n",
      "Epoch 16/20  Iteration 2824/3560 Training loss: 1.3880 0.7404 sec/batch\n",
      "Epoch 16/20  Iteration 2825/3560 Training loss: 1.3881 0.7608 sec/batch\n",
      "Epoch 16/20  Iteration 2826/3560 Training loss: 1.3880 0.7376 sec/batch\n",
      "Epoch 16/20  Iteration 2827/3560 Training loss: 1.3881 0.7324 sec/batch\n",
      "Epoch 16/20  Iteration 2828/3560 Training loss: 1.3881 0.7443 sec/batch\n",
      "Epoch 16/20  Iteration 2829/3560 Training loss: 1.3880 0.7580 sec/batch\n",
      "Epoch 16/20  Iteration 2830/3560 Training loss: 1.3881 0.7383 sec/batch\n",
      "Epoch 16/20  Iteration 2831/3560 Training loss: 1.3883 0.7935 sec/batch\n",
      "Epoch 16/20  Iteration 2832/3560 Training loss: 1.3883 0.7820 sec/batch\n",
      "Epoch 16/20  Iteration 2833/3560 Training loss: 1.3883 0.7436 sec/batch\n",
      "Epoch 16/20  Iteration 2834/3560 Training loss: 1.3883 0.7387 sec/batch\n",
      "Epoch 16/20  Iteration 2835/3560 Training loss: 1.3883 0.7783 sec/batch\n",
      "Epoch 16/20  Iteration 2836/3560 Training loss: 1.3883 0.8708 sec/batch\n",
      "Epoch 16/20  Iteration 2837/3560 Training loss: 1.3885 0.9681 sec/batch\n",
      "Epoch 16/20  Iteration 2838/3560 Training loss: 1.3889 0.9397 sec/batch\n",
      "Epoch 16/20  Iteration 2839/3560 Training loss: 1.3890 1.0518 sec/batch\n",
      "Epoch 16/20  Iteration 2840/3560 Training loss: 1.3890 0.7967 sec/batch\n",
      "Epoch 16/20  Iteration 2841/3560 Training loss: 1.3889 0.7472 sec/batch\n",
      "Epoch 16/20  Iteration 2842/3560 Training loss: 1.3888 0.7273 sec/batch\n",
      "Epoch 16/20  Iteration 2843/3560 Training loss: 1.3890 0.7618 sec/batch\n",
      "Epoch 16/20  Iteration 2844/3560 Training loss: 1.3890 0.8235 sec/batch\n",
      "Epoch 16/20  Iteration 2845/3560 Training loss: 1.3891 0.7368 sec/batch\n",
      "Epoch 16/20  Iteration 2846/3560 Training loss: 1.3890 0.7699 sec/batch\n",
      "Epoch 16/20  Iteration 2847/3560 Training loss: 1.3889 0.7542 sec/batch\n",
      "Epoch 16/20  Iteration 2848/3560 Training loss: 1.3891 0.7484 sec/batch\n",
      "Epoch 17/20  Iteration 2849/3560 Training loss: 1.4771 0.7341 sec/batch\n",
      "Epoch 17/20  Iteration 2850/3560 Training loss: 1.4387 0.7614 sec/batch\n",
      "Epoch 17/20  Iteration 2851/3560 Training loss: 1.4201 0.7574 sec/batch\n",
      "Epoch 17/20  Iteration 2852/3560 Training loss: 1.4157 0.7449 sec/batch\n",
      "Epoch 17/20  Iteration 2853/3560 Training loss: 1.4064 0.7635 sec/batch\n",
      "Epoch 17/20  Iteration 2854/3560 Training loss: 1.3977 0.7657 sec/batch\n",
      "Epoch 17/20  Iteration 2855/3560 Training loss: 1.3967 0.7532 sec/batch\n",
      "Epoch 17/20  Iteration 2856/3560 Training loss: 1.3952 0.7645 sec/batch\n",
      "Epoch 17/20  Iteration 2857/3560 Training loss: 1.3957 0.7667 sec/batch\n",
      "Epoch 17/20  Iteration 2858/3560 Training loss: 1.3942 0.7849 sec/batch\n",
      "Epoch 17/20  Iteration 2859/3560 Training loss: 1.3902 0.7835 sec/batch\n",
      "Epoch 17/20  Iteration 2860/3560 Training loss: 1.3900 0.7612 sec/batch\n",
      "Epoch 17/20  Iteration 2861/3560 Training loss: 1.3895 0.7917 sec/batch\n",
      "Epoch 17/20  Iteration 2862/3560 Training loss: 1.3907 0.8086 sec/batch\n",
      "Epoch 17/20  Iteration 2863/3560 Training loss: 1.3889 0.7416 sec/batch\n",
      "Epoch 17/20  Iteration 2864/3560 Training loss: 1.3875 0.7931 sec/batch\n",
      "Epoch 17/20  Iteration 2865/3560 Training loss: 1.3878 0.7374 sec/batch\n",
      "Epoch 17/20  Iteration 2866/3560 Training loss: 1.3891 0.7560 sec/batch\n",
      "Epoch 17/20  Iteration 2867/3560 Training loss: 1.3888 0.7698 sec/batch\n",
      "Epoch 17/20  Iteration 2868/3560 Training loss: 1.3900 0.7413 sec/batch\n",
      "Epoch 17/20  Iteration 2869/3560 Training loss: 1.3895 0.7320 sec/batch\n",
      "Epoch 17/20  Iteration 2870/3560 Training loss: 1.3902 0.7761 sec/batch\n",
      "Epoch 17/20  Iteration 2871/3560 Training loss: 1.3897 0.8101 sec/batch\n",
      "Epoch 17/20  Iteration 2872/3560 Training loss: 1.3895 0.7301 sec/batch\n",
      "Epoch 17/20  Iteration 2873/3560 Training loss: 1.3894 0.7776 sec/batch\n",
      "Epoch 17/20  Iteration 2874/3560 Training loss: 1.3875 0.7441 sec/batch\n",
      "Epoch 17/20  Iteration 2875/3560 Training loss: 1.3863 0.7583 sec/batch\n",
      "Epoch 17/20  Iteration 2876/3560 Training loss: 1.3870 0.7348 sec/batch\n",
      "Epoch 17/20  Iteration 2877/3560 Training loss: 1.3874 0.7313 sec/batch\n",
      "Epoch 17/20  Iteration 2878/3560 Training loss: 1.3876 0.7561 sec/batch\n",
      "Epoch 17/20  Iteration 2879/3560 Training loss: 1.3873 0.7587 sec/batch\n",
      "Epoch 17/20  Iteration 2880/3560 Training loss: 1.3863 0.7679 sec/batch\n",
      "Epoch 17/20  Iteration 2881/3560 Training loss: 1.3866 0.7876 sec/batch\n",
      "Epoch 17/20  Iteration 2882/3560 Training loss: 1.3866 0.7623 sec/batch\n",
      "Epoch 17/20  Iteration 2883/3560 Training loss: 1.3864 0.7846 sec/batch\n",
      "Epoch 17/20  Iteration 2884/3560 Training loss: 1.3862 0.7628 sec/batch\n",
      "Epoch 17/20  Iteration 2885/3560 Training loss: 1.3852 0.8382 sec/batch\n",
      "Epoch 17/20  Iteration 2886/3560 Training loss: 1.3844 0.7694 sec/batch\n",
      "Epoch 17/20  Iteration 2887/3560 Training loss: 1.3830 0.7845 sec/batch\n",
      "Epoch 17/20  Iteration 2888/3560 Training loss: 1.3827 0.7401 sec/batch\n",
      "Epoch 17/20  Iteration 2889/3560 Training loss: 1.3821 0.7593 sec/batch\n",
      "Epoch 17/20  Iteration 2890/3560 Training loss: 1.3829 0.7522 sec/batch\n",
      "Epoch 17/20  Iteration 2891/3560 Training loss: 1.3827 0.7374 sec/batch\n",
      "Epoch 17/20  Iteration 2892/3560 Training loss: 1.3821 0.7427 sec/batch\n",
      "Epoch 17/20  Iteration 2893/3560 Training loss: 1.3823 0.7687 sec/batch\n",
      "Epoch 17/20  Iteration 2894/3560 Training loss: 1.3815 0.7744 sec/batch\n",
      "Epoch 17/20  Iteration 2895/3560 Training loss: 1.3811 0.7448 sec/batch\n",
      "Epoch 17/20  Iteration 2896/3560 Training loss: 1.3806 0.7952 sec/batch\n",
      "Epoch 17/20  Iteration 2897/3560 Training loss: 1.3806 0.7484 sec/batch\n",
      "Epoch 17/20  Iteration 2898/3560 Training loss: 1.3809 0.7629 sec/batch\n",
      "Epoch 17/20  Iteration 2899/3560 Training loss: 1.3802 0.7475 sec/batch\n",
      "Epoch 17/20  Iteration 2900/3560 Training loss: 1.3809 0.7484 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 2901/3560 Training loss: 1.3809 0.7510 sec/batch\n",
      "Epoch 17/20  Iteration 2902/3560 Training loss: 1.3811 0.7274 sec/batch\n",
      "Epoch 17/20  Iteration 2903/3560 Training loss: 1.3810 0.7601 sec/batch\n",
      "Epoch 17/20  Iteration 2904/3560 Training loss: 1.3810 0.7599 sec/batch\n",
      "Epoch 17/20  Iteration 2905/3560 Training loss: 1.3814 0.7339 sec/batch\n",
      "Epoch 17/20  Iteration 2906/3560 Training loss: 1.3811 0.7805 sec/batch\n",
      "Epoch 17/20  Iteration 2907/3560 Training loss: 1.3807 0.7853 sec/batch\n",
      "Epoch 17/20  Iteration 2908/3560 Training loss: 1.3813 0.7724 sec/batch\n",
      "Epoch 17/20  Iteration 2909/3560 Training loss: 1.3813 0.7611 sec/batch\n",
      "Epoch 17/20  Iteration 2910/3560 Training loss: 1.3821 0.8234 sec/batch\n",
      "Epoch 17/20  Iteration 2911/3560 Training loss: 1.3826 0.7537 sec/batch\n",
      "Epoch 17/20  Iteration 2912/3560 Training loss: 1.3829 0.7782 sec/batch\n",
      "Epoch 17/20  Iteration 2913/3560 Training loss: 1.3829 0.7536 sec/batch\n",
      "Epoch 17/20  Iteration 2914/3560 Training loss: 1.3831 0.7872 sec/batch\n",
      "Epoch 17/20  Iteration 2915/3560 Training loss: 1.3833 0.7635 sec/batch\n",
      "Epoch 17/20  Iteration 2916/3560 Training loss: 1.3830 0.7761 sec/batch\n",
      "Epoch 17/20  Iteration 2917/3560 Training loss: 1.3831 0.7432 sec/batch\n",
      "Epoch 17/20  Iteration 2918/3560 Training loss: 1.3828 0.7622 sec/batch\n",
      "Epoch 17/20  Iteration 2919/3560 Training loss: 1.3834 0.7585 sec/batch\n",
      "Epoch 17/20  Iteration 2920/3560 Training loss: 1.3835 0.7607 sec/batch\n",
      "Epoch 17/20  Iteration 2921/3560 Training loss: 1.3840 0.7706 sec/batch\n",
      "Epoch 17/20  Iteration 2922/3560 Training loss: 1.3837 0.7350 sec/batch\n",
      "Epoch 17/20  Iteration 2923/3560 Training loss: 1.3836 0.8038 sec/batch\n",
      "Epoch 17/20  Iteration 2924/3560 Training loss: 1.3839 0.7838 sec/batch\n",
      "Epoch 17/20  Iteration 2925/3560 Training loss: 1.3838 0.7623 sec/batch\n",
      "Epoch 17/20  Iteration 2926/3560 Training loss: 1.3838 0.7389 sec/batch\n",
      "Epoch 17/20  Iteration 2927/3560 Training loss: 1.3833 0.7415 sec/batch\n",
      "Epoch 17/20  Iteration 2928/3560 Training loss: 1.3832 0.7644 sec/batch\n",
      "Epoch 17/20  Iteration 2929/3560 Training loss: 1.3829 0.7660 sec/batch\n",
      "Epoch 17/20  Iteration 2930/3560 Training loss: 1.3828 0.7517 sec/batch\n",
      "Epoch 17/20  Iteration 2931/3560 Training loss: 1.3824 0.7954 sec/batch\n",
      "Epoch 17/20  Iteration 2932/3560 Training loss: 1.3823 0.7945 sec/batch\n",
      "Epoch 17/20  Iteration 2933/3560 Training loss: 1.3820 0.7574 sec/batch\n",
      "Epoch 17/20  Iteration 2934/3560 Training loss: 1.3818 0.7717 sec/batch\n",
      "Epoch 17/20  Iteration 2935/3560 Training loss: 1.3817 0.7582 sec/batch\n",
      "Epoch 17/20  Iteration 2936/3560 Training loss: 1.3815 0.7524 sec/batch\n",
      "Epoch 17/20  Iteration 2937/3560 Training loss: 1.3811 0.7801 sec/batch\n",
      "Epoch 17/20  Iteration 2938/3560 Training loss: 1.3812 0.7246 sec/batch\n",
      "Epoch 17/20  Iteration 2939/3560 Training loss: 1.3809 0.7595 sec/batch\n",
      "Epoch 17/20  Iteration 2940/3560 Training loss: 1.3809 0.7524 sec/batch\n",
      "Epoch 17/20  Iteration 2941/3560 Training loss: 1.3805 0.7526 sec/batch\n",
      "Epoch 17/20  Iteration 2942/3560 Training loss: 1.3802 0.7371 sec/batch\n",
      "Epoch 17/20  Iteration 2943/3560 Training loss: 1.3799 0.7557 sec/batch\n",
      "Epoch 17/20  Iteration 2944/3560 Training loss: 1.3801 0.7574 sec/batch\n",
      "Epoch 17/20  Iteration 2945/3560 Training loss: 1.3801 0.7685 sec/batch\n",
      "Epoch 17/20  Iteration 2946/3560 Training loss: 1.3798 0.7397 sec/batch\n",
      "Epoch 17/20  Iteration 2947/3560 Training loss: 1.3794 0.7805 sec/batch\n",
      "Epoch 17/20  Iteration 2948/3560 Training loss: 1.3791 0.7679 sec/batch\n",
      "Epoch 17/20  Iteration 2949/3560 Training loss: 1.3791 0.7590 sec/batch\n",
      "Epoch 17/20  Iteration 2950/3560 Training loss: 1.3791 0.8149 sec/batch\n",
      "Epoch 17/20  Iteration 2951/3560 Training loss: 1.3790 0.7443 sec/batch\n",
      "Epoch 17/20  Iteration 2952/3560 Training loss: 1.3790 0.7524 sec/batch\n",
      "Epoch 17/20  Iteration 2953/3560 Training loss: 1.3788 0.7412 sec/batch\n",
      "Epoch 17/20  Iteration 2954/3560 Training loss: 1.3787 0.7566 sec/batch\n",
      "Epoch 17/20  Iteration 2955/3560 Training loss: 1.3786 0.7428 sec/batch\n",
      "Epoch 17/20  Iteration 2956/3560 Training loss: 1.3786 0.7410 sec/batch\n",
      "Epoch 17/20  Iteration 2957/3560 Training loss: 1.3785 0.7805 sec/batch\n",
      "Epoch 17/20  Iteration 2958/3560 Training loss: 1.3786 0.7555 sec/batch\n",
      "Epoch 17/20  Iteration 2959/3560 Training loss: 1.3785 0.7570 sec/batch\n",
      "Epoch 17/20  Iteration 2960/3560 Training loss: 1.3785 0.7776 sec/batch\n",
      "Epoch 17/20  Iteration 2961/3560 Training loss: 1.3785 0.7445 sec/batch\n",
      "Epoch 17/20  Iteration 2962/3560 Training loss: 1.3783 0.7505 sec/batch\n",
      "Epoch 17/20  Iteration 2963/3560 Training loss: 1.3779 0.8064 sec/batch\n",
      "Epoch 17/20  Iteration 2964/3560 Training loss: 1.3777 0.7685 sec/batch\n",
      "Epoch 17/20  Iteration 2965/3560 Training loss: 1.3778 0.7614 sec/batch\n",
      "Epoch 17/20  Iteration 2966/3560 Training loss: 1.3778 0.7669 sec/batch\n",
      "Epoch 17/20  Iteration 2967/3560 Training loss: 1.3778 0.7507 sec/batch\n",
      "Epoch 17/20  Iteration 2968/3560 Training loss: 1.3777 0.7357 sec/batch\n",
      "Epoch 17/20  Iteration 2969/3560 Training loss: 1.3776 0.7536 sec/batch\n",
      "Epoch 17/20  Iteration 2970/3560 Training loss: 1.3773 0.7359 sec/batch\n",
      "Epoch 17/20  Iteration 2971/3560 Training loss: 1.3769 0.7487 sec/batch\n",
      "Epoch 17/20  Iteration 2972/3560 Training loss: 1.3769 0.7264 sec/batch\n",
      "Epoch 17/20  Iteration 2973/3560 Training loss: 1.3769 0.7679 sec/batch\n",
      "Epoch 17/20  Iteration 2974/3560 Training loss: 1.3767 0.8185 sec/batch\n",
      "Epoch 17/20  Iteration 2975/3560 Training loss: 1.3768 0.7415 sec/batch\n",
      "Epoch 17/20  Iteration 2976/3560 Training loss: 1.3768 0.7631 sec/batch\n",
      "Epoch 17/20  Iteration 2977/3560 Training loss: 1.3766 0.7926 sec/batch\n",
      "Epoch 17/20  Iteration 2978/3560 Training loss: 1.3764 0.7456 sec/batch\n",
      "Epoch 17/20  Iteration 2979/3560 Training loss: 1.3759 0.7509 sec/batch\n",
      "Epoch 17/20  Iteration 2980/3560 Training loss: 1.3758 0.7509 sec/batch\n",
      "Epoch 17/20  Iteration 2981/3560 Training loss: 1.3759 0.7411 sec/batch\n",
      "Epoch 17/20  Iteration 2982/3560 Training loss: 1.3759 0.7482 sec/batch\n",
      "Epoch 17/20  Iteration 2983/3560 Training loss: 1.3759 0.8329 sec/batch\n",
      "Epoch 17/20  Iteration 2984/3560 Training loss: 1.3760 0.7719 sec/batch\n",
      "Epoch 17/20  Iteration 2985/3560 Training loss: 1.3762 0.7676 sec/batch\n",
      "Epoch 17/20  Iteration 2986/3560 Training loss: 1.3762 0.7697 sec/batch\n",
      "Epoch 17/20  Iteration 2987/3560 Training loss: 1.3763 0.7506 sec/batch\n",
      "Epoch 17/20  Iteration 2988/3560 Training loss: 1.3762 0.7639 sec/batch\n",
      "Epoch 17/20  Iteration 2989/3560 Training loss: 1.3766 0.7677 sec/batch\n",
      "Epoch 17/20  Iteration 2990/3560 Training loss: 1.3766 0.7687 sec/batch\n",
      "Epoch 17/20  Iteration 2991/3560 Training loss: 1.3765 0.7384 sec/batch\n",
      "Epoch 17/20  Iteration 2992/3560 Training loss: 1.3767 0.7267 sec/batch\n",
      "Epoch 17/20  Iteration 2993/3560 Training loss: 1.3766 0.8188 sec/batch\n",
      "Epoch 17/20  Iteration 2994/3560 Training loss: 1.3767 0.8078 sec/batch\n",
      "Epoch 17/20  Iteration 2995/3560 Training loss: 1.3768 0.7565 sec/batch\n",
      "Epoch 17/20  Iteration 2996/3560 Training loss: 1.3771 0.7551 sec/batch\n",
      "Epoch 17/20  Iteration 2997/3560 Training loss: 1.3772 0.7315 sec/batch\n",
      "Epoch 17/20  Iteration 2998/3560 Training loss: 1.3771 0.7425 sec/batch\n",
      "Epoch 17/20  Iteration 2999/3560 Training loss: 1.3768 0.7354 sec/batch\n",
      "Epoch 17/20  Iteration 3000/3560 Training loss: 1.3767 0.7831 sec/batch\n",
      "Epoch 17/20  Iteration 3001/3560 Training loss: 1.3768 0.7380 sec/batch\n",
      "Epoch 17/20  Iteration 3002/3560 Training loss: 1.3769 0.7448 sec/batch\n",
      "Epoch 17/20  Iteration 3003/3560 Training loss: 1.3768 0.7897 sec/batch\n",
      "Epoch 17/20  Iteration 3004/3560 Training loss: 1.3769 0.7560 sec/batch\n",
      "Epoch 17/20  Iteration 3005/3560 Training loss: 1.3769 0.7406 sec/batch\n",
      "Epoch 17/20  Iteration 3006/3560 Training loss: 1.3769 0.7529 sec/batch\n",
      "Epoch 17/20  Iteration 3007/3560 Training loss: 1.3767 0.7599 sec/batch\n",
      "Epoch 17/20  Iteration 3008/3560 Training loss: 1.3768 0.7575 sec/batch\n",
      "Epoch 17/20  Iteration 3009/3560 Training loss: 1.3770 0.7703 sec/batch\n",
      "Epoch 17/20  Iteration 3010/3560 Training loss: 1.3770 0.8258 sec/batch\n",
      "Epoch 17/20  Iteration 3011/3560 Training loss: 1.3770 0.7466 sec/batch\n",
      "Epoch 17/20  Iteration 3012/3560 Training loss: 1.3770 0.7714 sec/batch\n",
      "Epoch 17/20  Iteration 3013/3560 Training loss: 1.3770 0.7683 sec/batch\n",
      "Epoch 17/20  Iteration 3014/3560 Training loss: 1.3770 0.7618 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 3015/3560 Training loss: 1.3771 0.7617 sec/batch\n",
      "Epoch 17/20  Iteration 3016/3560 Training loss: 1.3775 0.8072 sec/batch\n",
      "Epoch 17/20  Iteration 3017/3560 Training loss: 1.3775 0.7597 sec/batch\n",
      "Epoch 17/20  Iteration 3018/3560 Training loss: 1.3776 0.7345 sec/batch\n",
      "Epoch 17/20  Iteration 3019/3560 Training loss: 1.3775 0.7660 sec/batch\n",
      "Epoch 17/20  Iteration 3020/3560 Training loss: 1.3774 0.7608 sec/batch\n",
      "Epoch 17/20  Iteration 3021/3560 Training loss: 1.3776 0.7611 sec/batch\n",
      "Epoch 17/20  Iteration 3022/3560 Training loss: 1.3776 0.7584 sec/batch\n",
      "Epoch 17/20  Iteration 3023/3560 Training loss: 1.3777 0.7448 sec/batch\n",
      "Epoch 17/20  Iteration 3024/3560 Training loss: 1.3775 0.7635 sec/batch\n",
      "Epoch 17/20  Iteration 3025/3560 Training loss: 1.3775 0.7602 sec/batch\n",
      "Epoch 17/20  Iteration 3026/3560 Training loss: 1.3776 0.7943 sec/batch\n",
      "Epoch 18/20  Iteration 3027/3560 Training loss: 1.4623 0.8061 sec/batch\n",
      "Epoch 18/20  Iteration 3028/3560 Training loss: 1.4294 0.7566 sec/batch\n",
      "Epoch 18/20  Iteration 3029/3560 Training loss: 1.4133 0.8403 sec/batch\n",
      "Epoch 18/20  Iteration 3030/3560 Training loss: 1.4081 0.7313 sec/batch\n",
      "Epoch 18/20  Iteration 3031/3560 Training loss: 1.3987 0.7358 sec/batch\n",
      "Epoch 18/20  Iteration 3032/3560 Training loss: 1.3886 0.8027 sec/batch\n",
      "Epoch 18/20  Iteration 3033/3560 Training loss: 1.3895 0.8727 sec/batch\n",
      "Epoch 18/20  Iteration 3034/3560 Training loss: 1.3877 0.8141 sec/batch\n",
      "Epoch 18/20  Iteration 3035/3560 Training loss: 1.3865 1.0999 sec/batch\n",
      "Epoch 18/20  Iteration 3036/3560 Training loss: 1.3853 1.0514 sec/batch\n",
      "Epoch 18/20  Iteration 3037/3560 Training loss: 1.3814 0.9840 sec/batch\n",
      "Epoch 18/20  Iteration 3038/3560 Training loss: 1.3806 0.8939 sec/batch\n",
      "Epoch 18/20  Iteration 3039/3560 Training loss: 1.3796 0.8142 sec/batch\n",
      "Epoch 18/20  Iteration 3040/3560 Training loss: 1.3807 0.8493 sec/batch\n",
      "Epoch 18/20  Iteration 3041/3560 Training loss: 1.3799 0.8644 sec/batch\n",
      "Epoch 18/20  Iteration 3042/3560 Training loss: 1.3775 0.7888 sec/batch\n",
      "Epoch 18/20  Iteration 3043/3560 Training loss: 1.3786 0.8150 sec/batch\n",
      "Epoch 18/20  Iteration 3044/3560 Training loss: 1.3799 0.8078 sec/batch\n",
      "Epoch 18/20  Iteration 3045/3560 Training loss: 1.3801 0.7655 sec/batch\n",
      "Epoch 18/20  Iteration 3046/3560 Training loss: 1.3812 0.7640 sec/batch\n",
      "Epoch 18/20  Iteration 3047/3560 Training loss: 1.3804 0.7387 sec/batch\n",
      "Epoch 18/20  Iteration 3048/3560 Training loss: 1.3805 0.7449 sec/batch\n",
      "Epoch 18/20  Iteration 3049/3560 Training loss: 1.3796 0.7677 sec/batch\n",
      "Epoch 18/20  Iteration 3050/3560 Training loss: 1.3797 0.7733 sec/batch\n",
      "Epoch 18/20  Iteration 3051/3560 Training loss: 1.3795 0.7726 sec/batch\n",
      "Epoch 18/20  Iteration 3052/3560 Training loss: 1.3779 0.7300 sec/batch\n",
      "Epoch 18/20  Iteration 3053/3560 Training loss: 1.3771 0.7540 sec/batch\n",
      "Epoch 18/20  Iteration 3054/3560 Training loss: 1.3774 0.7798 sec/batch\n",
      "Epoch 18/20  Iteration 3055/3560 Training loss: 1.3774 0.7428 sec/batch\n",
      "Epoch 18/20  Iteration 3056/3560 Training loss: 1.3774 0.7385 sec/batch\n",
      "Epoch 18/20  Iteration 3057/3560 Training loss: 1.3765 0.7314 sec/batch\n",
      "Epoch 18/20  Iteration 3058/3560 Training loss: 1.3756 0.7627 sec/batch\n",
      "Epoch 18/20  Iteration 3059/3560 Training loss: 1.3761 0.7644 sec/batch\n",
      "Epoch 18/20  Iteration 3060/3560 Training loss: 1.3766 0.7693 sec/batch\n",
      "Epoch 18/20  Iteration 3061/3560 Training loss: 1.3763 0.7928 sec/batch\n",
      "Epoch 18/20  Iteration 3062/3560 Training loss: 1.3762 0.8679 sec/batch\n",
      "Epoch 18/20  Iteration 3063/3560 Training loss: 1.3754 1.2878 sec/batch\n",
      "Epoch 18/20  Iteration 3064/3560 Training loss: 1.3743 0.9829 sec/batch\n",
      "Epoch 18/20  Iteration 3065/3560 Training loss: 1.3731 1.0190 sec/batch\n",
      "Epoch 18/20  Iteration 3066/3560 Training loss: 1.3727 1.0323 sec/batch\n",
      "Epoch 18/20  Iteration 3067/3560 Training loss: 1.3722 0.9622 sec/batch\n",
      "Epoch 18/20  Iteration 3068/3560 Training loss: 1.3729 1.1435 sec/batch\n",
      "Epoch 18/20  Iteration 3069/3560 Training loss: 1.3726 0.9806 sec/batch\n",
      "Epoch 18/20  Iteration 3070/3560 Training loss: 1.3718 1.1567 sec/batch\n",
      "Epoch 18/20  Iteration 3071/3560 Training loss: 1.3721 1.1190 sec/batch\n",
      "Epoch 18/20  Iteration 3072/3560 Training loss: 1.3712 1.1370 sec/batch\n",
      "Epoch 18/20  Iteration 3073/3560 Training loss: 1.3710 0.8521 sec/batch\n",
      "Epoch 18/20  Iteration 3074/3560 Training loss: 1.3706 0.8995 sec/batch\n",
      "Epoch 18/20  Iteration 3075/3560 Training loss: 1.3704 0.8676 sec/batch\n",
      "Epoch 18/20  Iteration 3076/3560 Training loss: 1.3708 0.8667 sec/batch\n",
      "Epoch 18/20  Iteration 3077/3560 Training loss: 1.3703 0.8003 sec/batch\n",
      "Epoch 18/20  Iteration 3078/3560 Training loss: 1.3710 0.7887 sec/batch\n",
      "Epoch 18/20  Iteration 3079/3560 Training loss: 1.3709 0.7709 sec/batch\n",
      "Epoch 18/20  Iteration 3080/3560 Training loss: 1.3712 0.8009 sec/batch\n",
      "Epoch 18/20  Iteration 3081/3560 Training loss: 1.3709 0.8110 sec/batch\n",
      "Epoch 18/20  Iteration 3082/3560 Training loss: 1.3709 0.8105 sec/batch\n",
      "Epoch 18/20  Iteration 3083/3560 Training loss: 1.3712 0.7956 sec/batch\n",
      "Epoch 18/20  Iteration 3084/3560 Training loss: 1.3709 0.8411 sec/batch\n",
      "Epoch 18/20  Iteration 3085/3560 Training loss: 1.3704 0.9725 sec/batch\n",
      "Epoch 18/20  Iteration 3086/3560 Training loss: 1.3711 0.8879 sec/batch\n",
      "Epoch 18/20  Iteration 3087/3560 Training loss: 1.3712 0.8384 sec/batch\n",
      "Epoch 18/20  Iteration 3088/3560 Training loss: 1.3721 0.9770 sec/batch\n",
      "Epoch 18/20  Iteration 3089/3560 Training loss: 1.3726 1.1545 sec/batch\n",
      "Epoch 18/20  Iteration 3090/3560 Training loss: 1.3728 1.1187 sec/batch\n",
      "Epoch 18/20  Iteration 3091/3560 Training loss: 1.3728 1.2051 sec/batch\n",
      "Epoch 18/20  Iteration 3092/3560 Training loss: 1.3729 0.9587 sec/batch\n",
      "Epoch 18/20  Iteration 3093/3560 Training loss: 1.3732 0.8118 sec/batch\n",
      "Epoch 18/20  Iteration 3094/3560 Training loss: 1.3729 0.8114 sec/batch\n",
      "Epoch 18/20  Iteration 3095/3560 Training loss: 1.3728 0.8256 sec/batch\n",
      "Epoch 18/20  Iteration 3096/3560 Training loss: 1.3725 0.7909 sec/batch\n",
      "Epoch 18/20  Iteration 3097/3560 Training loss: 1.3731 0.7823 sec/batch\n",
      "Epoch 18/20  Iteration 3098/3560 Training loss: 1.3733 0.7799 sec/batch\n",
      "Epoch 18/20  Iteration 3099/3560 Training loss: 1.3739 0.8324 sec/batch\n",
      "Epoch 18/20  Iteration 3100/3560 Training loss: 1.3735 0.7793 sec/batch\n",
      "Epoch 18/20  Iteration 3101/3560 Training loss: 1.3734 0.7830 sec/batch\n",
      "Epoch 18/20  Iteration 3102/3560 Training loss: 1.3735 0.7696 sec/batch\n",
      "Epoch 18/20  Iteration 3103/3560 Training loss: 1.3736 0.9807 sec/batch\n",
      "Epoch 18/20  Iteration 3104/3560 Training loss: 1.3735 1.0403 sec/batch\n",
      "Epoch 18/20  Iteration 3105/3560 Training loss: 1.3730 1.0188 sec/batch\n",
      "Epoch 18/20  Iteration 3106/3560 Training loss: 1.3730 1.0961 sec/batch\n",
      "Epoch 18/20  Iteration 3107/3560 Training loss: 1.3726 1.0513 sec/batch\n",
      "Epoch 18/20  Iteration 3108/3560 Training loss: 1.3726 1.1425 sec/batch\n",
      "Epoch 18/20  Iteration 3109/3560 Training loss: 1.3723 1.1797 sec/batch\n",
      "Epoch 18/20  Iteration 3110/3560 Training loss: 1.3722 0.9202 sec/batch\n",
      "Epoch 18/20  Iteration 3111/3560 Training loss: 1.3719 0.8578 sec/batch\n",
      "Epoch 18/20  Iteration 3112/3560 Training loss: 1.3718 0.8590 sec/batch\n",
      "Epoch 18/20  Iteration 3113/3560 Training loss: 1.3715 0.8817 sec/batch\n",
      "Epoch 18/20  Iteration 3114/3560 Training loss: 1.3713 0.8610 sec/batch\n",
      "Epoch 18/20  Iteration 3115/3560 Training loss: 1.3709 0.9165 sec/batch\n",
      "Epoch 18/20  Iteration 3116/3560 Training loss: 1.3709 0.9403 sec/batch\n",
      "Epoch 18/20  Iteration 3117/3560 Training loss: 1.3706 0.9653 sec/batch\n",
      "Epoch 18/20  Iteration 3118/3560 Training loss: 1.3705 1.0229 sec/batch\n",
      "Epoch 18/20  Iteration 3119/3560 Training loss: 1.3701 0.9809 sec/batch\n",
      "Epoch 18/20  Iteration 3120/3560 Training loss: 1.3699 1.1177 sec/batch\n",
      "Epoch 18/20  Iteration 3121/3560 Training loss: 1.3697 0.9512 sec/batch\n",
      "Epoch 18/20  Iteration 3122/3560 Training loss: 1.3698 1.0228 sec/batch\n",
      "Epoch 18/20  Iteration 3123/3560 Training loss: 1.3698 1.0613 sec/batch\n",
      "Epoch 18/20  Iteration 3124/3560 Training loss: 1.3695 0.8707 sec/batch\n",
      "Epoch 18/20  Iteration 3125/3560 Training loss: 1.3691 1.0127 sec/batch\n",
      "Epoch 18/20  Iteration 3126/3560 Training loss: 1.3688 1.0343 sec/batch\n",
      "Epoch 18/20  Iteration 3127/3560 Training loss: 1.3688 0.8587 sec/batch\n",
      "Epoch 18/20  Iteration 3128/3560 Training loss: 1.3688 0.7875 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3129/3560 Training loss: 1.3686 0.8164 sec/batch\n",
      "Epoch 18/20  Iteration 3130/3560 Training loss: 1.3685 0.8212 sec/batch\n",
      "Epoch 18/20  Iteration 3131/3560 Training loss: 1.3683 0.8254 sec/batch\n",
      "Epoch 18/20  Iteration 3132/3560 Training loss: 1.3681 0.8817 sec/batch\n",
      "Epoch 18/20  Iteration 3133/3560 Training loss: 1.3681 0.9522 sec/batch\n",
      "Epoch 18/20  Iteration 3134/3560 Training loss: 1.3681 0.9788 sec/batch\n",
      "Epoch 18/20  Iteration 3135/3560 Training loss: 1.3679 1.3279 sec/batch\n",
      "Epoch 18/20  Iteration 3136/3560 Training loss: 1.3679 1.2773 sec/batch\n",
      "Epoch 18/20  Iteration 3137/3560 Training loss: 1.3677 0.9039 sec/batch\n",
      "Epoch 18/20  Iteration 3138/3560 Training loss: 1.3677 1.0889 sec/batch\n",
      "Epoch 18/20  Iteration 3139/3560 Training loss: 1.3676 0.9611 sec/batch\n",
      "Epoch 18/20  Iteration 3140/3560 Training loss: 1.3675 0.7884 sec/batch\n",
      "Epoch 18/20  Iteration 3141/3560 Training loss: 1.3672 0.8464 sec/batch\n",
      "Epoch 18/20  Iteration 3142/3560 Training loss: 1.3670 0.7975 sec/batch\n",
      "Epoch 18/20  Iteration 3143/3560 Training loss: 1.3670 0.7975 sec/batch\n",
      "Epoch 18/20  Iteration 3144/3560 Training loss: 1.3671 0.9089 sec/batch\n",
      "Epoch 18/20  Iteration 3145/3560 Training loss: 1.3669 0.9800 sec/batch\n",
      "Epoch 18/20  Iteration 3146/3560 Training loss: 1.3669 0.9169 sec/batch\n",
      "Epoch 18/20  Iteration 3147/3560 Training loss: 1.3669 0.7827 sec/batch\n",
      "Epoch 18/20  Iteration 3148/3560 Training loss: 1.3666 0.8005 sec/batch\n",
      "Epoch 18/20  Iteration 3149/3560 Training loss: 1.3662 0.8106 sec/batch\n",
      "Epoch 18/20  Iteration 3150/3560 Training loss: 1.3662 0.8110 sec/batch\n",
      "Epoch 18/20  Iteration 3151/3560 Training loss: 1.3661 0.8298 sec/batch\n",
      "Epoch 18/20  Iteration 3152/3560 Training loss: 1.3657 0.8207 sec/batch\n",
      "Epoch 18/20  Iteration 3153/3560 Training loss: 1.3659 1.1000 sec/batch\n",
      "Epoch 18/20  Iteration 3154/3560 Training loss: 1.3659 0.9554 sec/batch\n",
      "Epoch 18/20  Iteration 3155/3560 Training loss: 1.3658 1.0312 sec/batch\n",
      "Epoch 18/20  Iteration 3156/3560 Training loss: 1.3655 0.9010 sec/batch\n",
      "Epoch 18/20  Iteration 3157/3560 Training loss: 1.3651 1.0826 sec/batch\n",
      "Epoch 18/20  Iteration 3158/3560 Training loss: 1.3649 0.9005 sec/batch\n",
      "Epoch 18/20  Iteration 3159/3560 Training loss: 1.3650 0.7681 sec/batch\n",
      "Epoch 18/20  Iteration 3160/3560 Training loss: 1.3650 0.7989 sec/batch\n",
      "Epoch 18/20  Iteration 3161/3560 Training loss: 1.3650 0.8510 sec/batch\n",
      "Epoch 18/20  Iteration 3162/3560 Training loss: 1.3650 0.7631 sec/batch\n",
      "Epoch 18/20  Iteration 3163/3560 Training loss: 1.3652 0.8378 sec/batch\n",
      "Epoch 18/20  Iteration 3164/3560 Training loss: 1.3654 1.0445 sec/batch\n",
      "Epoch 18/20  Iteration 3165/3560 Training loss: 1.3653 0.7791 sec/batch\n",
      "Epoch 18/20  Iteration 3166/3560 Training loss: 1.3653 0.8243 sec/batch\n",
      "Epoch 18/20  Iteration 3167/3560 Training loss: 1.3657 0.8304 sec/batch\n",
      "Epoch 18/20  Iteration 3168/3560 Training loss: 1.3657 1.1657 sec/batch\n",
      "Epoch 18/20  Iteration 3169/3560 Training loss: 1.3656 1.0671 sec/batch\n",
      "Epoch 18/20  Iteration 3170/3560 Training loss: 1.3658 0.9428 sec/batch\n",
      "Epoch 18/20  Iteration 3171/3560 Training loss: 1.3658 0.7913 sec/batch\n",
      "Epoch 18/20  Iteration 3172/3560 Training loss: 1.3659 0.8340 sec/batch\n",
      "Epoch 18/20  Iteration 3173/3560 Training loss: 1.3659 0.8944 sec/batch\n",
      "Epoch 18/20  Iteration 3174/3560 Training loss: 1.3662 0.8040 sec/batch\n",
      "Epoch 18/20  Iteration 3175/3560 Training loss: 1.3663 0.8160 sec/batch\n",
      "Epoch 18/20  Iteration 3176/3560 Training loss: 1.3662 0.8035 sec/batch\n",
      "Epoch 18/20  Iteration 3177/3560 Training loss: 1.3660 0.8500 sec/batch\n",
      "Epoch 18/20  Iteration 3178/3560 Training loss: 1.3659 0.9456 sec/batch\n",
      "Epoch 18/20  Iteration 3179/3560 Training loss: 1.3660 0.9990 sec/batch\n",
      "Epoch 18/20  Iteration 3180/3560 Training loss: 1.3661 0.9055 sec/batch\n",
      "Epoch 18/20  Iteration 3181/3560 Training loss: 1.3661 1.0442 sec/batch\n",
      "Epoch 18/20  Iteration 3182/3560 Training loss: 1.3661 1.1991 sec/batch\n",
      "Epoch 18/20  Iteration 3183/3560 Training loss: 1.3661 0.8775 sec/batch\n",
      "Epoch 18/20  Iteration 3184/3560 Training loss: 1.3662 0.9489 sec/batch\n",
      "Epoch 18/20  Iteration 3185/3560 Training loss: 1.3659 0.9253 sec/batch\n",
      "Epoch 18/20  Iteration 3186/3560 Training loss: 1.3660 1.0304 sec/batch\n",
      "Epoch 18/20  Iteration 3187/3560 Training loss: 1.3663 1.2639 sec/batch\n",
      "Epoch 18/20  Iteration 3188/3560 Training loss: 1.3663 1.1400 sec/batch\n",
      "Epoch 18/20  Iteration 3189/3560 Training loss: 1.3663 0.9647 sec/batch\n",
      "Epoch 18/20  Iteration 3190/3560 Training loss: 1.3664 0.8551 sec/batch\n",
      "Epoch 18/20  Iteration 3191/3560 Training loss: 1.3664 0.9883 sec/batch\n",
      "Epoch 18/20  Iteration 3192/3560 Training loss: 1.3663 1.0427 sec/batch\n",
      "Epoch 18/20  Iteration 3193/3560 Training loss: 1.3664 0.8987 sec/batch\n",
      "Epoch 18/20  Iteration 3194/3560 Training loss: 1.3669 0.8781 sec/batch\n",
      "Epoch 18/20  Iteration 3195/3560 Training loss: 1.3668 0.8405 sec/batch\n",
      "Epoch 18/20  Iteration 3196/3560 Training loss: 1.3669 1.0145 sec/batch\n",
      "Epoch 18/20  Iteration 3197/3560 Training loss: 1.3669 0.8445 sec/batch\n",
      "Epoch 18/20  Iteration 3198/3560 Training loss: 1.3667 0.8620 sec/batch\n",
      "Epoch 18/20  Iteration 3199/3560 Training loss: 1.3669 1.0964 sec/batch\n",
      "Epoch 18/20  Iteration 3200/3560 Training loss: 1.3670 1.2590 sec/batch\n",
      "Epoch 18/20  Iteration 3201/3560 Training loss: 1.3670 1.1401 sec/batch\n",
      "Epoch 18/20  Iteration 3202/3560 Training loss: 1.3670 1.1462 sec/batch\n",
      "Epoch 18/20  Iteration 3203/3560 Training loss: 1.3669 1.1929 sec/batch\n",
      "Epoch 18/20  Iteration 3204/3560 Training loss: 1.3670 1.1022 sec/batch\n",
      "Epoch 19/20  Iteration 3205/3560 Training loss: 1.4509 1.0701 sec/batch\n",
      "Epoch 19/20  Iteration 3206/3560 Training loss: 1.4164 1.1775 sec/batch\n",
      "Epoch 19/20  Iteration 3207/3560 Training loss: 1.3989 0.9376 sec/batch\n",
      "Epoch 19/20  Iteration 3208/3560 Training loss: 1.3959 0.8214 sec/batch\n",
      "Epoch 19/20  Iteration 3209/3560 Training loss: 1.3853 1.0272 sec/batch\n",
      "Epoch 19/20  Iteration 3210/3560 Training loss: 1.3744 0.9424 sec/batch\n",
      "Epoch 19/20  Iteration 3211/3560 Training loss: 1.3758 1.0639 sec/batch\n",
      "Epoch 19/20  Iteration 3212/3560 Training loss: 1.3742 1.0155 sec/batch\n",
      "Epoch 19/20  Iteration 3213/3560 Training loss: 1.3733 0.8212 sec/batch\n",
      "Epoch 19/20  Iteration 3214/3560 Training loss: 1.3726 0.7605 sec/batch\n",
      "Epoch 19/20  Iteration 3215/3560 Training loss: 1.3692 0.7531 sec/batch\n",
      "Epoch 19/20  Iteration 3216/3560 Training loss: 1.3689 0.7982 sec/batch\n",
      "Epoch 19/20  Iteration 3217/3560 Training loss: 1.3683 0.8165 sec/batch\n",
      "Epoch 19/20  Iteration 3218/3560 Training loss: 1.3687 0.7866 sec/batch\n",
      "Epoch 19/20  Iteration 3219/3560 Training loss: 1.3675 0.7714 sec/batch\n",
      "Epoch 19/20  Iteration 3220/3560 Training loss: 1.3657 0.7561 sec/batch\n",
      "Epoch 19/20  Iteration 3221/3560 Training loss: 1.3661 0.7941 sec/batch\n",
      "Epoch 19/20  Iteration 3222/3560 Training loss: 1.3672 0.7708 sec/batch\n",
      "Epoch 19/20  Iteration 3223/3560 Training loss: 1.3672 0.7747 sec/batch\n",
      "Epoch 19/20  Iteration 3224/3560 Training loss: 1.3681 0.8022 sec/batch\n",
      "Epoch 19/20  Iteration 3225/3560 Training loss: 1.3678 0.8296 sec/batch\n",
      "Epoch 19/20  Iteration 3226/3560 Training loss: 1.3682 0.9745 sec/batch\n",
      "Epoch 19/20  Iteration 3227/3560 Training loss: 1.3679 0.9686 sec/batch\n",
      "Epoch 19/20  Iteration 3228/3560 Training loss: 1.3676 1.1045 sec/batch\n",
      "Epoch 19/20  Iteration 3229/3560 Training loss: 1.3677 1.3241 sec/batch\n",
      "Epoch 19/20  Iteration 3230/3560 Training loss: 1.3662 1.0758 sec/batch\n",
      "Epoch 19/20  Iteration 3231/3560 Training loss: 1.3654 1.0233 sec/batch\n",
      "Epoch 19/20  Iteration 3232/3560 Training loss: 1.3658 1.1025 sec/batch\n",
      "Epoch 19/20  Iteration 3233/3560 Training loss: 1.3662 0.9855 sec/batch\n",
      "Epoch 19/20  Iteration 3234/3560 Training loss: 1.3662 1.0594 sec/batch\n",
      "Epoch 19/20  Iteration 3235/3560 Training loss: 1.3657 0.9764 sec/batch\n",
      "Epoch 19/20  Iteration 3236/3560 Training loss: 1.3643 0.9977 sec/batch\n",
      "Epoch 19/20  Iteration 3237/3560 Training loss: 1.3645 0.9821 sec/batch\n",
      "Epoch 19/20  Iteration 3238/3560 Training loss: 1.3648 1.0403 sec/batch\n",
      "Epoch 19/20  Iteration 3239/3560 Training loss: 1.3645 0.8614 sec/batch\n",
      "Epoch 19/20  Iteration 3240/3560 Training loss: 1.3643 0.8813 sec/batch\n",
      "Epoch 19/20  Iteration 3241/3560 Training loss: 1.3637 0.8840 sec/batch\n",
      "Epoch 19/20  Iteration 3242/3560 Training loss: 1.3626 0.8252 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3243/3560 Training loss: 1.3613 0.8639 sec/batch\n",
      "Epoch 19/20  Iteration 3244/3560 Training loss: 1.3608 0.7970 sec/batch\n",
      "Epoch 19/20  Iteration 3245/3560 Training loss: 1.3601 0.8102 sec/batch\n",
      "Epoch 19/20  Iteration 3246/3560 Training loss: 1.3609 0.8119 sec/batch\n",
      "Epoch 19/20  Iteration 3247/3560 Training loss: 1.3607 0.9432 sec/batch\n",
      "Epoch 19/20  Iteration 3248/3560 Training loss: 1.3602 0.8087 sec/batch\n",
      "Epoch 19/20  Iteration 3249/3560 Training loss: 1.3604 0.8629 sec/batch\n",
      "Epoch 19/20  Iteration 3250/3560 Training loss: 1.3598 0.9767 sec/batch\n",
      "Epoch 19/20  Iteration 3251/3560 Training loss: 1.3595 1.0812 sec/batch\n",
      "Epoch 19/20  Iteration 3252/3560 Training loss: 1.3591 0.9573 sec/batch\n",
      "Epoch 19/20  Iteration 3253/3560 Training loss: 1.3590 0.8867 sec/batch\n",
      "Epoch 19/20  Iteration 3254/3560 Training loss: 1.3594 0.7816 sec/batch\n",
      "Epoch 19/20  Iteration 3255/3560 Training loss: 1.3591 0.8006 sec/batch\n",
      "Epoch 19/20  Iteration 3256/3560 Training loss: 1.3599 0.7673 sec/batch\n",
      "Epoch 19/20  Iteration 3257/3560 Training loss: 1.3599 0.7798 sec/batch\n",
      "Epoch 19/20  Iteration 3258/3560 Training loss: 1.3601 0.7522 sec/batch\n",
      "Epoch 19/20  Iteration 3259/3560 Training loss: 1.3600 0.7707 sec/batch\n",
      "Epoch 19/20  Iteration 3260/3560 Training loss: 1.3600 0.7623 sec/batch\n",
      "Epoch 19/20  Iteration 3261/3560 Training loss: 1.3605 0.7549 sec/batch\n",
      "Epoch 19/20  Iteration 3262/3560 Training loss: 1.3602 0.8199 sec/batch\n",
      "Epoch 19/20  Iteration 3263/3560 Training loss: 1.3596 0.8357 sec/batch\n",
      "Epoch 19/20  Iteration 3264/3560 Training loss: 1.3605 0.8069 sec/batch\n",
      "Epoch 19/20  Iteration 3265/3560 Training loss: 1.3605 0.8258 sec/batch\n",
      "Epoch 19/20  Iteration 3266/3560 Training loss: 1.3613 0.9716 sec/batch\n",
      "Epoch 19/20  Iteration 3267/3560 Training loss: 1.3618 0.8123 sec/batch\n",
      "Epoch 19/20  Iteration 3268/3560 Training loss: 1.3620 0.8229 sec/batch\n",
      "Epoch 19/20  Iteration 3269/3560 Training loss: 1.3620 0.8331 sec/batch\n",
      "Epoch 19/20  Iteration 3270/3560 Training loss: 1.3622 0.8534 sec/batch\n",
      "Epoch 19/20  Iteration 3271/3560 Training loss: 1.3626 0.9132 sec/batch\n",
      "Epoch 19/20  Iteration 3272/3560 Training loss: 1.3624 1.0385 sec/batch\n",
      "Epoch 19/20  Iteration 3273/3560 Training loss: 1.3624 1.0008 sec/batch\n",
      "Epoch 19/20  Iteration 3274/3560 Training loss: 1.3623 0.9837 sec/batch\n",
      "Epoch 19/20  Iteration 3275/3560 Training loss: 1.3627 0.9148 sec/batch\n",
      "Epoch 19/20  Iteration 3276/3560 Training loss: 1.3629 0.8777 sec/batch\n",
      "Epoch 19/20  Iteration 3277/3560 Training loss: 1.3634 0.8638 sec/batch\n",
      "Epoch 19/20  Iteration 3278/3560 Training loss: 1.3630 0.7931 sec/batch\n",
      "Epoch 19/20  Iteration 3279/3560 Training loss: 1.3629 0.9484 sec/batch\n",
      "Epoch 19/20  Iteration 3280/3560 Training loss: 1.3630 0.9028 sec/batch\n",
      "Epoch 19/20  Iteration 3281/3560 Training loss: 1.3631 0.7981 sec/batch\n",
      "Epoch 19/20  Iteration 3282/3560 Training loss: 1.3630 0.8458 sec/batch\n",
      "Epoch 19/20  Iteration 3283/3560 Training loss: 1.3625 0.8122 sec/batch\n",
      "Epoch 19/20  Iteration 3284/3560 Training loss: 1.3623 1.0877 sec/batch\n",
      "Epoch 19/20  Iteration 3285/3560 Training loss: 1.3618 1.0297 sec/batch\n",
      "Epoch 19/20  Iteration 3286/3560 Training loss: 1.3618 1.1357 sec/batch\n",
      "Epoch 19/20  Iteration 3287/3560 Training loss: 1.3614 1.6720 sec/batch\n",
      "Epoch 19/20  Iteration 3288/3560 Training loss: 1.3615 1.0062 sec/batch\n",
      "Epoch 19/20  Iteration 3289/3560 Training loss: 1.3612 1.2381 sec/batch\n",
      "Epoch 19/20  Iteration 3290/3560 Training loss: 1.3611 0.9484 sec/batch\n",
      "Epoch 19/20  Iteration 3291/3560 Training loss: 1.3609 0.8517 sec/batch\n",
      "Epoch 19/20  Iteration 3292/3560 Training loss: 1.3605 0.7860 sec/batch\n",
      "Epoch 19/20  Iteration 3293/3560 Training loss: 1.3602 0.8558 sec/batch\n",
      "Epoch 19/20  Iteration 3294/3560 Training loss: 1.3603 0.8653 sec/batch\n",
      "Epoch 19/20  Iteration 3295/3560 Training loss: 1.3600 0.8198 sec/batch\n",
      "Epoch 19/20  Iteration 3296/3560 Training loss: 1.3599 0.7993 sec/batch\n",
      "Epoch 19/20  Iteration 3297/3560 Training loss: 1.3595 0.8657 sec/batch\n",
      "Epoch 19/20  Iteration 3298/3560 Training loss: 1.3592 0.8629 sec/batch\n",
      "Epoch 19/20  Iteration 3299/3560 Training loss: 1.3590 1.3162 sec/batch\n",
      "Epoch 19/20  Iteration 3300/3560 Training loss: 1.3591 1.0766 sec/batch\n",
      "Epoch 19/20  Iteration 3301/3560 Training loss: 1.3591 1.0049 sec/batch\n",
      "Epoch 19/20  Iteration 3302/3560 Training loss: 1.3588 1.2689 sec/batch\n",
      "Epoch 19/20  Iteration 3303/3560 Training loss: 1.3585 1.0968 sec/batch\n",
      "Epoch 19/20  Iteration 3304/3560 Training loss: 1.3582 1.0887 sec/batch\n",
      "Epoch 19/20  Iteration 3305/3560 Training loss: 1.3583 0.9659 sec/batch\n",
      "Epoch 19/20  Iteration 3306/3560 Training loss: 1.3582 0.8754 sec/batch\n",
      "Epoch 19/20  Iteration 3307/3560 Training loss: 1.3581 0.8367 sec/batch\n",
      "Epoch 19/20  Iteration 3308/3560 Training loss: 1.3579 1.0167 sec/batch\n",
      "Epoch 19/20  Iteration 3309/3560 Training loss: 1.3577 0.7988 sec/batch\n",
      "Epoch 19/20  Iteration 3310/3560 Training loss: 1.3576 0.8410 sec/batch\n",
      "Epoch 19/20  Iteration 3311/3560 Training loss: 1.3576 0.8292 sec/batch\n",
      "Epoch 19/20  Iteration 3312/3560 Training loss: 1.3576 0.8132 sec/batch\n",
      "Epoch 19/20  Iteration 3313/3560 Training loss: 1.3574 1.0033 sec/batch\n",
      "Epoch 19/20  Iteration 3314/3560 Training loss: 1.3575 0.8622 sec/batch\n",
      "Epoch 19/20  Iteration 3315/3560 Training loss: 1.3574 0.8842 sec/batch\n",
      "Epoch 19/20  Iteration 3316/3560 Training loss: 1.3574 0.8425 sec/batch\n",
      "Epoch 19/20  Iteration 3317/3560 Training loss: 1.3572 0.8690 sec/batch\n",
      "Epoch 19/20  Iteration 3318/3560 Training loss: 1.3572 0.8773 sec/batch\n",
      "Epoch 19/20  Iteration 3319/3560 Training loss: 1.3570 0.8443 sec/batch\n",
      "Epoch 19/20  Iteration 3320/3560 Training loss: 1.3566 0.8124 sec/batch\n",
      "Epoch 19/20  Iteration 3321/3560 Training loss: 1.3566 0.8446 sec/batch\n",
      "Epoch 19/20  Iteration 3322/3560 Training loss: 1.3567 0.8238 sec/batch\n",
      "Epoch 19/20  Iteration 3323/3560 Training loss: 1.3566 0.9124 sec/batch\n",
      "Epoch 19/20  Iteration 3324/3560 Training loss: 1.3565 0.8961 sec/batch\n",
      "Epoch 19/20  Iteration 3325/3560 Training loss: 1.3564 0.9112 sec/batch\n",
      "Epoch 19/20  Iteration 3326/3560 Training loss: 1.3561 0.8791 sec/batch\n",
      "Epoch 19/20  Iteration 3327/3560 Training loss: 1.3557 0.8687 sec/batch\n",
      "Epoch 19/20  Iteration 3328/3560 Training loss: 1.3558 0.7635 sec/batch\n",
      "Epoch 19/20  Iteration 3329/3560 Training loss: 1.3558 0.8265 sec/batch\n",
      "Epoch 19/20  Iteration 3330/3560 Training loss: 1.3554 0.7657 sec/batch\n",
      "Epoch 19/20  Iteration 3331/3560 Training loss: 1.3555 0.8424 sec/batch\n",
      "Epoch 19/20  Iteration 3332/3560 Training loss: 1.3555 0.8787 sec/batch\n",
      "Epoch 19/20  Iteration 3333/3560 Training loss: 1.3554 0.9126 sec/batch\n",
      "Epoch 19/20  Iteration 3334/3560 Training loss: 1.3551 0.8355 sec/batch\n",
      "Epoch 19/20  Iteration 3335/3560 Training loss: 1.3548 0.7810 sec/batch\n",
      "Epoch 19/20  Iteration 3336/3560 Training loss: 1.3546 0.8234 sec/batch\n",
      "Epoch 19/20  Iteration 3337/3560 Training loss: 1.3547 0.8172 sec/batch\n",
      "Epoch 19/20  Iteration 3338/3560 Training loss: 1.3547 0.7939 sec/batch\n",
      "Epoch 19/20  Iteration 3339/3560 Training loss: 1.3547 0.8267 sec/batch\n",
      "Epoch 19/20  Iteration 3340/3560 Training loss: 1.3548 0.7632 sec/batch\n",
      "Epoch 19/20  Iteration 3341/3560 Training loss: 1.3550 0.8453 sec/batch\n",
      "Epoch 19/20  Iteration 3342/3560 Training loss: 1.3551 0.9037 sec/batch\n",
      "Epoch 19/20  Iteration 3343/3560 Training loss: 1.3550 1.0114 sec/batch\n",
      "Epoch 19/20  Iteration 3344/3560 Training loss: 1.3551 1.0572 sec/batch\n",
      "Epoch 19/20  Iteration 3345/3560 Training loss: 1.3555 1.0459 sec/batch\n",
      "Epoch 19/20  Iteration 3346/3560 Training loss: 1.3555 0.9968 sec/batch\n",
      "Epoch 19/20  Iteration 3347/3560 Training loss: 1.3554 1.0106 sec/batch\n",
      "Epoch 19/20  Iteration 3348/3560 Training loss: 1.3557 1.0765 sec/batch\n",
      "Epoch 19/20  Iteration 3349/3560 Training loss: 1.3556 1.0281 sec/batch\n",
      "Epoch 19/20  Iteration 3350/3560 Training loss: 1.3559 0.8655 sec/batch\n",
      "Epoch 19/20  Iteration 3351/3560 Training loss: 1.3559 0.8380 sec/batch\n",
      "Epoch 19/20  Iteration 3352/3560 Training loss: 1.3562 0.8010 sec/batch\n",
      "Epoch 19/20  Iteration 3353/3560 Training loss: 1.3564 0.8006 sec/batch\n",
      "Epoch 19/20  Iteration 3354/3560 Training loss: 1.3563 0.7871 sec/batch\n",
      "Epoch 19/20  Iteration 3355/3560 Training loss: 1.3561 0.8075 sec/batch\n",
      "Epoch 19/20  Iteration 3356/3560 Training loss: 1.3559 0.8176 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3357/3560 Training loss: 1.3560 0.7901 sec/batch\n",
      "Epoch 19/20  Iteration 3358/3560 Training loss: 1.3560 0.8283 sec/batch\n",
      "Epoch 19/20  Iteration 3359/3560 Training loss: 1.3560 0.8218 sec/batch\n",
      "Epoch 19/20  Iteration 3360/3560 Training loss: 1.3561 0.7824 sec/batch\n",
      "Epoch 19/20  Iteration 3361/3560 Training loss: 1.3561 0.7755 sec/batch\n",
      "Epoch 19/20  Iteration 3362/3560 Training loss: 1.3561 0.7919 sec/batch\n",
      "Epoch 19/20  Iteration 3363/3560 Training loss: 1.3559 0.8968 sec/batch\n",
      "Epoch 19/20  Iteration 3364/3560 Training loss: 1.3561 0.7759 sec/batch\n",
      "Epoch 19/20  Iteration 3365/3560 Training loss: 1.3563 0.7665 sec/batch\n",
      "Epoch 19/20  Iteration 3366/3560 Training loss: 1.3563 0.7979 sec/batch\n",
      "Epoch 19/20  Iteration 3367/3560 Training loss: 1.3563 0.8344 sec/batch\n",
      "Epoch 19/20  Iteration 3368/3560 Training loss: 1.3563 0.7812 sec/batch\n",
      "Epoch 19/20  Iteration 3369/3560 Training loss: 1.3564 0.7911 sec/batch\n",
      "Epoch 19/20  Iteration 3370/3560 Training loss: 1.3564 0.8048 sec/batch\n",
      "Epoch 19/20  Iteration 3371/3560 Training loss: 1.3566 0.8028 sec/batch\n",
      "Epoch 19/20  Iteration 3372/3560 Training loss: 1.3571 0.8221 sec/batch\n",
      "Epoch 19/20  Iteration 3373/3560 Training loss: 1.3571 0.7941 sec/batch\n",
      "Epoch 19/20  Iteration 3374/3560 Training loss: 1.3571 0.8169 sec/batch\n",
      "Epoch 19/20  Iteration 3375/3560 Training loss: 1.3571 0.8158 sec/batch\n",
      "Epoch 19/20  Iteration 3376/3560 Training loss: 1.3570 0.8302 sec/batch\n",
      "Epoch 19/20  Iteration 3377/3560 Training loss: 1.3572 0.8160 sec/batch\n",
      "Epoch 19/20  Iteration 3378/3560 Training loss: 1.3572 0.8035 sec/batch\n",
      "Epoch 19/20  Iteration 3379/3560 Training loss: 1.3573 0.8111 sec/batch\n",
      "Epoch 19/20  Iteration 3380/3560 Training loss: 1.3572 0.8256 sec/batch\n",
      "Epoch 19/20  Iteration 3381/3560 Training loss: 1.3571 0.7953 sec/batch\n",
      "Epoch 19/20  Iteration 3382/3560 Training loss: 1.3574 0.7789 sec/batch\n",
      "Epoch 20/20  Iteration 3383/3560 Training loss: 1.4427 0.7987 sec/batch\n",
      "Epoch 20/20  Iteration 3384/3560 Training loss: 1.4057 0.8305 sec/batch\n",
      "Epoch 20/20  Iteration 3385/3560 Training loss: 1.3950 0.7800 sec/batch\n",
      "Epoch 20/20  Iteration 3386/3560 Training loss: 1.3915 0.7661 sec/batch\n",
      "Epoch 20/20  Iteration 3387/3560 Training loss: 1.3827 0.8115 sec/batch\n",
      "Epoch 20/20  Iteration 3388/3560 Training loss: 1.3712 0.7963 sec/batch\n",
      "Epoch 20/20  Iteration 3389/3560 Training loss: 1.3694 0.8135 sec/batch\n",
      "Epoch 20/20  Iteration 3390/3560 Training loss: 1.3651 0.7708 sec/batch\n",
      "Epoch 20/20  Iteration 3391/3560 Training loss: 1.3652 0.7593 sec/batch\n",
      "Epoch 20/20  Iteration 3392/3560 Training loss: 1.3638 0.8153 sec/batch\n",
      "Epoch 20/20  Iteration 3393/3560 Training loss: 1.3614 0.7815 sec/batch\n",
      "Epoch 20/20  Iteration 3394/3560 Training loss: 1.3611 0.7828 sec/batch\n",
      "Epoch 20/20  Iteration 3395/3560 Training loss: 1.3602 0.8041 sec/batch\n",
      "Epoch 20/20  Iteration 3396/3560 Training loss: 1.3605 0.8189 sec/batch\n",
      "Epoch 20/20  Iteration 3397/3560 Training loss: 1.3597 0.7910 sec/batch\n",
      "Epoch 20/20  Iteration 3398/3560 Training loss: 1.3580 0.7715 sec/batch\n",
      "Epoch 20/20  Iteration 3399/3560 Training loss: 1.3581 0.7816 sec/batch\n",
      "Epoch 20/20  Iteration 3400/3560 Training loss: 1.3589 0.9048 sec/batch\n",
      "Epoch 20/20  Iteration 3401/3560 Training loss: 1.3589 0.7805 sec/batch\n",
      "Epoch 20/20  Iteration 3402/3560 Training loss: 1.3594 0.7939 sec/batch\n",
      "Epoch 20/20  Iteration 3403/3560 Training loss: 1.3593 0.7617 sec/batch\n",
      "Epoch 20/20  Iteration 3404/3560 Training loss: 1.3590 0.8456 sec/batch\n",
      "Epoch 20/20  Iteration 3405/3560 Training loss: 1.3586 0.7709 sec/batch\n",
      "Epoch 20/20  Iteration 3406/3560 Training loss: 1.3588 0.7757 sec/batch\n",
      "Epoch 20/20  Iteration 3407/3560 Training loss: 1.3586 0.7700 sec/batch\n",
      "Epoch 20/20  Iteration 3408/3560 Training loss: 1.3568 0.8025 sec/batch\n",
      "Epoch 20/20  Iteration 3409/3560 Training loss: 1.3561 0.7930 sec/batch\n",
      "Epoch 20/20  Iteration 3410/3560 Training loss: 1.3568 0.8051 sec/batch\n",
      "Epoch 20/20  Iteration 3411/3560 Training loss: 1.3571 0.7892 sec/batch\n",
      "Epoch 20/20  Iteration 3412/3560 Training loss: 1.3574 0.7784 sec/batch\n",
      "Epoch 20/20  Iteration 3413/3560 Training loss: 1.3568 0.8473 sec/batch\n",
      "Epoch 20/20  Iteration 3414/3560 Training loss: 1.3556 0.7851 sec/batch\n",
      "Epoch 20/20  Iteration 3415/3560 Training loss: 1.3558 0.8928 sec/batch\n",
      "Epoch 20/20  Iteration 3416/3560 Training loss: 1.3561 0.8307 sec/batch\n",
      "Epoch 20/20  Iteration 3417/3560 Training loss: 1.3560 0.8206 sec/batch\n",
      "Epoch 20/20  Iteration 3418/3560 Training loss: 1.3559 0.8303 sec/batch\n",
      "Epoch 20/20  Iteration 3419/3560 Training loss: 1.3551 0.7644 sec/batch\n",
      "Epoch 20/20  Iteration 3420/3560 Training loss: 1.3542 0.7916 sec/batch\n",
      "Epoch 20/20  Iteration 3421/3560 Training loss: 1.3534 0.8246 sec/batch\n",
      "Epoch 20/20  Iteration 3422/3560 Training loss: 1.3529 0.7797 sec/batch\n",
      "Epoch 20/20  Iteration 3423/3560 Training loss: 1.3526 0.8098 sec/batch\n",
      "Epoch 20/20  Iteration 3424/3560 Training loss: 1.3533 0.8009 sec/batch\n",
      "Epoch 20/20  Iteration 3425/3560 Training loss: 1.3530 0.8575 sec/batch\n",
      "Epoch 20/20  Iteration 3426/3560 Training loss: 1.3522 0.8035 sec/batch\n",
      "Epoch 20/20  Iteration 3427/3560 Training loss: 1.3525 0.7745 sec/batch\n",
      "Epoch 20/20  Iteration 3428/3560 Training loss: 1.3517 0.7970 sec/batch\n",
      "Epoch 20/20  Iteration 3429/3560 Training loss: 1.3515 0.8016 sec/batch\n",
      "Epoch 20/20  Iteration 3430/3560 Training loss: 1.3511 0.7842 sec/batch\n",
      "Epoch 20/20  Iteration 3431/3560 Training loss: 1.3511 0.8001 sec/batch\n",
      "Epoch 20/20  Iteration 3432/3560 Training loss: 1.3513 0.7744 sec/batch\n",
      "Epoch 20/20  Iteration 3433/3560 Training loss: 1.3510 0.8460 sec/batch\n",
      "Epoch 20/20  Iteration 3434/3560 Training loss: 1.3517 0.7944 sec/batch\n",
      "Epoch 20/20  Iteration 3435/3560 Training loss: 1.3519 0.7807 sec/batch\n",
      "Epoch 20/20  Iteration 3436/3560 Training loss: 1.3521 0.8006 sec/batch\n",
      "Epoch 20/20  Iteration 3437/3560 Training loss: 1.3519 0.8244 sec/batch\n",
      "Epoch 20/20  Iteration 3438/3560 Training loss: 1.3520 0.8580 sec/batch\n",
      "Epoch 20/20  Iteration 3439/3560 Training loss: 1.3524 0.8016 sec/batch\n",
      "Epoch 20/20  Iteration 3440/3560 Training loss: 1.3523 0.7778 sec/batch\n",
      "Epoch 20/20  Iteration 3441/3560 Training loss: 1.3517 0.8339 sec/batch\n",
      "Epoch 20/20  Iteration 3442/3560 Training loss: 1.3524 0.7832 sec/batch\n",
      "Epoch 20/20  Iteration 3443/3560 Training loss: 1.3525 0.8066 sec/batch\n",
      "Epoch 20/20  Iteration 3444/3560 Training loss: 1.3533 0.7917 sec/batch\n",
      "Epoch 20/20  Iteration 3445/3560 Training loss: 1.3537 0.8355 sec/batch\n",
      "Epoch 20/20  Iteration 3446/3560 Training loss: 1.3537 0.7857 sec/batch\n",
      "Epoch 20/20  Iteration 3447/3560 Training loss: 1.3539 0.7738 sec/batch\n",
      "Epoch 20/20  Iteration 3448/3560 Training loss: 1.3541 0.7776 sec/batch\n",
      "Epoch 20/20  Iteration 3449/3560 Training loss: 1.3544 0.8340 sec/batch\n",
      "Epoch 20/20  Iteration 3450/3560 Training loss: 1.3541 0.8066 sec/batch\n",
      "Epoch 20/20  Iteration 3451/3560 Training loss: 1.3542 0.8159 sec/batch\n",
      "Epoch 20/20  Iteration 3452/3560 Training loss: 1.3541 0.8044 sec/batch\n",
      "Epoch 20/20  Iteration 3453/3560 Training loss: 1.3546 0.8350 sec/batch\n",
      "Epoch 20/20  Iteration 3454/3560 Training loss: 1.3548 0.7801 sec/batch\n",
      "Epoch 20/20  Iteration 3455/3560 Training loss: 1.3553 0.7801 sec/batch\n",
      "Epoch 20/20  Iteration 3456/3560 Training loss: 1.3550 0.7555 sec/batch\n",
      "Epoch 20/20  Iteration 3457/3560 Training loss: 1.3549 0.8292 sec/batch\n",
      "Epoch 20/20  Iteration 3458/3560 Training loss: 1.3550 0.7825 sec/batch\n",
      "Epoch 20/20  Iteration 3459/3560 Training loss: 1.3550 0.7947 sec/batch\n",
      "Epoch 20/20  Iteration 3460/3560 Training loss: 1.3547 0.7947 sec/batch\n",
      "Epoch 20/20  Iteration 3461/3560 Training loss: 1.3541 0.8337 sec/batch\n",
      "Epoch 20/20  Iteration 3462/3560 Training loss: 1.3539 0.7975 sec/batch\n",
      "Epoch 20/20  Iteration 3463/3560 Training loss: 1.3535 0.8081 sec/batch\n",
      "Epoch 20/20  Iteration 3464/3560 Training loss: 1.3535 0.7864 sec/batch\n",
      "Epoch 20/20  Iteration 3465/3560 Training loss: 1.3531 0.8010 sec/batch\n",
      "Epoch 20/20  Iteration 3466/3560 Training loss: 1.3531 0.7983 sec/batch\n",
      "Epoch 20/20  Iteration 3467/3560 Training loss: 1.3530 0.7916 sec/batch\n",
      "Epoch 20/20  Iteration 3468/3560 Training loss: 1.3528 0.7855 sec/batch\n",
      "Epoch 20/20  Iteration 3469/3560 Training loss: 1.3524 0.8030 sec/batch\n",
      "Epoch 20/20  Iteration 3470/3560 Training loss: 1.3523 0.8208 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3471/3560 Training loss: 1.3519 0.7833 sec/batch\n",
      "Epoch 20/20  Iteration 3472/3560 Training loss: 1.3520 0.7998 sec/batch\n",
      "Epoch 20/20  Iteration 3473/3560 Training loss: 1.3518 0.7713 sec/batch\n",
      "Epoch 20/20  Iteration 3474/3560 Training loss: 1.3517 0.8284 sec/batch\n",
      "Epoch 20/20  Iteration 3475/3560 Training loss: 1.3514 0.8166 sec/batch\n",
      "Epoch 20/20  Iteration 3476/3560 Training loss: 1.3513 0.8108 sec/batch\n",
      "Epoch 20/20  Iteration 3477/3560 Training loss: 1.3511 0.7632 sec/batch\n",
      "Epoch 20/20  Iteration 3478/3560 Training loss: 1.3512 0.8727 sec/batch\n",
      "Epoch 20/20  Iteration 3479/3560 Training loss: 1.3513 0.8126 sec/batch\n",
      "Epoch 20/20  Iteration 3480/3560 Training loss: 1.3510 0.8127 sec/batch\n",
      "Epoch 20/20  Iteration 3481/3560 Training loss: 1.3506 0.8097 sec/batch\n",
      "Epoch 20/20  Iteration 3482/3560 Training loss: 1.3502 0.8367 sec/batch\n",
      "Epoch 20/20  Iteration 3483/3560 Training loss: 1.3502 0.8425 sec/batch\n",
      "Epoch 20/20  Iteration 3484/3560 Training loss: 1.3502 0.7798 sec/batch\n",
      "Epoch 20/20  Iteration 3485/3560 Training loss: 1.3501 0.8073 sec/batch\n",
      "Epoch 20/20  Iteration 3486/3560 Training loss: 1.3500 0.8309 sec/batch\n",
      "Epoch 20/20  Iteration 3487/3560 Training loss: 1.3498 0.8343 sec/batch\n",
      "Epoch 20/20  Iteration 3488/3560 Training loss: 1.3497 0.9067 sec/batch\n",
      "Epoch 20/20  Iteration 3489/3560 Training loss: 1.3497 1.0312 sec/batch\n",
      "Epoch 20/20  Iteration 3490/3560 Training loss: 1.3496 0.8288 sec/batch\n",
      "Epoch 20/20  Iteration 3491/3560 Training loss: 1.3494 0.8162 sec/batch\n",
      "Epoch 20/20  Iteration 3492/3560 Training loss: 1.3495 0.8320 sec/batch\n",
      "Epoch 20/20  Iteration 3493/3560 Training loss: 1.3492 0.8207 sec/batch\n",
      "Epoch 20/20  Iteration 3494/3560 Training loss: 1.3492 0.8240 sec/batch\n",
      "Epoch 20/20  Iteration 3495/3560 Training loss: 1.3490 0.8259 sec/batch\n",
      "Epoch 20/20  Iteration 3496/3560 Training loss: 1.3489 0.8036 sec/batch\n",
      "Epoch 20/20  Iteration 3497/3560 Training loss: 1.3486 0.8107 sec/batch\n",
      "Epoch 20/20  Iteration 3498/3560 Training loss: 1.3484 0.8025 sec/batch\n",
      "Epoch 20/20  Iteration 3499/3560 Training loss: 1.3483 0.7991 sec/batch\n",
      "Epoch 20/20  Iteration 3500/3560 Training loss: 1.3483 0.8352 sec/batch\n",
      "Epoch 20/20  Iteration 3501/3560 Training loss: 1.3483 0.7814 sec/batch\n",
      "Epoch 20/20  Iteration 3502/3560 Training loss: 1.3481 0.7988 sec/batch\n",
      "Epoch 20/20  Iteration 3503/3560 Training loss: 1.3480 0.7851 sec/batch\n",
      "Epoch 20/20  Iteration 3504/3560 Training loss: 1.3477 0.8375 sec/batch\n",
      "Epoch 20/20  Iteration 3505/3560 Training loss: 1.3473 0.8469 sec/batch\n",
      "Epoch 20/20  Iteration 3506/3560 Training loss: 1.3473 0.7841 sec/batch\n",
      "Epoch 20/20  Iteration 3507/3560 Training loss: 1.3473 0.8322 sec/batch\n",
      "Epoch 20/20  Iteration 3508/3560 Training loss: 1.3470 0.7880 sec/batch\n",
      "Epoch 20/20  Iteration 3509/3560 Training loss: 1.3471 0.8435 sec/batch\n",
      "Epoch 20/20  Iteration 3510/3560 Training loss: 1.3471 1.2522 sec/batch\n",
      "Epoch 20/20  Iteration 3511/3560 Training loss: 1.3469 1.2020 sec/batch\n",
      "Epoch 20/20  Iteration 3512/3560 Training loss: 1.3467 1.2819 sec/batch\n",
      "Epoch 20/20  Iteration 3513/3560 Training loss: 1.3463 1.0772 sec/batch\n",
      "Epoch 20/20  Iteration 3514/3560 Training loss: 1.3461 1.0608 sec/batch\n",
      "Epoch 20/20  Iteration 3515/3560 Training loss: 1.3462 1.2651 sec/batch\n",
      "Epoch 20/20  Iteration 3516/3560 Training loss: 1.3462 1.0526 sec/batch\n",
      "Epoch 20/20  Iteration 3517/3560 Training loss: 1.3462 0.9013 sec/batch\n",
      "Epoch 20/20  Iteration 3518/3560 Training loss: 1.3462 0.9877 sec/batch\n",
      "Epoch 20/20  Iteration 3519/3560 Training loss: 1.3465 0.8949 sec/batch\n",
      "Epoch 20/20  Iteration 3520/3560 Training loss: 1.3466 0.7639 sec/batch\n",
      "Epoch 20/20  Iteration 3521/3560 Training loss: 1.3466 0.9975 sec/batch\n",
      "Epoch 20/20  Iteration 3522/3560 Training loss: 1.3466 0.8266 sec/batch\n",
      "Epoch 20/20  Iteration 3523/3560 Training loss: 1.3470 0.8081 sec/batch\n",
      "Epoch 20/20  Iteration 3524/3560 Training loss: 1.3470 0.9424 sec/batch\n",
      "Epoch 20/20  Iteration 3525/3560 Training loss: 1.3470 0.8011 sec/batch\n",
      "Epoch 20/20  Iteration 3526/3560 Training loss: 1.3472 0.8373 sec/batch\n",
      "Epoch 20/20  Iteration 3527/3560 Training loss: 1.3471 0.8276 sec/batch\n",
      "Epoch 20/20  Iteration 3528/3560 Training loss: 1.3473 0.8867 sec/batch\n",
      "Epoch 20/20  Iteration 3529/3560 Training loss: 1.3473 0.8293 sec/batch\n",
      "Epoch 20/20  Iteration 3530/3560 Training loss: 1.3475 0.9512 sec/batch\n",
      "Epoch 20/20  Iteration 3531/3560 Training loss: 1.3477 0.8844 sec/batch\n",
      "Epoch 20/20  Iteration 3532/3560 Training loss: 1.3476 0.9647 sec/batch\n",
      "Epoch 20/20  Iteration 3533/3560 Training loss: 1.3473 0.9835 sec/batch\n",
      "Epoch 20/20  Iteration 3534/3560 Training loss: 1.3472 0.9486 sec/batch\n",
      "Epoch 20/20  Iteration 3535/3560 Training loss: 1.3473 0.8396 sec/batch\n",
      "Epoch 20/20  Iteration 3536/3560 Training loss: 1.3473 0.7973 sec/batch\n",
      "Epoch 20/20  Iteration 3537/3560 Training loss: 1.3473 0.8267 sec/batch\n",
      "Epoch 20/20  Iteration 3538/3560 Training loss: 1.3473 0.8172 sec/batch\n",
      "Epoch 20/20  Iteration 3539/3560 Training loss: 1.3473 0.8471 sec/batch\n",
      "Epoch 20/20  Iteration 3540/3560 Training loss: 1.3472 0.7642 sec/batch\n",
      "Epoch 20/20  Iteration 3541/3560 Training loss: 1.3470 0.8011 sec/batch\n",
      "Epoch 20/20  Iteration 3542/3560 Training loss: 1.3471 0.8851 sec/batch\n",
      "Epoch 20/20  Iteration 3543/3560 Training loss: 1.3474 0.8793 sec/batch\n",
      "Epoch 20/20  Iteration 3544/3560 Training loss: 1.3474 0.9117 sec/batch\n",
      "Epoch 20/20  Iteration 3545/3560 Training loss: 1.3475 0.8439 sec/batch\n",
      "Epoch 20/20  Iteration 3546/3560 Training loss: 1.3475 0.8458 sec/batch\n",
      "Epoch 20/20  Iteration 3547/3560 Training loss: 1.3475 0.8453 sec/batch\n",
      "Epoch 20/20  Iteration 3548/3560 Training loss: 1.3475 0.8630 sec/batch\n",
      "Epoch 20/20  Iteration 3549/3560 Training loss: 1.3477 0.8086 sec/batch\n",
      "Epoch 20/20  Iteration 3550/3560 Training loss: 1.3481 0.7886 sec/batch\n",
      "Epoch 20/20  Iteration 3551/3560 Training loss: 1.3481 0.7935 sec/batch\n",
      "Epoch 20/20  Iteration 3552/3560 Training loss: 1.3482 0.7648 sec/batch\n",
      "Epoch 20/20  Iteration 3553/3560 Training loss: 1.3481 0.7967 sec/batch\n",
      "Epoch 20/20  Iteration 3554/3560 Training loss: 1.3480 0.8113 sec/batch\n",
      "Epoch 20/20  Iteration 3555/3560 Training loss: 1.3482 0.8237 sec/batch\n",
      "Epoch 20/20  Iteration 3556/3560 Training loss: 1.3482 1.1644 sec/batch\n",
      "Epoch 20/20  Iteration 3557/3560 Training loss: 1.3484 1.3351 sec/batch\n",
      "Epoch 20/20  Iteration 3558/3560 Training loss: 1.3483 0.8563 sec/batch\n",
      "Epoch 20/20  Iteration 3559/3560 Training loss: 1.3483 0.8067 sec/batch\n",
      "Epoch 20/20  Iteration 3560/3560 Training loss: 1.3484 0.7877 sec/batch\n",
      "Epoch 1/20  Iteration 1/3560 Training loss: 4.4173 0.8525 sec/batch\n",
      "Epoch 1/20  Iteration 2/3560 Training loss: 4.3971 0.7731 sec/batch\n",
      "Epoch 1/20  Iteration 3/3560 Training loss: 4.3623 0.7802 sec/batch\n",
      "Epoch 1/20  Iteration 4/3560 Training loss: 4.2732 0.8267 sec/batch\n",
      "Epoch 1/20  Iteration 5/3560 Training loss: 4.1654 1.0620 sec/batch\n",
      "Epoch 1/20  Iteration 6/3560 Training loss: 4.0796 1.1496 sec/batch\n",
      "Epoch 1/20  Iteration 7/3560 Training loss: 4.0014 0.9242 sec/batch\n",
      "Epoch 1/20  Iteration 8/3560 Training loss: 3.9360 0.9145 sec/batch\n",
      "Epoch 1/20  Iteration 9/3560 Training loss: 3.8787 0.9036 sec/batch\n",
      "Epoch 1/20  Iteration 10/3560 Training loss: 3.8309 0.8133 sec/batch\n",
      "Epoch 1/20  Iteration 11/3560 Training loss: 3.7878 0.8913 sec/batch\n",
      "Epoch 1/20  Iteration 12/3560 Training loss: 3.7525 0.8692 sec/batch\n",
      "Epoch 1/20  Iteration 13/3560 Training loss: 3.7204 0.8214 sec/batch\n",
      "Epoch 1/20  Iteration 14/3560 Training loss: 3.6924 1.0832 sec/batch\n",
      "Epoch 1/20  Iteration 15/3560 Training loss: 3.6668 1.0562 sec/batch\n",
      "Epoch 1/20  Iteration 16/3560 Training loss: 3.6440 0.8975 sec/batch\n",
      "Epoch 1/20  Iteration 17/3560 Training loss: 3.6228 1.0646 sec/batch\n",
      "Epoch 1/20  Iteration 18/3560 Training loss: 3.6051 0.9972 sec/batch\n",
      "Epoch 1/20  Iteration 19/3560 Training loss: 3.5885 0.9442 sec/batch\n",
      "Epoch 1/20  Iteration 20/3560 Training loss: 3.5711 0.8878 sec/batch\n",
      "Epoch 1/20  Iteration 21/3560 Training loss: 3.5563 0.9295 sec/batch\n",
      "Epoch 1/20  Iteration 22/3560 Training loss: 3.5422 0.8817 sec/batch\n",
      "Epoch 1/20  Iteration 23/3560 Training loss: 3.5290 0.9379 sec/batch\n",
      "Epoch 1/20  Iteration 24/3560 Training loss: 3.5169 0.8676 sec/batch\n",
      "Epoch 1/20  Iteration 25/3560 Training loss: 3.5053 0.9183 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 26/3560 Training loss: 3.4952 0.8895 sec/batch\n",
      "Epoch 1/20  Iteration 27/3560 Training loss: 3.4854 0.9135 sec/batch\n",
      "Epoch 1/20  Iteration 28/3560 Training loss: 3.4757 0.9482 sec/batch\n",
      "Epoch 1/20  Iteration 29/3560 Training loss: 3.4668 0.9194 sec/batch\n",
      "Epoch 1/20  Iteration 30/3560 Training loss: 3.4584 0.8731 sec/batch\n",
      "Epoch 1/20  Iteration 31/3560 Training loss: 3.4513 0.8706 sec/batch\n",
      "Epoch 1/20  Iteration 32/3560 Training loss: 3.4437 0.8903 sec/batch\n",
      "Epoch 1/20  Iteration 33/3560 Training loss: 3.4364 0.9289 sec/batch\n",
      "Epoch 1/20  Iteration 34/3560 Training loss: 3.4298 0.8955 sec/batch\n",
      "Epoch 1/20  Iteration 35/3560 Training loss: 3.4233 0.9075 sec/batch\n",
      "Epoch 1/20  Iteration 36/3560 Training loss: 3.4173 0.8788 sec/batch\n",
      "Epoch 1/20  Iteration 37/3560 Training loss: 3.4110 0.7955 sec/batch\n",
      "Epoch 1/20  Iteration 38/3560 Training loss: 3.4051 0.7707 sec/batch\n",
      "Epoch 1/20  Iteration 39/3560 Training loss: 3.3992 0.9051 sec/batch\n",
      "Epoch 1/20  Iteration 40/3560 Training loss: 3.3936 0.8191 sec/batch\n",
      "Epoch 1/20  Iteration 41/3560 Training loss: 3.3881 0.8110 sec/batch\n",
      "Epoch 1/20  Iteration 42/3560 Training loss: 3.3830 0.8010 sec/batch\n",
      "Epoch 1/20  Iteration 43/3560 Training loss: 3.3782 0.8144 sec/batch\n",
      "Epoch 1/20  Iteration 44/3560 Training loss: 3.3734 0.7750 sec/batch\n",
      "Epoch 1/20  Iteration 45/3560 Training loss: 3.3687 0.8294 sec/batch\n",
      "Epoch 1/20  Iteration 46/3560 Training loss: 3.3645 0.8004 sec/batch\n",
      "Epoch 1/20  Iteration 47/3560 Training loss: 3.3605 0.7997 sec/batch\n",
      "Epoch 1/20  Iteration 48/3560 Training loss: 3.3568 0.7811 sec/batch\n",
      "Epoch 1/20  Iteration 49/3560 Training loss: 3.3531 0.8393 sec/batch\n",
      "Epoch 1/20  Iteration 50/3560 Training loss: 3.3497 0.8153 sec/batch\n",
      "Epoch 1/20  Iteration 51/3560 Training loss: 3.3462 0.7900 sec/batch\n",
      "Epoch 1/20  Iteration 52/3560 Training loss: 3.3427 0.7878 sec/batch\n",
      "Epoch 1/20  Iteration 53/3560 Training loss: 3.3395 0.8367 sec/batch\n",
      "Epoch 1/20  Iteration 54/3560 Training loss: 3.3363 0.8598 sec/batch\n",
      "Epoch 1/20  Iteration 55/3560 Training loss: 3.3332 0.7770 sec/batch\n",
      "Epoch 1/20  Iteration 56/3560 Training loss: 3.3299 0.8139 sec/batch\n",
      "Epoch 1/20  Iteration 57/3560 Training loss: 3.3268 0.8133 sec/batch\n",
      "Epoch 1/20  Iteration 58/3560 Training loss: 3.3239 0.7773 sec/batch\n",
      "Epoch 1/20  Iteration 59/3560 Training loss: 3.3208 0.8158 sec/batch\n",
      "Epoch 1/20  Iteration 60/3560 Training loss: 3.3181 0.8044 sec/batch\n",
      "Epoch 1/20  Iteration 61/3560 Training loss: 3.3155 0.7954 sec/batch\n",
      "Epoch 1/20  Iteration 62/3560 Training loss: 3.3131 0.8363 sec/batch\n",
      "Epoch 1/20  Iteration 63/3560 Training loss: 3.3108 0.8006 sec/batch\n",
      "Epoch 1/20  Iteration 64/3560 Training loss: 3.3080 0.8150 sec/batch\n",
      "Epoch 1/20  Iteration 65/3560 Training loss: 3.3054 0.8779 sec/batch\n",
      "Epoch 1/20  Iteration 66/3560 Training loss: 3.3032 0.7813 sec/batch\n",
      "Epoch 1/20  Iteration 67/3560 Training loss: 3.3010 0.8357 sec/batch\n",
      "Epoch 1/20  Iteration 68/3560 Training loss: 3.2983 0.8364 sec/batch\n",
      "Epoch 1/20  Iteration 69/3560 Training loss: 3.2958 0.8525 sec/batch\n",
      "Epoch 1/20  Iteration 70/3560 Training loss: 3.2938 0.7920 sec/batch\n",
      "Epoch 1/20  Iteration 71/3560 Training loss: 3.2915 0.7930 sec/batch\n",
      "Epoch 1/20  Iteration 72/3560 Training loss: 3.2896 0.7743 sec/batch\n",
      "Epoch 1/20  Iteration 73/3560 Training loss: 3.2877 0.8055 sec/batch\n",
      "Epoch 1/20  Iteration 74/3560 Training loss: 3.2858 0.8231 sec/batch\n",
      "Epoch 1/20  Iteration 75/3560 Training loss: 3.2841 0.7761 sec/batch\n",
      "Epoch 1/20  Iteration 76/3560 Training loss: 3.2825 0.8481 sec/batch\n",
      "Epoch 1/20  Iteration 77/3560 Training loss: 3.2807 0.9536 sec/batch\n",
      "Epoch 1/20  Iteration 78/3560 Training loss: 3.2790 1.0506 sec/batch\n",
      "Epoch 1/20  Iteration 79/3560 Training loss: 3.2772 1.1232 sec/batch\n",
      "Epoch 1/20  Iteration 80/3560 Training loss: 3.2753 1.0963 sec/batch\n",
      "Epoch 1/20  Iteration 81/3560 Training loss: 3.2735 1.0301 sec/batch\n",
      "Epoch 1/20  Iteration 82/3560 Training loss: 3.2720 0.9273 sec/batch\n",
      "Epoch 1/20  Iteration 83/3560 Training loss: 3.2704 0.8961 sec/batch\n",
      "Epoch 1/20  Iteration 84/3560 Training loss: 3.2687 0.9538 sec/batch\n",
      "Epoch 1/20  Iteration 85/3560 Training loss: 3.2670 0.8730 sec/batch\n",
      "Epoch 1/20  Iteration 86/3560 Training loss: 3.2653 0.9024 sec/batch\n",
      "Epoch 1/20  Iteration 87/3560 Training loss: 3.2636 0.8821 sec/batch\n",
      "Epoch 1/20  Iteration 88/3560 Training loss: 3.2620 0.9049 sec/batch\n",
      "Epoch 1/20  Iteration 89/3560 Training loss: 3.2606 0.7638 sec/batch\n",
      "Epoch 1/20  Iteration 90/3560 Training loss: 3.2592 0.7838 sec/batch\n",
      "Epoch 1/20  Iteration 91/3560 Training loss: 3.2578 0.7933 sec/batch\n",
      "Epoch 1/20  Iteration 92/3560 Training loss: 3.2563 0.7768 sec/batch\n",
      "Epoch 1/20  Iteration 93/3560 Training loss: 3.2549 0.8320 sec/batch\n",
      "Epoch 1/20  Iteration 94/3560 Training loss: 3.2535 0.7838 sec/batch\n",
      "Epoch 1/20  Iteration 95/3560 Training loss: 3.2521 0.7955 sec/batch\n",
      "Epoch 1/20  Iteration 96/3560 Training loss: 3.2507 0.7935 sec/batch\n",
      "Epoch 1/20  Iteration 97/3560 Training loss: 3.2494 0.8627 sec/batch\n",
      "Epoch 1/20  Iteration 98/3560 Training loss: 3.2479 0.8104 sec/batch\n",
      "Epoch 1/20  Iteration 99/3560 Training loss: 3.2466 0.8043 sec/batch\n",
      "Epoch 1/20  Iteration 100/3560 Training loss: 3.2452 0.8345 sec/batch\n",
      "Epoch 1/20  Iteration 101/3560 Training loss: 3.2439 0.7723 sec/batch\n",
      "Epoch 1/20  Iteration 102/3560 Training loss: 3.2425 0.7602 sec/batch\n",
      "Epoch 1/20  Iteration 103/3560 Training loss: 3.2413 0.7968 sec/batch\n",
      "Epoch 1/20  Iteration 104/3560 Training loss: 3.2400 0.7687 sec/batch\n",
      "Epoch 1/20  Iteration 105/3560 Training loss: 3.2387 0.8102 sec/batch\n",
      "Epoch 1/20  Iteration 106/3560 Training loss: 3.2374 0.7702 sec/batch\n",
      "Epoch 1/20  Iteration 107/3560 Training loss: 3.2359 0.7672 sec/batch\n",
      "Epoch 1/20  Iteration 108/3560 Training loss: 3.2345 0.8085 sec/batch\n",
      "Epoch 1/20  Iteration 109/3560 Training loss: 3.2331 0.8106 sec/batch\n",
      "Epoch 1/20  Iteration 110/3560 Training loss: 3.2315 0.8114 sec/batch\n",
      "Epoch 1/20  Iteration 111/3560 Training loss: 3.2301 0.8051 sec/batch\n",
      "Epoch 1/20  Iteration 112/3560 Training loss: 3.2287 0.8963 sec/batch\n",
      "Epoch 1/20  Iteration 113/3560 Training loss: 3.2273 0.8325 sec/batch\n",
      "Epoch 1/20  Iteration 114/3560 Training loss: 3.2258 0.8004 sec/batch\n",
      "Epoch 1/20  Iteration 115/3560 Training loss: 3.2243 0.8110 sec/batch\n",
      "Epoch 1/20  Iteration 116/3560 Training loss: 3.2228 0.8163 sec/batch\n",
      "Epoch 1/20  Iteration 117/3560 Training loss: 3.2213 0.7805 sec/batch\n",
      "Epoch 1/20  Iteration 118/3560 Training loss: 3.2200 0.7924 sec/batch\n",
      "Epoch 1/20  Iteration 119/3560 Training loss: 3.2187 0.7761 sec/batch\n",
      "Epoch 1/20  Iteration 120/3560 Training loss: 3.2171 0.8102 sec/batch\n",
      "Epoch 1/20  Iteration 121/3560 Training loss: 3.2158 0.8381 sec/batch\n",
      "Epoch 1/20  Iteration 122/3560 Training loss: 3.2142 0.7919 sec/batch\n",
      "Epoch 1/20  Iteration 123/3560 Training loss: 3.2127 0.7698 sec/batch\n",
      "Epoch 1/20  Iteration 124/3560 Training loss: 3.2113 0.7780 sec/batch\n",
      "Epoch 1/20  Iteration 125/3560 Training loss: 3.2095 0.8377 sec/batch\n",
      "Epoch 1/20  Iteration 126/3560 Training loss: 3.2078 0.7964 sec/batch\n",
      "Epoch 1/20  Iteration 127/3560 Training loss: 3.2061 0.9221 sec/batch\n",
      "Epoch 1/20  Iteration 128/3560 Training loss: 3.2046 0.8142 sec/batch\n",
      "Epoch 1/20  Iteration 129/3560 Training loss: 3.2028 0.7761 sec/batch\n",
      "Epoch 1/20  Iteration 130/3560 Training loss: 3.2010 0.7935 sec/batch\n",
      "Epoch 1/20  Iteration 131/3560 Training loss: 3.1994 0.8049 sec/batch\n",
      "Epoch 1/20  Iteration 132/3560 Training loss: 3.1974 0.7709 sec/batch\n",
      "Epoch 1/20  Iteration 133/3560 Training loss: 3.1957 0.7671 sec/batch\n",
      "Epoch 1/20  Iteration 134/3560 Training loss: 3.1937 0.8131 sec/batch\n",
      "Epoch 1/20  Iteration 135/3560 Training loss: 3.1916 0.7732 sec/batch\n",
      "Epoch 1/20  Iteration 136/3560 Training loss: 3.1894 0.7854 sec/batch\n",
      "Epoch 1/20  Iteration 137/3560 Training loss: 3.1874 0.7997 sec/batch\n",
      "Epoch 1/20  Iteration 138/3560 Training loss: 3.1854 0.8273 sec/batch\n",
      "Epoch 1/20  Iteration 139/3560 Training loss: 3.1834 0.7640 sec/batch\n",
      "Epoch 1/20  Iteration 140/3560 Training loss: 3.1814 0.8126 sec/batch\n",
      "Epoch 1/20  Iteration 141/3560 Training loss: 3.1794 0.7919 sec/batch\n",
      "Epoch 1/20  Iteration 142/3560 Training loss: 3.1773 0.8887 sec/batch\n",
      "Epoch 1/20  Iteration 143/3560 Training loss: 3.1752 0.7855 sec/batch\n",
      "Epoch 1/20  Iteration 144/3560 Training loss: 3.1732 0.7743 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 145/3560 Training loss: 3.1711 0.7886 sec/batch\n",
      "Epoch 1/20  Iteration 146/3560 Training loss: 3.1690 0.8164 sec/batch\n",
      "Epoch 1/20  Iteration 147/3560 Training loss: 3.1669 0.7693 sec/batch\n",
      "Epoch 1/20  Iteration 148/3560 Training loss: 3.1650 0.7848 sec/batch\n",
      "Epoch 1/20  Iteration 149/3560 Training loss: 3.1628 0.7865 sec/batch\n",
      "Epoch 1/20  Iteration 150/3560 Training loss: 3.1607 0.8443 sec/batch\n",
      "Epoch 1/20  Iteration 151/3560 Training loss: 3.1587 0.8039 sec/batch\n",
      "Epoch 1/20  Iteration 152/3560 Training loss: 3.1569 0.7972 sec/batch\n",
      "Epoch 1/20  Iteration 153/3560 Training loss: 3.1548 0.7998 sec/batch\n",
      "Epoch 1/20  Iteration 154/3560 Training loss: 3.1526 0.8182 sec/batch\n",
      "Epoch 1/20  Iteration 155/3560 Training loss: 3.1504 0.7856 sec/batch\n",
      "Epoch 1/20  Iteration 156/3560 Training loss: 3.1482 0.7672 sec/batch\n",
      "Epoch 1/20  Iteration 157/3560 Training loss: 3.1458 0.8406 sec/batch\n",
      "Epoch 1/20  Iteration 158/3560 Training loss: 3.1435 0.7599 sec/batch\n",
      "Epoch 1/20  Iteration 159/3560 Training loss: 3.1411 0.7939 sec/batch\n",
      "Epoch 1/20  Iteration 160/3560 Training loss: 3.1388 0.7651 sec/batch\n",
      "Epoch 1/20  Iteration 161/3560 Training loss: 3.1365 0.7742 sec/batch\n",
      "Epoch 1/20  Iteration 162/3560 Training loss: 3.1340 0.7799 sec/batch\n",
      "Epoch 1/20  Iteration 163/3560 Training loss: 3.1315 0.8706 sec/batch\n",
      "Epoch 1/20  Iteration 164/3560 Training loss: 3.1291 0.7730 sec/batch\n",
      "Epoch 1/20  Iteration 165/3560 Training loss: 3.1268 0.7981 sec/batch\n",
      "Epoch 1/20  Iteration 166/3560 Training loss: 3.1245 0.8023 sec/batch\n",
      "Epoch 1/20  Iteration 167/3560 Training loss: 3.1222 0.8013 sec/batch\n",
      "Epoch 1/20  Iteration 168/3560 Training loss: 3.1198 0.7865 sec/batch\n",
      "Epoch 1/20  Iteration 169/3560 Training loss: 3.1174 0.7969 sec/batch\n",
      "Epoch 1/20  Iteration 170/3560 Training loss: 3.1149 0.7716 sec/batch\n",
      "Epoch 1/20  Iteration 171/3560 Training loss: 3.1125 0.7690 sec/batch\n",
      "Epoch 1/20  Iteration 172/3560 Training loss: 3.1103 0.8819 sec/batch\n",
      "Epoch 1/20  Iteration 173/3560 Training loss: 3.1081 0.8097 sec/batch\n",
      "Epoch 1/20  Iteration 174/3560 Training loss: 3.1059 0.7947 sec/batch\n",
      "Epoch 1/20  Iteration 175/3560 Training loss: 3.1036 0.7961 sec/batch\n",
      "Epoch 1/20  Iteration 176/3560 Training loss: 3.1012 0.8556 sec/batch\n",
      "Epoch 1/20  Iteration 177/3560 Training loss: 3.0987 0.7719 sec/batch\n",
      "Epoch 1/20  Iteration 178/3560 Training loss: 3.0961 0.7693 sec/batch\n",
      "Epoch 2/20  Iteration 179/3560 Training loss: 2.7245 0.7934 sec/batch\n",
      "Epoch 2/20  Iteration 180/3560 Training loss: 2.6673 0.7954 sec/batch\n",
      "Epoch 2/20  Iteration 181/3560 Training loss: 2.6521 0.7641 sec/batch\n",
      "Epoch 2/20  Iteration 182/3560 Training loss: 2.6445 0.7934 sec/batch\n",
      "Epoch 2/20  Iteration 183/3560 Training loss: 2.6381 0.7953 sec/batch\n",
      "Epoch 2/20  Iteration 184/3560 Training loss: 2.6320 0.8144 sec/batch\n",
      "Epoch 2/20  Iteration 185/3560 Training loss: 2.6288 0.7759 sec/batch\n",
      "Epoch 2/20  Iteration 186/3560 Training loss: 2.6272 0.8004 sec/batch\n",
      "Epoch 2/20  Iteration 187/3560 Training loss: 2.6244 1.0360 sec/batch\n",
      "Epoch 2/20  Iteration 188/3560 Training loss: 2.6203 1.0171 sec/batch\n",
      "Epoch 2/20  Iteration 189/3560 Training loss: 2.6152 1.1657 sec/batch\n",
      "Epoch 2/20  Iteration 190/3560 Training loss: 2.6143 1.1148 sec/batch\n",
      "Epoch 2/20  Iteration 191/3560 Training loss: 2.6114 1.0423 sec/batch\n",
      "Epoch 2/20  Iteration 192/3560 Training loss: 2.6105 1.0191 sec/batch\n",
      "Epoch 2/20  Iteration 193/3560 Training loss: 2.6080 1.0645 sec/batch\n",
      "Epoch 2/20  Iteration 194/3560 Training loss: 2.6058 1.0317 sec/batch\n",
      "Epoch 2/20  Iteration 195/3560 Training loss: 2.6032 1.0486 sec/batch\n",
      "Epoch 2/20  Iteration 196/3560 Training loss: 2.6037 1.0285 sec/batch\n",
      "Epoch 2/20  Iteration 197/3560 Training loss: 2.6018 1.0079 sec/batch\n",
      "Epoch 2/20  Iteration 198/3560 Training loss: 2.5984 1.1045 sec/batch\n",
      "Epoch 2/20  Iteration 199/3560 Training loss: 2.5957 1.0215 sec/batch\n",
      "Epoch 2/20  Iteration 200/3560 Training loss: 2.5947 1.0598 sec/batch\n",
      "Epoch 2/20  Iteration 201/3560 Training loss: 2.5924 0.9926 sec/batch\n",
      "Epoch 2/20  Iteration 202/3560 Training loss: 2.5900 0.7781 sec/batch\n",
      "Epoch 2/20  Iteration 203/3560 Training loss: 2.5874 0.7662 sec/batch\n",
      "Epoch 2/20  Iteration 204/3560 Training loss: 2.5856 0.7769 sec/batch\n",
      "Epoch 2/20  Iteration 205/3560 Training loss: 2.5830 0.7809 sec/batch\n",
      "Epoch 2/20  Iteration 206/3560 Training loss: 2.5811 0.7885 sec/batch\n",
      "Epoch 2/20  Iteration 207/3560 Training loss: 2.5797 0.7722 sec/batch\n",
      "Epoch 2/20  Iteration 208/3560 Training loss: 2.5777 0.7762 sec/batch\n",
      "Epoch 2/20  Iteration 209/3560 Training loss: 2.5767 0.8271 sec/batch\n",
      "Epoch 2/20  Iteration 210/3560 Training loss: 2.5744 0.7930 sec/batch\n",
      "Epoch 2/20  Iteration 211/3560 Training loss: 2.5720 0.7969 sec/batch\n",
      "Epoch 2/20  Iteration 212/3560 Training loss: 2.5703 0.8715 sec/batch\n",
      "Epoch 2/20  Iteration 213/3560 Training loss: 2.5683 0.7696 sec/batch\n",
      "Epoch 2/20  Iteration 214/3560 Training loss: 2.5667 0.7792 sec/batch\n",
      "Epoch 2/20  Iteration 215/3560 Training loss: 2.5646 0.8015 sec/batch\n",
      "Epoch 2/20  Iteration 216/3560 Training loss: 2.5618 0.7772 sec/batch\n",
      "Epoch 2/20  Iteration 217/3560 Training loss: 2.5596 0.8197 sec/batch\n",
      "Epoch 2/20  Iteration 218/3560 Training loss: 2.5574 0.7911 sec/batch\n",
      "Epoch 2/20  Iteration 219/3560 Training loss: 2.5553 0.7787 sec/batch\n",
      "Epoch 2/20  Iteration 220/3560 Training loss: 2.5533 0.7650 sec/batch\n",
      "Epoch 2/20  Iteration 221/3560 Training loss: 2.5512 0.8252 sec/batch\n",
      "Epoch 2/20  Iteration 222/3560 Training loss: 2.5490 0.8155 sec/batch\n",
      "Epoch 2/20  Iteration 223/3560 Training loss: 2.5471 0.7929 sec/batch\n",
      "Epoch 2/20  Iteration 224/3560 Training loss: 2.5445 0.7845 sec/batch\n",
      "Epoch 2/20  Iteration 225/3560 Training loss: 2.5432 0.7980 sec/batch\n",
      "Epoch 2/20  Iteration 226/3560 Training loss: 2.5413 0.8572 sec/batch\n",
      "Epoch 2/20  Iteration 227/3560 Training loss: 2.5395 0.7845 sec/batch\n",
      "Epoch 2/20  Iteration 228/3560 Training loss: 2.5382 0.7818 sec/batch\n",
      "Epoch 2/20  Iteration 229/3560 Training loss: 2.5366 0.7823 sec/batch\n",
      "Epoch 2/20  Iteration 230/3560 Training loss: 2.5351 0.8055 sec/batch\n",
      "Epoch 2/20  Iteration 231/3560 Training loss: 2.5334 0.7961 sec/batch\n",
      "Epoch 2/20  Iteration 232/3560 Training loss: 2.5318 0.8004 sec/batch\n",
      "Epoch 2/20  Iteration 233/3560 Training loss: 2.5300 0.7898 sec/batch\n",
      "Epoch 2/20  Iteration 234/3560 Training loss: 2.5286 0.8319 sec/batch\n",
      "Epoch 2/20  Iteration 235/3560 Training loss: 2.5271 0.8457 sec/batch\n",
      "Epoch 2/20  Iteration 236/3560 Training loss: 2.5254 0.7800 sec/batch\n",
      "Epoch 2/20  Iteration 237/3560 Training loss: 2.5237 0.7864 sec/batch\n",
      "Epoch 2/20  Iteration 238/3560 Training loss: 2.5225 0.7911 sec/batch\n",
      "Epoch 2/20  Iteration 239/3560 Training loss: 2.5210 0.7966 sec/batch\n",
      "Epoch 2/20  Iteration 240/3560 Training loss: 2.5197 0.7670 sec/batch\n",
      "Epoch 2/20  Iteration 241/3560 Training loss: 2.5188 0.8498 sec/batch\n",
      "Epoch 2/20  Iteration 242/3560 Training loss: 2.5174 0.7826 sec/batch\n",
      "Epoch 2/20  Iteration 243/3560 Training loss: 2.5157 0.7974 sec/batch\n",
      "Epoch 2/20  Iteration 244/3560 Training loss: 2.5146 0.7826 sec/batch\n",
      "Epoch 2/20  Iteration 245/3560 Training loss: 2.5132 0.7769 sec/batch\n",
      "Epoch 2/20  Iteration 246/3560 Training loss: 2.5113 0.7623 sec/batch\n",
      "Epoch 2/20  Iteration 247/3560 Training loss: 2.5097 0.8507 sec/batch\n",
      "Epoch 2/20  Iteration 248/3560 Training loss: 2.5086 0.7814 sec/batch\n",
      "Epoch 2/20  Iteration 249/3560 Training loss: 2.5075 0.7634 sec/batch\n",
      "Epoch 2/20  Iteration 250/3560 Training loss: 2.5063 0.7859 sec/batch\n",
      "Epoch 2/20  Iteration 251/3560 Training loss: 2.5050 0.7781 sec/batch\n",
      "Epoch 2/20  Iteration 252/3560 Training loss: 2.5035 0.7971 sec/batch\n",
      "Epoch 2/20  Iteration 253/3560 Training loss: 2.5021 0.7696 sec/batch\n",
      "Epoch 2/20  Iteration 254/3560 Training loss: 2.5012 0.7990 sec/batch\n",
      "Epoch 2/20  Iteration 255/3560 Training loss: 2.4999 0.8051 sec/batch\n",
      "Epoch 2/20  Iteration 256/3560 Training loss: 2.4989 0.8425 sec/batch\n",
      "Epoch 2/20  Iteration 257/3560 Training loss: 2.4974 0.8459 sec/batch\n",
      "Epoch 2/20  Iteration 258/3560 Training loss: 2.4961 0.8015 sec/batch\n",
      "Epoch 2/20  Iteration 259/3560 Training loss: 2.4947 0.8170 sec/batch\n",
      "Epoch 2/20  Iteration 260/3560 Training loss: 2.4937 0.8228 sec/batch\n",
      "Epoch 2/20  Iteration 261/3560 Training loss: 2.4924 0.8019 sec/batch\n",
      "Epoch 2/20  Iteration 262/3560 Training loss: 2.4911 0.8019 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 263/3560 Training loss: 2.4892 0.7907 sec/batch\n",
      "Epoch 2/20  Iteration 264/3560 Training loss: 2.4877 0.7698 sec/batch\n",
      "Epoch 2/20  Iteration 265/3560 Training loss: 2.4865 0.7890 sec/batch\n",
      "Epoch 2/20  Iteration 266/3560 Training loss: 2.4851 0.7715 sec/batch\n",
      "Epoch 2/20  Iteration 267/3560 Training loss: 2.4837 0.7719 sec/batch\n",
      "Epoch 2/20  Iteration 268/3560 Training loss: 2.4826 0.7928 sec/batch\n",
      "Epoch 2/20  Iteration 269/3560 Training loss: 2.4813 0.8112 sec/batch\n",
      "Epoch 2/20  Iteration 270/3560 Training loss: 2.4801 0.7992 sec/batch\n",
      "Epoch 2/20  Iteration 271/3560 Training loss: 2.4788 0.8666 sec/batch\n",
      "Epoch 2/20  Iteration 272/3560 Training loss: 2.4774 0.8876 sec/batch\n",
      "Epoch 2/20  Iteration 273/3560 Training loss: 2.4760 0.7937 sec/batch\n",
      "Epoch 2/20  Iteration 274/3560 Training loss: 2.4747 0.7908 sec/batch\n",
      "Epoch 2/20  Iteration 275/3560 Training loss: 2.4735 0.8184 sec/batch\n",
      "Epoch 2/20  Iteration 276/3560 Training loss: 2.4723 0.7692 sec/batch\n",
      "Epoch 2/20  Iteration 277/3560 Training loss: 2.4710 0.7831 sec/batch\n",
      "Epoch 2/20  Iteration 278/3560 Training loss: 2.4698 0.7777 sec/batch\n",
      "Epoch 2/20  Iteration 279/3560 Training loss: 2.4688 0.8057 sec/batch\n",
      "Epoch 2/20  Iteration 280/3560 Training loss: 2.4676 0.8013 sec/batch\n",
      "Epoch 2/20  Iteration 281/3560 Training loss: 2.4663 0.7854 sec/batch\n",
      "Epoch 2/20  Iteration 282/3560 Training loss: 2.4650 0.9058 sec/batch\n",
      "Epoch 2/20  Iteration 283/3560 Training loss: 2.4637 0.7833 sec/batch\n",
      "Epoch 2/20  Iteration 284/3560 Training loss: 2.4626 0.8555 sec/batch\n",
      "Epoch 2/20  Iteration 285/3560 Training loss: 2.4614 0.8066 sec/batch\n",
      "Epoch 2/20  Iteration 286/3560 Training loss: 2.4605 0.8795 sec/batch\n",
      "Epoch 2/20  Iteration 287/3560 Training loss: 2.4595 0.7637 sec/batch\n",
      "Epoch 2/20  Iteration 288/3560 Training loss: 2.4582 0.8047 sec/batch\n",
      "Epoch 2/20  Iteration 289/3560 Training loss: 2.4572 0.7854 sec/batch\n",
      "Epoch 2/20  Iteration 290/3560 Training loss: 2.4563 0.7829 sec/batch\n",
      "Epoch 2/20  Iteration 291/3560 Training loss: 2.4552 0.7967 sec/batch\n",
      "Epoch 2/20  Iteration 292/3560 Training loss: 2.4539 0.7981 sec/batch\n",
      "Epoch 2/20  Iteration 293/3560 Training loss: 2.4528 0.8010 sec/batch\n",
      "Epoch 2/20  Iteration 294/3560 Training loss: 2.4515 0.8351 sec/batch\n",
      "Epoch 2/20  Iteration 295/3560 Training loss: 2.4504 0.7970 sec/batch\n",
      "Epoch 2/20  Iteration 296/3560 Training loss: 2.4493 0.7709 sec/batch\n",
      "Epoch 2/20  Iteration 297/3560 Training loss: 2.4484 0.8498 sec/batch\n",
      "Epoch 2/20  Iteration 298/3560 Training loss: 2.4473 0.8009 sec/batch\n",
      "Epoch 2/20  Iteration 299/3560 Training loss: 2.4464 0.8370 sec/batch\n",
      "Epoch 2/20  Iteration 300/3560 Training loss: 2.4453 0.8315 sec/batch\n",
      "Epoch 2/20  Iteration 301/3560 Training loss: 2.4442 0.9065 sec/batch\n",
      "Epoch 2/20  Iteration 302/3560 Training loss: 2.4433 0.7839 sec/batch\n",
      "Epoch 2/20  Iteration 303/3560 Training loss: 2.4423 0.7989 sec/batch\n",
      "Epoch 2/20  Iteration 304/3560 Training loss: 2.4411 0.7666 sec/batch\n",
      "Epoch 2/20  Iteration 305/3560 Training loss: 2.4403 0.7760 sec/batch\n",
      "Epoch 2/20  Iteration 306/3560 Training loss: 2.4394 0.7986 sec/batch\n",
      "Epoch 2/20  Iteration 307/3560 Training loss: 2.4384 0.8187 sec/batch\n",
      "Epoch 2/20  Iteration 308/3560 Training loss: 2.4374 0.7812 sec/batch\n",
      "Epoch 2/20  Iteration 309/3560 Training loss: 2.4364 0.8386 sec/batch\n",
      "Epoch 2/20  Iteration 310/3560 Training loss: 2.4353 0.8115 sec/batch\n",
      "Epoch 2/20  Iteration 311/3560 Training loss: 2.4344 0.7905 sec/batch\n",
      "Epoch 2/20  Iteration 312/3560 Training loss: 2.4335 0.8091 sec/batch\n",
      "Epoch 2/20  Iteration 313/3560 Training loss: 2.4325 0.7652 sec/batch\n",
      "Epoch 2/20  Iteration 314/3560 Training loss: 2.4315 0.8030 sec/batch\n",
      "Epoch 2/20  Iteration 315/3560 Training loss: 2.4305 0.7873 sec/batch\n",
      "Epoch 2/20  Iteration 316/3560 Training loss: 2.4296 0.8862 sec/batch\n",
      "Epoch 2/20  Iteration 317/3560 Training loss: 2.4289 0.8267 sec/batch\n",
      "Epoch 2/20  Iteration 318/3560 Training loss: 2.4278 0.7988 sec/batch\n",
      "Epoch 2/20  Iteration 319/3560 Training loss: 2.4271 0.8640 sec/batch\n",
      "Epoch 2/20  Iteration 320/3560 Training loss: 2.4261 0.7692 sec/batch\n",
      "Epoch 2/20  Iteration 321/3560 Training loss: 2.4252 0.7675 sec/batch\n",
      "Epoch 2/20  Iteration 322/3560 Training loss: 2.4242 0.8658 sec/batch\n",
      "Epoch 2/20  Iteration 323/3560 Training loss: 2.4232 0.7995 sec/batch\n",
      "Epoch 2/20  Iteration 324/3560 Training loss: 2.4224 0.7809 sec/batch\n",
      "Epoch 2/20  Iteration 325/3560 Training loss: 2.4215 0.7719 sec/batch\n",
      "Epoch 2/20  Iteration 326/3560 Training loss: 2.4208 0.7919 sec/batch\n",
      "Epoch 2/20  Iteration 327/3560 Training loss: 2.4198 0.8092 sec/batch\n",
      "Epoch 2/20  Iteration 328/3560 Training loss: 2.4188 0.7972 sec/batch\n",
      "Epoch 2/20  Iteration 329/3560 Training loss: 2.4180 0.7732 sec/batch\n",
      "Epoch 2/20  Iteration 330/3560 Training loss: 2.4174 0.8117 sec/batch\n",
      "Epoch 2/20  Iteration 331/3560 Training loss: 2.4165 0.8711 sec/batch\n",
      "Epoch 2/20  Iteration 332/3560 Training loss: 2.4157 0.8215 sec/batch\n",
      "Epoch 2/20  Iteration 333/3560 Training loss: 2.4147 0.7705 sec/batch\n",
      "Epoch 2/20  Iteration 334/3560 Training loss: 2.4138 0.8202 sec/batch\n",
      "Epoch 2/20  Iteration 335/3560 Training loss: 2.4128 0.8156 sec/batch\n",
      "Epoch 2/20  Iteration 336/3560 Training loss: 2.4119 0.7982 sec/batch\n",
      "Epoch 2/20  Iteration 337/3560 Training loss: 2.4108 0.7992 sec/batch\n",
      "Epoch 2/20  Iteration 338/3560 Training loss: 2.4100 0.7872 sec/batch\n",
      "Epoch 2/20  Iteration 339/3560 Training loss: 2.4092 0.7650 sec/batch\n",
      "Epoch 2/20  Iteration 340/3560 Training loss: 2.4082 0.7898 sec/batch\n",
      "Epoch 2/20  Iteration 341/3560 Training loss: 2.4073 0.7968 sec/batch\n",
      "Epoch 2/20  Iteration 342/3560 Training loss: 2.4064 0.7967 sec/batch\n",
      "Epoch 2/20  Iteration 343/3560 Training loss: 2.4055 0.7917 sec/batch\n",
      "Epoch 2/20  Iteration 344/3560 Training loss: 2.4046 0.8108 sec/batch\n",
      "Epoch 2/20  Iteration 345/3560 Training loss: 2.4037 0.7921 sec/batch\n",
      "Epoch 2/20  Iteration 346/3560 Training loss: 2.4030 0.8621 sec/batch\n",
      "Epoch 2/20  Iteration 347/3560 Training loss: 2.4022 0.8450 sec/batch\n",
      "Epoch 2/20  Iteration 348/3560 Training loss: 2.4012 0.8046 sec/batch\n",
      "Epoch 2/20  Iteration 349/3560 Training loss: 2.4003 0.7927 sec/batch\n",
      "Epoch 2/20  Iteration 350/3560 Training loss: 2.3995 0.7975 sec/batch\n",
      "Epoch 2/20  Iteration 351/3560 Training loss: 2.3988 0.7864 sec/batch\n",
      "Epoch 2/20  Iteration 352/3560 Training loss: 2.3980 0.7971 sec/batch\n",
      "Epoch 2/20  Iteration 353/3560 Training loss: 2.3973 0.7665 sec/batch\n",
      "Epoch 2/20  Iteration 354/3560 Training loss: 2.3965 0.7980 sec/batch\n",
      "Epoch 2/20  Iteration 355/3560 Training loss: 2.3955 0.7795 sec/batch\n",
      "Epoch 2/20  Iteration 356/3560 Training loss: 2.3946 0.8194 sec/batch\n",
      "Epoch 3/20  Iteration 357/3560 Training loss: 2.3303 0.8073 sec/batch\n",
      "Epoch 3/20  Iteration 358/3560 Training loss: 2.2652 0.7742 sec/batch\n",
      "Epoch 3/20  Iteration 359/3560 Training loss: 2.2478 0.7928 sec/batch\n",
      "Epoch 3/20  Iteration 360/3560 Training loss: 2.2402 0.8844 sec/batch\n",
      "Epoch 3/20  Iteration 361/3560 Training loss: 2.2346 0.8277 sec/batch\n",
      "Epoch 3/20  Iteration 362/3560 Training loss: 2.2306 0.8543 sec/batch\n",
      "Epoch 3/20  Iteration 363/3560 Training loss: 2.2294 0.7783 sec/batch\n",
      "Epoch 3/20  Iteration 364/3560 Training loss: 2.2303 0.7978 sec/batch\n",
      "Epoch 3/20  Iteration 365/3560 Training loss: 2.2322 0.7731 sec/batch\n",
      "Epoch 3/20  Iteration 366/3560 Training loss: 2.2319 0.8080 sec/batch\n",
      "Epoch 3/20  Iteration 367/3560 Training loss: 2.2294 0.7680 sec/batch\n",
      "Epoch 3/20  Iteration 368/3560 Training loss: 2.2281 0.8020 sec/batch\n",
      "Epoch 3/20  Iteration 369/3560 Training loss: 2.2281 0.8101 sec/batch\n",
      "Epoch 3/20  Iteration 370/3560 Training loss: 2.2296 0.7568 sec/batch\n",
      "Epoch 3/20  Iteration 371/3560 Training loss: 2.2290 0.7880 sec/batch\n",
      "Epoch 3/20  Iteration 372/3560 Training loss: 2.2280 0.7719 sec/batch\n",
      "Epoch 3/20  Iteration 373/3560 Training loss: 2.2274 0.8295 sec/batch\n",
      "Epoch 3/20  Iteration 374/3560 Training loss: 2.2287 0.7879 sec/batch\n",
      "Epoch 3/20  Iteration 375/3560 Training loss: 2.2288 0.8607 sec/batch\n",
      "Epoch 3/20  Iteration 376/3560 Training loss: 2.2278 0.8606 sec/batch\n",
      "Epoch 3/20  Iteration 377/3560 Training loss: 2.2264 0.7976 sec/batch\n",
      "Epoch 3/20  Iteration 378/3560 Training loss: 2.2276 0.7991 sec/batch\n",
      "Epoch 3/20  Iteration 379/3560 Training loss: 2.2270 0.7601 sec/batch\n",
      "Epoch 3/20  Iteration 380/3560 Training loss: 2.2258 0.7860 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 381/3560 Training loss: 2.2248 0.8023 sec/batch\n",
      "Epoch 3/20  Iteration 382/3560 Training loss: 2.2235 0.7868 sec/batch\n",
      "Epoch 3/20  Iteration 383/3560 Training loss: 2.2219 0.7902 sec/batch\n",
      "Epoch 3/20  Iteration 384/3560 Training loss: 2.2214 0.8026 sec/batch\n",
      "Epoch 3/20  Iteration 385/3560 Training loss: 2.2216 0.8490 sec/batch\n",
      "Epoch 3/20  Iteration 386/3560 Training loss: 2.2208 0.8133 sec/batch\n",
      "Epoch 3/20  Iteration 387/3560 Training loss: 2.2205 0.7696 sec/batch\n",
      "Epoch 3/20  Iteration 388/3560 Training loss: 2.2194 0.7954 sec/batch\n",
      "Epoch 3/20  Iteration 389/3560 Training loss: 2.2184 0.7922 sec/batch\n",
      "Epoch 3/20  Iteration 390/3560 Training loss: 2.2184 0.8317 sec/batch\n",
      "Epoch 3/20  Iteration 391/3560 Training loss: 2.2173 0.8221 sec/batch\n",
      "Epoch 3/20  Iteration 392/3560 Training loss: 2.2167 0.7783 sec/batch\n",
      "Epoch 3/20  Iteration 393/3560 Training loss: 2.2159 0.7813 sec/batch\n",
      "Epoch 3/20  Iteration 394/3560 Training loss: 2.2140 0.7907 sec/batch\n",
      "Epoch 3/20  Iteration 395/3560 Training loss: 2.2127 0.7860 sec/batch\n",
      "Epoch 3/20  Iteration 396/3560 Training loss: 2.2115 0.8125 sec/batch\n",
      "Epoch 3/20  Iteration 397/3560 Training loss: 2.2104 0.8010 sec/batch\n",
      "Epoch 3/20  Iteration 398/3560 Training loss: 2.2096 0.7835 sec/batch\n",
      "Epoch 3/20  Iteration 399/3560 Training loss: 2.2084 0.7960 sec/batch\n",
      "Epoch 3/20  Iteration 400/3560 Training loss: 2.2071 0.7936 sec/batch\n",
      "Epoch 3/20  Iteration 401/3560 Training loss: 2.2063 0.7816 sec/batch\n",
      "Epoch 3/20  Iteration 402/3560 Training loss: 2.2045 0.7691 sec/batch\n",
      "Epoch 3/20  Iteration 403/3560 Training loss: 2.2041 0.7676 sec/batch\n",
      "Epoch 3/20  Iteration 404/3560 Training loss: 2.2031 0.7979 sec/batch\n",
      "Epoch 3/20  Iteration 405/3560 Training loss: 2.2023 0.8476 sec/batch\n",
      "Epoch 3/20  Iteration 406/3560 Training loss: 2.2022 0.8223 sec/batch\n",
      "Epoch 3/20  Iteration 407/3560 Training loss: 2.2011 1.0394 sec/batch\n",
      "Epoch 3/20  Iteration 408/3560 Training loss: 2.2011 1.0079 sec/batch\n",
      "Epoch 3/20  Iteration 409/3560 Training loss: 2.2000 1.0162 sec/batch\n",
      "Epoch 3/20  Iteration 410/3560 Training loss: 2.1992 1.0812 sec/batch\n",
      "Epoch 3/20  Iteration 411/3560 Training loss: 2.1983 1.0025 sec/batch\n",
      "Epoch 3/20  Iteration 412/3560 Training loss: 2.1978 1.0437 sec/batch\n",
      "Epoch 3/20  Iteration 413/3560 Training loss: 2.1974 1.0281 sec/batch\n",
      "Epoch 3/20  Iteration 414/3560 Training loss: 2.1967 1.0981 sec/batch\n",
      "Epoch 3/20  Iteration 415/3560 Training loss: 2.1958 0.7713 sec/batch\n",
      "Epoch 3/20  Iteration 416/3560 Training loss: 2.1956 0.7956 sec/batch\n",
      "Epoch 3/20  Iteration 417/3560 Training loss: 2.1948 0.7739 sec/batch\n",
      "Epoch 3/20  Iteration 418/3560 Training loss: 2.1947 0.7636 sec/batch\n",
      "Epoch 3/20  Iteration 419/3560 Training loss: 2.1947 0.7843 sec/batch\n",
      "Epoch 3/20  Iteration 420/3560 Training loss: 2.1941 0.8103 sec/batch\n",
      "Epoch 3/20  Iteration 421/3560 Training loss: 2.1932 0.7817 sec/batch\n",
      "Epoch 3/20  Iteration 422/3560 Training loss: 2.1927 0.7888 sec/batch\n",
      "Epoch 3/20  Iteration 423/3560 Training loss: 2.1923 0.7877 sec/batch\n",
      "Epoch 3/20  Iteration 424/3560 Training loss: 2.1914 0.7687 sec/batch\n",
      "Epoch 3/20  Iteration 425/3560 Training loss: 2.1905 0.7957 sec/batch\n",
      "Epoch 3/20  Iteration 426/3560 Training loss: 2.1901 0.7520 sec/batch\n",
      "Epoch 3/20  Iteration 427/3560 Training loss: 2.1900 0.7948 sec/batch\n",
      "Epoch 3/20  Iteration 428/3560 Training loss: 2.1895 0.8124 sec/batch\n",
      "Epoch 3/20  Iteration 429/3560 Training loss: 2.1892 0.8628 sec/batch\n",
      "Epoch 3/20  Iteration 430/3560 Training loss: 2.1883 0.8056 sec/batch\n",
      "Epoch 3/20  Iteration 431/3560 Training loss: 2.1878 0.8058 sec/batch\n",
      "Epoch 3/20  Iteration 432/3560 Training loss: 2.1876 0.7551 sec/batch\n",
      "Epoch 3/20  Iteration 433/3560 Training loss: 2.1869 0.8363 sec/batch\n",
      "Epoch 3/20  Iteration 434/3560 Training loss: 2.1867 0.7625 sec/batch\n",
      "Epoch 3/20  Iteration 435/3560 Training loss: 2.1858 0.8001 sec/batch\n",
      "Epoch 3/20  Iteration 436/3560 Training loss: 2.1852 0.7869 sec/batch\n",
      "Epoch 3/20  Iteration 437/3560 Training loss: 2.1843 0.7658 sec/batch\n",
      "Epoch 3/20  Iteration 438/3560 Training loss: 2.1840 0.7586 sec/batch\n",
      "Epoch 3/20  Iteration 439/3560 Training loss: 2.1831 0.7735 sec/batch\n",
      "Epoch 3/20  Iteration 440/3560 Training loss: 2.1825 0.7826 sec/batch\n",
      "Epoch 3/20  Iteration 441/3560 Training loss: 2.1815 0.8612 sec/batch\n",
      "Epoch 3/20  Iteration 442/3560 Training loss: 2.1806 0.7665 sec/batch\n",
      "Epoch 3/20  Iteration 443/3560 Training loss: 2.1801 0.8119 sec/batch\n",
      "Epoch 3/20  Iteration 444/3560 Training loss: 2.1792 0.7905 sec/batch\n",
      "Epoch 3/20  Iteration 445/3560 Training loss: 2.1784 0.8045 sec/batch\n",
      "Epoch 3/20  Iteration 446/3560 Training loss: 2.1779 0.8381 sec/batch\n",
      "Epoch 3/20  Iteration 447/3560 Training loss: 2.1771 0.7698 sec/batch\n",
      "Epoch 3/20  Iteration 448/3560 Training loss: 2.1765 0.7865 sec/batch\n",
      "Epoch 3/20  Iteration 449/3560 Training loss: 2.1756 0.7526 sec/batch\n",
      "Epoch 3/20  Iteration 450/3560 Training loss: 2.1747 0.7793 sec/batch\n",
      "Epoch 3/20  Iteration 451/3560 Training loss: 2.1739 0.7763 sec/batch\n",
      "Epoch 3/20  Iteration 452/3560 Training loss: 2.1733 0.8304 sec/batch\n",
      "Epoch 3/20  Iteration 453/3560 Training loss: 2.1726 0.7916 sec/batch\n",
      "Epoch 3/20  Iteration 454/3560 Training loss: 2.1719 0.8266 sec/batch\n",
      "Epoch 3/20  Iteration 455/3560 Training loss: 2.1709 0.7874 sec/batch\n",
      "Epoch 3/20  Iteration 456/3560 Training loss: 2.1700 0.7910 sec/batch\n",
      "Epoch 3/20  Iteration 457/3560 Training loss: 2.1696 0.8030 sec/batch\n",
      "Epoch 3/20  Iteration 458/3560 Training loss: 2.1690 0.8290 sec/batch\n",
      "Epoch 3/20  Iteration 459/3560 Training loss: 2.1682 0.8310 sec/batch\n",
      "Epoch 3/20  Iteration 460/3560 Training loss: 2.1674 0.7860 sec/batch\n",
      "Epoch 3/20  Iteration 461/3560 Training loss: 2.1667 0.7664 sec/batch\n",
      "Epoch 3/20  Iteration 462/3560 Training loss: 2.1661 0.7637 sec/batch\n",
      "Epoch 3/20  Iteration 463/3560 Training loss: 2.1655 0.7679 sec/batch\n",
      "Epoch 3/20  Iteration 464/3560 Training loss: 2.1650 0.7554 sec/batch\n",
      "Epoch 3/20  Iteration 465/3560 Training loss: 2.1645 0.8542 sec/batch\n",
      "Epoch 3/20  Iteration 466/3560 Training loss: 2.1638 0.9153 sec/batch\n",
      "Epoch 3/20  Iteration 467/3560 Training loss: 2.1633 0.8072 sec/batch\n",
      "Epoch 3/20  Iteration 468/3560 Training loss: 2.1627 0.7873 sec/batch\n",
      "Epoch 3/20  Iteration 469/3560 Training loss: 2.1621 0.8357 sec/batch\n",
      "Epoch 3/20  Iteration 470/3560 Training loss: 2.1615 0.7799 sec/batch\n",
      "Epoch 3/20  Iteration 471/3560 Training loss: 2.1608 0.8482 sec/batch\n",
      "Epoch 3/20  Iteration 472/3560 Training loss: 2.1599 0.7631 sec/batch\n",
      "Epoch 3/20  Iteration 473/3560 Training loss: 2.1593 0.7968 sec/batch\n",
      "Epoch 3/20  Iteration 474/3560 Training loss: 2.1587 0.7895 sec/batch\n",
      "Epoch 3/20  Iteration 475/3560 Training loss: 2.1582 0.7790 sec/batch\n",
      "Epoch 3/20  Iteration 476/3560 Training loss: 2.1577 0.7862 sec/batch\n",
      "Epoch 3/20  Iteration 477/3560 Training loss: 2.1574 0.7878 sec/batch\n",
      "Epoch 3/20  Iteration 478/3560 Training loss: 2.1566 0.7950 sec/batch\n",
      "Epoch 3/20  Iteration 479/3560 Training loss: 2.1560 0.7636 sec/batch\n",
      "Epoch 3/20  Iteration 480/3560 Training loss: 2.1556 0.7994 sec/batch\n",
      "Epoch 3/20  Iteration 481/3560 Training loss: 2.1551 0.8811 sec/batch\n",
      "Epoch 3/20  Iteration 482/3560 Training loss: 2.1543 0.7698 sec/batch\n",
      "Epoch 3/20  Iteration 483/3560 Training loss: 2.1538 0.8202 sec/batch\n",
      "Epoch 3/20  Iteration 484/3560 Training loss: 2.1534 0.8326 sec/batch\n",
      "Epoch 3/20  Iteration 485/3560 Training loss: 2.1530 0.7695 sec/batch\n",
      "Epoch 3/20  Iteration 486/3560 Training loss: 2.1525 0.7562 sec/batch\n",
      "Epoch 3/20  Iteration 487/3560 Training loss: 2.1519 0.7685 sec/batch\n",
      "Epoch 3/20  Iteration 488/3560 Training loss: 2.1512 0.7916 sec/batch\n",
      "Epoch 3/20  Iteration 489/3560 Training loss: 2.1508 0.7897 sec/batch\n",
      "Epoch 3/20  Iteration 490/3560 Training loss: 2.1504 0.7767 sec/batch\n",
      "Epoch 3/20  Iteration 491/3560 Training loss: 2.1499 0.7617 sec/batch\n",
      "Epoch 3/20  Iteration 492/3560 Training loss: 2.1494 0.7891 sec/batch\n",
      "Epoch 3/20  Iteration 493/3560 Training loss: 2.1490 0.7786 sec/batch\n",
      "Epoch 3/20  Iteration 494/3560 Training loss: 2.1485 0.7857 sec/batch\n",
      "Epoch 3/20  Iteration 495/3560 Training loss: 2.1482 0.7892 sec/batch\n",
      "Epoch 3/20  Iteration 496/3560 Training loss: 2.1477 0.8780 sec/batch\n",
      "Epoch 3/20  Iteration 497/3560 Training loss: 2.1473 0.8517 sec/batch\n",
      "Epoch 3/20  Iteration 498/3560 Training loss: 2.1468 0.7877 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 499/3560 Training loss: 2.1462 0.7647 sec/batch\n",
      "Epoch 3/20  Iteration 500/3560 Training loss: 2.1458 0.7818 sec/batch\n",
      "Epoch 3/20  Iteration 501/3560 Training loss: 2.1453 0.7937 sec/batch\n",
      "Epoch 3/20  Iteration 502/3560 Training loss: 2.1449 0.7688 sec/batch\n",
      "Epoch 3/20  Iteration 503/3560 Training loss: 2.1445 0.7934 sec/batch\n",
      "Epoch 3/20  Iteration 504/3560 Training loss: 2.1442 0.7847 sec/batch\n",
      "Epoch 3/20  Iteration 505/3560 Training loss: 2.1437 0.8495 sec/batch\n",
      "Epoch 3/20  Iteration 506/3560 Training loss: 2.1431 0.7943 sec/batch\n",
      "Epoch 3/20  Iteration 507/3560 Training loss: 2.1426 0.7973 sec/batch\n",
      "Epoch 3/20  Iteration 508/3560 Training loss: 2.1425 0.8439 sec/batch\n",
      "Epoch 3/20  Iteration 509/3560 Training loss: 2.1421 0.8354 sec/batch\n",
      "Epoch 3/20  Iteration 510/3560 Training loss: 2.1416 0.7972 sec/batch\n",
      "Epoch 3/20  Iteration 511/3560 Training loss: 2.1410 0.8924 sec/batch\n",
      "Epoch 3/20  Iteration 512/3560 Training loss: 2.1405 0.7859 sec/batch\n",
      "Epoch 3/20  Iteration 513/3560 Training loss: 2.1400 0.7832 sec/batch\n",
      "Epoch 3/20  Iteration 514/3560 Training loss: 2.1395 0.7861 sec/batch\n",
      "Epoch 3/20  Iteration 515/3560 Training loss: 2.1388 0.7771 sec/batch\n",
      "Epoch 3/20  Iteration 516/3560 Training loss: 2.1385 0.7872 sec/batch\n",
      "Epoch 3/20  Iteration 517/3560 Training loss: 2.1382 0.7604 sec/batch\n",
      "Epoch 3/20  Iteration 518/3560 Training loss: 2.1377 0.7920 sec/batch\n",
      "Epoch 3/20  Iteration 519/3560 Training loss: 2.1373 0.8096 sec/batch\n",
      "Epoch 3/20  Iteration 520/3560 Training loss: 2.1368 0.7857 sec/batch\n",
      "Epoch 3/20  Iteration 521/3560 Training loss: 2.1364 0.7740 sec/batch\n",
      "Epoch 3/20  Iteration 522/3560 Training loss: 2.1359 0.8326 sec/batch\n",
      "Epoch 3/20  Iteration 523/3560 Training loss: 2.1355 0.7747 sec/batch\n",
      "Epoch 3/20  Iteration 524/3560 Training loss: 2.1353 0.7490 sec/batch\n",
      "Epoch 3/20  Iteration 525/3560 Training loss: 2.1348 0.7765 sec/batch\n",
      "Epoch 3/20  Iteration 526/3560 Training loss: 2.1342 0.8544 sec/batch\n",
      "Epoch 3/20  Iteration 527/3560 Training loss: 2.1336 0.7641 sec/batch\n",
      "Epoch 3/20  Iteration 528/3560 Training loss: 2.1330 0.7876 sec/batch\n",
      "Epoch 3/20  Iteration 529/3560 Training loss: 2.1326 0.7980 sec/batch\n",
      "Epoch 3/20  Iteration 530/3560 Training loss: 2.1322 0.7667 sec/batch\n",
      "Epoch 3/20  Iteration 531/3560 Training loss: 2.1318 0.8033 sec/batch\n",
      "Epoch 3/20  Iteration 532/3560 Training loss: 2.1313 0.8077 sec/batch\n",
      "Epoch 3/20  Iteration 533/3560 Training loss: 2.1308 0.7558 sec/batch\n",
      "Epoch 3/20  Iteration 534/3560 Training loss: 2.1303 0.8308 sec/batch\n",
      "Epoch 4/20  Iteration 535/3560 Training loss: 2.1419 0.8355 sec/batch\n",
      "Epoch 4/20  Iteration 536/3560 Training loss: 2.0813 0.7835 sec/batch\n",
      "Epoch 4/20  Iteration 537/3560 Training loss: 2.0634 0.7875 sec/batch\n",
      "Epoch 4/20  Iteration 538/3560 Training loss: 2.0549 0.8305 sec/batch\n",
      "Epoch 4/20  Iteration 539/3560 Training loss: 2.0506 0.7820 sec/batch\n",
      "Epoch 4/20  Iteration 540/3560 Training loss: 2.0434 0.7871 sec/batch\n",
      "Epoch 4/20  Iteration 541/3560 Training loss: 2.0432 0.9445 sec/batch\n",
      "Epoch 4/20  Iteration 542/3560 Training loss: 2.0432 0.7837 sec/batch\n",
      "Epoch 4/20  Iteration 543/3560 Training loss: 2.0449 0.7605 sec/batch\n",
      "Epoch 4/20  Iteration 544/3560 Training loss: 2.0437 0.7867 sec/batch\n",
      "Epoch 4/20  Iteration 545/3560 Training loss: 2.0401 0.7943 sec/batch\n",
      "Epoch 4/20  Iteration 546/3560 Training loss: 2.0376 0.7828 sec/batch\n",
      "Epoch 4/20  Iteration 547/3560 Training loss: 2.0377 0.8186 sec/batch\n",
      "Epoch 4/20  Iteration 548/3560 Training loss: 2.0391 0.7773 sec/batch\n",
      "Epoch 4/20  Iteration 549/3560 Training loss: 2.0383 0.7801 sec/batch\n",
      "Epoch 4/20  Iteration 550/3560 Training loss: 2.0361 0.7970 sec/batch\n",
      "Epoch 4/20  Iteration 551/3560 Training loss: 2.0353 0.7925 sec/batch\n",
      "Epoch 4/20  Iteration 552/3560 Training loss: 2.0371 0.7764 sec/batch\n",
      "Epoch 4/20  Iteration 553/3560 Training loss: 2.0368 0.7974 sec/batch\n",
      "Epoch 4/20  Iteration 554/3560 Training loss: 2.0361 0.7833 sec/batch\n",
      "Epoch 4/20  Iteration 555/3560 Training loss: 2.0347 0.8031 sec/batch\n",
      "Epoch 4/20  Iteration 556/3560 Training loss: 2.0358 0.8381 sec/batch\n",
      "Epoch 4/20  Iteration 557/3560 Training loss: 2.0348 0.8514 sec/batch\n",
      "Epoch 4/20  Iteration 558/3560 Training loss: 2.0341 0.7651 sec/batch\n",
      "Epoch 4/20  Iteration 559/3560 Training loss: 2.0334 0.8108 sec/batch\n",
      "Epoch 4/20  Iteration 560/3560 Training loss: 2.0323 0.8091 sec/batch\n",
      "Epoch 4/20  Iteration 561/3560 Training loss: 2.0310 0.7920 sec/batch\n",
      "Epoch 4/20  Iteration 562/3560 Training loss: 2.0306 0.7708 sec/batch\n",
      "Epoch 4/20  Iteration 563/3560 Training loss: 2.0313 0.7866 sec/batch\n",
      "Epoch 4/20  Iteration 564/3560 Training loss: 2.0311 0.7981 sec/batch\n",
      "Epoch 4/20  Iteration 565/3560 Training loss: 2.0309 0.7995 sec/batch\n",
      "Epoch 4/20  Iteration 566/3560 Training loss: 2.0301 0.7582 sec/batch\n",
      "Epoch 4/20  Iteration 567/3560 Training loss: 2.0298 0.8061 sec/batch\n",
      "Epoch 4/20  Iteration 568/3560 Training loss: 2.0301 0.7908 sec/batch\n",
      "Epoch 4/20  Iteration 569/3560 Training loss: 2.0292 0.7730 sec/batch\n",
      "Epoch 4/20  Iteration 570/3560 Training loss: 2.0286 0.7912 sec/batch\n",
      "Epoch 4/20  Iteration 571/3560 Training loss: 2.0283 0.8760 sec/batch\n",
      "Epoch 4/20  Iteration 572/3560 Training loss: 2.0269 0.9040 sec/batch\n",
      "Epoch 4/20  Iteration 573/3560 Training loss: 2.0253 0.7959 sec/batch\n",
      "Epoch 4/20  Iteration 574/3560 Training loss: 2.0241 0.7492 sec/batch\n",
      "Epoch 4/20  Iteration 575/3560 Training loss: 2.0233 0.7623 sec/batch\n",
      "Epoch 4/20  Iteration 576/3560 Training loss: 2.0226 0.7863 sec/batch\n",
      "Epoch 4/20  Iteration 577/3560 Training loss: 2.0218 0.7827 sec/batch\n",
      "Epoch 4/20  Iteration 578/3560 Training loss: 2.0207 0.7593 sec/batch\n",
      "Epoch 4/20  Iteration 579/3560 Training loss: 2.0203 0.8006 sec/batch\n",
      "Epoch 4/20  Iteration 580/3560 Training loss: 2.0188 0.7961 sec/batch\n",
      "Epoch 4/20  Iteration 581/3560 Training loss: 2.0184 0.7872 sec/batch\n",
      "Epoch 4/20  Iteration 582/3560 Training loss: 2.0176 0.7993 sec/batch\n",
      "Epoch 4/20  Iteration 583/3560 Training loss: 2.0172 0.8241 sec/batch\n",
      "Epoch 4/20  Iteration 584/3560 Training loss: 2.0176 0.8014 sec/batch\n",
      "Epoch 4/20  Iteration 585/3560 Training loss: 2.0167 0.8916 sec/batch\n",
      "Epoch 4/20  Iteration 586/3560 Training loss: 2.0170 0.8288 sec/batch\n",
      "Epoch 4/20  Iteration 587/3560 Training loss: 2.0165 0.8020 sec/batch\n",
      "Epoch 4/20  Iteration 588/3560 Training loss: 2.0160 0.7501 sec/batch\n",
      "Epoch 4/20  Iteration 589/3560 Training loss: 2.0152 0.7787 sec/batch\n",
      "Epoch 4/20  Iteration 590/3560 Training loss: 2.0150 0.7562 sec/batch\n",
      "Epoch 4/20  Iteration 591/3560 Training loss: 2.0147 0.7795 sec/batch\n",
      "Epoch 4/20  Iteration 592/3560 Training loss: 2.0142 0.7885 sec/batch\n",
      "Epoch 4/20  Iteration 593/3560 Training loss: 2.0136 0.7830 sec/batch\n",
      "Epoch 4/20  Iteration 594/3560 Training loss: 2.0137 0.7636 sec/batch\n",
      "Epoch 4/20  Iteration 595/3560 Training loss: 2.0134 0.7631 sec/batch\n",
      "Epoch 4/20  Iteration 596/3560 Training loss: 2.0137 0.7657 sec/batch\n",
      "Epoch 4/20  Iteration 597/3560 Training loss: 2.0138 0.7740 sec/batch\n",
      "Epoch 4/20  Iteration 598/3560 Training loss: 2.0138 0.8409 sec/batch\n",
      "Epoch 4/20  Iteration 599/3560 Training loss: 2.0132 0.7502 sec/batch\n",
      "Epoch 4/20  Iteration 600/3560 Training loss: 2.0131 0.8212 sec/batch\n",
      "Epoch 4/20  Iteration 601/3560 Training loss: 2.0129 0.8341 sec/batch\n",
      "Epoch 4/20  Iteration 602/3560 Training loss: 2.0120 0.8003 sec/batch\n",
      "Epoch 4/20  Iteration 603/3560 Training loss: 2.0113 0.7765 sec/batch\n",
      "Epoch 4/20  Iteration 604/3560 Training loss: 2.0109 0.8045 sec/batch\n",
      "Epoch 4/20  Iteration 605/3560 Training loss: 2.0109 0.7815 sec/batch\n",
      "Epoch 4/20  Iteration 606/3560 Training loss: 2.0106 0.7633 sec/batch\n",
      "Epoch 4/20  Iteration 607/3560 Training loss: 2.0104 0.7763 sec/batch\n",
      "Epoch 4/20  Iteration 608/3560 Training loss: 2.0098 0.8146 sec/batch\n",
      "Epoch 4/20  Iteration 609/3560 Training loss: 2.0094 0.8018 sec/batch\n",
      "Epoch 4/20  Iteration 610/3560 Training loss: 2.0094 0.8085 sec/batch\n",
      "Epoch 4/20  Iteration 611/3560 Training loss: 2.0089 0.8359 sec/batch\n",
      "Epoch 4/20  Iteration 612/3560 Training loss: 2.0087 0.8004 sec/batch\n",
      "Epoch 4/20  Iteration 613/3560 Training loss: 2.0080 0.7718 sec/batch\n",
      "Epoch 4/20  Iteration 614/3560 Training loss: 2.0075 0.7780 sec/batch\n",
      "Epoch 4/20  Iteration 615/3560 Training loss: 2.0067 0.7773 sec/batch\n",
      "Epoch 4/20  Iteration 616/3560 Training loss: 2.0064 0.8487 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 617/3560 Training loss: 2.0056 0.7881 sec/batch\n",
      "Epoch 4/20  Iteration 618/3560 Training loss: 2.0051 0.7902 sec/batch\n",
      "Epoch 4/20  Iteration 619/3560 Training loss: 2.0043 0.7692 sec/batch\n",
      "Epoch 4/20  Iteration 620/3560 Training loss: 2.0037 0.8144 sec/batch\n",
      "Epoch 4/20  Iteration 621/3560 Training loss: 2.0032 0.7747 sec/batch\n",
      "Epoch 4/20  Iteration 622/3560 Training loss: 2.0026 0.7880 sec/batch\n",
      "Epoch 4/20  Iteration 623/3560 Training loss: 2.0018 0.8144 sec/batch\n",
      "Epoch 4/20  Iteration 624/3560 Training loss: 2.0016 0.7629 sec/batch\n",
      "Epoch 4/20  Iteration 625/3560 Training loss: 2.0010 0.8224 sec/batch\n",
      "Epoch 4/20  Iteration 626/3560 Training loss: 2.0005 0.8815 sec/batch\n",
      "Epoch 4/20  Iteration 627/3560 Training loss: 1.9997 0.7693 sec/batch\n",
      "Epoch 4/20  Iteration 628/3560 Training loss: 1.9990 0.7951 sec/batch\n",
      "Epoch 4/20  Iteration 629/3560 Training loss: 1.9983 0.7641 sec/batch\n",
      "Epoch 4/20  Iteration 630/3560 Training loss: 1.9979 0.7968 sec/batch\n",
      "Epoch 4/20  Iteration 631/3560 Training loss: 1.9974 1.0505 sec/batch\n",
      "Epoch 4/20  Iteration 632/3560 Training loss: 1.9967 0.9877 sec/batch\n",
      "Epoch 4/20  Iteration 633/3560 Training loss: 1.9959 1.0217 sec/batch\n",
      "Epoch 4/20  Iteration 634/3560 Training loss: 1.9951 1.0511 sec/batch\n",
      "Epoch 4/20  Iteration 635/3560 Training loss: 1.9949 1.0388 sec/batch\n",
      "Epoch 4/20  Iteration 636/3560 Training loss: 1.9944 0.9997 sec/batch\n",
      "Epoch 4/20  Iteration 637/3560 Training loss: 1.9938 1.0159 sec/batch\n",
      "Epoch 4/20  Iteration 638/3560 Training loss: 1.9933 1.1172 sec/batch\n",
      "Epoch 4/20  Iteration 639/3560 Training loss: 1.9927 0.8686 sec/batch\n",
      "Epoch 4/20  Iteration 640/3560 Training loss: 1.9924 0.7776 sec/batch\n",
      "Epoch 4/20  Iteration 641/3560 Training loss: 1.9919 0.7647 sec/batch\n",
      "Epoch 4/20  Iteration 642/3560 Training loss: 1.9917 0.7551 sec/batch\n",
      "Epoch 4/20  Iteration 643/3560 Training loss: 1.9914 0.7802 sec/batch\n",
      "Epoch 4/20  Iteration 644/3560 Training loss: 1.9910 0.7613 sec/batch\n",
      "Epoch 4/20  Iteration 645/3560 Training loss: 1.9907 0.8055 sec/batch\n",
      "Epoch 4/20  Iteration 646/3560 Training loss: 1.9902 0.8861 sec/batch\n",
      "Epoch 4/20  Iteration 647/3560 Training loss: 1.9897 0.7876 sec/batch\n",
      "Epoch 4/20  Iteration 648/3560 Training loss: 1.9893 0.7635 sec/batch\n",
      "Epoch 4/20  Iteration 649/3560 Training loss: 1.9888 0.7833 sec/batch\n",
      "Epoch 4/20  Iteration 650/3560 Training loss: 1.9880 0.7630 sec/batch\n",
      "Epoch 4/20  Iteration 651/3560 Training loss: 1.9877 0.8188 sec/batch\n",
      "Epoch 4/20  Iteration 652/3560 Training loss: 1.9873 0.7938 sec/batch\n",
      "Epoch 4/20  Iteration 653/3560 Training loss: 1.9871 0.8760 sec/batch\n",
      "Epoch 4/20  Iteration 654/3560 Training loss: 1.9867 0.8133 sec/batch\n",
      "Epoch 4/20  Iteration 655/3560 Training loss: 1.9864 0.7982 sec/batch\n",
      "Epoch 4/20  Iteration 656/3560 Training loss: 1.9857 0.8011 sec/batch\n",
      "Epoch 4/20  Iteration 657/3560 Training loss: 1.9852 0.7951 sec/batch\n",
      "Epoch 4/20  Iteration 658/3560 Training loss: 1.9850 0.7997 sec/batch\n",
      "Epoch 4/20  Iteration 659/3560 Training loss: 1.9847 0.7804 sec/batch\n",
      "Epoch 4/20  Iteration 660/3560 Training loss: 1.9841 0.7487 sec/batch\n",
      "Epoch 4/20  Iteration 661/3560 Training loss: 1.9839 0.7885 sec/batch\n",
      "Epoch 4/20  Iteration 662/3560 Training loss: 1.9835 0.7891 sec/batch\n",
      "Epoch 4/20  Iteration 663/3560 Training loss: 1.9832 0.7929 sec/batch\n",
      "Epoch 4/20  Iteration 664/3560 Training loss: 1.9829 0.7929 sec/batch\n",
      "Epoch 4/20  Iteration 665/3560 Training loss: 1.9824 0.8041 sec/batch\n",
      "Epoch 4/20  Iteration 666/3560 Training loss: 1.9818 0.7744 sec/batch\n",
      "Epoch 4/20  Iteration 667/3560 Training loss: 1.9816 0.7635 sec/batch\n",
      "Epoch 4/20  Iteration 668/3560 Training loss: 1.9813 0.8458 sec/batch\n",
      "Epoch 4/20  Iteration 669/3560 Training loss: 1.9810 0.8016 sec/batch\n",
      "Epoch 4/20  Iteration 670/3560 Training loss: 1.9808 0.8103 sec/batch\n",
      "Epoch 4/20  Iteration 671/3560 Training loss: 1.9805 0.8040 sec/batch\n",
      "Epoch 4/20  Iteration 672/3560 Training loss: 1.9802 0.8145 sec/batch\n",
      "Epoch 4/20  Iteration 673/3560 Training loss: 1.9801 0.7559 sec/batch\n",
      "Epoch 4/20  Iteration 674/3560 Training loss: 1.9797 0.8449 sec/batch\n",
      "Epoch 4/20  Iteration 675/3560 Training loss: 1.9796 0.7902 sec/batch\n",
      "Epoch 4/20  Iteration 676/3560 Training loss: 1.9793 0.8001 sec/batch\n",
      "Epoch 4/20  Iteration 677/3560 Training loss: 1.9789 0.7671 sec/batch\n",
      "Epoch 4/20  Iteration 678/3560 Training loss: 1.9786 0.7854 sec/batch\n",
      "Epoch 4/20  Iteration 679/3560 Training loss: 1.9782 0.7862 sec/batch\n",
      "Epoch 4/20  Iteration 680/3560 Training loss: 1.9780 0.7683 sec/batch\n",
      "Epoch 4/20  Iteration 681/3560 Training loss: 1.9777 0.8105 sec/batch\n",
      "Epoch 4/20  Iteration 682/3560 Training loss: 1.9775 0.7977 sec/batch\n",
      "Epoch 4/20  Iteration 683/3560 Training loss: 1.9773 0.8401 sec/batch\n",
      "Epoch 4/20  Iteration 684/3560 Training loss: 1.9768 0.8655 sec/batch\n",
      "Epoch 4/20  Iteration 685/3560 Training loss: 1.9764 0.7498 sec/batch\n",
      "Epoch 4/20  Iteration 686/3560 Training loss: 1.9763 0.7606 sec/batch\n",
      "Epoch 4/20  Iteration 687/3560 Training loss: 1.9760 0.7855 sec/batch\n",
      "Epoch 4/20  Iteration 688/3560 Training loss: 1.9757 0.7802 sec/batch\n",
      "Epoch 4/20  Iteration 689/3560 Training loss: 1.9754 0.8107 sec/batch\n",
      "Epoch 4/20  Iteration 690/3560 Training loss: 1.9751 0.7504 sec/batch\n",
      "Epoch 4/20  Iteration 691/3560 Training loss: 1.9748 0.7665 sec/batch\n",
      "Epoch 4/20  Iteration 692/3560 Training loss: 1.9745 0.7938 sec/batch\n",
      "Epoch 4/20  Iteration 693/3560 Training loss: 1.9740 0.7683 sec/batch\n",
      "Epoch 4/20  Iteration 694/3560 Training loss: 1.9739 0.7960 sec/batch\n",
      "Epoch 4/20  Iteration 695/3560 Training loss: 1.9738 0.7807 sec/batch\n",
      "Epoch 4/20  Iteration 696/3560 Training loss: 1.9734 0.8146 sec/batch\n",
      "Epoch 4/20  Iteration 697/3560 Training loss: 1.9732 0.8042 sec/batch\n",
      "Epoch 4/20  Iteration 698/3560 Training loss: 1.9729 0.8431 sec/batch\n",
      "Epoch 4/20  Iteration 699/3560 Training loss: 1.9726 0.8193 sec/batch\n",
      "Epoch 4/20  Iteration 700/3560 Training loss: 1.9722 0.7907 sec/batch\n",
      "Epoch 4/20  Iteration 701/3560 Training loss: 1.9720 0.7757 sec/batch\n",
      "Epoch 4/20  Iteration 702/3560 Training loss: 1.9719 0.7902 sec/batch\n",
      "Epoch 4/20  Iteration 703/3560 Training loss: 1.9716 0.7826 sec/batch\n",
      "Epoch 4/20  Iteration 704/3560 Training loss: 1.9713 0.7686 sec/batch\n",
      "Epoch 4/20  Iteration 705/3560 Training loss: 1.9708 0.7594 sec/batch\n",
      "Epoch 4/20  Iteration 706/3560 Training loss: 1.9704 0.7718 sec/batch\n",
      "Epoch 4/20  Iteration 707/3560 Training loss: 1.9702 0.7722 sec/batch\n",
      "Epoch 4/20  Iteration 708/3560 Training loss: 1.9700 0.7948 sec/batch\n",
      "Epoch 4/20  Iteration 709/3560 Training loss: 1.9697 0.8143 sec/batch\n",
      "Epoch 4/20  Iteration 710/3560 Training loss: 1.9694 0.7818 sec/batch\n",
      "Epoch 4/20  Iteration 711/3560 Training loss: 1.9689 0.7798 sec/batch\n",
      "Epoch 4/20  Iteration 712/3560 Training loss: 1.9686 0.7875 sec/batch\n",
      "Epoch 5/20  Iteration 713/3560 Training loss: 2.0054 0.7854 sec/batch\n",
      "Epoch 5/20  Iteration 714/3560 Training loss: 1.9499 0.8318 sec/batch\n",
      "Epoch 5/20  Iteration 715/3560 Training loss: 1.9341 0.7963 sec/batch\n",
      "Epoch 5/20  Iteration 716/3560 Training loss: 1.9255 0.7642 sec/batch\n",
      "Epoch 5/20  Iteration 717/3560 Training loss: 1.9186 0.7679 sec/batch\n",
      "Epoch 5/20  Iteration 718/3560 Training loss: 1.9095 0.7567 sec/batch\n",
      "Epoch 5/20  Iteration 719/3560 Training loss: 1.9103 0.7998 sec/batch\n",
      "Epoch 5/20  Iteration 720/3560 Training loss: 1.9099 0.7844 sec/batch\n",
      "Epoch 5/20  Iteration 721/3560 Training loss: 1.9127 0.8012 sec/batch\n",
      "Epoch 5/20  Iteration 722/3560 Training loss: 1.9120 0.7771 sec/batch\n",
      "Epoch 5/20  Iteration 723/3560 Training loss: 1.9083 0.8209 sec/batch\n",
      "Epoch 5/20  Iteration 724/3560 Training loss: 1.9065 0.7928 sec/batch\n",
      "Epoch 5/20  Iteration 725/3560 Training loss: 1.9066 0.7767 sec/batch\n",
      "Epoch 5/20  Iteration 726/3560 Training loss: 1.9087 0.7897 sec/batch\n",
      "Epoch 5/20  Iteration 727/3560 Training loss: 1.9081 0.7748 sec/batch\n",
      "Epoch 5/20  Iteration 728/3560 Training loss: 1.9063 0.7543 sec/batch\n",
      "Epoch 5/20  Iteration 729/3560 Training loss: 1.9057 0.9595 sec/batch\n",
      "Epoch 5/20  Iteration 730/3560 Training loss: 1.9077 0.7638 sec/batch\n",
      "Epoch 5/20  Iteration 731/3560 Training loss: 1.9073 0.8280 sec/batch\n",
      "Epoch 5/20  Iteration 732/3560 Training loss: 1.9069 0.7851 sec/batch\n",
      "Epoch 5/20  Iteration 733/3560 Training loss: 1.9057 0.8278 sec/batch\n",
      "Epoch 5/20  Iteration 734/3560 Training loss: 1.9070 0.7771 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 735/3560 Training loss: 1.9061 0.8008 sec/batch\n",
      "Epoch 5/20  Iteration 736/3560 Training loss: 1.9054 0.7710 sec/batch\n",
      "Epoch 5/20  Iteration 737/3560 Training loss: 1.9050 0.7612 sec/batch\n",
      "Epoch 5/20  Iteration 738/3560 Training loss: 1.9037 0.7682 sec/batch\n",
      "Epoch 5/20  Iteration 739/3560 Training loss: 1.9023 0.7985 sec/batch\n",
      "Epoch 5/20  Iteration 740/3560 Training loss: 1.9025 0.7775 sec/batch\n",
      "Epoch 5/20  Iteration 741/3560 Training loss: 1.9033 0.7866 sec/batch\n",
      "Epoch 5/20  Iteration 742/3560 Training loss: 1.9035 0.7979 sec/batch\n",
      "Epoch 5/20  Iteration 743/3560 Training loss: 1.9032 0.7864 sec/batch\n",
      "Epoch 5/20  Iteration 744/3560 Training loss: 1.9021 0.8795 sec/batch\n",
      "Epoch 5/20  Iteration 745/3560 Training loss: 1.9018 0.8047 sec/batch\n",
      "Epoch 5/20  Iteration 746/3560 Training loss: 1.9019 0.7948 sec/batch\n",
      "Epoch 5/20  Iteration 747/3560 Training loss: 1.9011 0.7836 sec/batch\n",
      "Epoch 5/20  Iteration 748/3560 Training loss: 1.9005 0.8116 sec/batch\n",
      "Epoch 5/20  Iteration 749/3560 Training loss: 1.8999 0.7818 sec/batch\n",
      "Epoch 5/20  Iteration 750/3560 Training loss: 1.8987 0.7946 sec/batch\n",
      "Epoch 5/20  Iteration 751/3560 Training loss: 1.8973 0.7727 sec/batch\n",
      "Epoch 5/20  Iteration 752/3560 Training loss: 1.8963 0.8090 sec/batch\n",
      "Epoch 5/20  Iteration 753/3560 Training loss: 1.8957 0.8066 sec/batch\n",
      "Epoch 5/20  Iteration 754/3560 Training loss: 1.8956 0.7596 sec/batch\n",
      "Epoch 5/20  Iteration 755/3560 Training loss: 1.8947 0.7612 sec/batch\n",
      "Epoch 5/20  Iteration 756/3560 Training loss: 1.8939 0.7535 sec/batch\n",
      "Epoch 5/20  Iteration 757/3560 Training loss: 1.8938 0.7808 sec/batch\n",
      "Epoch 5/20  Iteration 758/3560 Training loss: 1.8925 0.8167 sec/batch\n",
      "Epoch 5/20  Iteration 759/3560 Training loss: 1.8921 0.8879 sec/batch\n",
      "Epoch 5/20  Iteration 760/3560 Training loss: 1.8913 0.8076 sec/batch\n",
      "Epoch 5/20  Iteration 761/3560 Training loss: 1.8909 0.8326 sec/batch\n",
      "Epoch 5/20  Iteration 762/3560 Training loss: 1.8912 0.7925 sec/batch\n",
      "Epoch 5/20  Iteration 763/3560 Training loss: 1.8904 0.7564 sec/batch\n",
      "Epoch 5/20  Iteration 764/3560 Training loss: 1.8911 0.8038 sec/batch\n",
      "Epoch 5/20  Iteration 765/3560 Training loss: 1.8906 0.7971 sec/batch\n",
      "Epoch 5/20  Iteration 766/3560 Training loss: 1.8903 0.7829 sec/batch\n",
      "Epoch 5/20  Iteration 767/3560 Training loss: 1.8898 0.7669 sec/batch\n",
      "Epoch 5/20  Iteration 768/3560 Training loss: 1.8897 0.7623 sec/batch\n",
      "Epoch 5/20  Iteration 769/3560 Training loss: 1.8897 0.7756 sec/batch\n",
      "Epoch 5/20  Iteration 770/3560 Training loss: 1.8893 0.7807 sec/batch\n",
      "Epoch 5/20  Iteration 771/3560 Training loss: 1.8886 0.7848 sec/batch\n",
      "Epoch 5/20  Iteration 772/3560 Training loss: 1.8887 0.8028 sec/batch\n",
      "Epoch 5/20  Iteration 773/3560 Training loss: 1.8882 0.8069 sec/batch\n",
      "Epoch 5/20  Iteration 774/3560 Training loss: 1.8888 0.8505 sec/batch\n",
      "Epoch 5/20  Iteration 775/3560 Training loss: 1.8890 0.8041 sec/batch\n",
      "Epoch 5/20  Iteration 776/3560 Training loss: 1.8891 0.7844 sec/batch\n",
      "Epoch 5/20  Iteration 777/3560 Training loss: 1.8889 0.7705 sec/batch\n",
      "Epoch 5/20  Iteration 778/3560 Training loss: 1.8890 0.8072 sec/batch\n",
      "Epoch 5/20  Iteration 779/3560 Training loss: 1.8891 0.7630 sec/batch\n",
      "Epoch 5/20  Iteration 780/3560 Training loss: 1.8885 0.7654 sec/batch\n",
      "Epoch 5/20  Iteration 781/3560 Training loss: 1.8880 0.7831 sec/batch\n",
      "Epoch 5/20  Iteration 782/3560 Training loss: 1.8878 0.8010 sec/batch\n",
      "Epoch 5/20  Iteration 783/3560 Training loss: 1.8880 0.8039 sec/batch\n",
      "Epoch 5/20  Iteration 784/3560 Training loss: 1.8877 0.7703 sec/batch\n",
      "Epoch 5/20  Iteration 785/3560 Training loss: 1.8877 0.7725 sec/batch\n",
      "Epoch 5/20  Iteration 786/3560 Training loss: 1.8872 0.8027 sec/batch\n",
      "Epoch 5/20  Iteration 787/3560 Training loss: 1.8868 0.8024 sec/batch\n",
      "Epoch 5/20  Iteration 788/3560 Training loss: 1.8869 0.7559 sec/batch\n",
      "Epoch 5/20  Iteration 789/3560 Training loss: 1.8866 0.8100 sec/batch\n",
      "Epoch 5/20  Iteration 790/3560 Training loss: 1.8865 0.8463 sec/batch\n",
      "Epoch 5/20  Iteration 791/3560 Training loss: 1.8858 0.7945 sec/batch\n",
      "Epoch 5/20  Iteration 792/3560 Training loss: 1.8854 0.7513 sec/batch\n",
      "Epoch 5/20  Iteration 793/3560 Training loss: 1.8848 0.8038 sec/batch\n",
      "Epoch 5/20  Iteration 794/3560 Training loss: 1.8847 0.8140 sec/batch\n",
      "Epoch 5/20  Iteration 795/3560 Training loss: 1.8840 0.7809 sec/batch\n",
      "Epoch 5/20  Iteration 796/3560 Training loss: 1.8838 0.7795 sec/batch\n",
      "Epoch 5/20  Iteration 797/3560 Training loss: 1.8830 0.7958 sec/batch\n",
      "Epoch 5/20  Iteration 798/3560 Training loss: 1.8825 0.7829 sec/batch\n",
      "Epoch 5/20  Iteration 799/3560 Training loss: 1.8822 0.8289 sec/batch\n",
      "Epoch 5/20  Iteration 800/3560 Training loss: 1.8817 0.7708 sec/batch\n",
      "Epoch 5/20  Iteration 801/3560 Training loss: 1.8809 0.7672 sec/batch\n",
      "Epoch 5/20  Iteration 802/3560 Training loss: 1.8808 0.7541 sec/batch\n",
      "Epoch 5/20  Iteration 803/3560 Training loss: 1.8804 0.7599 sec/batch\n",
      "Epoch 5/20  Iteration 804/3560 Training loss: 1.8801 0.8406 sec/batch\n",
      "Epoch 5/20  Iteration 805/3560 Training loss: 1.8794 0.8075 sec/batch\n",
      "Epoch 5/20  Iteration 806/3560 Training loss: 1.8789 0.7882 sec/batch\n",
      "Epoch 5/20  Iteration 807/3560 Training loss: 1.8783 0.7645 sec/batch\n",
      "Epoch 5/20  Iteration 808/3560 Training loss: 1.8780 0.7836 sec/batch\n",
      "Epoch 5/20  Iteration 809/3560 Training loss: 1.8777 0.7969 sec/batch\n",
      "Epoch 5/20  Iteration 810/3560 Training loss: 1.8771 0.7787 sec/batch\n",
      "Epoch 5/20  Iteration 811/3560 Training loss: 1.8765 0.7972 sec/batch\n",
      "Epoch 5/20  Iteration 812/3560 Training loss: 1.8758 0.8051 sec/batch\n",
      "Epoch 5/20  Iteration 813/3560 Training loss: 1.8756 0.7807 sec/batch\n",
      "Epoch 5/20  Iteration 814/3560 Training loss: 1.8753 0.7614 sec/batch\n",
      "Epoch 5/20  Iteration 815/3560 Training loss: 1.8749 0.7619 sec/batch\n",
      "Epoch 5/20  Iteration 816/3560 Training loss: 1.8746 0.7760 sec/batch\n",
      "Epoch 5/20  Iteration 817/3560 Training loss: 1.8742 0.7690 sec/batch\n",
      "Epoch 5/20  Iteration 818/3560 Training loss: 1.8739 0.7877 sec/batch\n",
      "Epoch 5/20  Iteration 819/3560 Training loss: 1.8736 0.7898 sec/batch\n",
      "Epoch 5/20  Iteration 820/3560 Training loss: 1.8733 0.8493 sec/batch\n",
      "Epoch 5/20  Iteration 821/3560 Training loss: 1.8732 0.7656 sec/batch\n",
      "Epoch 5/20  Iteration 822/3560 Training loss: 1.8730 0.8001 sec/batch\n",
      "Epoch 5/20  Iteration 823/3560 Training loss: 1.8727 0.8128 sec/batch\n",
      "Epoch 5/20  Iteration 824/3560 Training loss: 1.8723 0.7945 sec/batch\n",
      "Epoch 5/20  Iteration 825/3560 Training loss: 1.8720 0.8309 sec/batch\n",
      "Epoch 5/20  Iteration 826/3560 Training loss: 1.8718 0.7882 sec/batch\n",
      "Epoch 5/20  Iteration 827/3560 Training loss: 1.8714 0.7819 sec/batch\n",
      "Epoch 5/20  Iteration 828/3560 Training loss: 1.8708 0.7841 sec/batch\n",
      "Epoch 5/20  Iteration 829/3560 Training loss: 1.8706 0.8018 sec/batch\n",
      "Epoch 5/20  Iteration 830/3560 Training loss: 1.8703 0.7605 sec/batch\n",
      "Epoch 5/20  Iteration 831/3560 Training loss: 1.8701 0.7659 sec/batch\n",
      "Epoch 5/20  Iteration 832/3560 Training loss: 1.8698 0.7917 sec/batch\n",
      "Epoch 5/20  Iteration 833/3560 Training loss: 1.8696 0.7631 sec/batch\n",
      "Epoch 5/20  Iteration 834/3560 Training loss: 1.8691 0.8155 sec/batch\n",
      "Epoch 5/20  Iteration 835/3560 Training loss: 1.8686 0.8832 sec/batch\n",
      "Epoch 5/20  Iteration 836/3560 Training loss: 1.8686 0.7995 sec/batch\n",
      "Epoch 5/20  Iteration 837/3560 Training loss: 1.8683 0.8364 sec/batch\n",
      "Epoch 5/20  Iteration 838/3560 Training loss: 1.8678 0.7921 sec/batch\n",
      "Epoch 5/20  Iteration 839/3560 Training loss: 1.8676 0.7529 sec/batch\n",
      "Epoch 5/20  Iteration 840/3560 Training loss: 1.8675 0.7946 sec/batch\n",
      "Epoch 5/20  Iteration 841/3560 Training loss: 1.8673 0.7965 sec/batch\n",
      "Epoch 5/20  Iteration 842/3560 Training loss: 1.8670 0.7779 sec/batch\n",
      "Epoch 5/20  Iteration 843/3560 Training loss: 1.8666 0.7774 sec/batch\n",
      "Epoch 5/20  Iteration 844/3560 Training loss: 1.8661 0.7569 sec/batch\n",
      "Epoch 5/20  Iteration 845/3560 Training loss: 1.8660 0.7938 sec/batch\n",
      "Epoch 5/20  Iteration 846/3560 Training loss: 1.8658 0.7880 sec/batch\n",
      "Epoch 5/20  Iteration 847/3560 Training loss: 1.8656 0.7990 sec/batch\n",
      "Epoch 5/20  Iteration 848/3560 Training loss: 1.8655 0.8011 sec/batch\n",
      "Epoch 5/20  Iteration 849/3560 Training loss: 1.8654 0.7690 sec/batch\n",
      "Epoch 5/20  Iteration 850/3560 Training loss: 1.8652 0.9276 sec/batch\n",
      "Epoch 5/20  Iteration 851/3560 Training loss: 1.8652 0.8687 sec/batch\n",
      "Epoch 5/20  Iteration 852/3560 Training loss: 1.8649 0.8658 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 853/3560 Training loss: 1.8649 0.8999 sec/batch\n",
      "Epoch 5/20  Iteration 854/3560 Training loss: 1.8647 1.1843 sec/batch\n",
      "Epoch 5/20  Iteration 855/3560 Training loss: 1.8644 1.0580 sec/batch\n",
      "Epoch 5/20  Iteration 856/3560 Training loss: 1.8643 1.0626 sec/batch\n",
      "Epoch 5/20  Iteration 857/3560 Training loss: 1.8640 1.1960 sec/batch\n",
      "Epoch 5/20  Iteration 858/3560 Training loss: 1.8639 1.0499 sec/batch\n",
      "Epoch 5/20  Iteration 859/3560 Training loss: 1.8637 1.0039 sec/batch\n",
      "Epoch 5/20  Iteration 860/3560 Training loss: 1.8636 1.1498 sec/batch\n",
      "Epoch 5/20  Iteration 861/3560 Training loss: 1.8635 1.0951 sec/batch\n",
      "Epoch 5/20  Iteration 862/3560 Training loss: 1.8632 0.9862 sec/batch\n",
      "Epoch 5/20  Iteration 863/3560 Training loss: 1.8629 0.7770 sec/batch\n",
      "Epoch 5/20  Iteration 864/3560 Training loss: 1.8629 0.7801 sec/batch\n",
      "Epoch 5/20  Iteration 865/3560 Training loss: 1.8627 0.7812 sec/batch\n",
      "Epoch 5/20  Iteration 866/3560 Training loss: 1.8626 0.7760 sec/batch\n",
      "Epoch 5/20  Iteration 867/3560 Training loss: 1.8624 0.7986 sec/batch\n",
      "Epoch 5/20  Iteration 868/3560 Training loss: 1.8621 0.7995 sec/batch\n",
      "Epoch 5/20  Iteration 869/3560 Training loss: 1.8620 0.7776 sec/batch\n",
      "Epoch 5/20  Iteration 870/3560 Training loss: 1.8618 0.7696 sec/batch\n",
      "Epoch 5/20  Iteration 871/3560 Training loss: 1.8614 0.7773 sec/batch\n",
      "Epoch 5/20  Iteration 872/3560 Training loss: 1.8614 0.8155 sec/batch\n",
      "Epoch 5/20  Iteration 873/3560 Training loss: 1.8614 0.7927 sec/batch\n",
      "Epoch 5/20  Iteration 874/3560 Training loss: 1.8612 0.8114 sec/batch\n",
      "Epoch 5/20  Iteration 875/3560 Training loss: 1.8611 0.7921 sec/batch\n",
      "Epoch 5/20  Iteration 876/3560 Training loss: 1.8609 0.8424 sec/batch\n",
      "Epoch 5/20  Iteration 877/3560 Training loss: 1.8607 0.8748 sec/batch\n",
      "Epoch 5/20  Iteration 878/3560 Training loss: 1.8605 0.7642 sec/batch\n",
      "Epoch 5/20  Iteration 879/3560 Training loss: 1.8603 0.7647 sec/batch\n",
      "Epoch 5/20  Iteration 880/3560 Training loss: 1.8604 0.7697 sec/batch\n",
      "Epoch 5/20  Iteration 881/3560 Training loss: 1.8602 0.7963 sec/batch\n",
      "Epoch 5/20  Iteration 882/3560 Training loss: 1.8600 0.7750 sec/batch\n",
      "Epoch 5/20  Iteration 883/3560 Training loss: 1.8597 0.7889 sec/batch\n",
      "Epoch 5/20  Iteration 884/3560 Training loss: 1.8594 0.8595 sec/batch\n",
      "Epoch 5/20  Iteration 885/3560 Training loss: 1.8593 0.7601 sec/batch\n",
      "Epoch 5/20  Iteration 886/3560 Training loss: 1.8592 0.7726 sec/batch\n",
      "Epoch 5/20  Iteration 887/3560 Training loss: 1.8590 0.7886 sec/batch\n",
      "Epoch 5/20  Iteration 888/3560 Training loss: 1.8588 0.7927 sec/batch\n",
      "Epoch 5/20  Iteration 889/3560 Training loss: 1.8585 0.7937 sec/batch\n",
      "Epoch 5/20  Iteration 890/3560 Training loss: 1.8584 0.7940 sec/batch\n",
      "Epoch 6/20  Iteration 891/3560 Training loss: 1.9204 0.8211 sec/batch\n",
      "Epoch 6/20  Iteration 892/3560 Training loss: 1.8650 0.8858 sec/batch\n",
      "Epoch 6/20  Iteration 893/3560 Training loss: 1.8502 0.7755 sec/batch\n",
      "Epoch 6/20  Iteration 894/3560 Training loss: 1.8428 0.8050 sec/batch\n",
      "Epoch 6/20  Iteration 895/3560 Training loss: 1.8371 0.7598 sec/batch\n",
      "Epoch 6/20  Iteration 896/3560 Training loss: 1.8257 0.7878 sec/batch\n",
      "Epoch 6/20  Iteration 897/3560 Training loss: 1.8261 0.8157 sec/batch\n",
      "Epoch 6/20  Iteration 898/3560 Training loss: 1.8244 0.7845 sec/batch\n",
      "Epoch 6/20  Iteration 899/3560 Training loss: 1.8257 0.7657 sec/batch\n",
      "Epoch 6/20  Iteration 900/3560 Training loss: 1.8248 0.7919 sec/batch\n",
      "Epoch 6/20  Iteration 901/3560 Training loss: 1.8202 0.8051 sec/batch\n",
      "Epoch 6/20  Iteration 902/3560 Training loss: 1.8189 0.7825 sec/batch\n",
      "Epoch 6/20  Iteration 903/3560 Training loss: 1.8182 0.7848 sec/batch\n",
      "Epoch 6/20  Iteration 904/3560 Training loss: 1.8206 0.8038 sec/batch\n",
      "Epoch 6/20  Iteration 905/3560 Training loss: 1.8188 0.7985 sec/batch\n",
      "Epoch 6/20  Iteration 906/3560 Training loss: 1.8162 0.8554 sec/batch\n",
      "Epoch 6/20  Iteration 907/3560 Training loss: 1.8163 0.8369 sec/batch\n",
      "Epoch 6/20  Iteration 908/3560 Training loss: 1.8177 0.7969 sec/batch\n",
      "Epoch 6/20  Iteration 909/3560 Training loss: 1.8173 0.7765 sec/batch\n",
      "Epoch 6/20  Iteration 910/3560 Training loss: 1.8176 0.8435 sec/batch\n",
      "Epoch 6/20  Iteration 911/3560 Training loss: 1.8165 0.7948 sec/batch\n",
      "Epoch 6/20  Iteration 912/3560 Training loss: 1.8179 0.8024 sec/batch\n",
      "Epoch 6/20  Iteration 913/3560 Training loss: 1.8168 0.7724 sec/batch\n",
      "Epoch 6/20  Iteration 914/3560 Training loss: 1.8163 0.7913 sec/batch\n",
      "Epoch 6/20  Iteration 915/3560 Training loss: 1.8155 0.8026 sec/batch\n",
      "Epoch 6/20  Iteration 916/3560 Training loss: 1.8140 0.7876 sec/batch\n",
      "Epoch 6/20  Iteration 917/3560 Training loss: 1.8128 0.8000 sec/batch\n",
      "Epoch 6/20  Iteration 918/3560 Training loss: 1.8127 0.7955 sec/batch\n",
      "Epoch 6/20  Iteration 919/3560 Training loss: 1.8136 0.7703 sec/batch\n",
      "Epoch 6/20  Iteration 920/3560 Training loss: 1.8135 0.7631 sec/batch\n",
      "Epoch 6/20  Iteration 921/3560 Training loss: 1.8133 0.8078 sec/batch\n",
      "Epoch 6/20  Iteration 922/3560 Training loss: 1.8126 0.8659 sec/batch\n",
      "Epoch 6/20  Iteration 923/3560 Training loss: 1.8128 0.8290 sec/batch\n",
      "Epoch 6/20  Iteration 924/3560 Training loss: 1.8132 0.7953 sec/batch\n",
      "Epoch 6/20  Iteration 925/3560 Training loss: 1.8131 0.7750 sec/batch\n",
      "Epoch 6/20  Iteration 926/3560 Training loss: 1.8128 0.7647 sec/batch\n",
      "Epoch 6/20  Iteration 927/3560 Training loss: 1.8121 0.7827 sec/batch\n",
      "Epoch 6/20  Iteration 928/3560 Training loss: 1.8107 0.7623 sec/batch\n",
      "Epoch 6/20  Iteration 929/3560 Training loss: 1.8094 0.7634 sec/batch\n",
      "Epoch 6/20  Iteration 930/3560 Training loss: 1.8084 0.7670 sec/batch\n",
      "Epoch 6/20  Iteration 931/3560 Training loss: 1.8076 0.7698 sec/batch\n",
      "Epoch 6/20  Iteration 932/3560 Training loss: 1.8076 0.7692 sec/batch\n",
      "Epoch 6/20  Iteration 933/3560 Training loss: 1.8070 0.7872 sec/batch\n",
      "Epoch 6/20  Iteration 934/3560 Training loss: 1.8062 0.7643 sec/batch\n",
      "Epoch 6/20  Iteration 935/3560 Training loss: 1.8064 0.8165 sec/batch\n",
      "Epoch 6/20  Iteration 936/3560 Training loss: 1.8049 0.7995 sec/batch\n",
      "Epoch 6/20  Iteration 937/3560 Training loss: 1.8046 0.9013 sec/batch\n",
      "Epoch 6/20  Iteration 938/3560 Training loss: 1.8039 0.8022 sec/batch\n",
      "Epoch 6/20  Iteration 939/3560 Training loss: 1.8035 0.7781 sec/batch\n",
      "Epoch 6/20  Iteration 940/3560 Training loss: 1.8040 0.7950 sec/batch\n",
      "Epoch 6/20  Iteration 941/3560 Training loss: 1.8034 0.7746 sec/batch\n",
      "Epoch 6/20  Iteration 942/3560 Training loss: 1.8042 0.7882 sec/batch\n",
      "Epoch 6/20  Iteration 943/3560 Training loss: 1.8038 0.7913 sec/batch\n",
      "Epoch 6/20  Iteration 944/3560 Training loss: 1.8035 0.7945 sec/batch\n",
      "Epoch 6/20  Iteration 945/3560 Training loss: 1.8029 0.7721 sec/batch\n",
      "Epoch 6/20  Iteration 946/3560 Training loss: 1.8028 0.7835 sec/batch\n",
      "Epoch 6/20  Iteration 947/3560 Training loss: 1.8030 0.7936 sec/batch\n",
      "Epoch 6/20  Iteration 948/3560 Training loss: 1.8024 0.8197 sec/batch\n",
      "Epoch 6/20  Iteration 949/3560 Training loss: 1.8019 0.7587 sec/batch\n",
      "Epoch 6/20  Iteration 950/3560 Training loss: 1.8024 0.7686 sec/batch\n",
      "Epoch 6/20  Iteration 951/3560 Training loss: 1.8022 0.7739 sec/batch\n",
      "Epoch 6/20  Iteration 952/3560 Training loss: 1.8028 0.9552 sec/batch\n",
      "Epoch 6/20  Iteration 953/3560 Training loss: 1.8030 0.7761 sec/batch\n",
      "Epoch 6/20  Iteration 954/3560 Training loss: 1.8031 0.8484 sec/batch\n",
      "Epoch 6/20  Iteration 955/3560 Training loss: 1.8027 0.7731 sec/batch\n",
      "Epoch 6/20  Iteration 956/3560 Training loss: 1.8026 0.7602 sec/batch\n",
      "Epoch 6/20  Iteration 957/3560 Training loss: 1.8026 0.7645 sec/batch\n",
      "Epoch 6/20  Iteration 958/3560 Training loss: 1.8023 0.7682 sec/batch\n",
      "Epoch 6/20  Iteration 959/3560 Training loss: 1.8021 0.7592 sec/batch\n",
      "Epoch 6/20  Iteration 960/3560 Training loss: 1.8017 0.7634 sec/batch\n",
      "Epoch 6/20  Iteration 961/3560 Training loss: 1.8021 0.8131 sec/batch\n",
      "Epoch 6/20  Iteration 962/3560 Training loss: 1.8021 0.7887 sec/batch\n",
      "Epoch 6/20  Iteration 963/3560 Training loss: 1.8022 0.7827 sec/batch\n",
      "Epoch 6/20  Iteration 964/3560 Training loss: 1.8018 0.7869 sec/batch\n",
      "Epoch 6/20  Iteration 965/3560 Training loss: 1.8016 0.7624 sec/batch\n",
      "Epoch 6/20  Iteration 966/3560 Training loss: 1.8017 0.7640 sec/batch\n",
      "Epoch 6/20  Iteration 967/3560 Training loss: 1.8015 0.8373 sec/batch\n",
      "Epoch 6/20  Iteration 968/3560 Training loss: 1.8014 0.7663 sec/batch\n",
      "Epoch 6/20  Iteration 969/3560 Training loss: 1.8008 0.7658 sec/batch\n",
      "Epoch 6/20  Iteration 970/3560 Training loss: 1.8005 0.7741 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 971/3560 Training loss: 1.7998 0.7951 sec/batch\n",
      "Epoch 6/20  Iteration 972/3560 Training loss: 1.7999 0.7879 sec/batch\n",
      "Epoch 6/20  Iteration 973/3560 Training loss: 1.7993 0.7959 sec/batch\n",
      "Epoch 6/20  Iteration 974/3560 Training loss: 1.7991 0.8151 sec/batch\n",
      "Epoch 6/20  Iteration 975/3560 Training loss: 1.7987 0.7569 sec/batch\n",
      "Epoch 6/20  Iteration 976/3560 Training loss: 1.7982 0.7966 sec/batch\n",
      "Epoch 6/20  Iteration 977/3560 Training loss: 1.7980 0.8167 sec/batch\n",
      "Epoch 6/20  Iteration 978/3560 Training loss: 1.7975 0.7781 sec/batch\n",
      "Epoch 6/20  Iteration 979/3560 Training loss: 1.7969 0.7598 sec/batch\n",
      "Epoch 6/20  Iteration 980/3560 Training loss: 1.7970 0.7534 sec/batch\n",
      "Epoch 6/20  Iteration 981/3560 Training loss: 1.7965 0.7868 sec/batch\n",
      "Epoch 6/20  Iteration 982/3560 Training loss: 1.7963 0.8723 sec/batch\n",
      "Epoch 6/20  Iteration 983/3560 Training loss: 1.7958 0.7857 sec/batch\n",
      "Epoch 6/20  Iteration 984/3560 Training loss: 1.7953 0.7939 sec/batch\n",
      "Epoch 6/20  Iteration 985/3560 Training loss: 1.7948 0.8071 sec/batch\n",
      "Epoch 6/20  Iteration 986/3560 Training loss: 1.7947 0.8248 sec/batch\n",
      "Epoch 6/20  Iteration 987/3560 Training loss: 1.7945 0.8206 sec/batch\n",
      "Epoch 6/20  Iteration 988/3560 Training loss: 1.7939 0.7781 sec/batch\n",
      "Epoch 6/20  Iteration 989/3560 Training loss: 1.7935 0.7776 sec/batch\n",
      "Epoch 6/20  Iteration 990/3560 Training loss: 1.7929 0.7906 sec/batch\n",
      "Epoch 6/20  Iteration 991/3560 Training loss: 1.7927 0.8021 sec/batch\n",
      "Epoch 6/20  Iteration 992/3560 Training loss: 1.7925 0.7559 sec/batch\n",
      "Epoch 6/20  Iteration 993/3560 Training loss: 1.7921 0.7685 sec/batch\n",
      "Epoch 6/20  Iteration 994/3560 Training loss: 1.7919 0.7665 sec/batch\n",
      "Epoch 6/20  Iteration 995/3560 Training loss: 1.7916 0.8008 sec/batch\n",
      "Epoch 6/20  Iteration 996/3560 Training loss: 1.7913 0.7772 sec/batch\n",
      "Epoch 6/20  Iteration 997/3560 Training loss: 1.7911 0.8249 sec/batch\n",
      "Epoch 6/20  Iteration 998/3560 Training loss: 1.7909 0.8048 sec/batch\n",
      "Epoch 6/20  Iteration 999/3560 Training loss: 1.7908 0.8140 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3560 Training loss: 1.7908 0.7882 sec/batch\n",
      "Epoch 6/20  Iteration 1001/3560 Training loss: 1.7906 0.7782 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3560 Training loss: 1.7903 0.7979 sec/batch\n",
      "Epoch 6/20  Iteration 1003/3560 Training loss: 1.7900 0.7640 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3560 Training loss: 1.7897 0.7843 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3560 Training loss: 1.7893 0.7534 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3560 Training loss: 1.7888 0.7666 sec/batch\n",
      "Epoch 6/20  Iteration 1007/3560 Training loss: 1.7886 0.7729 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3560 Training loss: 1.7884 0.7972 sec/batch\n",
      "Epoch 6/20  Iteration 1009/3560 Training loss: 1.7882 0.8081 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3560 Training loss: 1.7880 0.8003 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3560 Training loss: 1.7879 0.8030 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3560 Training loss: 1.7875 0.8990 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3560 Training loss: 1.7871 0.8022 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3560 Training loss: 1.7871 0.7818 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3560 Training loss: 1.7869 0.7861 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3560 Training loss: 1.7864 0.7948 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3560 Training loss: 1.7864 0.8086 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3560 Training loss: 1.7863 0.8050 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3560 Training loss: 1.7862 0.8110 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3560 Training loss: 1.7860 0.7624 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3560 Training loss: 1.7857 0.7641 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3560 Training loss: 1.7853 0.7869 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3560 Training loss: 1.7852 0.8028 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3560 Training loss: 1.7851 0.8048 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3560 Training loss: 1.7850 0.8080 sec/batch\n",
      "Epoch 6/20  Iteration 1026/3560 Training loss: 1.7850 0.7879 sec/batch\n",
      "Epoch 6/20  Iteration 1027/3560 Training loss: 1.7850 0.8704 sec/batch\n",
      "Epoch 6/20  Iteration 1028/3560 Training loss: 1.7849 0.8783 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3560 Training loss: 1.7848 0.8043 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3560 Training loss: 1.7846 0.8244 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3560 Training loss: 1.7848 0.7748 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3560 Training loss: 1.7845 0.7976 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3560 Training loss: 1.7844 0.7644 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3560 Training loss: 1.7843 0.8055 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3560 Training loss: 1.7841 0.7615 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3560 Training loss: 1.7841 0.7801 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3560 Training loss: 1.7840 0.8160 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3560 Training loss: 1.7840 0.7652 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3560 Training loss: 1.7839 0.7941 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3560 Training loss: 1.7837 0.7904 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3560 Training loss: 1.7834 0.7874 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3560 Training loss: 1.7834 0.8491 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3560 Training loss: 1.7833 0.8504 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3560 Training loss: 1.7832 0.7881 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3560 Training loss: 1.7831 0.7948 sec/batch\n",
      "Epoch 6/20  Iteration 1046/3560 Training loss: 1.7828 0.7819 sec/batch\n",
      "Epoch 6/20  Iteration 1047/3560 Training loss: 1.7828 0.7765 sec/batch\n",
      "Epoch 6/20  Iteration 1048/3560 Training loss: 1.7826 0.8084 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3560 Training loss: 1.7822 0.7889 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3560 Training loss: 1.7822 0.8174 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3560 Training loss: 1.7823 0.7934 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3560 Training loss: 1.7821 0.8012 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3560 Training loss: 1.7821 0.7587 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3560 Training loss: 1.7820 0.7753 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3560 Training loss: 1.7818 0.8081 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3560 Training loss: 1.7817 0.7981 sec/batch\n",
      "Epoch 6/20  Iteration 1057/3560 Training loss: 1.7816 0.8228 sec/batch\n",
      "Epoch 6/20  Iteration 1058/3560 Training loss: 1.7818 0.8446 sec/batch\n",
      "Epoch 6/20  Iteration 1059/3560 Training loss: 1.7816 0.7592 sec/batch\n",
      "Epoch 6/20  Iteration 1060/3560 Training loss: 1.7814 0.7734 sec/batch\n",
      "Epoch 6/20  Iteration 1061/3560 Training loss: 1.7812 0.7721 sec/batch\n",
      "Epoch 6/20  Iteration 1062/3560 Training loss: 1.7809 0.8008 sec/batch\n",
      "Epoch 6/20  Iteration 1063/3560 Training loss: 1.7809 0.8691 sec/batch\n",
      "Epoch 6/20  Iteration 1064/3560 Training loss: 1.7808 0.7952 sec/batch\n",
      "Epoch 6/20  Iteration 1065/3560 Training loss: 1.7808 0.7813 sec/batch\n",
      "Epoch 6/20  Iteration 1066/3560 Training loss: 1.7806 0.7628 sec/batch\n",
      "Epoch 6/20  Iteration 1067/3560 Training loss: 1.7804 0.7821 sec/batch\n",
      "Epoch 6/20  Iteration 1068/3560 Training loss: 1.7803 0.7714 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3560 Training loss: 1.8458 0.7651 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3560 Training loss: 1.7930 0.7987 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3560 Training loss: 1.7756 0.7743 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3560 Training loss: 1.7697 0.7967 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3560 Training loss: 1.7649 0.8638 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3560 Training loss: 1.7543 0.7757 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3560 Training loss: 1.7546 0.8150 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3560 Training loss: 1.7526 0.8294 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3560 Training loss: 1.7541 0.8203 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3560 Training loss: 1.7522 1.0047 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3560 Training loss: 1.7486 1.0382 sec/batch\n",
      "Epoch 7/20  Iteration 1080/3560 Training loss: 1.7480 0.9763 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3560 Training loss: 1.7477 1.0325 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3560 Training loss: 1.7502 1.0072 sec/batch\n",
      "Epoch 7/20  Iteration 1083/3560 Training loss: 1.7499 1.0385 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3560 Training loss: 1.7479 1.0385 sec/batch\n",
      "Epoch 7/20  Iteration 1085/3560 Training loss: 1.7480 1.1063 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3560 Training loss: 1.7495 0.9516 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1087/3560 Training loss: 1.7495 0.8013 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3560 Training loss: 1.7501 0.7741 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3560 Training loss: 1.7492 0.7711 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3560 Training loss: 1.7504 0.7939 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3560 Training loss: 1.7498 0.8060 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3560 Training loss: 1.7491 0.7799 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3560 Training loss: 1.7485 0.7789 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3560 Training loss: 1.7471 0.7988 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3560 Training loss: 1.7458 0.8107 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3560 Training loss: 1.7463 0.7975 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3560 Training loss: 1.7469 0.7611 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3560 Training loss: 1.7469 0.7925 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3560 Training loss: 1.7468 0.8185 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3560 Training loss: 1.7460 0.8875 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3560 Training loss: 1.7460 0.8144 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3560 Training loss: 1.7462 0.8021 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3560 Training loss: 1.7456 0.7938 sec/batch\n",
      "Epoch 7/20  Iteration 1104/3560 Training loss: 1.7455 0.8133 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3560 Training loss: 1.7446 0.7698 sec/batch\n",
      "Epoch 7/20  Iteration 1106/3560 Training loss: 1.7439 0.8043 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3560 Training loss: 1.7424 0.7877 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3560 Training loss: 1.7418 0.7970 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3560 Training loss: 1.7413 0.7695 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3560 Training loss: 1.7414 0.7885 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3560 Training loss: 1.7411 0.8423 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3560 Training loss: 1.7403 0.7985 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3560 Training loss: 1.7404 0.7916 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3560 Training loss: 1.7391 0.7936 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3560 Training loss: 1.7388 0.8890 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3560 Training loss: 1.7380 0.7949 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3560 Training loss: 1.7377 0.8026 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3560 Training loss: 1.7382 0.7604 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3560 Training loss: 1.7376 0.7750 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3560 Training loss: 1.7384 0.7753 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3560 Training loss: 1.7381 0.7899 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3560 Training loss: 1.7380 0.7763 sec/batch\n",
      "Epoch 7/20  Iteration 1123/3560 Training loss: 1.7377 0.8052 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3560 Training loss: 1.7376 0.8210 sec/batch\n",
      "Epoch 7/20  Iteration 1125/3560 Training loss: 1.7379 0.7633 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3560 Training loss: 1.7376 0.7529 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3560 Training loss: 1.7370 0.7632 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3560 Training loss: 1.7373 0.7848 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3560 Training loss: 1.7371 0.7693 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3560 Training loss: 1.7380 0.8668 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3560 Training loss: 1.7382 0.8292 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3560 Training loss: 1.7384 0.7564 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3560 Training loss: 1.7383 0.7953 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3560 Training loss: 1.7385 0.7852 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3560 Training loss: 1.7386 0.7928 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3560 Training loss: 1.7381 0.8772 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3560 Training loss: 1.7380 0.8329 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3560 Training loss: 1.7377 0.7694 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3560 Training loss: 1.7381 0.8104 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3560 Training loss: 1.7382 0.7899 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3560 Training loss: 1.7385 0.7834 sec/batch\n",
      "Epoch 7/20  Iteration 1142/3560 Training loss: 1.7382 0.8038 sec/batch\n",
      "Epoch 7/20  Iteration 1143/3560 Training loss: 1.7381 0.7780 sec/batch\n",
      "Epoch 7/20  Iteration 1144/3560 Training loss: 1.7382 0.7529 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3560 Training loss: 1.7380 0.8401 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3560 Training loss: 1.7380 0.7878 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3560 Training loss: 1.7374 0.7910 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3560 Training loss: 1.7372 0.7633 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3560 Training loss: 1.7367 0.8245 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3560 Training loss: 1.7368 0.7649 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3560 Training loss: 1.7363 0.7955 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3560 Training loss: 1.7360 0.7589 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3560 Training loss: 1.7356 0.7911 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3560 Training loss: 1.7353 0.7651 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3560 Training loss: 1.7349 0.7815 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3560 Training loss: 1.7345 0.7983 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3560 Training loss: 1.7338 0.7934 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3560 Training loss: 1.7339 0.7538 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3560 Training loss: 1.7335 0.8001 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3560 Training loss: 1.7333 0.8226 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3560 Training loss: 1.7327 0.8200 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3560 Training loss: 1.7322 0.8092 sec/batch\n",
      "Epoch 7/20  Iteration 1163/3560 Training loss: 1.7319 0.7983 sec/batch\n",
      "Epoch 7/20  Iteration 1164/3560 Training loss: 1.7318 0.7916 sec/batch\n",
      "Epoch 7/20  Iteration 1165/3560 Training loss: 1.7316 0.7690 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3560 Training loss: 1.7310 0.7679 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3560 Training loss: 1.7306 0.7772 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3560 Training loss: 1.7300 0.8351 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3560 Training loss: 1.7299 0.7594 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3560 Training loss: 1.7297 0.7940 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3560 Training loss: 1.7295 0.8038 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3560 Training loss: 1.7293 0.7931 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3560 Training loss: 1.7290 0.7976 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3560 Training loss: 1.7288 0.8051 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3560 Training loss: 1.7286 0.9866 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3560 Training loss: 1.7285 0.8447 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3560 Training loss: 1.7284 0.7902 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3560 Training loss: 1.7283 0.8196 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3560 Training loss: 1.7281 0.7937 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3560 Training loss: 1.7279 0.8017 sec/batch\n",
      "Epoch 7/20  Iteration 1181/3560 Training loss: 1.7277 0.7915 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3560 Training loss: 1.7276 0.7857 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3560 Training loss: 1.7273 0.7635 sec/batch\n",
      "Epoch 7/20  Iteration 1184/3560 Training loss: 1.7268 0.8131 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3560 Training loss: 1.7268 0.7577 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3560 Training loss: 1.7266 0.8100 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3560 Training loss: 1.7264 0.8236 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3560 Training loss: 1.7262 0.7759 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3560 Training loss: 1.7262 0.7818 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3560 Training loss: 1.7257 0.8465 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3560 Training loss: 1.7254 0.7846 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3560 Training loss: 1.7254 0.7860 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3560 Training loss: 1.7253 0.7822 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3560 Training loss: 1.7248 0.7850 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3560 Training loss: 1.7248 0.7524 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3560 Training loss: 1.7248 0.7794 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3560 Training loss: 1.7246 0.7905 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3560 Training loss: 1.7244 0.7824 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3560 Training loss: 1.7240 0.8104 sec/batch\n",
      "Epoch 7/20  Iteration 1200/3560 Training loss: 1.7237 0.8222 sec/batch\n",
      "Epoch 7/20  Iteration 1201/3560 Training loss: 1.7237 0.7528 sec/batch\n",
      "Epoch 7/20  Iteration 1202/3560 Training loss: 1.7235 0.7978 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1203/3560 Training loss: 1.7235 0.7959 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3560 Training loss: 1.7234 0.8481 sec/batch\n",
      "Epoch 7/20  Iteration 1205/3560 Training loss: 1.7234 0.8430 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3560 Training loss: 1.7233 0.7655 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3560 Training loss: 1.7233 0.7604 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3560 Training loss: 1.7231 0.7900 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3560 Training loss: 1.7233 0.7674 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3560 Training loss: 1.7232 0.8054 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3560 Training loss: 1.7230 0.7886 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3560 Training loss: 1.7231 0.8271 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3560 Training loss: 1.7228 0.8194 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3560 Training loss: 1.7229 0.7674 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3560 Training loss: 1.7229 0.7663 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3560 Training loss: 1.7230 0.7887 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3560 Training loss: 1.7229 0.7923 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3560 Training loss: 1.7227 0.8839 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3560 Training loss: 1.7223 0.7814 sec/batch\n",
      "Epoch 7/20  Iteration 1220/3560 Training loss: 1.7223 0.7587 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3560 Training loss: 1.7223 0.7929 sec/batch\n",
      "Epoch 7/20  Iteration 1222/3560 Training loss: 1.7222 0.7838 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3560 Training loss: 1.7221 0.8034 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3560 Training loss: 1.7220 0.7669 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3560 Training loss: 1.7219 0.8346 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3560 Training loss: 1.7218 0.7741 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3560 Training loss: 1.7214 0.7918 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3560 Training loss: 1.7214 0.8162 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3560 Training loss: 1.7215 0.7952 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3560 Training loss: 1.7213 0.7684 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3560 Training loss: 1.7214 0.8037 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3560 Training loss: 1.7212 0.7931 sec/batch\n",
      "Epoch 7/20  Iteration 1233/3560 Training loss: 1.7211 0.7740 sec/batch\n",
      "Epoch 7/20  Iteration 1234/3560 Training loss: 1.7209 0.8143 sec/batch\n",
      "Epoch 7/20  Iteration 1235/3560 Training loss: 1.7210 0.8019 sec/batch\n",
      "Epoch 7/20  Iteration 1236/3560 Training loss: 1.7213 0.7649 sec/batch\n",
      "Epoch 7/20  Iteration 1237/3560 Training loss: 1.7211 0.8259 sec/batch\n",
      "Epoch 7/20  Iteration 1238/3560 Training loss: 1.7210 0.8265 sec/batch\n",
      "Epoch 7/20  Iteration 1239/3560 Training loss: 1.7208 0.7865 sec/batch\n",
      "Epoch 7/20  Iteration 1240/3560 Training loss: 1.7206 0.8257 sec/batch\n",
      "Epoch 7/20  Iteration 1241/3560 Training loss: 1.7206 0.8358 sec/batch\n",
      "Epoch 7/20  Iteration 1242/3560 Training loss: 1.7206 0.8020 sec/batch\n",
      "Epoch 7/20  Iteration 1243/3560 Training loss: 1.7206 0.7802 sec/batch\n",
      "Epoch 7/20  Iteration 1244/3560 Training loss: 1.7204 0.7959 sec/batch\n",
      "Epoch 7/20  Iteration 1245/3560 Training loss: 1.7202 0.7743 sec/batch\n",
      "Epoch 7/20  Iteration 1246/3560 Training loss: 1.7201 0.7743 sec/batch\n",
      "Epoch 8/20  Iteration 1247/3560 Training loss: 1.7997 0.7664 sec/batch\n",
      "Epoch 8/20  Iteration 1248/3560 Training loss: 1.7470 0.8048 sec/batch\n",
      "Epoch 8/20  Iteration 1249/3560 Training loss: 1.7300 0.7923 sec/batch\n",
      "Epoch 8/20  Iteration 1250/3560 Training loss: 1.7240 0.8172 sec/batch\n",
      "Epoch 8/20  Iteration 1251/3560 Training loss: 1.7172 0.8225 sec/batch\n",
      "Epoch 8/20  Iteration 1252/3560 Training loss: 1.7058 0.7720 sec/batch\n",
      "Epoch 8/20  Iteration 1253/3560 Training loss: 1.7054 0.8079 sec/batch\n",
      "Epoch 8/20  Iteration 1254/3560 Training loss: 1.7036 0.7996 sec/batch\n",
      "Epoch 8/20  Iteration 1255/3560 Training loss: 1.7036 0.8571 sec/batch\n",
      "Epoch 8/20  Iteration 1256/3560 Training loss: 1.7017 0.7764 sec/batch\n",
      "Epoch 8/20  Iteration 1257/3560 Training loss: 1.6973 0.7885 sec/batch\n",
      "Epoch 8/20  Iteration 1258/3560 Training loss: 1.6964 0.7880 sec/batch\n",
      "Epoch 8/20  Iteration 1259/3560 Training loss: 1.6952 0.7595 sec/batch\n",
      "Epoch 8/20  Iteration 1260/3560 Training loss: 1.6978 0.8020 sec/batch\n",
      "Epoch 8/20  Iteration 1261/3560 Training loss: 1.6967 0.7560 sec/batch\n",
      "Epoch 8/20  Iteration 1262/3560 Training loss: 1.6947 0.7702 sec/batch\n",
      "Epoch 8/20  Iteration 1263/3560 Training loss: 1.6944 0.8360 sec/batch\n",
      "Epoch 8/20  Iteration 1264/3560 Training loss: 1.6958 0.8145 sec/batch\n",
      "Epoch 8/20  Iteration 1265/3560 Training loss: 1.6966 0.7645 sec/batch\n",
      "Epoch 8/20  Iteration 1266/3560 Training loss: 1.6974 0.7881 sec/batch\n",
      "Epoch 8/20  Iteration 1267/3560 Training loss: 1.6965 0.7933 sec/batch\n",
      "Epoch 8/20  Iteration 1268/3560 Training loss: 1.6975 0.7677 sec/batch\n",
      "Epoch 8/20  Iteration 1269/3560 Training loss: 1.6970 0.7871 sec/batch\n",
      "Epoch 8/20  Iteration 1270/3560 Training loss: 1.6966 0.8654 sec/batch\n",
      "Epoch 8/20  Iteration 1271/3560 Training loss: 1.6960 0.7934 sec/batch\n",
      "Epoch 8/20  Iteration 1272/3560 Training loss: 1.6947 0.7765 sec/batch\n",
      "Epoch 8/20  Iteration 1273/3560 Training loss: 1.6935 0.8374 sec/batch\n",
      "Epoch 8/20  Iteration 1274/3560 Training loss: 1.6935 0.7888 sec/batch\n",
      "Epoch 8/20  Iteration 1275/3560 Training loss: 1.6943 0.7781 sec/batch\n",
      "Epoch 8/20  Iteration 1276/3560 Training loss: 1.6947 0.8418 sec/batch\n",
      "Epoch 8/20  Iteration 1277/3560 Training loss: 1.6942 0.7964 sec/batch\n",
      "Epoch 8/20  Iteration 1278/3560 Training loss: 1.6932 0.7769 sec/batch\n",
      "Epoch 8/20  Iteration 1279/3560 Training loss: 1.6936 0.7920 sec/batch\n",
      "Epoch 8/20  Iteration 1280/3560 Training loss: 1.6945 0.8113 sec/batch\n",
      "Epoch 8/20  Iteration 1281/3560 Training loss: 1.6941 0.8166 sec/batch\n",
      "Epoch 8/20  Iteration 1282/3560 Training loss: 1.6940 0.8095 sec/batch\n",
      "Epoch 8/20  Iteration 1283/3560 Training loss: 1.6933 0.7858 sec/batch\n",
      "Epoch 8/20  Iteration 1284/3560 Training loss: 1.6922 0.7846 sec/batch\n",
      "Epoch 8/20  Iteration 1285/3560 Training loss: 1.6910 0.8317 sec/batch\n",
      "Epoch 8/20  Iteration 1286/3560 Training loss: 1.6903 0.8098 sec/batch\n",
      "Epoch 8/20  Iteration 1287/3560 Training loss: 1.6897 0.7787 sec/batch\n",
      "Epoch 8/20  Iteration 1288/3560 Training loss: 1.6901 0.7787 sec/batch\n",
      "Epoch 8/20  Iteration 1289/3560 Training loss: 1.6900 0.8428 sec/batch\n",
      "Epoch 8/20  Iteration 1290/3560 Training loss: 1.6890 0.7653 sec/batch\n",
      "Epoch 8/20  Iteration 1291/3560 Training loss: 1.6891 0.7880 sec/batch\n",
      "Epoch 8/20  Iteration 1292/3560 Training loss: 1.6881 0.7974 sec/batch\n",
      "Epoch 8/20  Iteration 1293/3560 Training loss: 1.6880 0.7605 sec/batch\n",
      "Epoch 8/20  Iteration 1294/3560 Training loss: 1.6874 0.8076 sec/batch\n",
      "Epoch 8/20  Iteration 1295/3560 Training loss: 1.6872 0.7888 sec/batch\n",
      "Epoch 8/20  Iteration 1296/3560 Training loss: 1.6878 0.7904 sec/batch\n",
      "Epoch 8/20  Iteration 1297/3560 Training loss: 1.6874 0.8278 sec/batch\n",
      "Epoch 8/20  Iteration 1298/3560 Training loss: 1.6882 0.8310 sec/batch\n",
      "Epoch 8/20  Iteration 1299/3560 Training loss: 1.6880 0.8090 sec/batch\n",
      "Epoch 8/20  Iteration 1300/3560 Training loss: 1.6880 0.8049 sec/batch\n",
      "Epoch 8/20  Iteration 1301/3560 Training loss: 1.6878 1.0294 sec/batch\n",
      "Epoch 8/20  Iteration 1302/3560 Training loss: 1.6878 1.0391 sec/batch\n",
      "Epoch 8/20  Iteration 1303/3560 Training loss: 1.6882 1.0528 sec/batch\n",
      "Epoch 8/20  Iteration 1304/3560 Training loss: 1.6880 1.0570 sec/batch\n",
      "Epoch 8/20  Iteration 1305/3560 Training loss: 1.6876 1.0101 sec/batch\n",
      "Epoch 8/20  Iteration 1306/3560 Training loss: 1.6880 1.0101 sec/batch\n",
      "Epoch 8/20  Iteration 1307/3560 Training loss: 1.6878 1.0177 sec/batch\n",
      "Epoch 8/20  Iteration 1308/3560 Training loss: 1.6886 1.0525 sec/batch\n",
      "Epoch 8/20  Iteration 1309/3560 Training loss: 1.6889 0.7992 sec/batch\n",
      "Epoch 8/20  Iteration 1310/3560 Training loss: 1.6889 0.8607 sec/batch\n",
      "Epoch 8/20  Iteration 1311/3560 Training loss: 1.6887 0.7932 sec/batch\n",
      "Epoch 8/20  Iteration 1312/3560 Training loss: 1.6889 0.8117 sec/batch\n",
      "Epoch 8/20  Iteration 1313/3560 Training loss: 1.6892 0.7811 sec/batch\n",
      "Epoch 8/20  Iteration 1314/3560 Training loss: 1.6888 0.7656 sec/batch\n",
      "Epoch 8/20  Iteration 1315/3560 Training loss: 1.6886 0.7730 sec/batch\n",
      "Epoch 8/20  Iteration 1316/3560 Training loss: 1.6886 0.7742 sec/batch\n",
      "Epoch 8/20  Iteration 1317/3560 Training loss: 1.6891 0.7720 sec/batch\n",
      "Epoch 8/20  Iteration 1318/3560 Training loss: 1.6892 0.7875 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1319/3560 Training loss: 1.6895 0.7622 sec/batch\n",
      "Epoch 8/20  Iteration 1320/3560 Training loss: 1.6891 0.7717 sec/batch\n",
      "Epoch 8/20  Iteration 1321/3560 Training loss: 1.6889 0.7466 sec/batch\n",
      "Epoch 8/20  Iteration 1322/3560 Training loss: 1.6892 0.8107 sec/batch\n",
      "Epoch 8/20  Iteration 1323/3560 Training loss: 1.6890 0.8067 sec/batch\n",
      "Epoch 8/20  Iteration 1324/3560 Training loss: 1.6890 0.7925 sec/batch\n",
      "Epoch 8/20  Iteration 1325/3560 Training loss: 1.6885 0.8864 sec/batch\n",
      "Epoch 8/20  Iteration 1326/3560 Training loss: 1.6883 0.8189 sec/batch\n",
      "Epoch 8/20  Iteration 1327/3560 Training loss: 1.6876 0.8054 sec/batch\n",
      "Epoch 8/20  Iteration 1328/3560 Training loss: 1.6877 0.8187 sec/batch\n",
      "Epoch 8/20  Iteration 1329/3560 Training loss: 1.6871 0.7942 sec/batch\n",
      "Epoch 8/20  Iteration 1330/3560 Training loss: 1.6870 0.7719 sec/batch\n",
      "Epoch 8/20  Iteration 1331/3560 Training loss: 1.6865 0.8020 sec/batch\n",
      "Epoch 8/20  Iteration 1332/3560 Training loss: 1.6861 0.7737 sec/batch\n",
      "Epoch 8/20  Iteration 1333/3560 Training loss: 1.6858 0.7921 sec/batch\n",
      "Epoch 8/20  Iteration 1334/3560 Training loss: 1.6854 0.7956 sec/batch\n",
      "Epoch 8/20  Iteration 1335/3560 Training loss: 1.6848 0.8031 sec/batch\n",
      "Epoch 8/20  Iteration 1336/3560 Training loss: 1.6849 0.7873 sec/batch\n",
      "Epoch 8/20  Iteration 1337/3560 Training loss: 1.6844 0.8135 sec/batch\n",
      "Epoch 8/20  Iteration 1338/3560 Training loss: 1.6842 0.7978 sec/batch\n",
      "Epoch 8/20  Iteration 1339/3560 Training loss: 1.6836 0.7603 sec/batch\n",
      "Epoch 8/20  Iteration 1340/3560 Training loss: 1.6833 0.8702 sec/batch\n",
      "Epoch 8/20  Iteration 1341/3560 Training loss: 1.6829 0.7898 sec/batch\n",
      "Epoch 8/20  Iteration 1342/3560 Training loss: 1.6827 0.7929 sec/batch\n",
      "Epoch 8/20  Iteration 1343/3560 Training loss: 1.6826 0.7726 sec/batch\n",
      "Epoch 8/20  Iteration 1344/3560 Training loss: 1.6821 0.7689 sec/batch\n",
      "Epoch 8/20  Iteration 1345/3560 Training loss: 1.6817 0.8035 sec/batch\n",
      "Epoch 8/20  Iteration 1346/3560 Training loss: 1.6812 0.7776 sec/batch\n",
      "Epoch 8/20  Iteration 1347/3560 Training loss: 1.6812 0.7747 sec/batch\n",
      "Epoch 8/20  Iteration 1348/3560 Training loss: 1.6808 0.7955 sec/batch\n",
      "Epoch 8/20  Iteration 1349/3560 Training loss: 1.6806 0.7565 sec/batch\n",
      "Epoch 8/20  Iteration 1350/3560 Training loss: 1.6803 0.8255 sec/batch\n",
      "Epoch 8/20  Iteration 1351/3560 Training loss: 1.6801 0.7578 sec/batch\n",
      "Epoch 8/20  Iteration 1352/3560 Training loss: 1.6800 0.7615 sec/batch\n",
      "Epoch 8/20  Iteration 1353/3560 Training loss: 1.6798 0.7832 sec/batch\n",
      "Epoch 8/20  Iteration 1354/3560 Training loss: 1.6797 0.7698 sec/batch\n",
      "Epoch 8/20  Iteration 1355/3560 Training loss: 1.6796 0.8004 sec/batch\n",
      "Epoch 8/20  Iteration 1356/3560 Training loss: 1.6795 0.8545 sec/batch\n",
      "Epoch 8/20  Iteration 1357/3560 Training loss: 1.6794 0.7836 sec/batch\n",
      "Epoch 8/20  Iteration 1358/3560 Training loss: 1.6792 0.7729 sec/batch\n",
      "Epoch 8/20  Iteration 1359/3560 Training loss: 1.6790 0.8053 sec/batch\n",
      "Epoch 8/20  Iteration 1360/3560 Training loss: 1.6786 0.7833 sec/batch\n",
      "Epoch 8/20  Iteration 1361/3560 Training loss: 1.6783 0.7941 sec/batch\n",
      "Epoch 8/20  Iteration 1362/3560 Training loss: 1.6779 0.7909 sec/batch\n",
      "Epoch 8/20  Iteration 1363/3560 Training loss: 1.6778 0.8429 sec/batch\n",
      "Epoch 8/20  Iteration 1364/3560 Training loss: 1.6777 0.7945 sec/batch\n",
      "Epoch 8/20  Iteration 1365/3560 Training loss: 1.6776 0.7882 sec/batch\n",
      "Epoch 8/20  Iteration 1366/3560 Training loss: 1.6775 0.7947 sec/batch\n",
      "Epoch 8/20  Iteration 1367/3560 Training loss: 1.6775 0.7645 sec/batch\n",
      "Epoch 8/20  Iteration 1368/3560 Training loss: 1.6771 0.7711 sec/batch\n",
      "Epoch 8/20  Iteration 1369/3560 Training loss: 1.6766 0.7929 sec/batch\n",
      "Epoch 8/20  Iteration 1370/3560 Training loss: 1.6767 0.7688 sec/batch\n",
      "Epoch 8/20  Iteration 1371/3560 Training loss: 1.6766 0.8823 sec/batch\n",
      "Epoch 8/20  Iteration 1372/3560 Training loss: 1.6761 0.7752 sec/batch\n",
      "Epoch 8/20  Iteration 1373/3560 Training loss: 1.6761 0.7968 sec/batch\n",
      "Epoch 8/20  Iteration 1374/3560 Training loss: 1.6761 0.7766 sec/batch\n",
      "Epoch 8/20  Iteration 1375/3560 Training loss: 1.6759 0.8543 sec/batch\n",
      "Epoch 8/20  Iteration 1376/3560 Training loss: 1.6757 0.7924 sec/batch\n",
      "Epoch 8/20  Iteration 1377/3560 Training loss: 1.6753 0.7849 sec/batch\n",
      "Epoch 8/20  Iteration 1378/3560 Training loss: 1.6750 0.7863 sec/batch\n",
      "Epoch 8/20  Iteration 1379/3560 Training loss: 1.6750 0.7971 sec/batch\n",
      "Epoch 8/20  Iteration 1380/3560 Training loss: 1.6748 0.7969 sec/batch\n",
      "Epoch 8/20  Iteration 1381/3560 Training loss: 1.6748 0.8156 sec/batch\n",
      "Epoch 8/20  Iteration 1382/3560 Training loss: 1.6748 0.7648 sec/batch\n",
      "Epoch 8/20  Iteration 1383/3560 Training loss: 1.6749 0.7799 sec/batch\n",
      "Epoch 8/20  Iteration 1384/3560 Training loss: 1.6749 0.7653 sec/batch\n",
      "Epoch 8/20  Iteration 1385/3560 Training loss: 1.6749 0.8240 sec/batch\n",
      "Epoch 8/20  Iteration 1386/3560 Training loss: 1.6748 0.8907 sec/batch\n",
      "Epoch 8/20  Iteration 1387/3560 Training loss: 1.6750 0.7785 sec/batch\n",
      "Epoch 8/20  Iteration 1388/3560 Training loss: 1.6749 0.8557 sec/batch\n",
      "Epoch 8/20  Iteration 1389/3560 Training loss: 1.6747 0.7749 sec/batch\n",
      "Epoch 8/20  Iteration 1390/3560 Training loss: 1.6747 0.8049 sec/batch\n",
      "Epoch 8/20  Iteration 1391/3560 Training loss: 1.6745 0.7678 sec/batch\n",
      "Epoch 8/20  Iteration 1392/3560 Training loss: 1.6745 0.7568 sec/batch\n",
      "Epoch 8/20  Iteration 1393/3560 Training loss: 1.6745 0.7959 sec/batch\n",
      "Epoch 8/20  Iteration 1394/3560 Training loss: 1.6747 0.8063 sec/batch\n",
      "Epoch 8/20  Iteration 1395/3560 Training loss: 1.6746 0.7894 sec/batch\n",
      "Epoch 8/20  Iteration 1396/3560 Training loss: 1.6744 0.8079 sec/batch\n",
      "Epoch 8/20  Iteration 1397/3560 Training loss: 1.6742 0.7872 sec/batch\n",
      "Epoch 8/20  Iteration 1398/3560 Training loss: 1.6742 0.7692 sec/batch\n",
      "Epoch 8/20  Iteration 1399/3560 Training loss: 1.6741 0.8728 sec/batch\n",
      "Epoch 8/20  Iteration 1400/3560 Training loss: 1.6741 0.8132 sec/batch\n",
      "Epoch 8/20  Iteration 1401/3560 Training loss: 1.6741 0.9465 sec/batch\n",
      "Epoch 8/20  Iteration 1402/3560 Training loss: 1.6740 0.7876 sec/batch\n",
      "Epoch 8/20  Iteration 1403/3560 Training loss: 1.6740 0.7921 sec/batch\n",
      "Epoch 8/20  Iteration 1404/3560 Training loss: 1.6739 0.7932 sec/batch\n",
      "Epoch 8/20  Iteration 1405/3560 Training loss: 1.6736 0.7886 sec/batch\n",
      "Epoch 8/20  Iteration 1406/3560 Training loss: 1.6736 0.7682 sec/batch\n",
      "Epoch 8/20  Iteration 1407/3560 Training loss: 1.6738 0.7730 sec/batch\n",
      "Epoch 8/20  Iteration 1408/3560 Training loss: 1.6737 0.7977 sec/batch\n",
      "Epoch 8/20  Iteration 1409/3560 Training loss: 1.6738 0.7873 sec/batch\n",
      "Epoch 8/20  Iteration 1410/3560 Training loss: 1.6737 0.7904 sec/batch\n",
      "Epoch 8/20  Iteration 1411/3560 Training loss: 1.6736 0.8112 sec/batch\n",
      "Epoch 8/20  Iteration 1412/3560 Training loss: 1.6734 0.7939 sec/batch\n",
      "Epoch 8/20  Iteration 1413/3560 Training loss: 1.6734 0.7986 sec/batch\n",
      "Epoch 8/20  Iteration 1414/3560 Training loss: 1.6738 0.8196 sec/batch\n",
      "Epoch 8/20  Iteration 1415/3560 Training loss: 1.6736 0.8384 sec/batch\n",
      "Epoch 8/20  Iteration 1416/3560 Training loss: 1.6736 0.8317 sec/batch\n",
      "Epoch 8/20  Iteration 1417/3560 Training loss: 1.6734 0.7795 sec/batch\n",
      "Epoch 8/20  Iteration 1418/3560 Training loss: 1.6732 0.8000 sec/batch\n",
      "Epoch 8/20  Iteration 1419/3560 Training loss: 1.6733 0.8052 sec/batch\n",
      "Epoch 8/20  Iteration 1420/3560 Training loss: 1.6733 0.7917 sec/batch\n",
      "Epoch 8/20  Iteration 1421/3560 Training loss: 1.6733 0.7924 sec/batch\n",
      "Epoch 8/20  Iteration 1422/3560 Training loss: 1.6731 0.7916 sec/batch\n",
      "Epoch 8/20  Iteration 1423/3560 Training loss: 1.6728 0.8111 sec/batch\n",
      "Epoch 8/20  Iteration 1424/3560 Training loss: 1.6728 0.7756 sec/batch\n",
      "Epoch 9/20  Iteration 1425/3560 Training loss: 1.7539 0.7602 sec/batch\n",
      "Epoch 9/20  Iteration 1426/3560 Training loss: 1.7064 0.8613 sec/batch\n",
      "Epoch 9/20  Iteration 1427/3560 Training loss: 1.6894 0.7970 sec/batch\n",
      "Epoch 9/20  Iteration 1428/3560 Training loss: 1.6815 0.7977 sec/batch\n",
      "Epoch 9/20  Iteration 1429/3560 Training loss: 1.6742 0.7650 sec/batch\n",
      "Epoch 9/20  Iteration 1430/3560 Training loss: 1.6648 0.8474 sec/batch\n",
      "Epoch 9/20  Iteration 1431/3560 Training loss: 1.6644 0.8238 sec/batch\n",
      "Epoch 9/20  Iteration 1432/3560 Training loss: 1.6611 0.7630 sec/batch\n",
      "Epoch 9/20  Iteration 1433/3560 Training loss: 1.6618 0.7682 sec/batch\n",
      "Epoch 9/20  Iteration 1434/3560 Training loss: 1.6611 0.7803 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1435/3560 Training loss: 1.6583 0.7649 sec/batch\n",
      "Epoch 9/20  Iteration 1436/3560 Training loss: 1.6564 0.7980 sec/batch\n",
      "Epoch 9/20  Iteration 1437/3560 Training loss: 1.6558 0.7640 sec/batch\n",
      "Epoch 9/20  Iteration 1438/3560 Training loss: 1.6578 0.7951 sec/batch\n",
      "Epoch 9/20  Iteration 1439/3560 Training loss: 1.6572 0.8661 sec/batch\n",
      "Epoch 9/20  Iteration 1440/3560 Training loss: 1.6546 0.7955 sec/batch\n",
      "Epoch 9/20  Iteration 1441/3560 Training loss: 1.6541 0.8027 sec/batch\n",
      "Epoch 9/20  Iteration 1442/3560 Training loss: 1.6561 0.7732 sec/batch\n",
      "Epoch 9/20  Iteration 1443/3560 Training loss: 1.6558 0.7488 sec/batch\n",
      "Epoch 9/20  Iteration 1444/3560 Training loss: 1.6566 0.7772 sec/batch\n",
      "Epoch 9/20  Iteration 1445/3560 Training loss: 1.6558 0.7961 sec/batch\n",
      "Epoch 9/20  Iteration 1446/3560 Training loss: 1.6566 0.8584 sec/batch\n",
      "Epoch 9/20  Iteration 1447/3560 Training loss: 1.6558 0.7817 sec/batch\n",
      "Epoch 9/20  Iteration 1448/3560 Training loss: 1.6553 0.8068 sec/batch\n",
      "Epoch 9/20  Iteration 1449/3560 Training loss: 1.6551 0.7750 sec/batch\n",
      "Epoch 9/20  Iteration 1450/3560 Training loss: 1.6537 0.7765 sec/batch\n",
      "Epoch 9/20  Iteration 1451/3560 Training loss: 1.6523 0.8411 sec/batch\n",
      "Epoch 9/20  Iteration 1452/3560 Training loss: 1.6530 0.7918 sec/batch\n",
      "Epoch 9/20  Iteration 1453/3560 Training loss: 1.6539 0.8023 sec/batch\n",
      "Epoch 9/20  Iteration 1454/3560 Training loss: 1.6541 0.7650 sec/batch\n",
      "Epoch 9/20  Iteration 1455/3560 Training loss: 1.6534 0.8017 sec/batch\n",
      "Epoch 9/20  Iteration 1456/3560 Training loss: 1.6525 0.7960 sec/batch\n",
      "Epoch 9/20  Iteration 1457/3560 Training loss: 1.6527 0.7991 sec/batch\n",
      "Epoch 9/20  Iteration 1458/3560 Training loss: 1.6534 0.7911 sec/batch\n",
      "Epoch 9/20  Iteration 1459/3560 Training loss: 1.6528 0.7597 sec/batch\n",
      "Epoch 9/20  Iteration 1460/3560 Training loss: 1.6528 0.8024 sec/batch\n",
      "Epoch 9/20  Iteration 1461/3560 Training loss: 1.6520 0.8998 sec/batch\n",
      "Epoch 9/20  Iteration 1462/3560 Training loss: 1.6509 0.7823 sec/batch\n",
      "Epoch 9/20  Iteration 1463/3560 Training loss: 1.6497 0.7571 sec/batch\n",
      "Epoch 9/20  Iteration 1464/3560 Training loss: 1.6491 0.8468 sec/batch\n",
      "Epoch 9/20  Iteration 1465/3560 Training loss: 1.6485 0.7991 sec/batch\n",
      "Epoch 9/20  Iteration 1466/3560 Training loss: 1.6489 0.8029 sec/batch\n",
      "Epoch 9/20  Iteration 1467/3560 Training loss: 1.6482 0.7552 sec/batch\n",
      "Epoch 9/20  Iteration 1468/3560 Training loss: 1.6475 0.7833 sec/batch\n",
      "Epoch 9/20  Iteration 1469/3560 Training loss: 1.6478 0.7555 sec/batch\n",
      "Epoch 9/20  Iteration 1470/3560 Training loss: 1.6468 0.8074 sec/batch\n",
      "Epoch 9/20  Iteration 1471/3560 Training loss: 1.6465 0.7661 sec/batch\n",
      "Epoch 9/20  Iteration 1472/3560 Training loss: 1.6459 0.7722 sec/batch\n",
      "Epoch 9/20  Iteration 1473/3560 Training loss: 1.6456 0.7875 sec/batch\n",
      "Epoch 9/20  Iteration 1474/3560 Training loss: 1.6463 0.7864 sec/batch\n",
      "Epoch 9/20  Iteration 1475/3560 Training loss: 1.6459 0.7906 sec/batch\n",
      "Epoch 9/20  Iteration 1476/3560 Training loss: 1.6468 0.8439 sec/batch\n",
      "Epoch 9/20  Iteration 1477/3560 Training loss: 1.6465 0.8405 sec/batch\n",
      "Epoch 9/20  Iteration 1478/3560 Training loss: 1.6466 0.7654 sec/batch\n",
      "Epoch 9/20  Iteration 1479/3560 Training loss: 1.6463 0.7808 sec/batch\n",
      "Epoch 9/20  Iteration 1480/3560 Training loss: 1.6463 0.8494 sec/batch\n",
      "Epoch 9/20  Iteration 1481/3560 Training loss: 1.6466 0.7905 sec/batch\n",
      "Epoch 9/20  Iteration 1482/3560 Training loss: 1.6462 0.7911 sec/batch\n",
      "Epoch 9/20  Iteration 1483/3560 Training loss: 1.6456 0.7882 sec/batch\n",
      "Epoch 9/20  Iteration 1484/3560 Training loss: 1.6459 0.7909 sec/batch\n",
      "Epoch 9/20  Iteration 1485/3560 Training loss: 1.6459 0.7874 sec/batch\n",
      "Epoch 9/20  Iteration 1486/3560 Training loss: 1.6466 0.7693 sec/batch\n",
      "Epoch 9/20  Iteration 1487/3560 Training loss: 1.6470 0.8079 sec/batch\n",
      "Epoch 9/20  Iteration 1488/3560 Training loss: 1.6472 0.7714 sec/batch\n",
      "Epoch 9/20  Iteration 1489/3560 Training loss: 1.6469 0.8014 sec/batch\n",
      "Epoch 9/20  Iteration 1490/3560 Training loss: 1.6472 0.8177 sec/batch\n",
      "Epoch 9/20  Iteration 1491/3560 Training loss: 1.6473 0.8692 sec/batch\n",
      "Epoch 9/20  Iteration 1492/3560 Training loss: 1.6469 0.8364 sec/batch\n",
      "Epoch 9/20  Iteration 1493/3560 Training loss: 1.6469 0.7896 sec/batch\n",
      "Epoch 9/20  Iteration 1494/3560 Training loss: 1.6466 0.7733 sec/batch\n",
      "Epoch 9/20  Iteration 1495/3560 Training loss: 1.6471 0.8014 sec/batch\n",
      "Epoch 9/20  Iteration 1496/3560 Training loss: 1.6472 0.7869 sec/batch\n",
      "Epoch 9/20  Iteration 1497/3560 Training loss: 1.6475 0.7840 sec/batch\n",
      "Epoch 9/20  Iteration 1498/3560 Training loss: 1.6472 0.8042 sec/batch\n",
      "Epoch 9/20  Iteration 1499/3560 Training loss: 1.6470 0.8086 sec/batch\n",
      "Epoch 9/20  Iteration 1500/3560 Training loss: 1.6472 0.7647 sec/batch\n",
      "Epoch 9/20  Iteration 1501/3560 Training loss: 1.6471 0.7655 sec/batch\n",
      "Epoch 9/20  Iteration 1502/3560 Training loss: 1.6471 0.8102 sec/batch\n",
      "Epoch 9/20  Iteration 1503/3560 Training loss: 1.6465 0.8365 sec/batch\n",
      "Epoch 9/20  Iteration 1504/3560 Training loss: 1.6463 0.8003 sec/batch\n",
      "Epoch 9/20  Iteration 1505/3560 Training loss: 1.6457 0.7924 sec/batch\n",
      "Epoch 9/20  Iteration 1506/3560 Training loss: 1.6459 0.8465 sec/batch\n",
      "Epoch 9/20  Iteration 1507/3560 Training loss: 1.6454 0.8472 sec/batch\n",
      "Epoch 9/20  Iteration 1508/3560 Training loss: 1.6452 0.7998 sec/batch\n",
      "Epoch 9/20  Iteration 1509/3560 Training loss: 1.6449 0.7958 sec/batch\n",
      "Epoch 9/20  Iteration 1510/3560 Training loss: 1.6445 0.7722 sec/batch\n",
      "Epoch 9/20  Iteration 1511/3560 Training loss: 1.6442 0.8024 sec/batch\n",
      "Epoch 9/20  Iteration 1512/3560 Training loss: 1.6438 0.7598 sec/batch\n",
      "Epoch 9/20  Iteration 1513/3560 Training loss: 1.6433 0.7567 sec/batch\n",
      "Epoch 9/20  Iteration 1514/3560 Training loss: 1.6433 0.7825 sec/batch\n",
      "Epoch 9/20  Iteration 1515/3560 Training loss: 1.6430 0.8824 sec/batch\n",
      "Epoch 9/20  Iteration 1516/3560 Training loss: 1.6428 0.7988 sec/batch\n",
      "Epoch 9/20  Iteration 1517/3560 Training loss: 1.6424 0.7861 sec/batch\n",
      "Epoch 9/20  Iteration 1518/3560 Training loss: 1.6421 0.7976 sec/batch\n",
      "Epoch 9/20  Iteration 1519/3560 Training loss: 1.6418 0.7656 sec/batch\n",
      "Epoch 9/20  Iteration 1520/3560 Training loss: 1.6417 0.7866 sec/batch\n",
      "Epoch 9/20  Iteration 1521/3560 Training loss: 1.6417 0.8433 sec/batch\n",
      "Epoch 9/20  Iteration 1522/3560 Training loss: 1.6412 0.7967 sec/batch\n",
      "Epoch 9/20  Iteration 1523/3560 Training loss: 1.6408 0.7680 sec/batch\n",
      "Epoch 9/20  Iteration 1524/3560 Training loss: 1.6403 0.9797 sec/batch\n",
      "Epoch 9/20  Iteration 1525/3560 Training loss: 1.6403 1.0369 sec/batch\n",
      "Epoch 9/20  Iteration 1526/3560 Training loss: 1.6402 1.0963 sec/batch\n",
      "Epoch 9/20  Iteration 1527/3560 Training loss: 1.6400 1.0910 sec/batch\n",
      "Epoch 9/20  Iteration 1528/3560 Training loss: 1.6400 1.0254 sec/batch\n",
      "Epoch 9/20  Iteration 1529/3560 Training loss: 1.6396 1.0222 sec/batch\n",
      "Epoch 9/20  Iteration 1530/3560 Training loss: 1.6395 1.0902 sec/batch\n",
      "Epoch 9/20  Iteration 1531/3560 Training loss: 1.6393 1.0574 sec/batch\n",
      "Epoch 9/20  Iteration 1532/3560 Training loss: 1.6392 0.8367 sec/batch\n",
      "Epoch 9/20  Iteration 1533/3560 Training loss: 1.6391 0.8204 sec/batch\n",
      "Epoch 9/20  Iteration 1534/3560 Training loss: 1.6390 0.8242 sec/batch\n",
      "Epoch 9/20  Iteration 1535/3560 Training loss: 1.6389 0.7731 sec/batch\n",
      "Epoch 9/20  Iteration 1536/3560 Training loss: 1.6388 0.7575 sec/batch\n",
      "Epoch 9/20  Iteration 1537/3560 Training loss: 1.6386 0.8002 sec/batch\n",
      "Epoch 9/20  Iteration 1538/3560 Training loss: 1.6385 0.8590 sec/batch\n",
      "Epoch 9/20  Iteration 1539/3560 Training loss: 1.6383 0.7801 sec/batch\n",
      "Epoch 9/20  Iteration 1540/3560 Training loss: 1.6378 0.7516 sec/batch\n",
      "Epoch 9/20  Iteration 1541/3560 Training loss: 1.6377 0.7703 sec/batch\n",
      "Epoch 9/20  Iteration 1542/3560 Training loss: 1.6376 0.7904 sec/batch\n",
      "Epoch 9/20  Iteration 1543/3560 Training loss: 1.6375 0.7941 sec/batch\n",
      "Epoch 9/20  Iteration 1544/3560 Training loss: 1.6374 0.8326 sec/batch\n",
      "Epoch 9/20  Iteration 1545/3560 Training loss: 1.6372 0.7951 sec/batch\n",
      "Epoch 9/20  Iteration 1546/3560 Training loss: 1.6368 0.7939 sec/batch\n",
      "Epoch 9/20  Iteration 1547/3560 Training loss: 1.6365 0.7766 sec/batch\n",
      "Epoch 9/20  Iteration 1548/3560 Training loss: 1.6366 0.8261 sec/batch\n",
      "Epoch 9/20  Iteration 1549/3560 Training loss: 1.6365 0.8040 sec/batch\n",
      "Epoch 9/20  Iteration 1550/3560 Training loss: 1.6360 0.8253 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1551/3560 Training loss: 1.6361 0.8486 sec/batch\n",
      "Epoch 9/20  Iteration 1552/3560 Training loss: 1.6361 0.8095 sec/batch\n",
      "Epoch 9/20  Iteration 1553/3560 Training loss: 1.6359 0.7650 sec/batch\n",
      "Epoch 9/20  Iteration 1554/3560 Training loss: 1.6357 0.7718 sec/batch\n",
      "Epoch 9/20  Iteration 1555/3560 Training loss: 1.6353 0.7944 sec/batch\n",
      "Epoch 9/20  Iteration 1556/3560 Training loss: 1.6351 0.8106 sec/batch\n",
      "Epoch 9/20  Iteration 1557/3560 Training loss: 1.6351 0.7578 sec/batch\n",
      "Epoch 9/20  Iteration 1558/3560 Training loss: 1.6351 0.7652 sec/batch\n",
      "Epoch 9/20  Iteration 1559/3560 Training loss: 1.6351 0.7983 sec/batch\n",
      "Epoch 9/20  Iteration 1560/3560 Training loss: 1.6351 0.7980 sec/batch\n",
      "Epoch 9/20  Iteration 1561/3560 Training loss: 1.6352 0.7859 sec/batch\n",
      "Epoch 9/20  Iteration 1562/3560 Training loss: 1.6352 0.8049 sec/batch\n",
      "Epoch 9/20  Iteration 1563/3560 Training loss: 1.6352 0.8795 sec/batch\n",
      "Epoch 9/20  Iteration 1564/3560 Training loss: 1.6351 0.7898 sec/batch\n",
      "Epoch 9/20  Iteration 1565/3560 Training loss: 1.6354 0.7572 sec/batch\n",
      "Epoch 9/20  Iteration 1566/3560 Training loss: 1.6353 0.7992 sec/batch\n",
      "Epoch 9/20  Iteration 1567/3560 Training loss: 1.6352 0.7924 sec/batch\n",
      "Epoch 9/20  Iteration 1568/3560 Training loss: 1.6353 0.7816 sec/batch\n",
      "Epoch 9/20  Iteration 1569/3560 Training loss: 1.6352 0.7822 sec/batch\n",
      "Epoch 9/20  Iteration 1570/3560 Training loss: 1.6353 0.7777 sec/batch\n",
      "Epoch 9/20  Iteration 1571/3560 Training loss: 1.6353 0.8002 sec/batch\n",
      "Epoch 9/20  Iteration 1572/3560 Training loss: 1.6354 0.8076 sec/batch\n",
      "Epoch 9/20  Iteration 1573/3560 Training loss: 1.6355 0.7818 sec/batch\n",
      "Epoch 9/20  Iteration 1574/3560 Training loss: 1.6353 0.7729 sec/batch\n",
      "Epoch 9/20  Iteration 1575/3560 Training loss: 1.6350 0.7657 sec/batch\n",
      "Epoch 9/20  Iteration 1576/3560 Training loss: 1.6350 0.8530 sec/batch\n",
      "Epoch 9/20  Iteration 1577/3560 Training loss: 1.6350 0.8489 sec/batch\n",
      "Epoch 9/20  Iteration 1578/3560 Training loss: 1.6350 0.8314 sec/batch\n",
      "Epoch 9/20  Iteration 1579/3560 Training loss: 1.6349 0.7920 sec/batch\n",
      "Epoch 9/20  Iteration 1580/3560 Training loss: 1.6348 0.7480 sec/batch\n",
      "Epoch 9/20  Iteration 1581/3560 Training loss: 1.6349 0.7834 sec/batch\n",
      "Epoch 9/20  Iteration 1582/3560 Training loss: 1.6348 0.7830 sec/batch\n",
      "Epoch 9/20  Iteration 1583/3560 Training loss: 1.6346 0.7776 sec/batch\n",
      "Epoch 9/20  Iteration 1584/3560 Training loss: 1.6346 0.7567 sec/batch\n",
      "Epoch 9/20  Iteration 1585/3560 Training loss: 1.6348 0.7563 sec/batch\n",
      "Epoch 9/20  Iteration 1586/3560 Training loss: 1.6347 0.7752 sec/batch\n",
      "Epoch 9/20  Iteration 1587/3560 Training loss: 1.6347 0.7742 sec/batch\n",
      "Epoch 9/20  Iteration 1588/3560 Training loss: 1.6347 0.7779 sec/batch\n",
      "Epoch 9/20  Iteration 1589/3560 Training loss: 1.6347 0.8513 sec/batch\n",
      "Epoch 9/20  Iteration 1590/3560 Training loss: 1.6346 0.7934 sec/batch\n",
      "Epoch 9/20  Iteration 1591/3560 Training loss: 1.6347 0.8089 sec/batch\n",
      "Epoch 9/20  Iteration 1592/3560 Training loss: 1.6350 0.8291 sec/batch\n",
      "Epoch 9/20  Iteration 1593/3560 Training loss: 1.6349 0.8180 sec/batch\n",
      "Epoch 9/20  Iteration 1594/3560 Training loss: 1.6348 0.7697 sec/batch\n",
      "Epoch 9/20  Iteration 1595/3560 Training loss: 1.6347 0.7860 sec/batch\n",
      "Epoch 9/20  Iteration 1596/3560 Training loss: 1.6344 0.7644 sec/batch\n",
      "Epoch 9/20  Iteration 1597/3560 Training loss: 1.6345 0.8065 sec/batch\n",
      "Epoch 9/20  Iteration 1598/3560 Training loss: 1.6345 0.7894 sec/batch\n",
      "Epoch 9/20  Iteration 1599/3560 Training loss: 1.6345 0.7756 sec/batch\n",
      "Epoch 9/20  Iteration 1600/3560 Training loss: 1.6343 0.7917 sec/batch\n",
      "Epoch 9/20  Iteration 1601/3560 Training loss: 1.6341 0.8056 sec/batch\n",
      "Epoch 9/20  Iteration 1602/3560 Training loss: 1.6342 0.8260 sec/batch\n",
      "Epoch 10/20  Iteration 1603/3560 Training loss: 1.7186 0.7947 sec/batch\n",
      "Epoch 10/20  Iteration 1604/3560 Training loss: 1.6652 0.7748 sec/batch\n",
      "Epoch 10/20  Iteration 1605/3560 Training loss: 1.6493 0.8774 sec/batch\n",
      "Epoch 10/20  Iteration 1606/3560 Training loss: 1.6444 0.7832 sec/batch\n",
      "Epoch 10/20  Iteration 1607/3560 Training loss: 1.6369 0.8056 sec/batch\n",
      "Epoch 10/20  Iteration 1608/3560 Training loss: 1.6254 0.8064 sec/batch\n",
      "Epoch 10/20  Iteration 1609/3560 Training loss: 1.6277 0.7699 sec/batch\n",
      "Epoch 10/20  Iteration 1610/3560 Training loss: 1.6252 0.7903 sec/batch\n",
      "Epoch 10/20  Iteration 1611/3560 Training loss: 1.6254 0.8087 sec/batch\n",
      "Epoch 10/20  Iteration 1612/3560 Training loss: 1.6253 0.8175 sec/batch\n",
      "Epoch 10/20  Iteration 1613/3560 Training loss: 1.6216 0.7820 sec/batch\n",
      "Epoch 10/20  Iteration 1614/3560 Training loss: 1.6199 0.7991 sec/batch\n",
      "Epoch 10/20  Iteration 1615/3560 Training loss: 1.6201 0.8249 sec/batch\n",
      "Epoch 10/20  Iteration 1616/3560 Training loss: 1.6218 0.7841 sec/batch\n",
      "Epoch 10/20  Iteration 1617/3560 Training loss: 1.6212 0.7954 sec/batch\n",
      "Epoch 10/20  Iteration 1618/3560 Training loss: 1.6192 0.7727 sec/batch\n",
      "Epoch 10/20  Iteration 1619/3560 Training loss: 1.6194 0.7872 sec/batch\n",
      "Epoch 10/20  Iteration 1620/3560 Training loss: 1.6207 0.8667 sec/batch\n",
      "Epoch 10/20  Iteration 1621/3560 Training loss: 1.6205 0.7807 sec/batch\n",
      "Epoch 10/20  Iteration 1622/3560 Training loss: 1.6212 0.8493 sec/batch\n",
      "Epoch 10/20  Iteration 1623/3560 Training loss: 1.6209 0.8011 sec/batch\n",
      "Epoch 10/20  Iteration 1624/3560 Training loss: 1.6216 0.8187 sec/batch\n",
      "Epoch 10/20  Iteration 1625/3560 Training loss: 1.6207 0.7892 sec/batch\n",
      "Epoch 10/20  Iteration 1626/3560 Training loss: 1.6203 0.8123 sec/batch\n",
      "Epoch 10/20  Iteration 1627/3560 Training loss: 1.6201 0.8467 sec/batch\n",
      "Epoch 10/20  Iteration 1628/3560 Training loss: 1.6188 0.7845 sec/batch\n",
      "Epoch 10/20  Iteration 1629/3560 Training loss: 1.6175 0.7879 sec/batch\n",
      "Epoch 10/20  Iteration 1630/3560 Training loss: 1.6178 0.7799 sec/batch\n",
      "Epoch 10/20  Iteration 1631/3560 Training loss: 1.6183 0.7831 sec/batch\n",
      "Epoch 10/20  Iteration 1632/3560 Training loss: 1.6187 0.8169 sec/batch\n",
      "Epoch 10/20  Iteration 1633/3560 Training loss: 1.6185 0.7798 sec/batch\n",
      "Epoch 10/20  Iteration 1634/3560 Training loss: 1.6173 0.7738 sec/batch\n",
      "Epoch 10/20  Iteration 1635/3560 Training loss: 1.6174 0.8610 sec/batch\n",
      "Epoch 10/20  Iteration 1636/3560 Training loss: 1.6181 0.8199 sec/batch\n",
      "Epoch 10/20  Iteration 1637/3560 Training loss: 1.6178 0.7876 sec/batch\n",
      "Epoch 10/20  Iteration 1638/3560 Training loss: 1.6176 0.7912 sec/batch\n",
      "Epoch 10/20  Iteration 1639/3560 Training loss: 1.6171 0.8237 sec/batch\n",
      "Epoch 10/20  Iteration 1640/3560 Training loss: 1.6161 0.7670 sec/batch\n",
      "Epoch 10/20  Iteration 1641/3560 Training loss: 1.6150 0.7907 sec/batch\n",
      "Epoch 10/20  Iteration 1642/3560 Training loss: 1.6141 0.7934 sec/batch\n",
      "Epoch 10/20  Iteration 1643/3560 Training loss: 1.6137 0.7933 sec/batch\n",
      "Epoch 10/20  Iteration 1644/3560 Training loss: 1.6141 0.7962 sec/batch\n",
      "Epoch 10/20  Iteration 1645/3560 Training loss: 1.6137 0.8059 sec/batch\n",
      "Epoch 10/20  Iteration 1646/3560 Training loss: 1.6132 0.7967 sec/batch\n",
      "Epoch 10/20  Iteration 1647/3560 Training loss: 1.6134 0.7649 sec/batch\n",
      "Epoch 10/20  Iteration 1648/3560 Training loss: 1.6123 0.7569 sec/batch\n",
      "Epoch 10/20  Iteration 1649/3560 Training loss: 1.6119 0.8026 sec/batch\n",
      "Epoch 10/20  Iteration 1650/3560 Training loss: 1.6115 0.8610 sec/batch\n",
      "Epoch 10/20  Iteration 1651/3560 Training loss: 1.6112 0.8092 sec/batch\n",
      "Epoch 10/20  Iteration 1652/3560 Training loss: 1.6115 0.8513 sec/batch\n",
      "Epoch 10/20  Iteration 1653/3560 Training loss: 1.6110 0.8115 sec/batch\n",
      "Epoch 10/20  Iteration 1654/3560 Training loss: 1.6119 0.7981 sec/batch\n",
      "Epoch 10/20  Iteration 1655/3560 Training loss: 1.6117 0.7881 sec/batch\n",
      "Epoch 10/20  Iteration 1656/3560 Training loss: 1.6116 0.7826 sec/batch\n",
      "Epoch 10/20  Iteration 1657/3560 Training loss: 1.6113 0.8089 sec/batch\n",
      "Epoch 10/20  Iteration 1658/3560 Training loss: 1.6112 0.8040 sec/batch\n",
      "Epoch 10/20  Iteration 1659/3560 Training loss: 1.6114 0.7734 sec/batch\n",
      "Epoch 10/20  Iteration 1660/3560 Training loss: 1.6110 0.7635 sec/batch\n",
      "Epoch 10/20  Iteration 1661/3560 Training loss: 1.6105 0.8017 sec/batch\n",
      "Epoch 10/20  Iteration 1662/3560 Training loss: 1.6109 0.7808 sec/batch\n",
      "Epoch 10/20  Iteration 1663/3560 Training loss: 1.6106 0.7753 sec/batch\n",
      "Epoch 10/20  Iteration 1664/3560 Training loss: 1.6115 0.7850 sec/batch\n",
      "Epoch 10/20  Iteration 1665/3560 Training loss: 1.6117 0.9084 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1666/3560 Training loss: 1.6118 0.8086 sec/batch\n",
      "Epoch 10/20  Iteration 1667/3560 Training loss: 1.6117 0.7937 sec/batch\n",
      "Epoch 10/20  Iteration 1668/3560 Training loss: 1.6119 0.7803 sec/batch\n",
      "Epoch 10/20  Iteration 1669/3560 Training loss: 1.6122 0.7843 sec/batch\n",
      "Epoch 10/20  Iteration 1670/3560 Training loss: 1.6119 0.7736 sec/batch\n",
      "Epoch 10/20  Iteration 1671/3560 Training loss: 1.6116 0.7912 sec/batch\n",
      "Epoch 10/20  Iteration 1672/3560 Training loss: 1.6115 0.8018 sec/batch\n",
      "Epoch 10/20  Iteration 1673/3560 Training loss: 1.6121 0.7992 sec/batch\n",
      "Epoch 10/20  Iteration 1674/3560 Training loss: 1.6122 0.7833 sec/batch\n",
      "Epoch 10/20  Iteration 1675/3560 Training loss: 1.6127 0.7802 sec/batch\n",
      "Epoch 10/20  Iteration 1676/3560 Training loss: 1.6124 0.8022 sec/batch\n",
      "Epoch 10/20  Iteration 1677/3560 Training loss: 1.6123 0.8334 sec/batch\n",
      "Epoch 10/20  Iteration 1678/3560 Training loss: 1.6125 0.8156 sec/batch\n",
      "Epoch 10/20  Iteration 1679/3560 Training loss: 1.6122 0.7711 sec/batch\n",
      "Epoch 10/20  Iteration 1680/3560 Training loss: 1.6123 0.8360 sec/batch\n",
      "Epoch 10/20  Iteration 1681/3560 Training loss: 1.6117 0.8188 sec/batch\n",
      "Epoch 10/20  Iteration 1682/3560 Training loss: 1.6117 0.8005 sec/batch\n",
      "Epoch 10/20  Iteration 1683/3560 Training loss: 1.6111 0.8125 sec/batch\n",
      "Epoch 10/20  Iteration 1684/3560 Training loss: 1.6111 0.8083 sec/batch\n",
      "Epoch 10/20  Iteration 1685/3560 Training loss: 1.6104 0.7755 sec/batch\n",
      "Epoch 10/20  Iteration 1686/3560 Training loss: 1.6102 0.7728 sec/batch\n",
      "Epoch 10/20  Iteration 1687/3560 Training loss: 1.6099 0.7771 sec/batch\n",
      "Epoch 10/20  Iteration 1688/3560 Training loss: 1.6097 0.7951 sec/batch\n",
      "Epoch 10/20  Iteration 1689/3560 Training loss: 1.6094 0.7879 sec/batch\n",
      "Epoch 10/20  Iteration 1690/3560 Training loss: 1.6091 0.7721 sec/batch\n",
      "Epoch 10/20  Iteration 1691/3560 Training loss: 1.6086 0.8116 sec/batch\n",
      "Epoch 10/20  Iteration 1692/3560 Training loss: 1.6087 0.7946 sec/batch\n",
      "Epoch 10/20  Iteration 1693/3560 Training loss: 1.6084 0.7579 sec/batch\n",
      "Epoch 10/20  Iteration 1694/3560 Training loss: 1.6082 0.7986 sec/batch\n",
      "Epoch 10/20  Iteration 1695/3560 Training loss: 1.6079 0.8529 sec/batch\n",
      "Epoch 10/20  Iteration 1696/3560 Training loss: 1.6077 0.8167 sec/batch\n",
      "Epoch 10/20  Iteration 1697/3560 Training loss: 1.6072 0.7823 sec/batch\n",
      "Epoch 10/20  Iteration 1698/3560 Training loss: 1.6072 0.7639 sec/batch\n",
      "Epoch 10/20  Iteration 1699/3560 Training loss: 1.6071 0.7946 sec/batch\n",
      "Epoch 10/20  Iteration 1700/3560 Training loss: 1.6067 0.8098 sec/batch\n",
      "Epoch 10/20  Iteration 1701/3560 Training loss: 1.6064 0.7914 sec/batch\n",
      "Epoch 10/20  Iteration 1702/3560 Training loss: 1.6060 0.8120 sec/batch\n",
      "Epoch 10/20  Iteration 1703/3560 Training loss: 1.6059 0.8245 sec/batch\n",
      "Epoch 10/20  Iteration 1704/3560 Training loss: 1.6057 0.7987 sec/batch\n",
      "Epoch 10/20  Iteration 1705/3560 Training loss: 1.6055 0.8017 sec/batch\n",
      "Epoch 10/20  Iteration 1706/3560 Training loss: 1.6053 0.8001 sec/batch\n",
      "Epoch 10/20  Iteration 1707/3560 Training loss: 1.6052 0.7908 sec/batch\n",
      "Epoch 10/20  Iteration 1708/3560 Training loss: 1.6051 0.7787 sec/batch\n",
      "Epoch 10/20  Iteration 1709/3560 Training loss: 1.6050 0.7677 sec/batch\n",
      "Epoch 10/20  Iteration 1710/3560 Training loss: 1.6050 0.8538 sec/batch\n",
      "Epoch 10/20  Iteration 1711/3560 Training loss: 1.6050 0.7919 sec/batch\n",
      "Epoch 10/20  Iteration 1712/3560 Training loss: 1.6050 0.7854 sec/batch\n",
      "Epoch 10/20  Iteration 1713/3560 Training loss: 1.6048 0.7746 sec/batch\n",
      "Epoch 10/20  Iteration 1714/3560 Training loss: 1.6047 0.7670 sec/batch\n",
      "Epoch 10/20  Iteration 1715/3560 Training loss: 1.6045 0.7831 sec/batch\n",
      "Epoch 10/20  Iteration 1716/3560 Training loss: 1.6043 0.8248 sec/batch\n",
      "Epoch 10/20  Iteration 1717/3560 Training loss: 1.6041 0.7934 sec/batch\n",
      "Epoch 10/20  Iteration 1718/3560 Training loss: 1.6037 0.7694 sec/batch\n",
      "Epoch 10/20  Iteration 1719/3560 Training loss: 1.6036 0.7645 sec/batch\n",
      "Epoch 10/20  Iteration 1720/3560 Training loss: 1.6036 0.7574 sec/batch\n",
      "Epoch 10/20  Iteration 1721/3560 Training loss: 1.6035 0.7724 sec/batch\n",
      "Epoch 10/20  Iteration 1722/3560 Training loss: 1.6034 0.7645 sec/batch\n",
      "Epoch 10/20  Iteration 1723/3560 Training loss: 1.6034 0.7604 sec/batch\n",
      "Epoch 10/20  Iteration 1724/3560 Training loss: 1.6030 0.7901 sec/batch\n",
      "Epoch 10/20  Iteration 1725/3560 Training loss: 1.6027 0.8453 sec/batch\n",
      "Epoch 10/20  Iteration 1726/3560 Training loss: 1.6027 0.8549 sec/batch\n",
      "Epoch 10/20  Iteration 1727/3560 Training loss: 1.6026 0.7793 sec/batch\n",
      "Epoch 10/20  Iteration 1728/3560 Training loss: 1.6021 0.8100 sec/batch\n",
      "Epoch 10/20  Iteration 1729/3560 Training loss: 1.6022 0.8249 sec/batch\n",
      "Epoch 10/20  Iteration 1730/3560 Training loss: 1.6023 0.7622 sec/batch\n",
      "Epoch 10/20  Iteration 1731/3560 Training loss: 1.6021 0.7951 sec/batch\n",
      "Epoch 10/20  Iteration 1732/3560 Training loss: 1.6019 0.7740 sec/batch\n",
      "Epoch 10/20  Iteration 1733/3560 Training loss: 1.6016 0.7947 sec/batch\n",
      "Epoch 10/20  Iteration 1734/3560 Training loss: 1.6013 0.8046 sec/batch\n",
      "Epoch 10/20  Iteration 1735/3560 Training loss: 1.6013 0.7760 sec/batch\n",
      "Epoch 10/20  Iteration 1736/3560 Training loss: 1.6013 0.7929 sec/batch\n",
      "Epoch 10/20  Iteration 1737/3560 Training loss: 1.6013 0.7580 sec/batch\n",
      "Epoch 10/20  Iteration 1738/3560 Training loss: 1.6014 0.7694 sec/batch\n",
      "Epoch 10/20  Iteration 1739/3560 Training loss: 1.6015 0.7856 sec/batch\n",
      "Epoch 10/20  Iteration 1740/3560 Training loss: 1.6015 0.8531 sec/batch\n",
      "Epoch 10/20  Iteration 1741/3560 Training loss: 1.6015 0.9060 sec/batch\n",
      "Epoch 10/20  Iteration 1742/3560 Training loss: 1.6014 0.8009 sec/batch\n",
      "Epoch 10/20  Iteration 1743/3560 Training loss: 1.6017 0.8053 sec/batch\n",
      "Epoch 10/20  Iteration 1744/3560 Training loss: 1.6016 0.7663 sec/batch\n",
      "Epoch 10/20  Iteration 1745/3560 Training loss: 1.6015 0.7885 sec/batch\n",
      "Epoch 10/20  Iteration 1746/3560 Training loss: 1.6016 0.7825 sec/batch\n",
      "Epoch 10/20  Iteration 1747/3560 Training loss: 1.6014 0.9002 sec/batch\n",
      "Epoch 10/20  Iteration 1748/3560 Training loss: 1.6016 1.0200 sec/batch\n",
      "Epoch 10/20  Iteration 1749/3560 Training loss: 1.6016 1.0386 sec/batch\n",
      "Epoch 10/20  Iteration 1750/3560 Training loss: 1.6018 1.0717 sec/batch\n",
      "Epoch 10/20  Iteration 1751/3560 Training loss: 1.6018 1.0073 sec/batch\n",
      "Epoch 10/20  Iteration 1752/3560 Training loss: 1.6016 1.1628 sec/batch\n",
      "Epoch 10/20  Iteration 1753/3560 Training loss: 1.6013 1.0997 sec/batch\n",
      "Epoch 10/20  Iteration 1754/3560 Training loss: 1.6013 1.0849 sec/batch\n",
      "Epoch 10/20  Iteration 1755/3560 Training loss: 1.6013 0.8972 sec/batch\n",
      "Epoch 10/20  Iteration 1756/3560 Training loss: 1.6013 0.7802 sec/batch\n",
      "Epoch 10/20  Iteration 1757/3560 Training loss: 1.6012 0.7689 sec/batch\n",
      "Epoch 10/20  Iteration 1758/3560 Training loss: 1.6012 0.8034 sec/batch\n",
      "Epoch 10/20  Iteration 1759/3560 Training loss: 1.6013 0.7610 sec/batch\n",
      "Epoch 10/20  Iteration 1760/3560 Training loss: 1.6012 0.7933 sec/batch\n",
      "Epoch 10/20  Iteration 1761/3560 Training loss: 1.6010 0.8125 sec/batch\n",
      "Epoch 10/20  Iteration 1762/3560 Training loss: 1.6010 0.7667 sec/batch\n",
      "Epoch 10/20  Iteration 1763/3560 Training loss: 1.6012 0.8096 sec/batch\n",
      "Epoch 10/20  Iteration 1764/3560 Training loss: 1.6011 0.8152 sec/batch\n",
      "Epoch 10/20  Iteration 1765/3560 Training loss: 1.6012 0.7797 sec/batch\n",
      "Epoch 10/20  Iteration 1766/3560 Training loss: 1.6011 0.7661 sec/batch\n",
      "Epoch 10/20  Iteration 1767/3560 Training loss: 1.6010 0.8043 sec/batch\n",
      "Epoch 10/20  Iteration 1768/3560 Training loss: 1.6009 0.9061 sec/batch\n",
      "Epoch 10/20  Iteration 1769/3560 Training loss: 1.6010 0.7617 sec/batch\n",
      "Epoch 10/20  Iteration 1770/3560 Training loss: 1.6015 0.7826 sec/batch\n",
      "Epoch 10/20  Iteration 1771/3560 Training loss: 1.6014 0.7616 sec/batch\n",
      "Epoch 10/20  Iteration 1772/3560 Training loss: 1.6013 0.7815 sec/batch\n",
      "Epoch 10/20  Iteration 1773/3560 Training loss: 1.6012 0.8033 sec/batch\n",
      "Epoch 10/20  Iteration 1774/3560 Training loss: 1.6009 0.7991 sec/batch\n",
      "Epoch 10/20  Iteration 1775/3560 Training loss: 1.6010 0.7702 sec/batch\n",
      "Epoch 10/20  Iteration 1776/3560 Training loss: 1.6010 0.8177 sec/batch\n",
      "Epoch 10/20  Iteration 1777/3560 Training loss: 1.6010 0.8306 sec/batch\n",
      "Epoch 10/20  Iteration 1778/3560 Training loss: 1.6010 0.7841 sec/batch\n",
      "Epoch 10/20  Iteration 1779/3560 Training loss: 1.6008 0.7898 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1780/3560 Training loss: 1.6008 0.7862 sec/batch\n",
      "Epoch 11/20  Iteration 1781/3560 Training loss: 1.6886 0.8015 sec/batch\n",
      "Epoch 11/20  Iteration 1782/3560 Training loss: 1.6416 0.8121 sec/batch\n",
      "Epoch 11/20  Iteration 1783/3560 Training loss: 1.6242 0.8700 sec/batch\n",
      "Epoch 11/20  Iteration 1784/3560 Training loss: 1.6168 0.8149 sec/batch\n",
      "Epoch 11/20  Iteration 1785/3560 Training loss: 1.6084 0.8005 sec/batch\n",
      "Epoch 11/20  Iteration 1786/3560 Training loss: 1.5985 0.7818 sec/batch\n",
      "Epoch 11/20  Iteration 1787/3560 Training loss: 1.5989 0.7874 sec/batch\n",
      "Epoch 11/20  Iteration 1788/3560 Training loss: 1.5963 0.8131 sec/batch\n",
      "Epoch 11/20  Iteration 1789/3560 Training loss: 1.5981 0.8369 sec/batch\n",
      "Epoch 11/20  Iteration 1790/3560 Training loss: 1.5973 0.8044 sec/batch\n",
      "Epoch 11/20  Iteration 1791/3560 Training loss: 1.5933 0.7589 sec/batch\n",
      "Epoch 11/20  Iteration 1792/3560 Training loss: 1.5912 0.7801 sec/batch\n",
      "Epoch 11/20  Iteration 1793/3560 Training loss: 1.5906 0.8021 sec/batch\n",
      "Epoch 11/20  Iteration 1794/3560 Training loss: 1.5924 0.7710 sec/batch\n",
      "Epoch 11/20  Iteration 1795/3560 Training loss: 1.5915 0.7958 sec/batch\n",
      "Epoch 11/20  Iteration 1796/3560 Training loss: 1.5896 0.8017 sec/batch\n",
      "Epoch 11/20  Iteration 1797/3560 Training loss: 1.5897 0.7876 sec/batch\n",
      "Epoch 11/20  Iteration 1798/3560 Training loss: 1.5913 0.8976 sec/batch\n",
      "Epoch 11/20  Iteration 1799/3560 Training loss: 1.5912 0.8228 sec/batch\n",
      "Epoch 11/20  Iteration 1800/3560 Training loss: 1.5920 0.7938 sec/batch\n",
      "Epoch 11/20  Iteration 1801/3560 Training loss: 1.5913 0.8084 sec/batch\n",
      "Epoch 11/20  Iteration 1802/3560 Training loss: 1.5918 0.7994 sec/batch\n",
      "Epoch 11/20  Iteration 1803/3560 Training loss: 1.5906 0.7974 sec/batch\n",
      "Epoch 11/20  Iteration 1804/3560 Training loss: 1.5904 0.7643 sec/batch\n",
      "Epoch 11/20  Iteration 1805/3560 Training loss: 1.5900 0.7985 sec/batch\n",
      "Epoch 11/20  Iteration 1806/3560 Training loss: 1.5889 0.7993 sec/batch\n",
      "Epoch 11/20  Iteration 1807/3560 Training loss: 1.5878 0.7948 sec/batch\n",
      "Epoch 11/20  Iteration 1808/3560 Training loss: 1.5880 0.7898 sec/batch\n",
      "Epoch 11/20  Iteration 1809/3560 Training loss: 1.5884 0.7585 sec/batch\n",
      "Epoch 11/20  Iteration 1810/3560 Training loss: 1.5890 0.7580 sec/batch\n",
      "Epoch 11/20  Iteration 1811/3560 Training loss: 1.5889 0.7807 sec/batch\n",
      "Epoch 11/20  Iteration 1812/3560 Training loss: 1.5880 0.8050 sec/batch\n",
      "Epoch 11/20  Iteration 1813/3560 Training loss: 1.5883 0.8775 sec/batch\n",
      "Epoch 11/20  Iteration 1814/3560 Training loss: 1.5888 0.8645 sec/batch\n",
      "Epoch 11/20  Iteration 1815/3560 Training loss: 1.5885 0.7610 sec/batch\n",
      "Epoch 11/20  Iteration 1816/3560 Training loss: 1.5884 0.7747 sec/batch\n",
      "Epoch 11/20  Iteration 1817/3560 Training loss: 1.5875 0.7845 sec/batch\n",
      "Epoch 11/20  Iteration 1818/3560 Training loss: 1.5862 0.7966 sec/batch\n",
      "Epoch 11/20  Iteration 1819/3560 Training loss: 1.5849 0.7917 sec/batch\n",
      "Epoch 11/20  Iteration 1820/3560 Training loss: 1.5843 0.7763 sec/batch\n",
      "Epoch 11/20  Iteration 1821/3560 Training loss: 1.5839 0.7948 sec/batch\n",
      "Epoch 11/20  Iteration 1822/3560 Training loss: 1.5844 0.7773 sec/batch\n",
      "Epoch 11/20  Iteration 1823/3560 Training loss: 1.5839 0.7677 sec/batch\n",
      "Epoch 11/20  Iteration 1824/3560 Training loss: 1.5832 0.7692 sec/batch\n",
      "Epoch 11/20  Iteration 1825/3560 Training loss: 1.5834 0.7677 sec/batch\n",
      "Epoch 11/20  Iteration 1826/3560 Training loss: 1.5823 0.7601 sec/batch\n",
      "Epoch 11/20  Iteration 1827/3560 Training loss: 1.5821 0.8766 sec/batch\n",
      "Epoch 11/20  Iteration 1828/3560 Training loss: 1.5818 0.8495 sec/batch\n",
      "Epoch 11/20  Iteration 1829/3560 Training loss: 1.5814 0.7662 sec/batch\n",
      "Epoch 11/20  Iteration 1830/3560 Training loss: 1.5819 0.8169 sec/batch\n",
      "Epoch 11/20  Iteration 1831/3560 Training loss: 1.5816 0.7934 sec/batch\n",
      "Epoch 11/20  Iteration 1832/3560 Training loss: 1.5826 0.8014 sec/batch\n",
      "Epoch 11/20  Iteration 1833/3560 Training loss: 1.5825 0.7919 sec/batch\n",
      "Epoch 11/20  Iteration 1834/3560 Training loss: 1.5827 0.7855 sec/batch\n",
      "Epoch 11/20  Iteration 1835/3560 Training loss: 1.5827 0.7680 sec/batch\n",
      "Epoch 11/20  Iteration 1836/3560 Training loss: 1.5827 0.7994 sec/batch\n",
      "Epoch 11/20  Iteration 1837/3560 Training loss: 1.5830 0.8067 sec/batch\n",
      "Epoch 11/20  Iteration 1838/3560 Training loss: 1.5828 0.7699 sec/batch\n",
      "Epoch 11/20  Iteration 1839/3560 Training loss: 1.5824 0.8142 sec/batch\n",
      "Epoch 11/20  Iteration 1840/3560 Training loss: 1.5828 0.8370 sec/batch\n",
      "Epoch 11/20  Iteration 1841/3560 Training loss: 1.5828 0.7869 sec/batch\n",
      "Epoch 11/20  Iteration 1842/3560 Training loss: 1.5835 0.7855 sec/batch\n",
      "Epoch 11/20  Iteration 1843/3560 Training loss: 1.5840 0.8703 sec/batch\n",
      "Epoch 11/20  Iteration 1844/3560 Training loss: 1.5843 0.7915 sec/batch\n",
      "Epoch 11/20  Iteration 1845/3560 Training loss: 1.5841 0.8471 sec/batch\n",
      "Epoch 11/20  Iteration 1846/3560 Training loss: 1.5843 0.7711 sec/batch\n",
      "Epoch 11/20  Iteration 1847/3560 Training loss: 1.5844 0.8538 sec/batch\n",
      "Epoch 11/20  Iteration 1848/3560 Training loss: 1.5839 0.7920 sec/batch\n",
      "Epoch 11/20  Iteration 1849/3560 Training loss: 1.5839 0.8117 sec/batch\n",
      "Epoch 11/20  Iteration 1850/3560 Training loss: 1.5837 0.8051 sec/batch\n",
      "Epoch 11/20  Iteration 1851/3560 Training loss: 1.5842 0.7652 sec/batch\n",
      "Epoch 11/20  Iteration 1852/3560 Training loss: 1.5845 0.7983 sec/batch\n",
      "Epoch 11/20  Iteration 1853/3560 Training loss: 1.5849 0.7881 sec/batch\n",
      "Epoch 11/20  Iteration 1854/3560 Training loss: 1.5846 0.7808 sec/batch\n",
      "Epoch 11/20  Iteration 1855/3560 Training loss: 1.5845 0.7663 sec/batch\n",
      "Epoch 11/20  Iteration 1856/3560 Training loss: 1.5847 0.7623 sec/batch\n",
      "Epoch 11/20  Iteration 1857/3560 Training loss: 1.5846 0.8058 sec/batch\n",
      "Epoch 11/20  Iteration 1858/3560 Training loss: 1.5843 0.8606 sec/batch\n",
      "Epoch 11/20  Iteration 1859/3560 Training loss: 1.5837 0.8191 sec/batch\n",
      "Epoch 11/20  Iteration 1860/3560 Training loss: 1.5836 0.7941 sec/batch\n",
      "Epoch 11/20  Iteration 1861/3560 Training loss: 1.5830 0.7861 sec/batch\n",
      "Epoch 11/20  Iteration 1862/3560 Training loss: 1.5830 0.7724 sec/batch\n",
      "Epoch 11/20  Iteration 1863/3560 Training loss: 1.5824 0.7717 sec/batch\n",
      "Epoch 11/20  Iteration 1864/3560 Training loss: 1.5824 0.7810 sec/batch\n",
      "Epoch 11/20  Iteration 1865/3560 Training loss: 1.5820 0.8249 sec/batch\n",
      "Epoch 11/20  Iteration 1866/3560 Training loss: 1.5818 0.7612 sec/batch\n",
      "Epoch 11/20  Iteration 1867/3560 Training loss: 1.5815 0.7874 sec/batch\n",
      "Epoch 11/20  Iteration 1868/3560 Training loss: 1.5811 0.7718 sec/batch\n",
      "Epoch 11/20  Iteration 1869/3560 Training loss: 1.5807 0.8038 sec/batch\n",
      "Epoch 11/20  Iteration 1870/3560 Training loss: 1.5808 0.7649 sec/batch\n",
      "Epoch 11/20  Iteration 1871/3560 Training loss: 1.5803 0.8101 sec/batch\n",
      "Epoch 11/20  Iteration 1872/3560 Training loss: 1.5802 0.7868 sec/batch\n",
      "Epoch 11/20  Iteration 1873/3560 Training loss: 1.5798 0.8482 sec/batch\n",
      "Epoch 11/20  Iteration 1874/3560 Training loss: 1.5796 0.8170 sec/batch\n",
      "Epoch 11/20  Iteration 1875/3560 Training loss: 1.5792 0.8091 sec/batch\n",
      "Epoch 11/20  Iteration 1876/3560 Training loss: 1.5793 0.7723 sec/batch\n",
      "Epoch 11/20  Iteration 1877/3560 Training loss: 1.5792 0.8132 sec/batch\n",
      "Epoch 11/20  Iteration 1878/3560 Training loss: 1.5787 0.7998 sec/batch\n",
      "Epoch 11/20  Iteration 1879/3560 Training loss: 1.5784 0.7658 sec/batch\n",
      "Epoch 11/20  Iteration 1880/3560 Training loss: 1.5779 0.7941 sec/batch\n",
      "Epoch 11/20  Iteration 1881/3560 Training loss: 1.5779 0.7943 sec/batch\n",
      "Epoch 11/20  Iteration 1882/3560 Training loss: 1.5777 0.7922 sec/batch\n",
      "Epoch 11/20  Iteration 1883/3560 Training loss: 1.5776 0.7719 sec/batch\n",
      "Epoch 11/20  Iteration 1884/3560 Training loss: 1.5775 0.7966 sec/batch\n",
      "Epoch 11/20  Iteration 1885/3560 Training loss: 1.5773 0.7929 sec/batch\n",
      "Epoch 11/20  Iteration 1886/3560 Training loss: 1.5771 0.7520 sec/batch\n",
      "Epoch 11/20  Iteration 1887/3560 Training loss: 1.5770 0.7881 sec/batch\n",
      "Epoch 11/20  Iteration 1888/3560 Training loss: 1.5770 0.8583 sec/batch\n",
      "Epoch 11/20  Iteration 1889/3560 Training loss: 1.5768 0.8378 sec/batch\n",
      "Epoch 11/20  Iteration 1890/3560 Training loss: 1.5769 0.8318 sec/batch\n",
      "Epoch 11/20  Iteration 1891/3560 Training loss: 1.5768 0.8544 sec/batch\n",
      "Epoch 11/20  Iteration 1892/3560 Training loss: 1.5766 0.7721 sec/batch\n",
      "Epoch 11/20  Iteration 1893/3560 Training loss: 1.5764 0.7841 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1894/3560 Training loss: 1.5763 0.7876 sec/batch\n",
      "Epoch 11/20  Iteration 1895/3560 Training loss: 1.5759 0.7811 sec/batch\n",
      "Epoch 11/20  Iteration 1896/3560 Training loss: 1.5756 0.7911 sec/batch\n",
      "Epoch 11/20  Iteration 1897/3560 Training loss: 1.5756 0.7846 sec/batch\n",
      "Epoch 11/20  Iteration 1898/3560 Training loss: 1.5754 0.7881 sec/batch\n",
      "Epoch 11/20  Iteration 1899/3560 Training loss: 1.5753 0.7546 sec/batch\n",
      "Epoch 11/20  Iteration 1900/3560 Training loss: 1.5752 0.8394 sec/batch\n",
      "Epoch 11/20  Iteration 1901/3560 Training loss: 1.5751 0.7897 sec/batch\n",
      "Epoch 11/20  Iteration 1902/3560 Training loss: 1.5746 0.7835 sec/batch\n",
      "Epoch 11/20  Iteration 1903/3560 Training loss: 1.5743 0.9393 sec/batch\n",
      "Epoch 11/20  Iteration 1904/3560 Training loss: 1.5744 0.7666 sec/batch\n",
      "Epoch 11/20  Iteration 1905/3560 Training loss: 1.5743 0.7707 sec/batch\n",
      "Epoch 11/20  Iteration 1906/3560 Training loss: 1.5738 0.7928 sec/batch\n",
      "Epoch 11/20  Iteration 1907/3560 Training loss: 1.5739 0.7974 sec/batch\n",
      "Epoch 11/20  Iteration 1908/3560 Training loss: 1.5739 0.7954 sec/batch\n",
      "Epoch 11/20  Iteration 1909/3560 Training loss: 1.5738 0.7849 sec/batch\n",
      "Epoch 11/20  Iteration 1910/3560 Training loss: 1.5736 0.7949 sec/batch\n",
      "Epoch 11/20  Iteration 1911/3560 Training loss: 1.5731 0.8091 sec/batch\n",
      "Epoch 11/20  Iteration 1912/3560 Training loss: 1.5728 0.7913 sec/batch\n",
      "Epoch 11/20  Iteration 1913/3560 Training loss: 1.5729 0.7793 sec/batch\n",
      "Epoch 11/20  Iteration 1914/3560 Training loss: 1.5728 0.7880 sec/batch\n",
      "Epoch 11/20  Iteration 1915/3560 Training loss: 1.5728 0.8420 sec/batch\n",
      "Epoch 11/20  Iteration 1916/3560 Training loss: 1.5729 0.7868 sec/batch\n",
      "Epoch 11/20  Iteration 1917/3560 Training loss: 1.5731 0.7969 sec/batch\n",
      "Epoch 11/20  Iteration 1918/3560 Training loss: 1.5731 0.8842 sec/batch\n",
      "Epoch 11/20  Iteration 1919/3560 Training loss: 1.5731 0.8005 sec/batch\n",
      "Epoch 11/20  Iteration 1920/3560 Training loss: 1.5730 0.7626 sec/batch\n",
      "Epoch 11/20  Iteration 1921/3560 Training loss: 1.5733 0.8047 sec/batch\n",
      "Epoch 11/20  Iteration 1922/3560 Training loss: 1.5733 0.7616 sec/batch\n",
      "Epoch 11/20  Iteration 1923/3560 Training loss: 1.5731 0.8086 sec/batch\n",
      "Epoch 11/20  Iteration 1924/3560 Training loss: 1.5733 0.7963 sec/batch\n",
      "Epoch 11/20  Iteration 1925/3560 Training loss: 1.5732 0.7857 sec/batch\n",
      "Epoch 11/20  Iteration 1926/3560 Training loss: 1.5734 0.7802 sec/batch\n",
      "Epoch 11/20  Iteration 1927/3560 Training loss: 1.5734 0.7715 sec/batch\n",
      "Epoch 11/20  Iteration 1928/3560 Training loss: 1.5735 0.8286 sec/batch\n",
      "Epoch 11/20  Iteration 1929/3560 Training loss: 1.5735 0.8034 sec/batch\n",
      "Epoch 11/20  Iteration 1930/3560 Training loss: 1.5734 0.7796 sec/batch\n",
      "Epoch 11/20  Iteration 1931/3560 Training loss: 1.5731 0.7908 sec/batch\n",
      "Epoch 11/20  Iteration 1932/3560 Training loss: 1.5731 0.7902 sec/batch\n",
      "Epoch 11/20  Iteration 1933/3560 Training loss: 1.5731 0.8896 sec/batch\n",
      "Epoch 11/20  Iteration 1934/3560 Training loss: 1.5731 0.7996 sec/batch\n",
      "Epoch 11/20  Iteration 1935/3560 Training loss: 1.5731 0.7727 sec/batch\n",
      "Epoch 11/20  Iteration 1936/3560 Training loss: 1.5730 0.7690 sec/batch\n",
      "Epoch 11/20  Iteration 1937/3560 Training loss: 1.5731 0.7894 sec/batch\n",
      "Epoch 11/20  Iteration 1938/3560 Training loss: 1.5730 0.7932 sec/batch\n",
      "Epoch 11/20  Iteration 1939/3560 Training loss: 1.5728 0.7617 sec/batch\n",
      "Epoch 11/20  Iteration 1940/3560 Training loss: 1.5729 0.7874 sec/batch\n",
      "Epoch 11/20  Iteration 1941/3560 Training loss: 1.5731 0.8159 sec/batch\n",
      "Epoch 11/20  Iteration 1942/3560 Training loss: 1.5730 0.7959 sec/batch\n",
      "Epoch 11/20  Iteration 1943/3560 Training loss: 1.5731 0.7874 sec/batch\n",
      "Epoch 11/20  Iteration 1944/3560 Training loss: 1.5730 0.7889 sec/batch\n",
      "Epoch 11/20  Iteration 1945/3560 Training loss: 1.5730 0.8082 sec/batch\n",
      "Epoch 11/20  Iteration 1946/3560 Training loss: 1.5730 0.9124 sec/batch\n",
      "Epoch 11/20  Iteration 1947/3560 Training loss: 1.5731 0.7602 sec/batch\n",
      "Epoch 11/20  Iteration 1948/3560 Training loss: 1.5735 0.7668 sec/batch\n",
      "Epoch 11/20  Iteration 1949/3560 Training loss: 1.5734 0.8068 sec/batch\n",
      "Epoch 11/20  Iteration 1950/3560 Training loss: 1.5734 0.7789 sec/batch\n",
      "Epoch 11/20  Iteration 1951/3560 Training loss: 1.5732 0.7712 sec/batch\n",
      "Epoch 11/20  Iteration 1952/3560 Training loss: 1.5731 0.7912 sec/batch\n",
      "Epoch 11/20  Iteration 1953/3560 Training loss: 1.5732 0.7795 sec/batch\n",
      "Epoch 11/20  Iteration 1954/3560 Training loss: 1.5732 0.8246 sec/batch\n",
      "Epoch 11/20  Iteration 1955/3560 Training loss: 1.5732 0.7916 sec/batch\n",
      "Epoch 11/20  Iteration 1956/3560 Training loss: 1.5731 0.7661 sec/batch\n",
      "Epoch 11/20  Iteration 1957/3560 Training loss: 1.5729 0.7926 sec/batch\n",
      "Epoch 11/20  Iteration 1958/3560 Training loss: 1.5729 0.7943 sec/batch\n",
      "Epoch 12/20  Iteration 1959/3560 Training loss: 1.6525 0.8149 sec/batch\n",
      "Epoch 12/20  Iteration 1960/3560 Training loss: 1.6093 0.7612 sec/batch\n",
      "Epoch 12/20  Iteration 1961/3560 Training loss: 1.5926 0.8125 sec/batch\n",
      "Epoch 12/20  Iteration 1962/3560 Training loss: 1.5887 0.8255 sec/batch\n",
      "Epoch 12/20  Iteration 1963/3560 Training loss: 1.5805 0.8631 sec/batch\n",
      "Epoch 12/20  Iteration 1964/3560 Training loss: 1.5701 0.7966 sec/batch\n",
      "Epoch 12/20  Iteration 1965/3560 Training loss: 1.5706 0.8061 sec/batch\n",
      "Epoch 12/20  Iteration 1966/3560 Training loss: 1.5682 0.8708 sec/batch\n",
      "Epoch 12/20  Iteration 1967/3560 Training loss: 1.5695 0.7766 sec/batch\n",
      "Epoch 12/20  Iteration 1968/3560 Training loss: 1.5684 0.7777 sec/batch\n",
      "Epoch 12/20  Iteration 1969/3560 Training loss: 1.5645 0.7950 sec/batch\n",
      "Epoch 12/20  Iteration 1970/3560 Training loss: 1.5630 0.8680 sec/batch\n",
      "Epoch 12/20  Iteration 1971/3560 Training loss: 1.5631 1.0315 sec/batch\n",
      "Epoch 12/20  Iteration 1972/3560 Training loss: 1.5653 1.0151 sec/batch\n",
      "Epoch 12/20  Iteration 1973/3560 Training loss: 1.5657 0.9920 sec/batch\n",
      "Epoch 12/20  Iteration 1974/3560 Training loss: 1.5632 1.1036 sec/batch\n",
      "Epoch 12/20  Iteration 1975/3560 Training loss: 1.5637 1.1264 sec/batch\n",
      "Epoch 12/20  Iteration 1976/3560 Training loss: 1.5652 1.0192 sec/batch\n",
      "Epoch 12/20  Iteration 1977/3560 Training loss: 1.5654 1.0760 sec/batch\n",
      "Epoch 12/20  Iteration 1978/3560 Training loss: 1.5667 0.9736 sec/batch\n",
      "Epoch 12/20  Iteration 1979/3560 Training loss: 1.5657 0.7680 sec/batch\n",
      "Epoch 12/20  Iteration 1980/3560 Training loss: 1.5663 0.8014 sec/batch\n",
      "Epoch 12/20  Iteration 1981/3560 Training loss: 1.5652 0.8011 sec/batch\n",
      "Epoch 12/20  Iteration 1982/3560 Training loss: 1.5654 0.7601 sec/batch\n",
      "Epoch 12/20  Iteration 1983/3560 Training loss: 1.5652 0.7636 sec/batch\n",
      "Epoch 12/20  Iteration 1984/3560 Training loss: 1.5640 0.8206 sec/batch\n",
      "Epoch 12/20  Iteration 1985/3560 Training loss: 1.5623 0.7793 sec/batch\n",
      "Epoch 12/20  Iteration 1986/3560 Training loss: 1.5626 0.7742 sec/batch\n",
      "Epoch 12/20  Iteration 1987/3560 Training loss: 1.5631 0.7814 sec/batch\n",
      "Epoch 12/20  Iteration 1988/3560 Training loss: 1.5635 0.8755 sec/batch\n",
      "Epoch 12/20  Iteration 1989/3560 Training loss: 1.5636 0.8954 sec/batch\n",
      "Epoch 12/20  Iteration 1990/3560 Training loss: 1.5625 0.7940 sec/batch\n",
      "Epoch 12/20  Iteration 1991/3560 Training loss: 1.5631 0.7576 sec/batch\n",
      "Epoch 12/20  Iteration 1992/3560 Training loss: 1.5635 0.7710 sec/batch\n",
      "Epoch 12/20  Iteration 1993/3560 Training loss: 1.5632 0.8038 sec/batch\n",
      "Epoch 12/20  Iteration 1994/3560 Training loss: 1.5629 0.7690 sec/batch\n",
      "Epoch 12/20  Iteration 1995/3560 Training loss: 1.5621 0.7549 sec/batch\n",
      "Epoch 12/20  Iteration 1996/3560 Training loss: 1.5607 0.7883 sec/batch\n",
      "Epoch 12/20  Iteration 1997/3560 Training loss: 1.5593 0.7626 sec/batch\n",
      "Epoch 12/20  Iteration 1998/3560 Training loss: 1.5587 0.7738 sec/batch\n",
      "Epoch 12/20  Iteration 1999/3560 Training loss: 1.5581 0.7955 sec/batch\n",
      "Epoch 12/20  Iteration 2000/3560 Training loss: 1.5585 0.7897 sec/batch\n",
      "Epoch 12/20  Iteration 2001/3560 Training loss: 1.5582 0.7670 sec/batch\n",
      "Epoch 12/20  Iteration 2002/3560 Training loss: 1.5575 0.8394 sec/batch\n",
      "Epoch 12/20  Iteration 2003/3560 Training loss: 1.5576 0.8391 sec/batch\n",
      "Epoch 12/20  Iteration 2004/3560 Training loss: 1.5568 0.8476 sec/batch\n",
      "Epoch 12/20  Iteration 2005/3560 Training loss: 1.5566 0.7650 sec/batch\n",
      "Epoch 12/20  Iteration 2006/3560 Training loss: 1.5560 0.8274 sec/batch\n",
      "Epoch 12/20  Iteration 2007/3560 Training loss: 1.5559 0.7946 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 2008/3560 Training loss: 1.5561 0.7836 sec/batch\n",
      "Epoch 12/20  Iteration 2009/3560 Training loss: 1.5556 0.7696 sec/batch\n",
      "Epoch 12/20  Iteration 2010/3560 Training loss: 1.5563 0.7885 sec/batch\n",
      "Epoch 12/20  Iteration 2011/3560 Training loss: 1.5563 0.7964 sec/batch\n",
      "Epoch 12/20  Iteration 2012/3560 Training loss: 1.5563 0.8007 sec/batch\n",
      "Epoch 12/20  Iteration 2013/3560 Training loss: 1.5563 0.7853 sec/batch\n",
      "Epoch 12/20  Iteration 2014/3560 Training loss: 1.5564 0.7765 sec/batch\n",
      "Epoch 12/20  Iteration 2015/3560 Training loss: 1.5567 0.8039 sec/batch\n",
      "Epoch 12/20  Iteration 2016/3560 Training loss: 1.5564 0.8054 sec/batch\n",
      "Epoch 12/20  Iteration 2017/3560 Training loss: 1.5560 0.7939 sec/batch\n",
      "Epoch 12/20  Iteration 2018/3560 Training loss: 1.5564 0.7966 sec/batch\n",
      "Epoch 12/20  Iteration 2019/3560 Training loss: 1.5565 0.8533 sec/batch\n",
      "Epoch 12/20  Iteration 2020/3560 Training loss: 1.5574 0.7805 sec/batch\n",
      "Epoch 12/20  Iteration 2021/3560 Training loss: 1.5579 0.7567 sec/batch\n",
      "Epoch 12/20  Iteration 2022/3560 Training loss: 1.5580 0.8224 sec/batch\n",
      "Epoch 12/20  Iteration 2023/3560 Training loss: 1.5578 0.7982 sec/batch\n",
      "Epoch 12/20  Iteration 2024/3560 Training loss: 1.5580 0.7906 sec/batch\n",
      "Epoch 12/20  Iteration 2025/3560 Training loss: 1.5582 0.7543 sec/batch\n",
      "Epoch 12/20  Iteration 2026/3560 Training loss: 1.5580 0.7639 sec/batch\n",
      "Epoch 12/20  Iteration 2027/3560 Training loss: 1.5580 0.7931 sec/batch\n",
      "Epoch 12/20  Iteration 2028/3560 Training loss: 1.5579 0.8372 sec/batch\n",
      "Epoch 12/20  Iteration 2029/3560 Training loss: 1.5585 0.7710 sec/batch\n",
      "Epoch 12/20  Iteration 2030/3560 Training loss: 1.5587 0.7948 sec/batch\n",
      "Epoch 12/20  Iteration 2031/3560 Training loss: 1.5591 0.7949 sec/batch\n",
      "Epoch 12/20  Iteration 2032/3560 Training loss: 1.5589 0.8007 sec/batch\n",
      "Epoch 12/20  Iteration 2033/3560 Training loss: 1.5588 0.8496 sec/batch\n",
      "Epoch 12/20  Iteration 2034/3560 Training loss: 1.5590 0.8191 sec/batch\n",
      "Epoch 12/20  Iteration 2035/3560 Training loss: 1.5588 0.8027 sec/batch\n",
      "Epoch 12/20  Iteration 2036/3560 Training loss: 1.5589 0.7710 sec/batch\n",
      "Epoch 12/20  Iteration 2037/3560 Training loss: 1.5583 0.8092 sec/batch\n",
      "Epoch 12/20  Iteration 2038/3560 Training loss: 1.5580 0.7634 sec/batch\n",
      "Epoch 12/20  Iteration 2039/3560 Training loss: 1.5575 0.7916 sec/batch\n",
      "Epoch 12/20  Iteration 2040/3560 Training loss: 1.5576 0.8760 sec/batch\n",
      "Epoch 12/20  Iteration 2041/3560 Training loss: 1.5571 0.7997 sec/batch\n",
      "Epoch 12/20  Iteration 2042/3560 Training loss: 1.5570 0.7865 sec/batch\n",
      "Epoch 12/20  Iteration 2043/3560 Training loss: 1.5567 0.8288 sec/batch\n",
      "Epoch 12/20  Iteration 2044/3560 Training loss: 1.5564 0.7953 sec/batch\n",
      "Epoch 12/20  Iteration 2045/3560 Training loss: 1.5561 0.7865 sec/batch\n",
      "Epoch 12/20  Iteration 2046/3560 Training loss: 1.5558 0.7614 sec/batch\n",
      "Epoch 12/20  Iteration 2047/3560 Training loss: 1.5554 0.7896 sec/batch\n",
      "Epoch 12/20  Iteration 2048/3560 Training loss: 1.5554 0.7857 sec/batch\n",
      "Epoch 12/20  Iteration 2049/3560 Training loss: 1.5552 0.7884 sec/batch\n",
      "Epoch 12/20  Iteration 2050/3560 Training loss: 1.5550 0.7939 sec/batch\n",
      "Epoch 12/20  Iteration 2051/3560 Training loss: 1.5547 0.8007 sec/batch\n",
      "Epoch 12/20  Iteration 2052/3560 Training loss: 1.5543 0.8011 sec/batch\n",
      "Epoch 12/20  Iteration 2053/3560 Training loss: 1.5540 0.8090 sec/batch\n",
      "Epoch 12/20  Iteration 2054/3560 Training loss: 1.5540 0.8055 sec/batch\n",
      "Epoch 12/20  Iteration 2055/3560 Training loss: 1.5539 0.8041 sec/batch\n",
      "Epoch 12/20  Iteration 2056/3560 Training loss: 1.5536 0.7727 sec/batch\n",
      "Epoch 12/20  Iteration 2057/3560 Training loss: 1.5533 0.8510 sec/batch\n",
      "Epoch 12/20  Iteration 2058/3560 Training loss: 1.5527 0.7950 sec/batch\n",
      "Epoch 12/20  Iteration 2059/3560 Training loss: 1.5526 0.7669 sec/batch\n",
      "Epoch 12/20  Iteration 2060/3560 Training loss: 1.5525 0.8213 sec/batch\n",
      "Epoch 12/20  Iteration 2061/3560 Training loss: 1.5524 0.7599 sec/batch\n",
      "Epoch 12/20  Iteration 2062/3560 Training loss: 1.5521 0.7726 sec/batch\n",
      "Epoch 12/20  Iteration 2063/3560 Training loss: 1.5520 0.7656 sec/batch\n",
      "Epoch 12/20  Iteration 2064/3560 Training loss: 1.5520 0.7575 sec/batch\n",
      "Epoch 12/20  Iteration 2065/3560 Training loss: 1.5520 0.8250 sec/batch\n",
      "Epoch 12/20  Iteration 2066/3560 Training loss: 1.5519 0.8345 sec/batch\n",
      "Epoch 12/20  Iteration 2067/3560 Training loss: 1.5518 0.7489 sec/batch\n",
      "Epoch 12/20  Iteration 2068/3560 Training loss: 1.5518 0.8392 sec/batch\n",
      "Epoch 12/20  Iteration 2069/3560 Training loss: 1.5517 0.7707 sec/batch\n",
      "Epoch 12/20  Iteration 2070/3560 Training loss: 1.5515 0.8285 sec/batch\n",
      "Epoch 12/20  Iteration 2071/3560 Training loss: 1.5513 0.7804 sec/batch\n",
      "Epoch 12/20  Iteration 2072/3560 Training loss: 1.5511 0.8061 sec/batch\n",
      "Epoch 12/20  Iteration 2073/3560 Training loss: 1.5509 0.8732 sec/batch\n",
      "Epoch 12/20  Iteration 2074/3560 Training loss: 1.5503 0.7883 sec/batch\n",
      "Epoch 12/20  Iteration 2075/3560 Training loss: 1.5502 0.7975 sec/batch\n",
      "Epoch 12/20  Iteration 2076/3560 Training loss: 1.5503 0.7874 sec/batch\n",
      "Epoch 12/20  Iteration 2077/3560 Training loss: 1.5501 0.7743 sec/batch\n",
      "Epoch 12/20  Iteration 2078/3560 Training loss: 1.5499 0.8349 sec/batch\n",
      "Epoch 12/20  Iteration 2079/3560 Training loss: 1.5499 0.7713 sec/batch\n",
      "Epoch 12/20  Iteration 2080/3560 Training loss: 1.5494 0.7903 sec/batch\n",
      "Epoch 12/20  Iteration 2081/3560 Training loss: 1.5490 0.7871 sec/batch\n",
      "Epoch 12/20  Iteration 2082/3560 Training loss: 1.5491 0.7523 sec/batch\n",
      "Epoch 12/20  Iteration 2083/3560 Training loss: 1.5490 0.7648 sec/batch\n",
      "Epoch 12/20  Iteration 2084/3560 Training loss: 1.5486 0.7912 sec/batch\n",
      "Epoch 12/20  Iteration 2085/3560 Training loss: 1.5487 0.8107 sec/batch\n",
      "Epoch 12/20  Iteration 2086/3560 Training loss: 1.5487 0.8002 sec/batch\n",
      "Epoch 12/20  Iteration 2087/3560 Training loss: 1.5486 0.8084 sec/batch\n",
      "Epoch 12/20  Iteration 2088/3560 Training loss: 1.5484 0.9084 sec/batch\n",
      "Epoch 12/20  Iteration 2089/3560 Training loss: 1.5479 0.7804 sec/batch\n",
      "Epoch 12/20  Iteration 2090/3560 Training loss: 1.5476 0.8156 sec/batch\n",
      "Epoch 12/20  Iteration 2091/3560 Training loss: 1.5478 0.8213 sec/batch\n",
      "Epoch 12/20  Iteration 2092/3560 Training loss: 1.5478 0.8244 sec/batch\n",
      "Epoch 12/20  Iteration 2093/3560 Training loss: 1.5477 0.7673 sec/batch\n",
      "Epoch 12/20  Iteration 2094/3560 Training loss: 1.5478 0.7864 sec/batch\n",
      "Epoch 12/20  Iteration 2095/3560 Training loss: 1.5479 0.7669 sec/batch\n",
      "Epoch 12/20  Iteration 2096/3560 Training loss: 1.5480 0.8037 sec/batch\n",
      "Epoch 12/20  Iteration 2097/3560 Training loss: 1.5480 0.7894 sec/batch\n",
      "Epoch 12/20  Iteration 2098/3560 Training loss: 1.5480 0.8191 sec/batch\n",
      "Epoch 12/20  Iteration 2099/3560 Training loss: 1.5482 0.7712 sec/batch\n",
      "Epoch 12/20  Iteration 2100/3560 Training loss: 1.5482 0.8211 sec/batch\n",
      "Epoch 12/20  Iteration 2101/3560 Training loss: 1.5482 0.7933 sec/batch\n",
      "Epoch 12/20  Iteration 2102/3560 Training loss: 1.5482 0.8115 sec/batch\n",
      "Epoch 12/20  Iteration 2103/3560 Training loss: 1.5481 0.9103 sec/batch\n",
      "Epoch 12/20  Iteration 2104/3560 Training loss: 1.5483 0.8532 sec/batch\n",
      "Epoch 12/20  Iteration 2105/3560 Training loss: 1.5482 0.7848 sec/batch\n",
      "Epoch 12/20  Iteration 2106/3560 Training loss: 1.5484 0.7919 sec/batch\n",
      "Epoch 12/20  Iteration 2107/3560 Training loss: 1.5484 0.7618 sec/batch\n",
      "Epoch 12/20  Iteration 2108/3560 Training loss: 1.5482 0.7889 sec/batch\n",
      "Epoch 12/20  Iteration 2109/3560 Training loss: 1.5478 0.7641 sec/batch\n",
      "Epoch 12/20  Iteration 2110/3560 Training loss: 1.5479 0.7673 sec/batch\n",
      "Epoch 12/20  Iteration 2111/3560 Training loss: 1.5478 0.8010 sec/batch\n",
      "Epoch 12/20  Iteration 2112/3560 Training loss: 1.5478 0.7568 sec/batch\n",
      "Epoch 12/20  Iteration 2113/3560 Training loss: 1.5478 0.8058 sec/batch\n",
      "Epoch 12/20  Iteration 2114/3560 Training loss: 1.5478 0.7669 sec/batch\n",
      "Epoch 12/20  Iteration 2115/3560 Training loss: 1.5478 0.7956 sec/batch\n",
      "Epoch 12/20  Iteration 2116/3560 Training loss: 1.5478 0.8618 sec/batch\n",
      "Epoch 12/20  Iteration 2117/3560 Training loss: 1.5475 0.8285 sec/batch\n",
      "Epoch 12/20  Iteration 2118/3560 Training loss: 1.5476 0.8644 sec/batch\n",
      "Epoch 12/20  Iteration 2119/3560 Training loss: 1.5478 0.7821 sec/batch\n",
      "Epoch 12/20  Iteration 2120/3560 Training loss: 1.5478 0.7869 sec/batch\n",
      "Epoch 12/20  Iteration 2121/3560 Training loss: 1.5478 0.7776 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 2122/3560 Training loss: 1.5477 0.7973 sec/batch\n",
      "Epoch 12/20  Iteration 2123/3560 Training loss: 1.5477 0.7815 sec/batch\n",
      "Epoch 12/20  Iteration 2124/3560 Training loss: 1.5476 0.7795 sec/batch\n",
      "Epoch 12/20  Iteration 2125/3560 Training loss: 1.5478 0.7688 sec/batch\n",
      "Epoch 12/20  Iteration 2126/3560 Training loss: 1.5482 0.8023 sec/batch\n",
      "Epoch 12/20  Iteration 2127/3560 Training loss: 1.5481 0.7969 sec/batch\n",
      "Epoch 12/20  Iteration 2128/3560 Training loss: 1.5482 0.7783 sec/batch\n",
      "Epoch 12/20  Iteration 2129/3560 Training loss: 1.5480 0.8209 sec/batch\n",
      "Epoch 12/20  Iteration 2130/3560 Training loss: 1.5478 0.8239 sec/batch\n",
      "Epoch 12/20  Iteration 2131/3560 Training loss: 1.5479 0.7642 sec/batch\n",
      "Epoch 12/20  Iteration 2132/3560 Training loss: 1.5479 0.7821 sec/batch\n",
      "Epoch 12/20  Iteration 2133/3560 Training loss: 1.5480 0.8285 sec/batch\n",
      "Epoch 12/20  Iteration 2134/3560 Training loss: 1.5479 0.8170 sec/batch\n",
      "Epoch 12/20  Iteration 2135/3560 Training loss: 1.5477 0.7925 sec/batch\n",
      "Epoch 12/20  Iteration 2136/3560 Training loss: 1.5478 0.7940 sec/batch\n",
      "Epoch 13/20  Iteration 2137/3560 Training loss: 1.6391 0.7713 sec/batch\n",
      "Epoch 13/20  Iteration 2138/3560 Training loss: 1.5899 0.8195 sec/batch\n",
      "Epoch 13/20  Iteration 2139/3560 Training loss: 1.5736 0.7606 sec/batch\n",
      "Epoch 13/20  Iteration 2140/3560 Training loss: 1.5687 0.8064 sec/batch\n",
      "Epoch 13/20  Iteration 2141/3560 Training loss: 1.5605 0.8342 sec/batch\n",
      "Epoch 13/20  Iteration 2142/3560 Training loss: 1.5502 0.7733 sec/batch\n",
      "Epoch 13/20  Iteration 2143/3560 Training loss: 1.5509 0.7584 sec/batch\n",
      "Epoch 13/20  Iteration 2144/3560 Training loss: 1.5480 0.8036 sec/batch\n",
      "Epoch 13/20  Iteration 2145/3560 Training loss: 1.5493 0.8176 sec/batch\n",
      "Epoch 13/20  Iteration 2146/3560 Training loss: 1.5485 0.7749 sec/batch\n",
      "Epoch 13/20  Iteration 2147/3560 Training loss: 1.5450 0.7636 sec/batch\n",
      "Epoch 13/20  Iteration 2148/3560 Training loss: 1.5443 0.7878 sec/batch\n",
      "Epoch 13/20  Iteration 2149/3560 Training loss: 1.5434 0.7634 sec/batch\n",
      "Epoch 13/20  Iteration 2150/3560 Training loss: 1.5448 0.7913 sec/batch\n",
      "Epoch 13/20  Iteration 2151/3560 Training loss: 1.5434 0.7976 sec/batch\n",
      "Epoch 13/20  Iteration 2152/3560 Training loss: 1.5416 0.7741 sec/batch\n",
      "Epoch 13/20  Iteration 2153/3560 Training loss: 1.5420 0.7639 sec/batch\n",
      "Epoch 13/20  Iteration 2154/3560 Training loss: 1.5440 0.8147 sec/batch\n",
      "Epoch 13/20  Iteration 2155/3560 Training loss: 1.5435 0.8376 sec/batch\n",
      "Epoch 13/20  Iteration 2156/3560 Training loss: 1.5444 0.7615 sec/batch\n",
      "Epoch 13/20  Iteration 2157/3560 Training loss: 1.5438 0.7959 sec/batch\n",
      "Epoch 13/20  Iteration 2158/3560 Training loss: 1.5447 0.7635 sec/batch\n",
      "Epoch 13/20  Iteration 2159/3560 Training loss: 1.5437 0.7961 sec/batch\n",
      "Epoch 13/20  Iteration 2160/3560 Training loss: 1.5439 0.7666 sec/batch\n",
      "Epoch 13/20  Iteration 2161/3560 Training loss: 1.5436 0.7695 sec/batch\n",
      "Epoch 13/20  Iteration 2162/3560 Training loss: 1.5420 0.7904 sec/batch\n",
      "Epoch 13/20  Iteration 2163/3560 Training loss: 1.5406 0.7586 sec/batch\n",
      "Epoch 13/20  Iteration 2164/3560 Training loss: 1.5409 0.7777 sec/batch\n",
      "Epoch 13/20  Iteration 2165/3560 Training loss: 1.5411 0.7892 sec/batch\n",
      "Epoch 13/20  Iteration 2166/3560 Training loss: 1.5416 0.7754 sec/batch\n",
      "Epoch 13/20  Iteration 2167/3560 Training loss: 1.5415 0.8212 sec/batch\n",
      "Epoch 13/20  Iteration 2168/3560 Training loss: 1.5408 0.8060 sec/batch\n",
      "Epoch 13/20  Iteration 2169/3560 Training loss: 1.5411 0.7669 sec/batch\n",
      "Epoch 13/20  Iteration 2170/3560 Training loss: 1.5417 0.7951 sec/batch\n",
      "Epoch 13/20  Iteration 2171/3560 Training loss: 1.5415 0.8122 sec/batch\n",
      "Epoch 13/20  Iteration 2172/3560 Training loss: 1.5412 0.8100 sec/batch\n",
      "Epoch 13/20  Iteration 2173/3560 Training loss: 1.5403 0.7964 sec/batch\n",
      "Epoch 13/20  Iteration 2174/3560 Training loss: 1.5391 0.7721 sec/batch\n",
      "Epoch 13/20  Iteration 2175/3560 Training loss: 1.5379 0.7744 sec/batch\n",
      "Epoch 13/20  Iteration 2176/3560 Training loss: 1.5373 0.7951 sec/batch\n",
      "Epoch 13/20  Iteration 2177/3560 Training loss: 1.5368 0.7985 sec/batch\n",
      "Epoch 13/20  Iteration 2178/3560 Training loss: 1.5374 0.7981 sec/batch\n",
      "Epoch 13/20  Iteration 2179/3560 Training loss: 1.5369 0.8129 sec/batch\n",
      "Epoch 13/20  Iteration 2180/3560 Training loss: 1.5361 0.8099 sec/batch\n",
      "Epoch 13/20  Iteration 2181/3560 Training loss: 1.5362 0.7706 sec/batch\n",
      "Epoch 13/20  Iteration 2182/3560 Training loss: 1.5352 0.7882 sec/batch\n",
      "Epoch 13/20  Iteration 2183/3560 Training loss: 1.5349 0.7702 sec/batch\n",
      "Epoch 13/20  Iteration 2184/3560 Training loss: 1.5345 0.7973 sec/batch\n",
      "Epoch 13/20  Iteration 2185/3560 Training loss: 1.5341 0.8017 sec/batch\n",
      "Epoch 13/20  Iteration 2186/3560 Training loss: 1.5345 0.7696 sec/batch\n",
      "Epoch 13/20  Iteration 2187/3560 Training loss: 1.5340 0.7810 sec/batch\n",
      "Epoch 13/20  Iteration 2188/3560 Training loss: 1.5349 0.7885 sec/batch\n",
      "Epoch 13/20  Iteration 2189/3560 Training loss: 1.5348 0.7946 sec/batch\n",
      "Epoch 13/20  Iteration 2190/3560 Training loss: 1.5349 0.7817 sec/batch\n",
      "Epoch 13/20  Iteration 2191/3560 Training loss: 1.5347 0.7972 sec/batch\n",
      "Epoch 13/20  Iteration 2192/3560 Training loss: 1.5349 0.7876 sec/batch\n",
      "Epoch 13/20  Iteration 2193/3560 Training loss: 1.5353 0.8590 sec/batch\n",
      "Epoch 13/20  Iteration 2194/3560 Training loss: 1.5350 0.9463 sec/batch\n",
      "Epoch 13/20  Iteration 2195/3560 Training loss: 1.5346 1.0144 sec/batch\n",
      "Epoch 13/20  Iteration 2196/3560 Training loss: 1.5350 0.9781 sec/batch\n",
      "Epoch 13/20  Iteration 2197/3560 Training loss: 1.5350 1.0463 sec/batch\n",
      "Epoch 13/20  Iteration 2198/3560 Training loss: 1.5359 1.0350 sec/batch\n",
      "Epoch 13/20  Iteration 2199/3560 Training loss: 1.5363 1.0362 sec/batch\n",
      "Epoch 13/20  Iteration 2200/3560 Training loss: 1.5363 1.0150 sec/batch\n",
      "Epoch 13/20  Iteration 2201/3560 Training loss: 1.5361 1.0106 sec/batch\n",
      "Epoch 13/20  Iteration 2202/3560 Training loss: 1.5362 0.8413 sec/batch\n",
      "Epoch 13/20  Iteration 2203/3560 Training loss: 1.5364 0.8156 sec/batch\n",
      "Epoch 13/20  Iteration 2204/3560 Training loss: 1.5362 0.8373 sec/batch\n",
      "Epoch 13/20  Iteration 2205/3560 Training loss: 1.5362 0.7921 sec/batch\n",
      "Epoch 13/20  Iteration 2206/3560 Training loss: 1.5360 0.7907 sec/batch\n",
      "Epoch 13/20  Iteration 2207/3560 Training loss: 1.5365 0.7619 sec/batch\n",
      "Epoch 13/20  Iteration 2208/3560 Training loss: 1.5367 0.7748 sec/batch\n",
      "Epoch 13/20  Iteration 2209/3560 Training loss: 1.5372 0.7796 sec/batch\n",
      "Epoch 13/20  Iteration 2210/3560 Training loss: 1.5370 0.7882 sec/batch\n",
      "Epoch 13/20  Iteration 2211/3560 Training loss: 1.5368 0.7682 sec/batch\n",
      "Epoch 13/20  Iteration 2212/3560 Training loss: 1.5371 0.7543 sec/batch\n",
      "Epoch 13/20  Iteration 2213/3560 Training loss: 1.5369 0.7853 sec/batch\n",
      "Epoch 13/20  Iteration 2214/3560 Training loss: 1.5370 0.7666 sec/batch\n",
      "Epoch 13/20  Iteration 2215/3560 Training loss: 1.5363 0.7689 sec/batch\n",
      "Epoch 13/20  Iteration 2216/3560 Training loss: 1.5362 0.8365 sec/batch\n",
      "Epoch 13/20  Iteration 2217/3560 Training loss: 1.5356 0.7768 sec/batch\n",
      "Epoch 13/20  Iteration 2218/3560 Training loss: 1.5355 0.8162 sec/batch\n",
      "Epoch 13/20  Iteration 2219/3560 Training loss: 1.5349 0.7857 sec/batch\n",
      "Epoch 13/20  Iteration 2220/3560 Training loss: 1.5348 0.7906 sec/batch\n",
      "Epoch 13/20  Iteration 2221/3560 Training loss: 1.5345 0.7887 sec/batch\n",
      "Epoch 13/20  Iteration 2222/3560 Training loss: 1.5342 0.8037 sec/batch\n",
      "Epoch 13/20  Iteration 2223/3560 Training loss: 1.5340 0.7537 sec/batch\n",
      "Epoch 13/20  Iteration 2224/3560 Training loss: 1.5338 0.8097 sec/batch\n",
      "Epoch 13/20  Iteration 2225/3560 Training loss: 1.5334 0.7598 sec/batch\n",
      "Epoch 13/20  Iteration 2226/3560 Training loss: 1.5335 0.7783 sec/batch\n",
      "Epoch 13/20  Iteration 2227/3560 Training loss: 1.5332 0.7687 sec/batch\n",
      "Epoch 13/20  Iteration 2228/3560 Training loss: 1.5330 0.8018 sec/batch\n",
      "Epoch 13/20  Iteration 2229/3560 Training loss: 1.5327 0.8317 sec/batch\n",
      "Epoch 13/20  Iteration 2230/3560 Training loss: 1.5324 0.7990 sec/batch\n",
      "Epoch 13/20  Iteration 2231/3560 Training loss: 1.5321 0.7994 sec/batch\n",
      "Epoch 13/20  Iteration 2232/3560 Training loss: 1.5322 0.7700 sec/batch\n",
      "Epoch 13/20  Iteration 2233/3560 Training loss: 1.5322 0.7918 sec/batch\n",
      "Epoch 13/20  Iteration 2234/3560 Training loss: 1.5317 0.8148 sec/batch\n",
      "Epoch 13/20  Iteration 2235/3560 Training loss: 1.5314 0.7695 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2236/3560 Training loss: 1.5310 0.7983 sec/batch\n",
      "Epoch 13/20  Iteration 2237/3560 Training loss: 1.5310 0.7911 sec/batch\n",
      "Epoch 13/20  Iteration 2238/3560 Training loss: 1.5307 0.7896 sec/batch\n",
      "Epoch 13/20  Iteration 2239/3560 Training loss: 1.5307 0.8117 sec/batch\n",
      "Epoch 13/20  Iteration 2240/3560 Training loss: 1.5306 0.7799 sec/batch\n",
      "Epoch 13/20  Iteration 2241/3560 Training loss: 1.5305 0.7962 sec/batch\n",
      "Epoch 13/20  Iteration 2242/3560 Training loss: 1.5304 0.8400 sec/batch\n",
      "Epoch 13/20  Iteration 2243/3560 Training loss: 1.5303 0.7747 sec/batch\n",
      "Epoch 13/20  Iteration 2244/3560 Training loss: 1.5303 0.7628 sec/batch\n",
      "Epoch 13/20  Iteration 2245/3560 Training loss: 1.5302 0.7625 sec/batch\n",
      "Epoch 13/20  Iteration 2246/3560 Training loss: 1.5303 0.7716 sec/batch\n",
      "Epoch 13/20  Iteration 2247/3560 Training loss: 1.5301 0.7983 sec/batch\n",
      "Epoch 13/20  Iteration 2248/3560 Training loss: 1.5301 0.7595 sec/batch\n",
      "Epoch 13/20  Iteration 2249/3560 Training loss: 1.5298 0.7758 sec/batch\n",
      "Epoch 13/20  Iteration 2250/3560 Training loss: 1.5296 0.7625 sec/batch\n",
      "Epoch 13/20  Iteration 2251/3560 Training loss: 1.5294 0.7566 sec/batch\n",
      "Epoch 13/20  Iteration 2252/3560 Training loss: 1.5290 0.8013 sec/batch\n",
      "Epoch 13/20  Iteration 2253/3560 Training loss: 1.5289 0.7693 sec/batch\n",
      "Epoch 13/20  Iteration 2254/3560 Training loss: 1.5289 0.7849 sec/batch\n",
      "Epoch 13/20  Iteration 2255/3560 Training loss: 1.5288 0.8374 sec/batch\n",
      "Epoch 13/20  Iteration 2256/3560 Training loss: 1.5286 0.7878 sec/batch\n",
      "Epoch 13/20  Iteration 2257/3560 Training loss: 1.5285 0.7920 sec/batch\n",
      "Epoch 13/20  Iteration 2258/3560 Training loss: 1.5281 0.7631 sec/batch\n",
      "Epoch 13/20  Iteration 2259/3560 Training loss: 1.5277 0.7923 sec/batch\n",
      "Epoch 13/20  Iteration 2260/3560 Training loss: 1.5277 0.7761 sec/batch\n",
      "Epoch 13/20  Iteration 2261/3560 Training loss: 1.5275 0.7836 sec/batch\n",
      "Epoch 13/20  Iteration 2262/3560 Training loss: 1.5271 0.7882 sec/batch\n",
      "Epoch 13/20  Iteration 2263/3560 Training loss: 1.5272 0.7973 sec/batch\n",
      "Epoch 13/20  Iteration 2264/3560 Training loss: 1.5273 0.8075 sec/batch\n",
      "Epoch 13/20  Iteration 2265/3560 Training loss: 1.5271 0.8219 sec/batch\n",
      "Epoch 13/20  Iteration 2266/3560 Training loss: 1.5269 0.7905 sec/batch\n",
      "Epoch 13/20  Iteration 2267/3560 Training loss: 1.5265 0.8123 sec/batch\n",
      "Epoch 13/20  Iteration 2268/3560 Training loss: 1.5263 0.8521 sec/batch\n",
      "Epoch 13/20  Iteration 2269/3560 Training loss: 1.5264 0.7899 sec/batch\n",
      "Epoch 13/20  Iteration 2270/3560 Training loss: 1.5263 0.8044 sec/batch\n",
      "Epoch 13/20  Iteration 2271/3560 Training loss: 1.5263 0.7481 sec/batch\n",
      "Epoch 13/20  Iteration 2272/3560 Training loss: 1.5263 0.8162 sec/batch\n",
      "Epoch 13/20  Iteration 2273/3560 Training loss: 1.5264 0.8041 sec/batch\n",
      "Epoch 13/20  Iteration 2274/3560 Training loss: 1.5265 0.7829 sec/batch\n",
      "Epoch 13/20  Iteration 2275/3560 Training loss: 1.5265 0.8118 sec/batch\n",
      "Epoch 13/20  Iteration 2276/3560 Training loss: 1.5264 0.7960 sec/batch\n",
      "Epoch 13/20  Iteration 2277/3560 Training loss: 1.5267 0.8060 sec/batch\n",
      "Epoch 13/20  Iteration 2278/3560 Training loss: 1.5267 0.7916 sec/batch\n",
      "Epoch 13/20  Iteration 2279/3560 Training loss: 1.5267 0.7912 sec/batch\n",
      "Epoch 13/20  Iteration 2280/3560 Training loss: 1.5268 0.8201 sec/batch\n",
      "Epoch 13/20  Iteration 2281/3560 Training loss: 1.5267 0.8296 sec/batch\n",
      "Epoch 13/20  Iteration 2282/3560 Training loss: 1.5268 0.8006 sec/batch\n",
      "Epoch 13/20  Iteration 2283/3560 Training loss: 1.5268 0.7951 sec/batch\n",
      "Epoch 13/20  Iteration 2284/3560 Training loss: 1.5269 0.7747 sec/batch\n",
      "Epoch 13/20  Iteration 2285/3560 Training loss: 1.5270 0.8199 sec/batch\n",
      "Epoch 13/20  Iteration 2286/3560 Training loss: 1.5268 0.7686 sec/batch\n",
      "Epoch 13/20  Iteration 2287/3560 Training loss: 1.5266 0.7872 sec/batch\n",
      "Epoch 13/20  Iteration 2288/3560 Training loss: 1.5266 0.7951 sec/batch\n",
      "Epoch 13/20  Iteration 2289/3560 Training loss: 1.5266 0.7766 sec/batch\n",
      "Epoch 13/20  Iteration 2290/3560 Training loss: 1.5266 0.8235 sec/batch\n",
      "Epoch 13/20  Iteration 2291/3560 Training loss: 1.5266 0.7966 sec/batch\n",
      "Epoch 13/20  Iteration 2292/3560 Training loss: 1.5266 0.9183 sec/batch\n",
      "Epoch 13/20  Iteration 2293/3560 Training loss: 1.5267 0.8971 sec/batch\n",
      "Epoch 13/20  Iteration 2294/3560 Training loss: 1.5267 0.8661 sec/batch\n",
      "Epoch 13/20  Iteration 2295/3560 Training loss: 1.5264 0.8396 sec/batch\n",
      "Epoch 13/20  Iteration 2296/3560 Training loss: 1.5265 0.8360 sec/batch\n",
      "Epoch 13/20  Iteration 2297/3560 Training loss: 1.5268 0.8354 sec/batch\n",
      "Epoch 13/20  Iteration 2298/3560 Training loss: 1.5267 0.8537 sec/batch\n",
      "Epoch 13/20  Iteration 2299/3560 Training loss: 1.5267 0.8053 sec/batch\n",
      "Epoch 13/20  Iteration 2300/3560 Training loss: 1.5267 0.7660 sec/batch\n",
      "Epoch 13/20  Iteration 2301/3560 Training loss: 1.5268 0.7867 sec/batch\n",
      "Epoch 13/20  Iteration 2302/3560 Training loss: 1.5267 0.7972 sec/batch\n",
      "Epoch 13/20  Iteration 2303/3560 Training loss: 1.5268 0.7918 sec/batch\n",
      "Epoch 13/20  Iteration 2304/3560 Training loss: 1.5272 0.7684 sec/batch\n",
      "Epoch 13/20  Iteration 2305/3560 Training loss: 1.5271 0.8243 sec/batch\n",
      "Epoch 13/20  Iteration 2306/3560 Training loss: 1.5271 0.7768 sec/batch\n",
      "Epoch 13/20  Iteration 2307/3560 Training loss: 1.5270 0.8278 sec/batch\n",
      "Epoch 13/20  Iteration 2308/3560 Training loss: 1.5268 0.7988 sec/batch\n",
      "Epoch 13/20  Iteration 2309/3560 Training loss: 1.5269 0.8135 sec/batch\n",
      "Epoch 13/20  Iteration 2310/3560 Training loss: 1.5269 0.8016 sec/batch\n",
      "Epoch 13/20  Iteration 2311/3560 Training loss: 1.5269 0.8773 sec/batch\n",
      "Epoch 13/20  Iteration 2312/3560 Training loss: 1.5268 0.7951 sec/batch\n",
      "Epoch 13/20  Iteration 2313/3560 Training loss: 1.5267 0.7660 sec/batch\n",
      "Epoch 13/20  Iteration 2314/3560 Training loss: 1.5268 0.7579 sec/batch\n",
      "Epoch 14/20  Iteration 2315/3560 Training loss: 1.6227 0.8066 sec/batch\n",
      "Epoch 14/20  Iteration 2316/3560 Training loss: 1.5736 0.7705 sec/batch\n",
      "Epoch 14/20  Iteration 2317/3560 Training loss: 1.5563 0.8087 sec/batch\n",
      "Epoch 14/20  Iteration 2318/3560 Training loss: 1.5511 0.8299 sec/batch\n",
      "Epoch 14/20  Iteration 2319/3560 Training loss: 1.5416 0.7632 sec/batch\n",
      "Epoch 14/20  Iteration 2320/3560 Training loss: 1.5318 0.7883 sec/batch\n",
      "Epoch 14/20  Iteration 2321/3560 Training loss: 1.5325 0.8114 sec/batch\n",
      "Epoch 14/20  Iteration 2322/3560 Training loss: 1.5302 0.8036 sec/batch\n",
      "Epoch 14/20  Iteration 2323/3560 Training loss: 1.5316 0.7651 sec/batch\n",
      "Epoch 14/20  Iteration 2324/3560 Training loss: 1.5292 0.7885 sec/batch\n",
      "Epoch 14/20  Iteration 2325/3560 Training loss: 1.5260 0.8035 sec/batch\n",
      "Epoch 14/20  Iteration 2326/3560 Training loss: 1.5244 0.7892 sec/batch\n",
      "Epoch 14/20  Iteration 2327/3560 Training loss: 1.5244 0.7988 sec/batch\n",
      "Epoch 14/20  Iteration 2328/3560 Training loss: 1.5262 0.8001 sec/batch\n",
      "Epoch 14/20  Iteration 2329/3560 Training loss: 1.5252 0.8058 sec/batch\n",
      "Epoch 14/20  Iteration 2330/3560 Training loss: 1.5230 0.7853 sec/batch\n",
      "Epoch 14/20  Iteration 2331/3560 Training loss: 1.5236 0.8167 sec/batch\n",
      "Epoch 14/20  Iteration 2332/3560 Training loss: 1.5250 0.7899 sec/batch\n",
      "Epoch 14/20  Iteration 2333/3560 Training loss: 1.5247 0.8048 sec/batch\n",
      "Epoch 14/20  Iteration 2334/3560 Training loss: 1.5254 0.7934 sec/batch\n",
      "Epoch 14/20  Iteration 2335/3560 Training loss: 1.5248 0.7931 sec/batch\n",
      "Epoch 14/20  Iteration 2336/3560 Training loss: 1.5255 0.7938 sec/batch\n",
      "Epoch 14/20  Iteration 2337/3560 Training loss: 1.5247 0.8181 sec/batch\n",
      "Epoch 14/20  Iteration 2338/3560 Training loss: 1.5243 0.7770 sec/batch\n",
      "Epoch 14/20  Iteration 2339/3560 Training loss: 1.5236 0.8210 sec/batch\n",
      "Epoch 14/20  Iteration 2340/3560 Training loss: 1.5220 0.9861 sec/batch\n",
      "Epoch 14/20  Iteration 2341/3560 Training loss: 1.5210 0.8973 sec/batch\n",
      "Epoch 14/20  Iteration 2342/3560 Training loss: 1.5213 0.8445 sec/batch\n",
      "Epoch 14/20  Iteration 2343/3560 Training loss: 1.5219 1.0537 sec/batch\n",
      "Epoch 14/20  Iteration 2344/3560 Training loss: 1.5222 0.7769 sec/batch\n",
      "Epoch 14/20  Iteration 2345/3560 Training loss: 1.5223 0.8122 sec/batch\n",
      "Epoch 14/20  Iteration 2346/3560 Training loss: 1.5215 0.8461 sec/batch\n",
      "Epoch 14/20  Iteration 2347/3560 Training loss: 1.5219 0.7910 sec/batch\n",
      "Epoch 14/20  Iteration 2348/3560 Training loss: 1.5225 0.8011 sec/batch\n",
      "Epoch 14/20  Iteration 2349/3560 Training loss: 1.5222 0.8408 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2350/3560 Training loss: 1.5218 0.8095 sec/batch\n",
      "Epoch 14/20  Iteration 2351/3560 Training loss: 1.5211 0.9473 sec/batch\n",
      "Epoch 14/20  Iteration 2352/3560 Training loss: 1.5201 1.1611 sec/batch\n",
      "Epoch 14/20  Iteration 2353/3560 Training loss: 1.5188 1.0092 sec/batch\n",
      "Epoch 14/20  Iteration 2354/3560 Training loss: 1.5178 1.0005 sec/batch\n",
      "Epoch 14/20  Iteration 2355/3560 Training loss: 1.5174 0.9048 sec/batch\n",
      "Epoch 14/20  Iteration 2356/3560 Training loss: 1.5180 1.0697 sec/batch\n",
      "Epoch 14/20  Iteration 2357/3560 Training loss: 1.5176 0.9771 sec/batch\n",
      "Epoch 14/20  Iteration 2358/3560 Training loss: 1.5170 0.9474 sec/batch\n",
      "Epoch 14/20  Iteration 2359/3560 Training loss: 1.5171 1.0611 sec/batch\n",
      "Epoch 14/20  Iteration 2360/3560 Training loss: 1.5164 0.9872 sec/batch\n",
      "Epoch 14/20  Iteration 2361/3560 Training loss: 1.5162 0.9362 sec/batch\n",
      "Epoch 14/20  Iteration 2362/3560 Training loss: 1.5158 0.9497 sec/batch\n",
      "Epoch 14/20  Iteration 2363/3560 Training loss: 1.5157 0.8717 sec/batch\n",
      "Epoch 14/20  Iteration 2364/3560 Training loss: 1.5159 0.9919 sec/batch\n",
      "Epoch 14/20  Iteration 2365/3560 Training loss: 1.5157 0.9500 sec/batch\n",
      "Epoch 14/20  Iteration 2366/3560 Training loss: 1.5163 0.8808 sec/batch\n",
      "Epoch 14/20  Iteration 2367/3560 Training loss: 1.5161 0.8567 sec/batch\n",
      "Epoch 14/20  Iteration 2368/3560 Training loss: 1.5161 0.9381 sec/batch\n",
      "Epoch 14/20  Iteration 2369/3560 Training loss: 1.5160 0.9006 sec/batch\n",
      "Epoch 14/20  Iteration 2370/3560 Training loss: 1.5161 0.8796 sec/batch\n",
      "Epoch 14/20  Iteration 2371/3560 Training loss: 1.5165 0.8792 sec/batch\n",
      "Epoch 14/20  Iteration 2372/3560 Training loss: 1.5162 0.9072 sec/batch\n",
      "Epoch 14/20  Iteration 2373/3560 Training loss: 1.5156 0.8949 sec/batch\n",
      "Epoch 14/20  Iteration 2374/3560 Training loss: 1.5160 0.8635 sec/batch\n",
      "Epoch 14/20  Iteration 2375/3560 Training loss: 1.5160 0.8774 sec/batch\n",
      "Epoch 14/20  Iteration 2376/3560 Training loss: 1.5169 0.8480 sec/batch\n",
      "Epoch 14/20  Iteration 2377/3560 Training loss: 1.5173 0.7789 sec/batch\n",
      "Epoch 14/20  Iteration 2378/3560 Training loss: 1.5174 0.7976 sec/batch\n",
      "Epoch 14/20  Iteration 2379/3560 Training loss: 1.5174 0.8215 sec/batch\n",
      "Epoch 14/20  Iteration 2380/3560 Training loss: 1.5177 0.8506 sec/batch\n",
      "Epoch 14/20  Iteration 2381/3560 Training loss: 1.5178 0.8001 sec/batch\n",
      "Epoch 14/20  Iteration 2382/3560 Training loss: 1.5176 0.7839 sec/batch\n",
      "Epoch 14/20  Iteration 2383/3560 Training loss: 1.5175 0.7927 sec/batch\n",
      "Epoch 14/20  Iteration 2384/3560 Training loss: 1.5172 0.8029 sec/batch\n",
      "Epoch 14/20  Iteration 2385/3560 Training loss: 1.5177 0.7669 sec/batch\n",
      "Epoch 14/20  Iteration 2386/3560 Training loss: 1.5179 0.7834 sec/batch\n",
      "Epoch 14/20  Iteration 2387/3560 Training loss: 1.5183 0.8088 sec/batch\n",
      "Epoch 14/20  Iteration 2388/3560 Training loss: 1.5181 0.8556 sec/batch\n",
      "Epoch 14/20  Iteration 2389/3560 Training loss: 1.5180 0.8575 sec/batch\n",
      "Epoch 14/20  Iteration 2390/3560 Training loss: 1.5181 0.8157 sec/batch\n",
      "Epoch 14/20  Iteration 2391/3560 Training loss: 1.5180 0.7937 sec/batch\n",
      "Epoch 14/20  Iteration 2392/3560 Training loss: 1.5179 0.7760 sec/batch\n",
      "Epoch 14/20  Iteration 2393/3560 Training loss: 1.5173 0.8010 sec/batch\n",
      "Epoch 14/20  Iteration 2394/3560 Training loss: 1.5170 0.8046 sec/batch\n",
      "Epoch 14/20  Iteration 2395/3560 Training loss: 1.5164 0.7947 sec/batch\n",
      "Epoch 14/20  Iteration 2396/3560 Training loss: 1.5164 0.7997 sec/batch\n",
      "Epoch 14/20  Iteration 2397/3560 Training loss: 1.5160 0.7660 sec/batch\n",
      "Epoch 14/20  Iteration 2398/3560 Training loss: 1.5159 0.8059 sec/batch\n",
      "Epoch 14/20  Iteration 2399/3560 Training loss: 1.5156 0.7714 sec/batch\n",
      "Epoch 14/20  Iteration 2400/3560 Training loss: 1.5153 0.8111 sec/batch\n",
      "Epoch 14/20  Iteration 2401/3560 Training loss: 1.5150 0.8225 sec/batch\n",
      "Epoch 14/20  Iteration 2402/3560 Training loss: 1.5147 0.8033 sec/batch\n",
      "Epoch 14/20  Iteration 2403/3560 Training loss: 1.5143 0.8831 sec/batch\n",
      "Epoch 14/20  Iteration 2404/3560 Training loss: 1.5145 0.7988 sec/batch\n",
      "Epoch 14/20  Iteration 2405/3560 Training loss: 1.5142 0.7793 sec/batch\n",
      "Epoch 14/20  Iteration 2406/3560 Training loss: 1.5140 0.7689 sec/batch\n",
      "Epoch 14/20  Iteration 2407/3560 Training loss: 1.5137 0.7954 sec/batch\n",
      "Epoch 14/20  Iteration 2408/3560 Training loss: 1.5134 0.7824 sec/batch\n",
      "Epoch 14/20  Iteration 2409/3560 Training loss: 1.5130 0.7948 sec/batch\n",
      "Epoch 14/20  Iteration 2410/3560 Training loss: 1.5131 0.8217 sec/batch\n",
      "Epoch 14/20  Iteration 2411/3560 Training loss: 1.5131 0.7873 sec/batch\n",
      "Epoch 14/20  Iteration 2412/3560 Training loss: 1.5126 1.0200 sec/batch\n",
      "Epoch 14/20  Iteration 2413/3560 Training loss: 1.5124 1.0881 sec/batch\n",
      "Epoch 14/20  Iteration 2414/3560 Training loss: 1.5119 1.0057 sec/batch\n",
      "Epoch 14/20  Iteration 2415/3560 Training loss: 1.5120 1.0213 sec/batch\n",
      "Epoch 14/20  Iteration 2416/3560 Training loss: 1.5119 1.0650 sec/batch\n",
      "Epoch 14/20  Iteration 2417/3560 Training loss: 1.5118 1.0604 sec/batch\n",
      "Epoch 14/20  Iteration 2418/3560 Training loss: 1.5117 1.0170 sec/batch\n",
      "Epoch 14/20  Iteration 2419/3560 Training loss: 1.5116 1.0097 sec/batch\n",
      "Epoch 14/20  Iteration 2420/3560 Training loss: 1.5115 0.9651 sec/batch\n",
      "Epoch 14/20  Iteration 2421/3560 Training loss: 1.5114 0.8037 sec/batch\n",
      "Epoch 14/20  Iteration 2422/3560 Training loss: 1.5114 0.7738 sec/batch\n",
      "Epoch 14/20  Iteration 2423/3560 Training loss: 1.5113 0.7852 sec/batch\n",
      "Epoch 14/20  Iteration 2424/3560 Training loss: 1.5113 0.8340 sec/batch\n",
      "Epoch 14/20  Iteration 2425/3560 Training loss: 1.5111 0.7829 sec/batch\n",
      "Epoch 14/20  Iteration 2426/3560 Training loss: 1.5109 0.7977 sec/batch\n",
      "Epoch 14/20  Iteration 2427/3560 Training loss: 1.5106 0.7849 sec/batch\n",
      "Epoch 14/20  Iteration 2428/3560 Training loss: 1.5103 0.7729 sec/batch\n",
      "Epoch 14/20  Iteration 2429/3560 Training loss: 1.5101 0.7927 sec/batch\n",
      "Epoch 14/20  Iteration 2430/3560 Training loss: 1.5098 0.8393 sec/batch\n",
      "Epoch 14/20  Iteration 2431/3560 Training loss: 1.5098 0.8488 sec/batch\n",
      "Epoch 14/20  Iteration 2432/3560 Training loss: 1.5097 0.8041 sec/batch\n",
      "Epoch 14/20  Iteration 2433/3560 Training loss: 1.5095 0.8384 sec/batch\n",
      "Epoch 14/20  Iteration 2434/3560 Training loss: 1.5094 0.7971 sec/batch\n",
      "Epoch 14/20  Iteration 2435/3560 Training loss: 1.5094 0.8153 sec/batch\n",
      "Epoch 14/20  Iteration 2436/3560 Training loss: 1.5090 0.7977 sec/batch\n",
      "Epoch 14/20  Iteration 2437/3560 Training loss: 1.5086 0.7756 sec/batch\n",
      "Epoch 14/20  Iteration 2438/3560 Training loss: 1.5086 0.7800 sec/batch\n",
      "Epoch 14/20  Iteration 2439/3560 Training loss: 1.5086 0.7862 sec/batch\n",
      "Epoch 14/20  Iteration 2440/3560 Training loss: 1.5082 0.7648 sec/batch\n",
      "Epoch 14/20  Iteration 2441/3560 Training loss: 1.5082 0.7639 sec/batch\n",
      "Epoch 14/20  Iteration 2442/3560 Training loss: 1.5083 0.8251 sec/batch\n",
      "Epoch 14/20  Iteration 2443/3560 Training loss: 1.5082 0.8404 sec/batch\n",
      "Epoch 14/20  Iteration 2444/3560 Training loss: 1.5080 0.7817 sec/batch\n",
      "Epoch 14/20  Iteration 2445/3560 Training loss: 1.5077 0.7919 sec/batch\n",
      "Epoch 14/20  Iteration 2446/3560 Training loss: 1.5075 0.8009 sec/batch\n",
      "Epoch 14/20  Iteration 2447/3560 Training loss: 1.5075 0.7866 sec/batch\n",
      "Epoch 14/20  Iteration 2448/3560 Training loss: 1.5075 0.8474 sec/batch\n",
      "Epoch 14/20  Iteration 2449/3560 Training loss: 1.5075 0.7745 sec/batch\n",
      "Epoch 14/20  Iteration 2450/3560 Training loss: 1.5075 0.7761 sec/batch\n",
      "Epoch 14/20  Iteration 2451/3560 Training loss: 1.5077 0.7880 sec/batch\n",
      "Epoch 14/20  Iteration 2452/3560 Training loss: 1.5078 0.7819 sec/batch\n",
      "Epoch 14/20  Iteration 2453/3560 Training loss: 1.5078 0.7961 sec/batch\n",
      "Epoch 14/20  Iteration 2454/3560 Training loss: 1.5078 0.7653 sec/batch\n",
      "Epoch 14/20  Iteration 2455/3560 Training loss: 1.5081 0.7722 sec/batch\n",
      "Epoch 14/20  Iteration 2456/3560 Training loss: 1.5081 0.8016 sec/batch\n",
      "Epoch 14/20  Iteration 2457/3560 Training loss: 1.5080 0.8755 sec/batch\n",
      "Epoch 14/20  Iteration 2458/3560 Training loss: 1.5081 0.7824 sec/batch\n",
      "Epoch 14/20  Iteration 2459/3560 Training loss: 1.5081 0.7925 sec/batch\n",
      "Epoch 14/20  Iteration 2460/3560 Training loss: 1.5082 0.7712 sec/batch\n",
      "Epoch 14/20  Iteration 2461/3560 Training loss: 1.5082 0.8319 sec/batch\n",
      "Epoch 14/20  Iteration 2462/3560 Training loss: 1.5085 0.8041 sec/batch\n",
      "Epoch 14/20  Iteration 2463/3560 Training loss: 1.5086 0.7781 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2464/3560 Training loss: 1.5084 0.8011 sec/batch\n",
      "Epoch 14/20  Iteration 2465/3560 Training loss: 1.5082 0.7540 sec/batch\n",
      "Epoch 14/20  Iteration 2466/3560 Training loss: 1.5082 0.7930 sec/batch\n",
      "Epoch 14/20  Iteration 2467/3560 Training loss: 1.5082 0.7975 sec/batch\n",
      "Epoch 14/20  Iteration 2468/3560 Training loss: 1.5081 0.7769 sec/batch\n",
      "Epoch 14/20  Iteration 2469/3560 Training loss: 1.5081 0.7702 sec/batch\n",
      "Epoch 14/20  Iteration 2470/3560 Training loss: 1.5081 0.8289 sec/batch\n",
      "Epoch 14/20  Iteration 2471/3560 Training loss: 1.5082 0.8087 sec/batch\n",
      "Epoch 14/20  Iteration 2472/3560 Training loss: 1.5082 0.8117 sec/batch\n",
      "Epoch 14/20  Iteration 2473/3560 Training loss: 1.5079 0.7680 sec/batch\n",
      "Epoch 14/20  Iteration 2474/3560 Training loss: 1.5080 0.8115 sec/batch\n",
      "Epoch 14/20  Iteration 2475/3560 Training loss: 1.5082 0.8008 sec/batch\n",
      "Epoch 14/20  Iteration 2476/3560 Training loss: 1.5081 0.7715 sec/batch\n",
      "Epoch 14/20  Iteration 2477/3560 Training loss: 1.5081 0.7992 sec/batch\n",
      "Epoch 14/20  Iteration 2478/3560 Training loss: 1.5081 0.7620 sec/batch\n",
      "Epoch 14/20  Iteration 2479/3560 Training loss: 1.5081 0.7999 sec/batch\n",
      "Epoch 14/20  Iteration 2480/3560 Training loss: 1.5080 0.8019 sec/batch\n",
      "Epoch 14/20  Iteration 2481/3560 Training loss: 1.5081 0.7974 sec/batch\n",
      "Epoch 14/20  Iteration 2482/3560 Training loss: 1.5085 0.7932 sec/batch\n",
      "Epoch 14/20  Iteration 2483/3560 Training loss: 1.5085 0.7981 sec/batch\n",
      "Epoch 14/20  Iteration 2484/3560 Training loss: 1.5085 0.8084 sec/batch\n",
      "Epoch 14/20  Iteration 2485/3560 Training loss: 1.5084 0.9081 sec/batch\n",
      "Epoch 14/20  Iteration 2486/3560 Training loss: 1.5082 0.8511 sec/batch\n",
      "Epoch 14/20  Iteration 2487/3560 Training loss: 1.5083 0.8156 sec/batch\n",
      "Epoch 14/20  Iteration 2488/3560 Training loss: 1.5083 0.7767 sec/batch\n",
      "Epoch 14/20  Iteration 2489/3560 Training loss: 1.5084 0.7977 sec/batch\n",
      "Epoch 14/20  Iteration 2490/3560 Training loss: 1.5082 0.7851 sec/batch\n",
      "Epoch 14/20  Iteration 2491/3560 Training loss: 1.5081 0.7968 sec/batch\n",
      "Epoch 14/20  Iteration 2492/3560 Training loss: 1.5082 0.7841 sec/batch\n",
      "Epoch 15/20  Iteration 2493/3560 Training loss: 1.5999 0.7989 sec/batch\n",
      "Epoch 15/20  Iteration 2494/3560 Training loss: 1.5512 0.7611 sec/batch\n",
      "Epoch 15/20  Iteration 2495/3560 Training loss: 1.5378 0.8118 sec/batch\n",
      "Epoch 15/20  Iteration 2496/3560 Training loss: 1.5312 0.7933 sec/batch\n",
      "Epoch 15/20  Iteration 2497/3560 Training loss: 1.5205 0.8249 sec/batch\n",
      "Epoch 15/20  Iteration 2498/3560 Training loss: 1.5088 0.7925 sec/batch\n",
      "Epoch 15/20  Iteration 2499/3560 Training loss: 1.5093 0.8232 sec/batch\n",
      "Epoch 15/20  Iteration 2500/3560 Training loss: 1.5070 0.8399 sec/batch\n",
      "Epoch 15/20  Iteration 2501/3560 Training loss: 1.5076 0.8084 sec/batch\n",
      "Epoch 15/20  Iteration 2502/3560 Training loss: 1.5077 0.7724 sec/batch\n",
      "Epoch 15/20  Iteration 2503/3560 Training loss: 1.5046 0.8006 sec/batch\n",
      "Epoch 15/20  Iteration 2504/3560 Training loss: 1.5033 0.7997 sec/batch\n",
      "Epoch 15/20  Iteration 2505/3560 Training loss: 1.5024 0.7660 sec/batch\n",
      "Epoch 15/20  Iteration 2506/3560 Training loss: 1.5045 0.8032 sec/batch\n",
      "Epoch 15/20  Iteration 2507/3560 Training loss: 1.5033 0.7722 sec/batch\n",
      "Epoch 15/20  Iteration 2508/3560 Training loss: 1.5020 0.8003 sec/batch\n",
      "Epoch 15/20  Iteration 2509/3560 Training loss: 1.5023 0.8878 sec/batch\n",
      "Epoch 15/20  Iteration 2510/3560 Training loss: 1.5035 0.7897 sec/batch\n",
      "Epoch 15/20  Iteration 2511/3560 Training loss: 1.5035 0.8228 sec/batch\n",
      "Epoch 15/20  Iteration 2512/3560 Training loss: 1.5043 0.8586 sec/batch\n",
      "Epoch 15/20  Iteration 2513/3560 Training loss: 1.5037 0.7709 sec/batch\n",
      "Epoch 15/20  Iteration 2514/3560 Training loss: 1.5045 0.8061 sec/batch\n",
      "Epoch 15/20  Iteration 2515/3560 Training loss: 1.5038 0.8136 sec/batch\n",
      "Epoch 15/20  Iteration 2516/3560 Training loss: 1.5042 0.8645 sec/batch\n",
      "Epoch 15/20  Iteration 2517/3560 Training loss: 1.5040 0.7980 sec/batch\n",
      "Epoch 15/20  Iteration 2518/3560 Training loss: 1.5025 0.8024 sec/batch\n",
      "Epoch 15/20  Iteration 2519/3560 Training loss: 1.5015 0.7663 sec/batch\n",
      "Epoch 15/20  Iteration 2520/3560 Training loss: 1.5018 0.7886 sec/batch\n",
      "Epoch 15/20  Iteration 2521/3560 Training loss: 1.5022 0.8061 sec/batch\n",
      "Epoch 15/20  Iteration 2522/3560 Training loss: 1.5024 0.8149 sec/batch\n",
      "Epoch 15/20  Iteration 2523/3560 Training loss: 1.5022 0.7578 sec/batch\n",
      "Epoch 15/20  Iteration 2524/3560 Training loss: 1.5011 0.7791 sec/batch\n",
      "Epoch 15/20  Iteration 2525/3560 Training loss: 1.5014 0.7997 sec/batch\n",
      "Epoch 15/20  Iteration 2526/3560 Training loss: 1.5019 0.7685 sec/batch\n",
      "Epoch 15/20  Iteration 2527/3560 Training loss: 1.5018 0.7869 sec/batch\n",
      "Epoch 15/20  Iteration 2528/3560 Training loss: 1.5016 0.7696 sec/batch\n",
      "Epoch 15/20  Iteration 2529/3560 Training loss: 1.5010 0.8083 sec/batch\n",
      "Epoch 15/20  Iteration 2530/3560 Training loss: 1.5001 0.7622 sec/batch\n",
      "Epoch 15/20  Iteration 2531/3560 Training loss: 1.4988 0.8233 sec/batch\n",
      "Epoch 15/20  Iteration 2532/3560 Training loss: 1.4982 0.7946 sec/batch\n",
      "Epoch 15/20  Iteration 2533/3560 Training loss: 1.4976 0.8378 sec/batch\n",
      "Epoch 15/20  Iteration 2534/3560 Training loss: 1.4984 0.7942 sec/batch\n",
      "Epoch 15/20  Iteration 2535/3560 Training loss: 1.4977 0.7823 sec/batch\n",
      "Epoch 15/20  Iteration 2536/3560 Training loss: 1.4972 0.8077 sec/batch\n",
      "Epoch 15/20  Iteration 2537/3560 Training loss: 1.4974 0.8322 sec/batch\n",
      "Epoch 15/20  Iteration 2538/3560 Training loss: 1.4967 0.7780 sec/batch\n",
      "Epoch 15/20  Iteration 2539/3560 Training loss: 1.4964 0.8023 sec/batch\n",
      "Epoch 15/20  Iteration 2540/3560 Training loss: 1.4960 0.8116 sec/batch\n",
      "Epoch 15/20  Iteration 2541/3560 Training loss: 1.4959 0.7697 sec/batch\n",
      "Epoch 15/20  Iteration 2542/3560 Training loss: 1.4963 0.7743 sec/batch\n",
      "Epoch 15/20  Iteration 2543/3560 Training loss: 1.4959 0.7913 sec/batch\n",
      "Epoch 15/20  Iteration 2544/3560 Training loss: 1.4967 0.8052 sec/batch\n",
      "Epoch 15/20  Iteration 2545/3560 Training loss: 1.4967 0.7974 sec/batch\n",
      "Epoch 15/20  Iteration 2546/3560 Training loss: 1.4968 0.8231 sec/batch\n",
      "Epoch 15/20  Iteration 2547/3560 Training loss: 1.4966 0.7863 sec/batch\n",
      "Epoch 15/20  Iteration 2548/3560 Training loss: 1.4967 0.8340 sec/batch\n",
      "Epoch 15/20  Iteration 2549/3560 Training loss: 1.4970 0.8349 sec/batch\n",
      "Epoch 15/20  Iteration 2550/3560 Training loss: 1.4967 0.8768 sec/batch\n",
      "Epoch 15/20  Iteration 2551/3560 Training loss: 1.4962 0.8261 sec/batch\n",
      "Epoch 15/20  Iteration 2552/3560 Training loss: 1.4966 0.8084 sec/batch\n",
      "Epoch 15/20  Iteration 2553/3560 Training loss: 1.4967 0.8032 sec/batch\n",
      "Epoch 15/20  Iteration 2554/3560 Training loss: 1.4976 0.7980 sec/batch\n",
      "Epoch 15/20  Iteration 2555/3560 Training loss: 1.4978 0.7895 sec/batch\n",
      "Epoch 15/20  Iteration 2556/3560 Training loss: 1.4980 0.7733 sec/batch\n",
      "Epoch 15/20  Iteration 2557/3560 Training loss: 1.4979 0.7954 sec/batch\n",
      "Epoch 15/20  Iteration 2558/3560 Training loss: 1.4981 0.7803 sec/batch\n",
      "Epoch 15/20  Iteration 2559/3560 Training loss: 1.4984 0.8137 sec/batch\n",
      "Epoch 15/20  Iteration 2560/3560 Training loss: 1.4982 0.7944 sec/batch\n",
      "Epoch 15/20  Iteration 2561/3560 Training loss: 1.4983 0.7949 sec/batch\n",
      "Epoch 15/20  Iteration 2562/3560 Training loss: 1.4980 0.8123 sec/batch\n",
      "Epoch 15/20  Iteration 2563/3560 Training loss: 1.4985 0.9066 sec/batch\n",
      "Epoch 15/20  Iteration 2564/3560 Training loss: 1.4987 0.8045 sec/batch\n",
      "Epoch 15/20  Iteration 2565/3560 Training loss: 1.4991 0.7923 sec/batch\n",
      "Epoch 15/20  Iteration 2566/3560 Training loss: 1.4989 0.7963 sec/batch\n",
      "Epoch 15/20  Iteration 2567/3560 Training loss: 1.4988 0.7899 sec/batch\n",
      "Epoch 15/20  Iteration 2568/3560 Training loss: 1.4990 0.7765 sec/batch\n",
      "Epoch 15/20  Iteration 2569/3560 Training loss: 1.4988 0.7979 sec/batch\n",
      "Epoch 15/20  Iteration 2570/3560 Training loss: 1.4988 0.8023 sec/batch\n",
      "Epoch 15/20  Iteration 2571/3560 Training loss: 1.4982 0.7699 sec/batch\n",
      "Epoch 15/20  Iteration 2572/3560 Training loss: 1.4981 0.8429 sec/batch\n",
      "Epoch 15/20  Iteration 2573/3560 Training loss: 1.4976 0.8118 sec/batch\n",
      "Epoch 15/20  Iteration 2574/3560 Training loss: 1.4975 0.8350 sec/batch\n",
      "Epoch 15/20  Iteration 2575/3560 Training loss: 1.4970 0.8342 sec/batch\n",
      "Epoch 15/20  Iteration 2576/3560 Training loss: 1.4969 0.7895 sec/batch\n",
      "Epoch 15/20  Iteration 2577/3560 Training loss: 1.4966 0.7915 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2578/3560 Training loss: 1.4965 0.8081 sec/batch\n",
      "Epoch 15/20  Iteration 2579/3560 Training loss: 1.4963 0.7955 sec/batch\n",
      "Epoch 15/20  Iteration 2580/3560 Training loss: 1.4961 0.7993 sec/batch\n",
      "Epoch 15/20  Iteration 2581/3560 Training loss: 1.4957 0.8054 sec/batch\n",
      "Epoch 15/20  Iteration 2582/3560 Training loss: 1.4959 0.8063 sec/batch\n",
      "Epoch 15/20  Iteration 2583/3560 Training loss: 1.4956 0.7997 sec/batch\n",
      "Epoch 15/20  Iteration 2584/3560 Training loss: 1.4955 0.7825 sec/batch\n",
      "Epoch 15/20  Iteration 2585/3560 Training loss: 1.4952 0.7963 sec/batch\n",
      "Epoch 15/20  Iteration 2586/3560 Training loss: 1.4950 0.8928 sec/batch\n",
      "Epoch 15/20  Iteration 2587/3560 Training loss: 1.4947 1.0433 sec/batch\n",
      "Epoch 15/20  Iteration 2588/3560 Training loss: 1.4948 1.1302 sec/batch\n",
      "Epoch 15/20  Iteration 2589/3560 Training loss: 1.4948 0.8418 sec/batch\n",
      "Epoch 15/20  Iteration 2590/3560 Training loss: 1.4944 0.8268 sec/batch\n",
      "Epoch 15/20  Iteration 2591/3560 Training loss: 1.4941 0.8027 sec/batch\n",
      "Epoch 15/20  Iteration 2592/3560 Training loss: 1.4937 0.7867 sec/batch\n",
      "Epoch 15/20  Iteration 2593/3560 Training loss: 1.4937 0.7993 sec/batch\n",
      "Epoch 15/20  Iteration 2594/3560 Training loss: 1.4935 0.8336 sec/batch\n",
      "Epoch 15/20  Iteration 2595/3560 Training loss: 1.4934 0.8294 sec/batch\n",
      "Epoch 15/20  Iteration 2596/3560 Training loss: 1.4933 0.8057 sec/batch\n",
      "Epoch 15/20  Iteration 2597/3560 Training loss: 1.4932 0.8229 sec/batch\n",
      "Epoch 15/20  Iteration 2598/3560 Training loss: 1.4930 0.7679 sec/batch\n",
      "Epoch 15/20  Iteration 2599/3560 Training loss: 1.4930 0.8616 sec/batch\n",
      "Epoch 15/20  Iteration 2600/3560 Training loss: 1.4929 0.9263 sec/batch\n",
      "Epoch 15/20  Iteration 2601/3560 Training loss: 1.4928 0.8337 sec/batch\n",
      "Epoch 15/20  Iteration 2602/3560 Training loss: 1.4928 0.8172 sec/batch\n",
      "Epoch 15/20  Iteration 2603/3560 Training loss: 1.4926 0.8205 sec/batch\n",
      "Epoch 15/20  Iteration 2604/3560 Training loss: 1.4926 0.8028 sec/batch\n",
      "Epoch 15/20  Iteration 2605/3560 Training loss: 1.4925 0.7918 sec/batch\n",
      "Epoch 15/20  Iteration 2606/3560 Training loss: 1.4924 0.8251 sec/batch\n",
      "Epoch 15/20  Iteration 2607/3560 Training loss: 1.4921 0.7832 sec/batch\n",
      "Epoch 15/20  Iteration 2608/3560 Training loss: 1.4918 0.8143 sec/batch\n",
      "Epoch 15/20  Iteration 2609/3560 Training loss: 1.4918 0.7860 sec/batch\n",
      "Epoch 15/20  Iteration 2610/3560 Training loss: 1.4917 0.7981 sec/batch\n",
      "Epoch 15/20  Iteration 2611/3560 Training loss: 1.4917 0.8504 sec/batch\n",
      "Epoch 15/20  Iteration 2612/3560 Training loss: 1.4915 0.8148 sec/batch\n",
      "Epoch 15/20  Iteration 2613/3560 Training loss: 1.4914 0.8044 sec/batch\n",
      "Epoch 15/20  Iteration 2614/3560 Training loss: 1.4910 0.8853 sec/batch\n",
      "Epoch 15/20  Iteration 2615/3560 Training loss: 1.4906 0.9007 sec/batch\n",
      "Epoch 15/20  Iteration 2616/3560 Training loss: 1.4907 0.7917 sec/batch\n",
      "Epoch 15/20  Iteration 2617/3560 Training loss: 1.4906 0.7998 sec/batch\n",
      "Epoch 15/20  Iteration 2618/3560 Training loss: 1.4902 0.7959 sec/batch\n",
      "Epoch 15/20  Iteration 2619/3560 Training loss: 1.4903 0.7969 sec/batch\n",
      "Epoch 15/20  Iteration 2620/3560 Training loss: 1.4903 0.8065 sec/batch\n",
      "Epoch 15/20  Iteration 2621/3560 Training loss: 1.4902 0.8257 sec/batch\n",
      "Epoch 15/20  Iteration 2622/3560 Training loss: 1.4899 0.7957 sec/batch\n",
      "Epoch 15/20  Iteration 2623/3560 Training loss: 1.4895 0.8214 sec/batch\n",
      "Epoch 15/20  Iteration 2624/3560 Training loss: 1.4893 0.8551 sec/batch\n",
      "Epoch 15/20  Iteration 2625/3560 Training loss: 1.4895 0.8058 sec/batch\n",
      "Epoch 15/20  Iteration 2626/3560 Training loss: 1.4895 0.8080 sec/batch\n",
      "Epoch 15/20  Iteration 2627/3560 Training loss: 1.4894 0.8162 sec/batch\n",
      "Epoch 15/20  Iteration 2628/3560 Training loss: 1.4895 0.7990 sec/batch\n",
      "Epoch 15/20  Iteration 2629/3560 Training loss: 1.4896 0.8928 sec/batch\n",
      "Epoch 15/20  Iteration 2630/3560 Training loss: 1.4897 0.9158 sec/batch\n",
      "Epoch 15/20  Iteration 2631/3560 Training loss: 1.4897 0.9073 sec/batch\n",
      "Epoch 15/20  Iteration 2632/3560 Training loss: 1.4897 1.0832 sec/batch\n",
      "Epoch 15/20  Iteration 2633/3560 Training loss: 1.4899 1.0474 sec/batch\n",
      "Epoch 15/20  Iteration 2634/3560 Training loss: 1.4899 1.0954 sec/batch\n",
      "Epoch 15/20  Iteration 2635/3560 Training loss: 1.4898 1.1407 sec/batch\n",
      "Epoch 15/20  Iteration 2636/3560 Training loss: 1.4899 1.0661 sec/batch\n",
      "Epoch 15/20  Iteration 2637/3560 Training loss: 1.4899 1.0010 sec/batch\n",
      "Epoch 15/20  Iteration 2638/3560 Training loss: 1.4900 1.0797 sec/batch\n",
      "Epoch 15/20  Iteration 2639/3560 Training loss: 1.4900 1.0713 sec/batch\n",
      "Epoch 15/20  Iteration 2640/3560 Training loss: 1.4902 0.8558 sec/batch\n",
      "Epoch 15/20  Iteration 2641/3560 Training loss: 1.4903 0.8884 sec/batch\n",
      "Epoch 15/20  Iteration 2642/3560 Training loss: 1.4902 0.8588 sec/batch\n",
      "Epoch 15/20  Iteration 2643/3560 Training loss: 1.4899 0.8016 sec/batch\n",
      "Epoch 15/20  Iteration 2644/3560 Training loss: 1.4898 0.7877 sec/batch\n",
      "Epoch 15/20  Iteration 2645/3560 Training loss: 1.4899 0.8662 sec/batch\n",
      "Epoch 15/20  Iteration 2646/3560 Training loss: 1.4899 0.8219 sec/batch\n",
      "Epoch 15/20  Iteration 2647/3560 Training loss: 1.4899 0.8148 sec/batch\n",
      "Epoch 15/20  Iteration 2648/3560 Training loss: 1.4899 0.8225 sec/batch\n",
      "Epoch 15/20  Iteration 2649/3560 Training loss: 1.4900 0.8278 sec/batch\n",
      "Epoch 15/20  Iteration 2650/3560 Training loss: 1.4900 0.8180 sec/batch\n",
      "Epoch 15/20  Iteration 2651/3560 Training loss: 1.4898 0.8201 sec/batch\n",
      "Epoch 15/20  Iteration 2652/3560 Training loss: 1.4899 0.8319 sec/batch\n",
      "Epoch 15/20  Iteration 2653/3560 Training loss: 1.4901 0.8207 sec/batch\n",
      "Epoch 15/20  Iteration 2654/3560 Training loss: 1.4901 0.8321 sec/batch\n",
      "Epoch 15/20  Iteration 2655/3560 Training loss: 1.4901 0.9171 sec/batch\n",
      "Epoch 15/20  Iteration 2656/3560 Training loss: 1.4901 0.8945 sec/batch\n",
      "Epoch 15/20  Iteration 2657/3560 Training loss: 1.4901 0.8290 sec/batch\n",
      "Epoch 15/20  Iteration 2658/3560 Training loss: 1.4901 0.7951 sec/batch\n",
      "Epoch 15/20  Iteration 2659/3560 Training loss: 1.4903 0.7889 sec/batch\n",
      "Epoch 15/20  Iteration 2660/3560 Training loss: 1.4907 0.8250 sec/batch\n",
      "Epoch 15/20  Iteration 2661/3560 Training loss: 1.4906 0.8014 sec/batch\n",
      "Epoch 15/20  Iteration 2662/3560 Training loss: 1.4907 0.7871 sec/batch\n",
      "Epoch 15/20  Iteration 2663/3560 Training loss: 1.4906 0.8208 sec/batch\n",
      "Epoch 15/20  Iteration 2664/3560 Training loss: 1.4904 0.8393 sec/batch\n",
      "Epoch 15/20  Iteration 2665/3560 Training loss: 1.4906 0.8093 sec/batch\n",
      "Epoch 15/20  Iteration 2666/3560 Training loss: 1.4906 0.8260 sec/batch\n",
      "Epoch 15/20  Iteration 2667/3560 Training loss: 1.4907 0.8169 sec/batch\n",
      "Epoch 15/20  Iteration 2668/3560 Training loss: 1.4906 0.7977 sec/batch\n",
      "Epoch 15/20  Iteration 2669/3560 Training loss: 1.4904 0.9136 sec/batch\n",
      "Epoch 15/20  Iteration 2670/3560 Training loss: 1.4905 0.9171 sec/batch\n",
      "Epoch 16/20  Iteration 2671/3560 Training loss: 1.5710 0.7964 sec/batch\n",
      "Epoch 16/20  Iteration 2672/3560 Training loss: 1.5299 0.8037 sec/batch\n",
      "Epoch 16/20  Iteration 2673/3560 Training loss: 1.5175 0.8246 sec/batch\n",
      "Epoch 16/20  Iteration 2674/3560 Training loss: 1.5132 0.8119 sec/batch\n",
      "Epoch 16/20  Iteration 2675/3560 Training loss: 1.5037 0.7867 sec/batch\n",
      "Epoch 16/20  Iteration 2676/3560 Training loss: 1.4925 0.8241 sec/batch\n",
      "Epoch 16/20  Iteration 2677/3560 Training loss: 1.4939 0.7899 sec/batch\n",
      "Epoch 16/20  Iteration 2678/3560 Training loss: 1.4923 0.8361 sec/batch\n",
      "Epoch 16/20  Iteration 2679/3560 Training loss: 1.4923 0.8168 sec/batch\n",
      "Epoch 16/20  Iteration 2680/3560 Training loss: 1.4915 0.7897 sec/batch\n",
      "Epoch 16/20  Iteration 2681/3560 Training loss: 1.4884 0.8384 sec/batch\n",
      "Epoch 16/20  Iteration 2682/3560 Training loss: 1.4878 0.8467 sec/batch\n",
      "Epoch 16/20  Iteration 2683/3560 Training loss: 1.4871 0.8330 sec/batch\n",
      "Epoch 16/20  Iteration 2684/3560 Training loss: 1.4885 0.9533 sec/batch\n",
      "Epoch 16/20  Iteration 2685/3560 Training loss: 1.4869 0.8268 sec/batch\n",
      "Epoch 16/20  Iteration 2686/3560 Training loss: 1.4849 0.7813 sec/batch\n",
      "Epoch 16/20  Iteration 2687/3560 Training loss: 1.4853 0.8331 sec/batch\n",
      "Epoch 16/20  Iteration 2688/3560 Training loss: 1.4872 0.8022 sec/batch\n",
      "Epoch 16/20  Iteration 2689/3560 Training loss: 1.4872 0.8139 sec/batch\n",
      "Epoch 16/20  Iteration 2690/3560 Training loss: 1.4880 0.8120 sec/batch\n",
      "Epoch 16/20  Iteration 2691/3560 Training loss: 1.4879 0.8249 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2692/3560 Training loss: 1.4884 0.8123 sec/batch\n",
      "Epoch 16/20  Iteration 2693/3560 Training loss: 1.4871 0.8505 sec/batch\n",
      "Epoch 16/20  Iteration 2694/3560 Training loss: 1.4868 0.8624 sec/batch\n",
      "Epoch 16/20  Iteration 2695/3560 Training loss: 1.4869 0.7865 sec/batch\n",
      "Epoch 16/20  Iteration 2696/3560 Training loss: 1.4856 0.8229 sec/batch\n",
      "Epoch 16/20  Iteration 2697/3560 Training loss: 1.4842 0.8023 sec/batch\n",
      "Epoch 16/20  Iteration 2698/3560 Training loss: 1.4847 0.8605 sec/batch\n",
      "Epoch 16/20  Iteration 2699/3560 Training loss: 1.4851 0.9041 sec/batch\n",
      "Epoch 16/20  Iteration 2700/3560 Training loss: 1.4855 0.8123 sec/batch\n",
      "Epoch 16/20  Iteration 2701/3560 Training loss: 1.4851 0.8067 sec/batch\n",
      "Epoch 16/20  Iteration 2702/3560 Training loss: 1.4843 0.8317 sec/batch\n",
      "Epoch 16/20  Iteration 2703/3560 Training loss: 1.4847 0.8284 sec/batch\n",
      "Epoch 16/20  Iteration 2704/3560 Training loss: 1.4850 0.8118 sec/batch\n",
      "Epoch 16/20  Iteration 2705/3560 Training loss: 1.4850 0.7997 sec/batch\n",
      "Epoch 16/20  Iteration 2706/3560 Training loss: 1.4847 0.8273 sec/batch\n",
      "Epoch 16/20  Iteration 2707/3560 Training loss: 1.4839 0.8653 sec/batch\n",
      "Epoch 16/20  Iteration 2708/3560 Training loss: 1.4831 0.7906 sec/batch\n",
      "Epoch 16/20  Iteration 2709/3560 Training loss: 1.4817 0.8169 sec/batch\n",
      "Epoch 16/20  Iteration 2710/3560 Training loss: 1.4814 0.8163 sec/batch\n",
      "Epoch 16/20  Iteration 2711/3560 Training loss: 1.4808 0.7821 sec/batch\n",
      "Epoch 16/20  Iteration 2712/3560 Training loss: 1.4814 0.8314 sec/batch\n",
      "Epoch 16/20  Iteration 2713/3560 Training loss: 1.4811 0.8470 sec/batch\n",
      "Epoch 16/20  Iteration 2714/3560 Training loss: 1.4804 0.8776 sec/batch\n",
      "Epoch 16/20  Iteration 2715/3560 Training loss: 1.4807 0.8479 sec/batch\n",
      "Epoch 16/20  Iteration 2716/3560 Training loss: 1.4800 0.8010 sec/batch\n",
      "Epoch 16/20  Iteration 2717/3560 Training loss: 1.4797 0.8404 sec/batch\n",
      "Epoch 16/20  Iteration 2718/3560 Training loss: 1.4795 0.8575 sec/batch\n",
      "Epoch 16/20  Iteration 2719/3560 Training loss: 1.4792 0.7754 sec/batch\n",
      "Epoch 16/20  Iteration 2720/3560 Training loss: 1.4795 0.8303 sec/batch\n",
      "Epoch 16/20  Iteration 2721/3560 Training loss: 1.4793 0.8288 sec/batch\n",
      "Epoch 16/20  Iteration 2722/3560 Training loss: 1.4800 0.7895 sec/batch\n",
      "Epoch 16/20  Iteration 2723/3560 Training loss: 1.4800 0.7921 sec/batch\n",
      "Epoch 16/20  Iteration 2724/3560 Training loss: 1.4801 0.8123 sec/batch\n",
      "Epoch 16/20  Iteration 2725/3560 Training loss: 1.4799 0.8779 sec/batch\n",
      "Epoch 16/20  Iteration 2726/3560 Training loss: 1.4800 0.9790 sec/batch\n",
      "Epoch 16/20  Iteration 2727/3560 Training loss: 1.4805 0.8161 sec/batch\n",
      "Epoch 16/20  Iteration 2728/3560 Training loss: 1.4801 0.8830 sec/batch\n",
      "Epoch 16/20  Iteration 2729/3560 Training loss: 1.4795 0.8120 sec/batch\n",
      "Epoch 16/20  Iteration 2730/3560 Training loss: 1.4800 0.8516 sec/batch\n",
      "Epoch 16/20  Iteration 2731/3560 Training loss: 1.4801 0.8043 sec/batch\n",
      "Epoch 16/20  Iteration 2732/3560 Training loss: 1.4810 0.8028 sec/batch\n",
      "Epoch 16/20  Iteration 2733/3560 Training loss: 1.4812 0.8240 sec/batch\n",
      "Epoch 16/20  Iteration 2734/3560 Training loss: 1.4814 0.8034 sec/batch\n",
      "Epoch 16/20  Iteration 2735/3560 Training loss: 1.4813 0.8056 sec/batch\n",
      "Epoch 16/20  Iteration 2736/3560 Training loss: 1.4815 0.8170 sec/batch\n",
      "Epoch 16/20  Iteration 2737/3560 Training loss: 1.4816 0.8194 sec/batch\n",
      "Epoch 16/20  Iteration 2738/3560 Training loss: 1.4814 0.7793 sec/batch\n",
      "Epoch 16/20  Iteration 2739/3560 Training loss: 1.4816 0.8290 sec/batch\n",
      "Epoch 16/20  Iteration 2740/3560 Training loss: 1.4816 0.8787 sec/batch\n",
      "Epoch 16/20  Iteration 2741/3560 Training loss: 1.4821 0.8772 sec/batch\n",
      "Epoch 16/20  Iteration 2742/3560 Training loss: 1.4824 0.8206 sec/batch\n",
      "Epoch 16/20  Iteration 2743/3560 Training loss: 1.4827 0.8316 sec/batch\n",
      "Epoch 16/20  Iteration 2744/3560 Training loss: 1.4825 0.7886 sec/batch\n",
      "Epoch 16/20  Iteration 2745/3560 Training loss: 1.4824 0.7974 sec/batch\n",
      "Epoch 16/20  Iteration 2746/3560 Training loss: 1.4826 0.8193 sec/batch\n",
      "Epoch 16/20  Iteration 2747/3560 Training loss: 1.4825 0.8274 sec/batch\n",
      "Epoch 16/20  Iteration 2748/3560 Training loss: 1.4825 0.7926 sec/batch\n",
      "Epoch 16/20  Iteration 2749/3560 Training loss: 1.4819 0.7900 sec/batch\n",
      "Epoch 16/20  Iteration 2750/3560 Training loss: 1.4818 0.8182 sec/batch\n",
      "Epoch 16/20  Iteration 2751/3560 Training loss: 1.4812 0.8342 sec/batch\n",
      "Epoch 16/20  Iteration 2752/3560 Training loss: 1.4812 0.8225 sec/batch\n",
      "Epoch 16/20  Iteration 2753/3560 Training loss: 1.4807 0.7964 sec/batch\n",
      "Epoch 16/20  Iteration 2754/3560 Training loss: 1.4806 0.8837 sec/batch\n",
      "Epoch 16/20  Iteration 2755/3560 Training loss: 1.4803 0.9403 sec/batch\n",
      "Epoch 16/20  Iteration 2756/3560 Training loss: 1.4801 0.8400 sec/batch\n",
      "Epoch 16/20  Iteration 2757/3560 Training loss: 1.4797 0.8346 sec/batch\n",
      "Epoch 16/20  Iteration 2758/3560 Training loss: 1.4795 0.8149 sec/batch\n",
      "Epoch 16/20  Iteration 2759/3560 Training loss: 1.4791 0.8104 sec/batch\n",
      "Epoch 16/20  Iteration 2760/3560 Training loss: 1.4793 0.7930 sec/batch\n",
      "Epoch 16/20  Iteration 2761/3560 Training loss: 1.4790 0.7777 sec/batch\n",
      "Epoch 16/20  Iteration 2762/3560 Training loss: 1.4789 0.8041 sec/batch\n",
      "Epoch 16/20  Iteration 2763/3560 Training loss: 1.4786 0.8098 sec/batch\n",
      "Epoch 16/20  Iteration 2764/3560 Training loss: 1.4784 0.8025 sec/batch\n",
      "Epoch 16/20  Iteration 2765/3560 Training loss: 1.4782 0.8128 sec/batch\n",
      "Epoch 16/20  Iteration 2766/3560 Training loss: 1.4783 0.8054 sec/batch\n",
      "Epoch 16/20  Iteration 2767/3560 Training loss: 1.4783 0.8400 sec/batch\n",
      "Epoch 16/20  Iteration 2768/3560 Training loss: 1.4779 0.8048 sec/batch\n",
      "Epoch 16/20  Iteration 2769/3560 Training loss: 1.4776 0.8597 sec/batch\n",
      "Epoch 16/20  Iteration 2770/3560 Training loss: 1.4772 0.8046 sec/batch\n",
      "Epoch 16/20  Iteration 2771/3560 Training loss: 1.4772 0.8136 sec/batch\n",
      "Epoch 16/20  Iteration 2772/3560 Training loss: 1.4770 0.8115 sec/batch\n",
      "Epoch 16/20  Iteration 2773/3560 Training loss: 1.4768 0.7814 sec/batch\n",
      "Epoch 16/20  Iteration 2774/3560 Training loss: 1.4767 0.7965 sec/batch\n",
      "Epoch 16/20  Iteration 2775/3560 Training loss: 1.4765 0.8290 sec/batch\n",
      "Epoch 16/20  Iteration 2776/3560 Training loss: 1.4764 0.7961 sec/batch\n",
      "Epoch 16/20  Iteration 2777/3560 Training loss: 1.4764 0.8376 sec/batch\n",
      "Epoch 16/20  Iteration 2778/3560 Training loss: 1.4763 0.8730 sec/batch\n",
      "Epoch 16/20  Iteration 2779/3560 Training loss: 1.4762 0.8676 sec/batch\n",
      "Epoch 16/20  Iteration 2780/3560 Training loss: 1.4762 0.8758 sec/batch\n",
      "Epoch 16/20  Iteration 2781/3560 Training loss: 1.4760 0.8705 sec/batch\n",
      "Epoch 16/20  Iteration 2782/3560 Training loss: 1.4759 0.8657 sec/batch\n",
      "Epoch 16/20  Iteration 2783/3560 Training loss: 1.4759 0.8316 sec/batch\n",
      "Epoch 16/20  Iteration 2784/3560 Training loss: 1.4757 0.8355 sec/batch\n",
      "Epoch 16/20  Iteration 2785/3560 Training loss: 1.4755 0.8181 sec/batch\n",
      "Epoch 16/20  Iteration 2786/3560 Training loss: 1.4751 0.8339 sec/batch\n",
      "Epoch 16/20  Iteration 2787/3560 Training loss: 1.4752 0.8012 sec/batch\n",
      "Epoch 16/20  Iteration 2788/3560 Training loss: 1.4752 0.8026 sec/batch\n",
      "Epoch 16/20  Iteration 2789/3560 Training loss: 1.4751 0.8293 sec/batch\n",
      "Epoch 16/20  Iteration 2790/3560 Training loss: 1.4751 0.8182 sec/batch\n",
      "Epoch 16/20  Iteration 2791/3560 Training loss: 1.4750 0.8183 sec/batch\n",
      "Epoch 16/20  Iteration 2792/3560 Training loss: 1.4747 0.8999 sec/batch\n",
      "Epoch 16/20  Iteration 2793/3560 Training loss: 1.4744 0.8176 sec/batch\n",
      "Epoch 16/20  Iteration 2794/3560 Training loss: 1.4744 0.8249 sec/batch\n",
      "Epoch 16/20  Iteration 2795/3560 Training loss: 1.4743 0.7980 sec/batch\n",
      "Epoch 16/20  Iteration 2796/3560 Training loss: 1.4739 0.8166 sec/batch\n",
      "Epoch 16/20  Iteration 2797/3560 Training loss: 1.4739 0.8526 sec/batch\n",
      "Epoch 16/20  Iteration 2798/3560 Training loss: 1.4740 0.8009 sec/batch\n",
      "Epoch 16/20  Iteration 2799/3560 Training loss: 1.4738 0.8172 sec/batch\n",
      "Epoch 16/20  Iteration 2800/3560 Training loss: 1.4736 0.7932 sec/batch\n",
      "Epoch 16/20  Iteration 2801/3560 Training loss: 1.4732 0.8497 sec/batch\n",
      "Epoch 16/20  Iteration 2802/3560 Training loss: 1.4730 0.8014 sec/batch\n",
      "Epoch 16/20  Iteration 2803/3560 Training loss: 1.4730 0.8535 sec/batch\n",
      "Epoch 16/20  Iteration 2804/3560 Training loss: 1.4731 0.8472 sec/batch\n",
      "Epoch 16/20  Iteration 2805/3560 Training loss: 1.4731 0.7709 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2806/3560 Training loss: 1.4733 0.7932 sec/batch\n",
      "Epoch 16/20  Iteration 2807/3560 Training loss: 1.4735 0.7828 sec/batch\n",
      "Epoch 16/20  Iteration 2808/3560 Training loss: 1.4735 0.7743 sec/batch\n",
      "Epoch 16/20  Iteration 2809/3560 Training loss: 1.4735 0.8240 sec/batch\n",
      "Epoch 16/20  Iteration 2810/3560 Training loss: 1.4735 0.7980 sec/batch\n",
      "Epoch 16/20  Iteration 2811/3560 Training loss: 1.4739 0.7951 sec/batch\n",
      "Epoch 16/20  Iteration 2812/3560 Training loss: 1.4739 0.7920 sec/batch\n",
      "Epoch 16/20  Iteration 2813/3560 Training loss: 1.4738 0.7979 sec/batch\n",
      "Epoch 16/20  Iteration 2814/3560 Training loss: 1.4740 0.8011 sec/batch\n",
      "Epoch 16/20  Iteration 2815/3560 Training loss: 1.4739 0.7809 sec/batch\n",
      "Epoch 16/20  Iteration 2816/3560 Training loss: 1.4741 0.8191 sec/batch\n",
      "Epoch 16/20  Iteration 2817/3560 Training loss: 1.4741 0.8497 sec/batch\n",
      "Epoch 16/20  Iteration 2818/3560 Training loss: 1.4744 0.7767 sec/batch\n",
      "Epoch 16/20  Iteration 2819/3560 Training loss: 1.4745 0.7778 sec/batch\n",
      "Epoch 16/20  Iteration 2820/3560 Training loss: 1.4744 0.7684 sec/batch\n",
      "Epoch 16/20  Iteration 2821/3560 Training loss: 1.4741 0.7810 sec/batch\n",
      "Epoch 16/20  Iteration 2822/3560 Training loss: 1.4740 0.7907 sec/batch\n",
      "Epoch 16/20  Iteration 2823/3560 Training loss: 1.4740 0.7686 sec/batch\n",
      "Epoch 16/20  Iteration 2824/3560 Training loss: 1.4740 0.7872 sec/batch\n",
      "Epoch 16/20  Iteration 2825/3560 Training loss: 1.4740 0.8071 sec/batch\n",
      "Epoch 16/20  Iteration 2826/3560 Training loss: 1.4740 0.7701 sec/batch\n",
      "Epoch 16/20  Iteration 2827/3560 Training loss: 1.4742 0.7809 sec/batch\n",
      "Epoch 16/20  Iteration 2828/3560 Training loss: 1.4742 0.7996 sec/batch\n",
      "Epoch 16/20  Iteration 2829/3560 Training loss: 1.4739 0.8372 sec/batch\n",
      "Epoch 16/20  Iteration 2830/3560 Training loss: 1.4741 0.7916 sec/batch\n",
      "Epoch 16/20  Iteration 2831/3560 Training loss: 1.4743 0.7721 sec/batch\n",
      "Epoch 16/20  Iteration 2832/3560 Training loss: 1.4743 0.8047 sec/batch\n",
      "Epoch 16/20  Iteration 2833/3560 Training loss: 1.4743 0.8000 sec/batch\n",
      "Epoch 16/20  Iteration 2834/3560 Training loss: 1.4743 0.7964 sec/batch\n",
      "Epoch 16/20  Iteration 2835/3560 Training loss: 1.4744 0.7733 sec/batch\n",
      "Epoch 16/20  Iteration 2836/3560 Training loss: 1.4743 0.7599 sec/batch\n",
      "Epoch 16/20  Iteration 2837/3560 Training loss: 1.4745 0.7961 sec/batch\n",
      "Epoch 16/20  Iteration 2838/3560 Training loss: 1.4749 0.8150 sec/batch\n",
      "Epoch 16/20  Iteration 2839/3560 Training loss: 1.4749 0.7784 sec/batch\n",
      "Epoch 16/20  Iteration 2840/3560 Training loss: 1.4750 0.7571 sec/batch\n",
      "Epoch 16/20  Iteration 2841/3560 Training loss: 1.4749 0.7917 sec/batch\n",
      "Epoch 16/20  Iteration 2842/3560 Training loss: 1.4748 0.7970 sec/batch\n",
      "Epoch 16/20  Iteration 2843/3560 Training loss: 1.4750 0.7667 sec/batch\n",
      "Epoch 16/20  Iteration 2844/3560 Training loss: 1.4750 0.7866 sec/batch\n",
      "Epoch 16/20  Iteration 2845/3560 Training loss: 1.4751 0.8089 sec/batch\n",
      "Epoch 16/20  Iteration 2846/3560 Training loss: 1.4749 0.7709 sec/batch\n",
      "Epoch 16/20  Iteration 2847/3560 Training loss: 1.4748 0.7933 sec/batch\n",
      "Epoch 16/20  Iteration 2848/3560 Training loss: 1.4749 0.8532 sec/batch\n",
      "Epoch 17/20  Iteration 2849/3560 Training loss: 1.5729 0.9852 sec/batch\n",
      "Epoch 17/20  Iteration 2850/3560 Training loss: 1.5224 1.0404 sec/batch\n",
      "Epoch 17/20  Iteration 2851/3560 Training loss: 1.5047 1.0554 sec/batch\n",
      "Epoch 17/20  Iteration 2852/3560 Training loss: 1.5024 1.0168 sec/batch\n",
      "Epoch 17/20  Iteration 2853/3560 Training loss: 1.4946 1.1389 sec/batch\n",
      "Epoch 17/20  Iteration 2854/3560 Training loss: 1.4849 1.0300 sec/batch\n",
      "Epoch 17/20  Iteration 2855/3560 Training loss: 1.4853 1.0275 sec/batch\n",
      "Epoch 17/20  Iteration 2856/3560 Training loss: 1.4830 1.0669 sec/batch\n",
      "Epoch 17/20  Iteration 2857/3560 Training loss: 1.4815 0.8363 sec/batch\n",
      "Epoch 17/20  Iteration 2858/3560 Training loss: 1.4802 0.7910 sec/batch\n",
      "Epoch 17/20  Iteration 2859/3560 Training loss: 1.4769 0.8017 sec/batch\n",
      "Epoch 17/20  Iteration 2860/3560 Training loss: 1.4755 0.7822 sec/batch\n",
      "Epoch 17/20  Iteration 2861/3560 Training loss: 1.4755 0.8033 sec/batch\n",
      "Epoch 17/20  Iteration 2862/3560 Training loss: 1.4766 0.8399 sec/batch\n",
      "Epoch 17/20  Iteration 2863/3560 Training loss: 1.4756 0.7797 sec/batch\n",
      "Epoch 17/20  Iteration 2864/3560 Training loss: 1.4735 0.8282 sec/batch\n",
      "Epoch 17/20  Iteration 2865/3560 Training loss: 1.4737 0.8124 sec/batch\n",
      "Epoch 17/20  Iteration 2866/3560 Training loss: 1.4748 0.7632 sec/batch\n",
      "Epoch 17/20  Iteration 2867/3560 Training loss: 1.4748 0.7807 sec/batch\n",
      "Epoch 17/20  Iteration 2868/3560 Training loss: 1.4759 0.7843 sec/batch\n",
      "Epoch 17/20  Iteration 2869/3560 Training loss: 1.4754 0.7653 sec/batch\n",
      "Epoch 17/20  Iteration 2870/3560 Training loss: 1.4756 0.7743 sec/batch\n",
      "Epoch 17/20  Iteration 2871/3560 Training loss: 1.4751 0.7663 sec/batch\n",
      "Epoch 17/20  Iteration 2872/3560 Training loss: 1.4751 0.8015 sec/batch\n",
      "Epoch 17/20  Iteration 2873/3560 Training loss: 1.4750 0.7908 sec/batch\n",
      "Epoch 17/20  Iteration 2874/3560 Training loss: 1.4733 0.7945 sec/batch\n",
      "Epoch 17/20  Iteration 2875/3560 Training loss: 1.4720 0.7900 sec/batch\n",
      "Epoch 17/20  Iteration 2876/3560 Training loss: 1.4726 0.7595 sec/batch\n",
      "Epoch 17/20  Iteration 2877/3560 Training loss: 1.4728 0.8206 sec/batch\n",
      "Epoch 17/20  Iteration 2878/3560 Training loss: 1.4731 0.7982 sec/batch\n",
      "Epoch 17/20  Iteration 2879/3560 Training loss: 1.4726 0.7765 sec/batch\n",
      "Epoch 17/20  Iteration 2880/3560 Training loss: 1.4714 0.8018 sec/batch\n",
      "Epoch 17/20  Iteration 2881/3560 Training loss: 1.4719 0.7767 sec/batch\n",
      "Epoch 17/20  Iteration 2882/3560 Training loss: 1.4720 0.8032 sec/batch\n",
      "Epoch 17/20  Iteration 2883/3560 Training loss: 1.4717 0.8094 sec/batch\n",
      "Epoch 17/20  Iteration 2884/3560 Training loss: 1.4714 0.7769 sec/batch\n",
      "Epoch 17/20  Iteration 2885/3560 Training loss: 1.4707 0.7731 sec/batch\n",
      "Epoch 17/20  Iteration 2886/3560 Training loss: 1.4696 0.7772 sec/batch\n",
      "Epoch 17/20  Iteration 2887/3560 Training loss: 1.4684 0.8097 sec/batch\n",
      "Epoch 17/20  Iteration 2888/3560 Training loss: 1.4677 0.8015 sec/batch\n",
      "Epoch 17/20  Iteration 2889/3560 Training loss: 1.4672 0.7965 sec/batch\n",
      "Epoch 17/20  Iteration 2890/3560 Training loss: 1.4681 0.8130 sec/batch\n",
      "Epoch 17/20  Iteration 2891/3560 Training loss: 1.4678 0.8111 sec/batch\n",
      "Epoch 17/20  Iteration 2892/3560 Training loss: 1.4669 0.8104 sec/batch\n",
      "Epoch 17/20  Iteration 2893/3560 Training loss: 1.4671 0.7997 sec/batch\n",
      "Epoch 17/20  Iteration 2894/3560 Training loss: 1.4662 0.8026 sec/batch\n",
      "Epoch 17/20  Iteration 2895/3560 Training loss: 1.4659 0.8061 sec/batch\n",
      "Epoch 17/20  Iteration 2896/3560 Training loss: 1.4657 0.8059 sec/batch\n",
      "Epoch 17/20  Iteration 2897/3560 Training loss: 1.4655 0.7899 sec/batch\n",
      "Epoch 17/20  Iteration 2898/3560 Training loss: 1.4657 0.8169 sec/batch\n",
      "Epoch 17/20  Iteration 2899/3560 Training loss: 1.4653 0.7838 sec/batch\n",
      "Epoch 17/20  Iteration 2900/3560 Training loss: 1.4661 0.8026 sec/batch\n",
      "Epoch 17/20  Iteration 2901/3560 Training loss: 1.4660 0.7921 sec/batch\n",
      "Epoch 17/20  Iteration 2902/3560 Training loss: 1.4662 0.8464 sec/batch\n",
      "Epoch 17/20  Iteration 2903/3560 Training loss: 1.4660 0.7798 sec/batch\n",
      "Epoch 17/20  Iteration 2904/3560 Training loss: 1.4661 0.7993 sec/batch\n",
      "Epoch 17/20  Iteration 2905/3560 Training loss: 1.4665 0.7945 sec/batch\n",
      "Epoch 17/20  Iteration 2906/3560 Training loss: 1.4662 0.8000 sec/batch\n",
      "Epoch 17/20  Iteration 2907/3560 Training loss: 1.4656 0.7985 sec/batch\n",
      "Epoch 17/20  Iteration 2908/3560 Training loss: 1.4661 0.8006 sec/batch\n",
      "Epoch 17/20  Iteration 2909/3560 Training loss: 1.4660 0.7856 sec/batch\n",
      "Epoch 17/20  Iteration 2910/3560 Training loss: 1.4670 0.8082 sec/batch\n",
      "Epoch 17/20  Iteration 2911/3560 Training loss: 1.4674 0.7681 sec/batch\n",
      "Epoch 17/20  Iteration 2912/3560 Training loss: 1.4675 0.8092 sec/batch\n",
      "Epoch 17/20  Iteration 2913/3560 Training loss: 1.4676 0.8122 sec/batch\n",
      "Epoch 17/20  Iteration 2914/3560 Training loss: 1.4679 0.7979 sec/batch\n",
      "Epoch 17/20  Iteration 2915/3560 Training loss: 1.4681 0.8341 sec/batch\n",
      "Epoch 17/20  Iteration 2916/3560 Training loss: 1.4679 0.8111 sec/batch\n",
      "Epoch 17/20  Iteration 2917/3560 Training loss: 1.4680 0.7959 sec/batch\n",
      "Epoch 17/20  Iteration 2918/3560 Training loss: 1.4678 0.7999 sec/batch\n",
      "Epoch 17/20  Iteration 2919/3560 Training loss: 1.4683 0.8017 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 2920/3560 Training loss: 1.4685 0.7807 sec/batch\n",
      "Epoch 17/20  Iteration 2921/3560 Training loss: 1.4690 0.7999 sec/batch\n",
      "Epoch 17/20  Iteration 2922/3560 Training loss: 1.4688 0.8059 sec/batch\n",
      "Epoch 17/20  Iteration 2923/3560 Training loss: 1.4687 0.7614 sec/batch\n",
      "Epoch 17/20  Iteration 2924/3560 Training loss: 1.4689 0.7651 sec/batch\n",
      "Epoch 17/20  Iteration 2925/3560 Training loss: 1.4688 0.8217 sec/batch\n",
      "Epoch 17/20  Iteration 2926/3560 Training loss: 1.4687 0.8136 sec/batch\n",
      "Epoch 17/20  Iteration 2927/3560 Training loss: 1.4682 0.8260 sec/batch\n",
      "Epoch 17/20  Iteration 2928/3560 Training loss: 1.4681 0.8534 sec/batch\n",
      "Epoch 17/20  Iteration 2929/3560 Training loss: 1.4675 0.8043 sec/batch\n",
      "Epoch 17/20  Iteration 2930/3560 Training loss: 1.4675 0.7746 sec/batch\n",
      "Epoch 17/20  Iteration 2931/3560 Training loss: 1.4670 0.7612 sec/batch\n",
      "Epoch 17/20  Iteration 2932/3560 Training loss: 1.4668 0.7720 sec/batch\n",
      "Epoch 17/20  Iteration 2933/3560 Training loss: 1.4666 0.7777 sec/batch\n",
      "Epoch 17/20  Iteration 2934/3560 Training loss: 1.4665 0.7923 sec/batch\n",
      "Epoch 17/20  Iteration 2935/3560 Training loss: 1.4663 0.8235 sec/batch\n",
      "Epoch 17/20  Iteration 2936/3560 Training loss: 1.4660 0.7993 sec/batch\n",
      "Epoch 17/20  Iteration 2937/3560 Training loss: 1.4657 0.8164 sec/batch\n",
      "Epoch 17/20  Iteration 2938/3560 Training loss: 1.4657 0.7820 sec/batch\n",
      "Epoch 17/20  Iteration 2939/3560 Training loss: 1.4654 0.7996 sec/batch\n",
      "Epoch 17/20  Iteration 2940/3560 Training loss: 1.4653 0.8232 sec/batch\n",
      "Epoch 17/20  Iteration 2941/3560 Training loss: 1.4649 0.7694 sec/batch\n",
      "Epoch 17/20  Iteration 2942/3560 Training loss: 1.4646 0.7945 sec/batch\n",
      "Epoch 17/20  Iteration 2943/3560 Training loss: 1.4644 0.7996 sec/batch\n",
      "Epoch 17/20  Iteration 2944/3560 Training loss: 1.4644 0.8005 sec/batch\n",
      "Epoch 17/20  Iteration 2945/3560 Training loss: 1.4643 0.8192 sec/batch\n",
      "Epoch 17/20  Iteration 2946/3560 Training loss: 1.4639 0.8724 sec/batch\n",
      "Epoch 17/20  Iteration 2947/3560 Training loss: 1.4636 0.8322 sec/batch\n",
      "Epoch 17/20  Iteration 2948/3560 Training loss: 1.4633 0.8042 sec/batch\n",
      "Epoch 17/20  Iteration 2949/3560 Training loss: 1.4634 0.7808 sec/batch\n",
      "Epoch 17/20  Iteration 2950/3560 Training loss: 1.4634 0.7952 sec/batch\n",
      "Epoch 17/20  Iteration 2951/3560 Training loss: 1.4633 0.7960 sec/batch\n",
      "Epoch 17/20  Iteration 2952/3560 Training loss: 1.4632 0.7731 sec/batch\n",
      "Epoch 17/20  Iteration 2953/3560 Training loss: 1.4630 0.8081 sec/batch\n",
      "Epoch 17/20  Iteration 2954/3560 Training loss: 1.4629 0.7714 sec/batch\n",
      "Epoch 17/20  Iteration 2955/3560 Training loss: 1.4629 0.7989 sec/batch\n",
      "Epoch 17/20  Iteration 2956/3560 Training loss: 1.4629 0.7946 sec/batch\n",
      "Epoch 17/20  Iteration 2957/3560 Training loss: 1.4627 0.8080 sec/batch\n",
      "Epoch 17/20  Iteration 2958/3560 Training loss: 1.4628 0.7721 sec/batch\n",
      "Epoch 17/20  Iteration 2959/3560 Training loss: 1.4626 0.7970 sec/batch\n",
      "Epoch 17/20  Iteration 2960/3560 Training loss: 1.4625 0.7950 sec/batch\n",
      "Epoch 17/20  Iteration 2961/3560 Training loss: 1.4625 0.7839 sec/batch\n",
      "Epoch 17/20  Iteration 2962/3560 Training loss: 1.4623 0.7861 sec/batch\n",
      "Epoch 17/20  Iteration 2963/3560 Training loss: 1.4620 0.7849 sec/batch\n",
      "Epoch 17/20  Iteration 2964/3560 Training loss: 1.4616 0.7681 sec/batch\n",
      "Epoch 17/20  Iteration 2965/3560 Training loss: 1.4616 0.8170 sec/batch\n",
      "Epoch 17/20  Iteration 2966/3560 Training loss: 1.4616 0.8291 sec/batch\n",
      "Epoch 17/20  Iteration 2967/3560 Training loss: 1.4616 0.7950 sec/batch\n",
      "Epoch 17/20  Iteration 2968/3560 Training loss: 1.4614 0.7992 sec/batch\n",
      "Epoch 17/20  Iteration 2969/3560 Training loss: 1.4614 0.7788 sec/batch\n",
      "Epoch 17/20  Iteration 2970/3560 Training loss: 1.4610 0.8168 sec/batch\n",
      "Epoch 17/20  Iteration 2971/3560 Training loss: 1.4606 0.7830 sec/batch\n",
      "Epoch 17/20  Iteration 2972/3560 Training loss: 1.4605 0.7998 sec/batch\n",
      "Epoch 17/20  Iteration 2973/3560 Training loss: 1.4605 0.7890 sec/batch\n",
      "Epoch 17/20  Iteration 2974/3560 Training loss: 1.4601 0.7882 sec/batch\n",
      "Epoch 17/20  Iteration 2975/3560 Training loss: 1.4601 0.8436 sec/batch\n",
      "Epoch 17/20  Iteration 2976/3560 Training loss: 1.4601 0.8022 sec/batch\n",
      "Epoch 17/20  Iteration 2977/3560 Training loss: 1.4600 0.7624 sec/batch\n",
      "Epoch 17/20  Iteration 2978/3560 Training loss: 1.4598 0.8354 sec/batch\n",
      "Epoch 17/20  Iteration 2979/3560 Training loss: 1.4593 0.7702 sec/batch\n",
      "Epoch 17/20  Iteration 2980/3560 Training loss: 1.4591 0.7813 sec/batch\n",
      "Epoch 17/20  Iteration 2981/3560 Training loss: 1.4592 0.7957 sec/batch\n",
      "Epoch 17/20  Iteration 2982/3560 Training loss: 1.4593 0.7725 sec/batch\n",
      "Epoch 17/20  Iteration 2983/3560 Training loss: 1.4593 0.7730 sec/batch\n",
      "Epoch 17/20  Iteration 2984/3560 Training loss: 1.4594 0.7569 sec/batch\n",
      "Epoch 17/20  Iteration 2985/3560 Training loss: 1.4596 0.7834 sec/batch\n",
      "Epoch 17/20  Iteration 2986/3560 Training loss: 1.4597 0.7840 sec/batch\n",
      "Epoch 17/20  Iteration 2987/3560 Training loss: 1.4597 0.7998 sec/batch\n",
      "Epoch 17/20  Iteration 2988/3560 Training loss: 1.4598 0.7889 sec/batch\n",
      "Epoch 17/20  Iteration 2989/3560 Training loss: 1.4602 0.7756 sec/batch\n",
      "Epoch 17/20  Iteration 2990/3560 Training loss: 1.4601 0.7691 sec/batch\n",
      "Epoch 17/20  Iteration 2991/3560 Training loss: 1.4600 0.8183 sec/batch\n",
      "Epoch 17/20  Iteration 2992/3560 Training loss: 1.4603 0.7727 sec/batch\n",
      "Epoch 17/20  Iteration 2993/3560 Training loss: 1.4602 0.7915 sec/batch\n",
      "Epoch 17/20  Iteration 2994/3560 Training loss: 1.4604 0.7752 sec/batch\n",
      "Epoch 17/20  Iteration 2995/3560 Training loss: 1.4604 0.8119 sec/batch\n",
      "Epoch 17/20  Iteration 2996/3560 Training loss: 1.4607 0.7757 sec/batch\n",
      "Epoch 17/20  Iteration 2997/3560 Training loss: 1.4608 0.7987 sec/batch\n",
      "Epoch 17/20  Iteration 2998/3560 Training loss: 1.4607 0.8031 sec/batch\n",
      "Epoch 17/20  Iteration 2999/3560 Training loss: 1.4605 0.7935 sec/batch\n",
      "Epoch 17/20  Iteration 3000/3560 Training loss: 1.4604 0.7911 sec/batch\n",
      "Epoch 17/20  Iteration 3001/3560 Training loss: 1.4605 0.8140 sec/batch\n",
      "Epoch 17/20  Iteration 3002/3560 Training loss: 1.4605 0.7995 sec/batch\n",
      "Epoch 17/20  Iteration 3003/3560 Training loss: 1.4605 0.8530 sec/batch\n",
      "Epoch 17/20  Iteration 3004/3560 Training loss: 1.4605 0.8326 sec/batch\n",
      "Epoch 17/20  Iteration 3005/3560 Training loss: 1.4606 0.7755 sec/batch\n",
      "Epoch 17/20  Iteration 3006/3560 Training loss: 1.4606 0.8167 sec/batch\n",
      "Epoch 17/20  Iteration 3007/3560 Training loss: 1.4604 0.7707 sec/batch\n",
      "Epoch 17/20  Iteration 3008/3560 Training loss: 1.4605 0.8040 sec/batch\n",
      "Epoch 17/20  Iteration 3009/3560 Training loss: 1.4608 0.7653 sec/batch\n",
      "Epoch 17/20  Iteration 3010/3560 Training loss: 1.4608 0.8104 sec/batch\n",
      "Epoch 17/20  Iteration 3011/3560 Training loss: 1.4608 0.7876 sec/batch\n",
      "Epoch 17/20  Iteration 3012/3560 Training loss: 1.4608 0.8044 sec/batch\n",
      "Epoch 17/20  Iteration 3013/3560 Training loss: 1.4608 0.8149 sec/batch\n",
      "Epoch 17/20  Iteration 3014/3560 Training loss: 1.4608 0.8036 sec/batch\n",
      "Epoch 17/20  Iteration 3015/3560 Training loss: 1.4609 0.7825 sec/batch\n",
      "Epoch 17/20  Iteration 3016/3560 Training loss: 1.4614 0.8736 sec/batch\n",
      "Epoch 17/20  Iteration 3017/3560 Training loss: 1.4614 0.8001 sec/batch\n",
      "Epoch 17/20  Iteration 3018/3560 Training loss: 1.4614 0.7745 sec/batch\n",
      "Epoch 17/20  Iteration 3019/3560 Training loss: 1.4613 0.8017 sec/batch\n",
      "Epoch 17/20  Iteration 3020/3560 Training loss: 1.4611 0.7909 sec/batch\n",
      "Epoch 17/20  Iteration 3021/3560 Training loss: 1.4613 0.7985 sec/batch\n",
      "Epoch 17/20  Iteration 3022/3560 Training loss: 1.4613 0.8016 sec/batch\n",
      "Epoch 17/20  Iteration 3023/3560 Training loss: 1.4614 0.7836 sec/batch\n",
      "Epoch 17/20  Iteration 3024/3560 Training loss: 1.4613 0.8073 sec/batch\n",
      "Epoch 17/20  Iteration 3025/3560 Training loss: 1.4612 0.7720 sec/batch\n",
      "Epoch 17/20  Iteration 3026/3560 Training loss: 1.4614 0.8053 sec/batch\n",
      "Epoch 18/20  Iteration 3027/3560 Training loss: 1.5521 0.7904 sec/batch\n",
      "Epoch 18/20  Iteration 3028/3560 Training loss: 1.5069 0.8024 sec/batch\n",
      "Epoch 18/20  Iteration 3029/3560 Training loss: 1.4936 0.8357 sec/batch\n",
      "Epoch 18/20  Iteration 3030/3560 Training loss: 1.4887 0.7792 sec/batch\n",
      "Epoch 18/20  Iteration 3031/3560 Training loss: 1.4781 0.7727 sec/batch\n",
      "Epoch 18/20  Iteration 3032/3560 Training loss: 1.4677 0.7720 sec/batch\n",
      "Epoch 18/20  Iteration 3033/3560 Training loss: 1.4687 0.7971 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3034/3560 Training loss: 1.4655 0.7661 sec/batch\n",
      "Epoch 18/20  Iteration 3035/3560 Training loss: 1.4659 0.7649 sec/batch\n",
      "Epoch 18/20  Iteration 3036/3560 Training loss: 1.4652 0.8132 sec/batch\n",
      "Epoch 18/20  Iteration 3037/3560 Training loss: 1.4617 0.7797 sec/batch\n",
      "Epoch 18/20  Iteration 3038/3560 Training loss: 1.4617 0.8088 sec/batch\n",
      "Epoch 18/20  Iteration 3039/3560 Training loss: 1.4610 0.7846 sec/batch\n",
      "Epoch 18/20  Iteration 3040/3560 Training loss: 1.4620 0.7819 sec/batch\n",
      "Epoch 18/20  Iteration 3041/3560 Training loss: 1.4611 0.7929 sec/batch\n",
      "Epoch 18/20  Iteration 3042/3560 Training loss: 1.4590 0.8167 sec/batch\n",
      "Epoch 18/20  Iteration 3043/3560 Training loss: 1.4600 0.7944 sec/batch\n",
      "Epoch 18/20  Iteration 3044/3560 Training loss: 1.4618 0.7739 sec/batch\n",
      "Epoch 18/20  Iteration 3045/3560 Training loss: 1.4619 0.8104 sec/batch\n",
      "Epoch 18/20  Iteration 3046/3560 Training loss: 1.4623 0.8087 sec/batch\n",
      "Epoch 18/20  Iteration 3047/3560 Training loss: 1.4614 0.7556 sec/batch\n",
      "Epoch 18/20  Iteration 3048/3560 Training loss: 1.4623 0.7702 sec/batch\n",
      "Epoch 18/20  Iteration 3049/3560 Training loss: 1.4614 0.7957 sec/batch\n",
      "Epoch 18/20  Iteration 3050/3560 Training loss: 1.4615 0.8001 sec/batch\n",
      "Epoch 18/20  Iteration 3051/3560 Training loss: 1.4615 0.8082 sec/batch\n",
      "Epoch 18/20  Iteration 3052/3560 Training loss: 1.4596 0.8160 sec/batch\n",
      "Epoch 18/20  Iteration 3053/3560 Training loss: 1.4581 0.7966 sec/batch\n",
      "Epoch 18/20  Iteration 3054/3560 Training loss: 1.4588 0.8456 sec/batch\n",
      "Epoch 18/20  Iteration 3055/3560 Training loss: 1.4593 0.7859 sec/batch\n",
      "Epoch 18/20  Iteration 3056/3560 Training loss: 1.4594 0.7943 sec/batch\n",
      "Epoch 18/20  Iteration 3057/3560 Training loss: 1.4591 0.8001 sec/batch\n",
      "Epoch 18/20  Iteration 3058/3560 Training loss: 1.4581 0.7979 sec/batch\n",
      "Epoch 18/20  Iteration 3059/3560 Training loss: 1.4585 0.7828 sec/batch\n",
      "Epoch 18/20  Iteration 3060/3560 Training loss: 1.4588 0.7825 sec/batch\n",
      "Epoch 18/20  Iteration 3061/3560 Training loss: 1.4586 0.7777 sec/batch\n",
      "Epoch 18/20  Iteration 3062/3560 Training loss: 1.4584 0.7588 sec/batch\n",
      "Epoch 18/20  Iteration 3063/3560 Training loss: 1.4575 0.7731 sec/batch\n",
      "Epoch 18/20  Iteration 3064/3560 Training loss: 1.4564 0.7679 sec/batch\n",
      "Epoch 18/20  Iteration 3065/3560 Training loss: 1.4553 0.8015 sec/batch\n",
      "Epoch 18/20  Iteration 3066/3560 Training loss: 1.4549 0.7869 sec/batch\n",
      "Epoch 18/20  Iteration 3067/3560 Training loss: 1.4543 0.8098 sec/batch\n",
      "Epoch 18/20  Iteration 3068/3560 Training loss: 1.4552 0.7854 sec/batch\n",
      "Epoch 18/20  Iteration 3069/3560 Training loss: 1.4549 0.8049 sec/batch\n",
      "Epoch 18/20  Iteration 3070/3560 Training loss: 1.4543 0.7641 sec/batch\n",
      "Epoch 18/20  Iteration 3071/3560 Training loss: 1.4548 0.8043 sec/batch\n",
      "Epoch 18/20  Iteration 3072/3560 Training loss: 1.4539 1.0092 sec/batch\n",
      "Epoch 18/20  Iteration 3073/3560 Training loss: 1.4538 1.1530 sec/batch\n",
      "Epoch 18/20  Iteration 3074/3560 Training loss: 1.4534 1.1001 sec/batch\n",
      "Epoch 18/20  Iteration 3075/3560 Training loss: 1.4533 1.2152 sec/batch\n",
      "Epoch 18/20  Iteration 3076/3560 Training loss: 1.4535 1.0884 sec/batch\n",
      "Epoch 18/20  Iteration 3077/3560 Training loss: 1.4532 1.2573 sec/batch\n",
      "Epoch 18/20  Iteration 3078/3560 Training loss: 1.4541 1.1793 sec/batch\n",
      "Epoch 18/20  Iteration 3079/3560 Training loss: 1.4539 1.1188 sec/batch\n",
      "Epoch 18/20  Iteration 3080/3560 Training loss: 1.4541 0.9705 sec/batch\n",
      "Epoch 18/20  Iteration 3081/3560 Training loss: 1.4537 0.8348 sec/batch\n",
      "Epoch 18/20  Iteration 3082/3560 Training loss: 1.4539 0.8228 sec/batch\n",
      "Epoch 18/20  Iteration 3083/3560 Training loss: 1.4543 0.7720 sec/batch\n",
      "Epoch 18/20  Iteration 3084/3560 Training loss: 1.4542 0.7693 sec/batch\n",
      "Epoch 18/20  Iteration 3085/3560 Training loss: 1.4537 0.8201 sec/batch\n",
      "Epoch 18/20  Iteration 3086/3560 Training loss: 1.4542 0.8545 sec/batch\n",
      "Epoch 18/20  Iteration 3087/3560 Training loss: 1.4542 0.8344 sec/batch\n",
      "Epoch 18/20  Iteration 3088/3560 Training loss: 1.4551 0.8077 sec/batch\n",
      "Epoch 18/20  Iteration 3089/3560 Training loss: 1.4555 0.8334 sec/batch\n",
      "Epoch 18/20  Iteration 3090/3560 Training loss: 1.4556 0.7706 sec/batch\n",
      "Epoch 18/20  Iteration 3091/3560 Training loss: 1.4557 0.7643 sec/batch\n",
      "Epoch 18/20  Iteration 3092/3560 Training loss: 1.4559 0.7946 sec/batch\n",
      "Epoch 18/20  Iteration 3093/3560 Training loss: 1.4560 0.7658 sec/batch\n",
      "Epoch 18/20  Iteration 3094/3560 Training loss: 1.4558 0.8531 sec/batch\n",
      "Epoch 18/20  Iteration 3095/3560 Training loss: 1.4559 0.8377 sec/batch\n",
      "Epoch 18/20  Iteration 3096/3560 Training loss: 1.4558 0.7714 sec/batch\n",
      "Epoch 18/20  Iteration 3097/3560 Training loss: 1.4563 0.8062 sec/batch\n",
      "Epoch 18/20  Iteration 3098/3560 Training loss: 1.4566 0.8017 sec/batch\n",
      "Epoch 18/20  Iteration 3099/3560 Training loss: 1.4571 0.8250 sec/batch\n",
      "Epoch 18/20  Iteration 3100/3560 Training loss: 1.4568 0.8165 sec/batch\n",
      "Epoch 18/20  Iteration 3101/3560 Training loss: 1.4566 0.8380 sec/batch\n",
      "Epoch 18/20  Iteration 3102/3560 Training loss: 1.4569 0.8029 sec/batch\n",
      "Epoch 18/20  Iteration 3103/3560 Training loss: 1.4568 0.7887 sec/batch\n",
      "Epoch 18/20  Iteration 3104/3560 Training loss: 1.4568 0.7789 sec/batch\n",
      "Epoch 18/20  Iteration 3105/3560 Training loss: 1.4562 0.8831 sec/batch\n",
      "Epoch 18/20  Iteration 3106/3560 Training loss: 1.4561 0.8197 sec/batch\n",
      "Epoch 18/20  Iteration 3107/3560 Training loss: 1.4556 0.9759 sec/batch\n",
      "Epoch 18/20  Iteration 3108/3560 Training loss: 1.4555 0.7980 sec/batch\n",
      "Epoch 18/20  Iteration 3109/3560 Training loss: 1.4548 0.8589 sec/batch\n",
      "Epoch 18/20  Iteration 3110/3560 Training loss: 1.4547 0.8037 sec/batch\n",
      "Epoch 18/20  Iteration 3111/3560 Training loss: 1.4544 0.7942 sec/batch\n",
      "Epoch 18/20  Iteration 3112/3560 Training loss: 1.4543 0.7966 sec/batch\n",
      "Epoch 18/20  Iteration 3113/3560 Training loss: 1.4541 0.8400 sec/batch\n",
      "Epoch 18/20  Iteration 3114/3560 Training loss: 1.4539 0.8059 sec/batch\n",
      "Epoch 18/20  Iteration 3115/3560 Training loss: 1.4535 0.9007 sec/batch\n",
      "Epoch 18/20  Iteration 3116/3560 Training loss: 1.4536 0.9609 sec/batch\n",
      "Epoch 18/20  Iteration 3117/3560 Training loss: 1.4534 0.9056 sec/batch\n",
      "Epoch 18/20  Iteration 3118/3560 Training loss: 1.4532 0.8477 sec/batch\n",
      "Epoch 18/20  Iteration 3119/3560 Training loss: 1.4530 0.8187 sec/batch\n",
      "Epoch 18/20  Iteration 3120/3560 Training loss: 1.4527 0.8317 sec/batch\n",
      "Epoch 18/20  Iteration 3121/3560 Training loss: 1.4524 0.9319 sec/batch\n",
      "Epoch 18/20  Iteration 3122/3560 Training loss: 1.4526 0.8670 sec/batch\n",
      "Epoch 18/20  Iteration 3123/3560 Training loss: 1.4526 0.8494 sec/batch\n",
      "Epoch 18/20  Iteration 3124/3560 Training loss: 1.4522 0.8256 sec/batch\n",
      "Epoch 18/20  Iteration 3125/3560 Training loss: 1.4518 0.8263 sec/batch\n",
      "Epoch 18/20  Iteration 3126/3560 Training loss: 1.4514 0.8329 sec/batch\n",
      "Epoch 18/20  Iteration 3127/3560 Training loss: 1.4515 0.8059 sec/batch\n",
      "Epoch 18/20  Iteration 3128/3560 Training loss: 1.4514 0.9566 sec/batch\n",
      "Epoch 18/20  Iteration 3129/3560 Training loss: 1.4513 0.8724 sec/batch\n",
      "Epoch 18/20  Iteration 3130/3560 Training loss: 1.4512 0.8402 sec/batch\n",
      "Epoch 18/20  Iteration 3131/3560 Training loss: 1.4511 0.7582 sec/batch\n",
      "Epoch 18/20  Iteration 3132/3560 Training loss: 1.4509 0.7937 sec/batch\n",
      "Epoch 18/20  Iteration 3133/3560 Training loss: 1.4509 0.8330 sec/batch\n",
      "Epoch 18/20  Iteration 3134/3560 Training loss: 1.4509 0.8411 sec/batch\n",
      "Epoch 18/20  Iteration 3135/3560 Training loss: 1.4507 0.7813 sec/batch\n",
      "Epoch 18/20  Iteration 3136/3560 Training loss: 1.4508 0.7951 sec/batch\n",
      "Epoch 18/20  Iteration 3137/3560 Training loss: 1.4507 0.7940 sec/batch\n",
      "Epoch 18/20  Iteration 3138/3560 Training loss: 1.4506 0.7995 sec/batch\n",
      "Epoch 18/20  Iteration 3139/3560 Training loss: 1.4505 0.7626 sec/batch\n",
      "Epoch 18/20  Iteration 3140/3560 Training loss: 1.4502 0.7843 sec/batch\n",
      "Epoch 18/20  Iteration 3141/3560 Training loss: 1.4500 0.7563 sec/batch\n",
      "Epoch 18/20  Iteration 3142/3560 Training loss: 1.4496 0.8193 sec/batch\n",
      "Epoch 18/20  Iteration 3143/3560 Training loss: 1.4496 0.7865 sec/batch\n",
      "Epoch 18/20  Iteration 3144/3560 Training loss: 1.4496 0.7864 sec/batch\n",
      "Epoch 18/20  Iteration 3145/3560 Training loss: 1.4496 0.7868 sec/batch\n",
      "Epoch 18/20  Iteration 3146/3560 Training loss: 1.4496 0.7709 sec/batch\n",
      "Epoch 18/20  Iteration 3147/3560 Training loss: 1.4495 0.7921 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3148/3560 Training loss: 1.4491 0.7667 sec/batch\n",
      "Epoch 18/20  Iteration 3149/3560 Training loss: 1.4488 0.7951 sec/batch\n",
      "Epoch 18/20  Iteration 3150/3560 Training loss: 1.4488 0.8824 sec/batch\n",
      "Epoch 18/20  Iteration 3151/3560 Training loss: 1.4488 0.7739 sec/batch\n",
      "Epoch 18/20  Iteration 3152/3560 Training loss: 1.4484 0.7808 sec/batch\n",
      "Epoch 18/20  Iteration 3153/3560 Training loss: 1.4485 0.7652 sec/batch\n",
      "Epoch 18/20  Iteration 3154/3560 Training loss: 1.4485 0.7765 sec/batch\n",
      "Epoch 18/20  Iteration 3155/3560 Training loss: 1.4484 0.8183 sec/batch\n",
      "Epoch 18/20  Iteration 3156/3560 Training loss: 1.4481 0.7975 sec/batch\n",
      "Epoch 18/20  Iteration 3157/3560 Training loss: 1.4477 0.7824 sec/batch\n",
      "Epoch 18/20  Iteration 3158/3560 Training loss: 1.4476 0.7959 sec/batch\n",
      "Epoch 18/20  Iteration 3159/3560 Training loss: 1.4478 0.7792 sec/batch\n",
      "Epoch 18/20  Iteration 3160/3560 Training loss: 1.4478 0.7538 sec/batch\n",
      "Epoch 18/20  Iteration 3161/3560 Training loss: 1.4478 0.7664 sec/batch\n",
      "Epoch 18/20  Iteration 3162/3560 Training loss: 1.4478 0.7609 sec/batch\n",
      "Epoch 18/20  Iteration 3163/3560 Training loss: 1.4480 0.8430 sec/batch\n",
      "Epoch 18/20  Iteration 3164/3560 Training loss: 1.4481 0.7860 sec/batch\n",
      "Epoch 18/20  Iteration 3165/3560 Training loss: 1.4481 0.8032 sec/batch\n",
      "Epoch 18/20  Iteration 3166/3560 Training loss: 1.4481 0.7662 sec/batch\n",
      "Epoch 18/20  Iteration 3167/3560 Training loss: 1.4485 0.8671 sec/batch\n",
      "Epoch 18/20  Iteration 3168/3560 Training loss: 1.4484 0.7758 sec/batch\n",
      "Epoch 18/20  Iteration 3169/3560 Training loss: 1.4483 0.8421 sec/batch\n",
      "Epoch 18/20  Iteration 3170/3560 Training loss: 1.4485 0.7595 sec/batch\n",
      "Epoch 18/20  Iteration 3171/3560 Training loss: 1.4483 0.7866 sec/batch\n",
      "Epoch 18/20  Iteration 3172/3560 Training loss: 1.4486 0.7920 sec/batch\n",
      "Epoch 18/20  Iteration 3173/3560 Training loss: 1.4487 0.7750 sec/batch\n",
      "Epoch 18/20  Iteration 3174/3560 Training loss: 1.4489 0.7744 sec/batch\n",
      "Epoch 18/20  Iteration 3175/3560 Training loss: 1.4490 0.8291 sec/batch\n",
      "Epoch 18/20  Iteration 3176/3560 Training loss: 1.4489 0.7995 sec/batch\n",
      "Epoch 18/20  Iteration 3177/3560 Training loss: 1.4486 0.7570 sec/batch\n",
      "Epoch 18/20  Iteration 3178/3560 Training loss: 1.4485 0.7837 sec/batch\n",
      "Epoch 18/20  Iteration 3179/3560 Training loss: 1.4486 0.8116 sec/batch\n",
      "Epoch 18/20  Iteration 3180/3560 Training loss: 1.4486 0.7831 sec/batch\n",
      "Epoch 18/20  Iteration 3181/3560 Training loss: 1.4486 0.8117 sec/batch\n",
      "Epoch 18/20  Iteration 3182/3560 Training loss: 1.4486 0.7720 sec/batch\n",
      "Epoch 18/20  Iteration 3183/3560 Training loss: 1.4486 0.7914 sec/batch\n",
      "Epoch 18/20  Iteration 3184/3560 Training loss: 1.4486 0.7987 sec/batch\n",
      "Epoch 18/20  Iteration 3185/3560 Training loss: 1.4484 0.8030 sec/batch\n",
      "Epoch 18/20  Iteration 3186/3560 Training loss: 1.4485 0.8018 sec/batch\n",
      "Epoch 18/20  Iteration 3187/3560 Training loss: 1.4487 0.8011 sec/batch\n",
      "Epoch 18/20  Iteration 3188/3560 Training loss: 1.4488 0.8115 sec/batch\n",
      "Epoch 18/20  Iteration 3189/3560 Training loss: 1.4488 0.7737 sec/batch\n",
      "Epoch 18/20  Iteration 3190/3560 Training loss: 1.4488 0.7808 sec/batch\n",
      "Epoch 18/20  Iteration 3191/3560 Training loss: 1.4488 0.8017 sec/batch\n",
      "Epoch 18/20  Iteration 3192/3560 Training loss: 1.4487 0.7653 sec/batch\n",
      "Epoch 18/20  Iteration 3193/3560 Training loss: 1.4489 0.7917 sec/batch\n",
      "Epoch 18/20  Iteration 3194/3560 Training loss: 1.4494 0.7884 sec/batch\n",
      "Epoch 18/20  Iteration 3195/3560 Training loss: 1.4494 0.7890 sec/batch\n",
      "Epoch 18/20  Iteration 3196/3560 Training loss: 1.4495 0.8098 sec/batch\n",
      "Epoch 18/20  Iteration 3197/3560 Training loss: 1.4495 0.7662 sec/batch\n",
      "Epoch 18/20  Iteration 3198/3560 Training loss: 1.4493 0.7973 sec/batch\n",
      "Epoch 18/20  Iteration 3199/3560 Training loss: 1.4494 0.7834 sec/batch\n",
      "Epoch 18/20  Iteration 3200/3560 Training loss: 1.4495 0.8134 sec/batch\n",
      "Epoch 18/20  Iteration 3201/3560 Training loss: 1.4496 0.8030 sec/batch\n",
      "Epoch 18/20  Iteration 3202/3560 Training loss: 1.4495 0.7570 sec/batch\n",
      "Epoch 18/20  Iteration 3203/3560 Training loss: 1.4493 0.7770 sec/batch\n",
      "Epoch 18/20  Iteration 3204/3560 Training loss: 1.4494 0.8089 sec/batch\n",
      "Epoch 19/20  Iteration 3205/3560 Training loss: 1.5417 0.7856 sec/batch\n",
      "Epoch 19/20  Iteration 3206/3560 Training loss: 1.4988 0.8107 sec/batch\n",
      "Epoch 19/20  Iteration 3207/3560 Training loss: 1.4824 0.7555 sec/batch\n",
      "Epoch 19/20  Iteration 3208/3560 Training loss: 1.4736 0.7959 sec/batch\n",
      "Epoch 19/20  Iteration 3209/3560 Training loss: 1.4642 0.7756 sec/batch\n",
      "Epoch 19/20  Iteration 3210/3560 Training loss: 1.4541 0.7555 sec/batch\n",
      "Epoch 19/20  Iteration 3211/3560 Training loss: 1.4542 0.7761 sec/batch\n",
      "Epoch 19/20  Iteration 3212/3560 Training loss: 1.4521 0.7730 sec/batch\n",
      "Epoch 19/20  Iteration 3213/3560 Training loss: 1.4526 0.7952 sec/batch\n",
      "Epoch 19/20  Iteration 3214/3560 Training loss: 1.4511 0.8389 sec/batch\n",
      "Epoch 19/20  Iteration 3215/3560 Training loss: 1.4482 0.7689 sec/batch\n",
      "Epoch 19/20  Iteration 3216/3560 Training loss: 1.4470 0.7971 sec/batch\n",
      "Epoch 19/20  Iteration 3217/3560 Training loss: 1.4464 0.8103 sec/batch\n",
      "Epoch 19/20  Iteration 3218/3560 Training loss: 1.4479 0.7647 sec/batch\n",
      "Epoch 19/20  Iteration 3219/3560 Training loss: 1.4469 0.7983 sec/batch\n",
      "Epoch 19/20  Iteration 3220/3560 Training loss: 1.4450 0.7488 sec/batch\n",
      "Epoch 19/20  Iteration 3221/3560 Training loss: 1.4454 0.8020 sec/batch\n",
      "Epoch 19/20  Iteration 3222/3560 Training loss: 1.4466 0.7976 sec/batch\n",
      "Epoch 19/20  Iteration 3223/3560 Training loss: 1.4471 0.7541 sec/batch\n",
      "Epoch 19/20  Iteration 3224/3560 Training loss: 1.4487 0.7680 sec/batch\n",
      "Epoch 19/20  Iteration 3225/3560 Training loss: 1.4481 0.8242 sec/batch\n",
      "Epoch 19/20  Iteration 3226/3560 Training loss: 1.4484 0.8156 sec/batch\n",
      "Epoch 19/20  Iteration 3227/3560 Training loss: 1.4481 0.8253 sec/batch\n",
      "Epoch 19/20  Iteration 3228/3560 Training loss: 1.4482 0.7805 sec/batch\n",
      "Epoch 19/20  Iteration 3229/3560 Training loss: 1.4481 0.7938 sec/batch\n",
      "Epoch 19/20  Iteration 3230/3560 Training loss: 1.4469 0.7818 sec/batch\n",
      "Epoch 19/20  Iteration 3231/3560 Training loss: 1.4455 0.7685 sec/batch\n",
      "Epoch 19/20  Iteration 3232/3560 Training loss: 1.4462 0.7999 sec/batch\n",
      "Epoch 19/20  Iteration 3233/3560 Training loss: 1.4465 0.8018 sec/batch\n",
      "Epoch 19/20  Iteration 3234/3560 Training loss: 1.4471 0.7926 sec/batch\n",
      "Epoch 19/20  Iteration 3235/3560 Training loss: 1.4468 0.7837 sec/batch\n",
      "Epoch 19/20  Iteration 3236/3560 Training loss: 1.4456 0.7702 sec/batch\n",
      "Epoch 19/20  Iteration 3237/3560 Training loss: 1.4460 0.7852 sec/batch\n",
      "Epoch 19/20  Iteration 3238/3560 Training loss: 1.4466 0.7541 sec/batch\n",
      "Epoch 19/20  Iteration 3239/3560 Training loss: 1.4463 0.8493 sec/batch\n",
      "Epoch 19/20  Iteration 3240/3560 Training loss: 1.4459 0.7861 sec/batch\n",
      "Epoch 19/20  Iteration 3241/3560 Training loss: 1.4452 0.7765 sec/batch\n",
      "Epoch 19/20  Iteration 3242/3560 Training loss: 1.4440 0.7957 sec/batch\n",
      "Epoch 19/20  Iteration 3243/3560 Training loss: 1.4428 0.7831 sec/batch\n",
      "Epoch 19/20  Iteration 3244/3560 Training loss: 1.4422 0.8034 sec/batch\n",
      "Epoch 19/20  Iteration 3245/3560 Training loss: 1.4417 0.7724 sec/batch\n",
      "Epoch 19/20  Iteration 3246/3560 Training loss: 1.4424 0.7862 sec/batch\n",
      "Epoch 19/20  Iteration 3247/3560 Training loss: 1.4421 0.7584 sec/batch\n",
      "Epoch 19/20  Iteration 3248/3560 Training loss: 1.4416 0.7845 sec/batch\n",
      "Epoch 19/20  Iteration 3249/3560 Training loss: 1.4418 0.7803 sec/batch\n",
      "Epoch 19/20  Iteration 3250/3560 Training loss: 1.4409 0.8121 sec/batch\n",
      "Epoch 19/20  Iteration 3251/3560 Training loss: 1.4408 0.7952 sec/batch\n",
      "Epoch 19/20  Iteration 3252/3560 Training loss: 1.4405 0.8136 sec/batch\n",
      "Epoch 19/20  Iteration 3253/3560 Training loss: 1.4405 0.7786 sec/batch\n",
      "Epoch 19/20  Iteration 3254/3560 Training loss: 1.4409 0.7949 sec/batch\n",
      "Epoch 19/20  Iteration 3255/3560 Training loss: 1.4403 0.7723 sec/batch\n",
      "Epoch 19/20  Iteration 3256/3560 Training loss: 1.4410 0.7744 sec/batch\n",
      "Epoch 19/20  Iteration 3257/3560 Training loss: 1.4410 0.7701 sec/batch\n",
      "Epoch 19/20  Iteration 3258/3560 Training loss: 1.4413 0.8128 sec/batch\n",
      "Epoch 19/20  Iteration 3259/3560 Training loss: 1.4412 0.7762 sec/batch\n",
      "Epoch 19/20  Iteration 3260/3560 Training loss: 1.4413 0.7957 sec/batch\n",
      "Epoch 19/20  Iteration 3261/3560 Training loss: 1.4416 0.7811 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3262/3560 Training loss: 1.4414 0.8004 sec/batch\n",
      "Epoch 19/20  Iteration 3263/3560 Training loss: 1.4408 0.8046 sec/batch\n",
      "Epoch 19/20  Iteration 3264/3560 Training loss: 1.4415 0.7781 sec/batch\n",
      "Epoch 19/20  Iteration 3265/3560 Training loss: 1.4414 0.7955 sec/batch\n",
      "Epoch 19/20  Iteration 3266/3560 Training loss: 1.4424 0.7679 sec/batch\n",
      "Epoch 19/20  Iteration 3267/3560 Training loss: 1.4428 0.7873 sec/batch\n",
      "Epoch 19/20  Iteration 3268/3560 Training loss: 1.4430 0.7810 sec/batch\n",
      "Epoch 19/20  Iteration 3269/3560 Training loss: 1.4429 0.7689 sec/batch\n",
      "Epoch 19/20  Iteration 3270/3560 Training loss: 1.4431 0.7776 sec/batch\n",
      "Epoch 19/20  Iteration 3271/3560 Training loss: 1.4432 0.7869 sec/batch\n",
      "Epoch 19/20  Iteration 3272/3560 Training loss: 1.4430 0.7721 sec/batch\n",
      "Epoch 19/20  Iteration 3273/3560 Training loss: 1.4431 0.7909 sec/batch\n",
      "Epoch 19/20  Iteration 3274/3560 Training loss: 1.4430 0.7587 sec/batch\n",
      "Epoch 19/20  Iteration 3275/3560 Training loss: 1.4434 0.8098 sec/batch\n",
      "Epoch 19/20  Iteration 3276/3560 Training loss: 1.4436 0.7597 sec/batch\n",
      "Epoch 19/20  Iteration 3277/3560 Training loss: 1.4441 0.8418 sec/batch\n",
      "Epoch 19/20  Iteration 3278/3560 Training loss: 1.4439 0.8092 sec/batch\n",
      "Epoch 19/20  Iteration 3279/3560 Training loss: 1.4437 0.8089 sec/batch\n",
      "Epoch 19/20  Iteration 3280/3560 Training loss: 1.4439 0.7900 sec/batch\n",
      "Epoch 19/20  Iteration 3281/3560 Training loss: 1.4438 0.7794 sec/batch\n",
      "Epoch 19/20  Iteration 3282/3560 Training loss: 1.4438 0.7568 sec/batch\n",
      "Epoch 19/20  Iteration 3283/3560 Training loss: 1.4433 0.8061 sec/batch\n",
      "Epoch 19/20  Iteration 3284/3560 Training loss: 1.4431 0.7609 sec/batch\n",
      "Epoch 19/20  Iteration 3285/3560 Training loss: 1.4427 0.7763 sec/batch\n",
      "Epoch 19/20  Iteration 3286/3560 Training loss: 1.4426 0.7862 sec/batch\n",
      "Epoch 19/20  Iteration 3287/3560 Training loss: 1.4421 0.8165 sec/batch\n",
      "Epoch 19/20  Iteration 3288/3560 Training loss: 1.4420 0.7792 sec/batch\n",
      "Epoch 19/20  Iteration 3289/3560 Training loss: 1.4417 0.7966 sec/batch\n",
      "Epoch 19/20  Iteration 3290/3560 Training loss: 1.4415 0.7756 sec/batch\n",
      "Epoch 19/20  Iteration 3291/3560 Training loss: 1.4412 0.8105 sec/batch\n",
      "Epoch 19/20  Iteration 3292/3560 Training loss: 1.4411 0.7874 sec/batch\n",
      "Epoch 19/20  Iteration 3293/3560 Training loss: 1.4406 0.9128 sec/batch\n",
      "Epoch 19/20  Iteration 3294/3560 Training loss: 1.4408 0.9835 sec/batch\n",
      "Epoch 19/20  Iteration 3295/3560 Training loss: 1.4406 1.0011 sec/batch\n",
      "Epoch 19/20  Iteration 3296/3560 Training loss: 1.4404 1.0339 sec/batch\n",
      "Epoch 19/20  Iteration 3297/3560 Training loss: 1.4402 0.9968 sec/batch\n",
      "Epoch 19/20  Iteration 3298/3560 Training loss: 1.4399 1.0086 sec/batch\n",
      "Epoch 19/20  Iteration 3299/3560 Training loss: 1.4397 1.0289 sec/batch\n",
      "Epoch 19/20  Iteration 3300/3560 Training loss: 1.4398 1.0506 sec/batch\n",
      "Epoch 19/20  Iteration 3301/3560 Training loss: 1.4398 1.0744 sec/batch\n",
      "Epoch 19/20  Iteration 3302/3560 Training loss: 1.4395 0.7635 sec/batch\n",
      "Epoch 19/20  Iteration 3303/3560 Training loss: 1.4391 0.7893 sec/batch\n",
      "Epoch 19/20  Iteration 3304/3560 Training loss: 1.4387 0.7963 sec/batch\n",
      "Epoch 19/20  Iteration 3305/3560 Training loss: 1.4388 0.7773 sec/batch\n",
      "Epoch 19/20  Iteration 3306/3560 Training loss: 1.4387 0.7598 sec/batch\n",
      "Epoch 19/20  Iteration 3307/3560 Training loss: 1.4386 0.8035 sec/batch\n",
      "Epoch 19/20  Iteration 3308/3560 Training loss: 1.4385 0.7923 sec/batch\n",
      "Epoch 19/20  Iteration 3309/3560 Training loss: 1.4384 0.7631 sec/batch\n",
      "Epoch 19/20  Iteration 3310/3560 Training loss: 1.4383 0.8373 sec/batch\n",
      "Epoch 19/20  Iteration 3311/3560 Training loss: 1.4384 1.0095 sec/batch\n",
      "Epoch 19/20  Iteration 3312/3560 Training loss: 1.4383 0.8386 sec/batch\n",
      "Epoch 19/20  Iteration 3313/3560 Training loss: 1.4382 0.8961 sec/batch\n",
      "Epoch 19/20  Iteration 3314/3560 Training loss: 1.4383 0.7848 sec/batch\n",
      "Epoch 19/20  Iteration 3315/3560 Training loss: 1.4381 0.8573 sec/batch\n",
      "Epoch 19/20  Iteration 3316/3560 Training loss: 1.4380 0.7906 sec/batch\n",
      "Epoch 19/20  Iteration 3317/3560 Training loss: 1.4380 0.9401 sec/batch\n",
      "Epoch 19/20  Iteration 3318/3560 Training loss: 1.4378 1.0305 sec/batch\n",
      "Epoch 19/20  Iteration 3319/3560 Training loss: 1.4376 0.8797 sec/batch\n",
      "Epoch 19/20  Iteration 3320/3560 Training loss: 1.4373 1.0373 sec/batch\n",
      "Epoch 19/20  Iteration 3321/3560 Training loss: 1.4372 0.9107 sec/batch\n",
      "Epoch 19/20  Iteration 3322/3560 Training loss: 1.4372 0.7929 sec/batch\n",
      "Epoch 19/20  Iteration 3323/3560 Training loss: 1.4371 0.7777 sec/batch\n",
      "Epoch 19/20  Iteration 3324/3560 Training loss: 1.4370 0.8083 sec/batch\n",
      "Epoch 19/20  Iteration 3325/3560 Training loss: 1.4369 0.7970 sec/batch\n",
      "Epoch 19/20  Iteration 3326/3560 Training loss: 1.4366 0.7863 sec/batch\n",
      "Epoch 19/20  Iteration 3327/3560 Training loss: 1.4362 0.8215 sec/batch\n",
      "Epoch 19/20  Iteration 3328/3560 Training loss: 1.4362 0.7998 sec/batch\n",
      "Epoch 19/20  Iteration 3329/3560 Training loss: 1.4363 0.7746 sec/batch\n",
      "Epoch 19/20  Iteration 3330/3560 Training loss: 1.4359 0.7581 sec/batch\n",
      "Epoch 19/20  Iteration 3331/3560 Training loss: 1.4360 0.7849 sec/batch\n",
      "Epoch 19/20  Iteration 3332/3560 Training loss: 1.4361 0.7463 sec/batch\n",
      "Epoch 19/20  Iteration 3333/3560 Training loss: 1.4360 0.7757 sec/batch\n",
      "Epoch 19/20  Iteration 3334/3560 Training loss: 1.4358 0.7642 sec/batch\n",
      "Epoch 19/20  Iteration 3335/3560 Training loss: 1.4354 0.8075 sec/batch\n",
      "Epoch 19/20  Iteration 3336/3560 Training loss: 1.4352 0.8197 sec/batch\n",
      "Epoch 19/20  Iteration 3337/3560 Training loss: 1.4354 0.7816 sec/batch\n",
      "Epoch 19/20  Iteration 3338/3560 Training loss: 1.4354 0.8352 sec/batch\n",
      "Epoch 19/20  Iteration 3339/3560 Training loss: 1.4354 0.7736 sec/batch\n",
      "Epoch 19/20  Iteration 3340/3560 Training loss: 1.4355 0.9299 sec/batch\n",
      "Epoch 19/20  Iteration 3341/3560 Training loss: 1.4358 0.8419 sec/batch\n",
      "Epoch 19/20  Iteration 3342/3560 Training loss: 1.4359 0.7980 sec/batch\n",
      "Epoch 19/20  Iteration 3343/3560 Training loss: 1.4359 0.8365 sec/batch\n",
      "Epoch 19/20  Iteration 3344/3560 Training loss: 1.4360 0.7715 sec/batch\n",
      "Epoch 19/20  Iteration 3345/3560 Training loss: 1.4363 0.7763 sec/batch\n",
      "Epoch 19/20  Iteration 3346/3560 Training loss: 1.4363 0.7772 sec/batch\n",
      "Epoch 19/20  Iteration 3347/3560 Training loss: 1.4363 0.8067 sec/batch\n",
      "Epoch 19/20  Iteration 3348/3560 Training loss: 1.4365 0.7644 sec/batch\n",
      "Epoch 19/20  Iteration 3349/3560 Training loss: 1.4364 0.7646 sec/batch\n",
      "Epoch 19/20  Iteration 3350/3560 Training loss: 1.4367 0.8616 sec/batch\n",
      "Epoch 19/20  Iteration 3351/3560 Training loss: 1.4368 0.8195 sec/batch\n",
      "Epoch 19/20  Iteration 3352/3560 Training loss: 1.4370 0.7637 sec/batch\n",
      "Epoch 19/20  Iteration 3353/3560 Training loss: 1.4372 0.7843 sec/batch\n",
      "Epoch 19/20  Iteration 3354/3560 Training loss: 1.4371 0.7880 sec/batch\n",
      "Epoch 19/20  Iteration 3355/3560 Training loss: 1.4368 0.8306 sec/batch\n",
      "Epoch 19/20  Iteration 3356/3560 Training loss: 1.4368 0.7616 sec/batch\n",
      "Epoch 19/20  Iteration 3357/3560 Training loss: 1.4369 0.7835 sec/batch\n",
      "Epoch 19/20  Iteration 3358/3560 Training loss: 1.4368 0.7486 sec/batch\n",
      "Epoch 19/20  Iteration 3359/3560 Training loss: 1.4368 0.8016 sec/batch\n",
      "Epoch 19/20  Iteration 3360/3560 Training loss: 1.4368 0.7980 sec/batch\n",
      "Epoch 19/20  Iteration 3361/3560 Training loss: 1.4369 0.7590 sec/batch\n",
      "Epoch 19/20  Iteration 3362/3560 Training loss: 1.4369 0.7601 sec/batch\n",
      "Epoch 19/20  Iteration 3363/3560 Training loss: 1.4367 0.8510 sec/batch\n",
      "Epoch 19/20  Iteration 3364/3560 Training loss: 1.4368 0.7978 sec/batch\n",
      "Epoch 19/20  Iteration 3365/3560 Training loss: 1.4371 0.7881 sec/batch\n",
      "Epoch 19/20  Iteration 3366/3560 Training loss: 1.4371 0.7788 sec/batch\n",
      "Epoch 19/20  Iteration 3367/3560 Training loss: 1.4371 0.7831 sec/batch\n",
      "Epoch 19/20  Iteration 3368/3560 Training loss: 1.4371 0.8494 sec/batch\n",
      "Epoch 19/20  Iteration 3369/3560 Training loss: 1.4372 0.8749 sec/batch\n",
      "Epoch 19/20  Iteration 3370/3560 Training loss: 1.4372 0.8042 sec/batch\n",
      "Epoch 19/20  Iteration 3371/3560 Training loss: 1.4374 0.7539 sec/batch\n",
      "Epoch 19/20  Iteration 3372/3560 Training loss: 1.4378 0.8816 sec/batch\n",
      "Epoch 19/20  Iteration 3373/3560 Training loss: 1.4378 0.8029 sec/batch\n",
      "Epoch 19/20  Iteration 3374/3560 Training loss: 1.4378 0.7522 sec/batch\n",
      "Epoch 19/20  Iteration 3375/3560 Training loss: 1.4377 0.8042 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3376/3560 Training loss: 1.4376 0.9033 sec/batch\n",
      "Epoch 19/20  Iteration 3377/3560 Training loss: 1.4378 0.7636 sec/batch\n",
      "Epoch 19/20  Iteration 3378/3560 Training loss: 1.4378 0.7887 sec/batch\n",
      "Epoch 19/20  Iteration 3379/3560 Training loss: 1.4380 0.7854 sec/batch\n",
      "Epoch 19/20  Iteration 3380/3560 Training loss: 1.4379 0.8200 sec/batch\n",
      "Epoch 19/20  Iteration 3381/3560 Training loss: 1.4377 0.7715 sec/batch\n",
      "Epoch 19/20  Iteration 3382/3560 Training loss: 1.4378 0.8099 sec/batch\n",
      "Epoch 20/20  Iteration 3383/3560 Training loss: 1.5255 0.7980 sec/batch\n",
      "Epoch 20/20  Iteration 3384/3560 Training loss: 1.4853 0.8073 sec/batch\n",
      "Epoch 20/20  Iteration 3385/3560 Training loss: 1.4684 0.7728 sec/batch\n",
      "Epoch 20/20  Iteration 3386/3560 Training loss: 1.4639 0.7715 sec/batch\n",
      "Epoch 20/20  Iteration 3387/3560 Training loss: 1.4534 0.8047 sec/batch\n",
      "Epoch 20/20  Iteration 3388/3560 Training loss: 1.4440 0.9128 sec/batch\n",
      "Epoch 20/20  Iteration 3389/3560 Training loss: 1.4435 0.9035 sec/batch\n",
      "Epoch 20/20  Iteration 3390/3560 Training loss: 1.4412 0.8154 sec/batch\n",
      "Epoch 20/20  Iteration 3391/3560 Training loss: 1.4418 0.8368 sec/batch\n",
      "Epoch 20/20  Iteration 3392/3560 Training loss: 1.4410 0.8390 sec/batch\n",
      "Epoch 20/20  Iteration 3393/3560 Training loss: 1.4385 0.7919 sec/batch\n",
      "Epoch 20/20  Iteration 3394/3560 Training loss: 1.4374 0.8018 sec/batch\n",
      "Epoch 20/20  Iteration 3395/3560 Training loss: 1.4372 0.9093 sec/batch\n",
      "Epoch 20/20  Iteration 3396/3560 Training loss: 1.4385 0.8482 sec/batch\n",
      "Epoch 20/20  Iteration 3397/3560 Training loss: 1.4375 0.8091 sec/batch\n",
      "Epoch 20/20  Iteration 3398/3560 Training loss: 1.4352 0.7630 sec/batch\n",
      "Epoch 20/20  Iteration 3399/3560 Training loss: 1.4356 0.8254 sec/batch\n",
      "Epoch 20/20  Iteration 3400/3560 Training loss: 1.4374 0.9046 sec/batch\n",
      "Epoch 20/20  Iteration 3401/3560 Training loss: 1.4370 0.8380 sec/batch\n",
      "Epoch 20/20  Iteration 3402/3560 Training loss: 1.4379 0.8684 sec/batch\n",
      "Epoch 20/20  Iteration 3403/3560 Training loss: 1.4375 0.7595 sec/batch\n",
      "Epoch 20/20  Iteration 3404/3560 Training loss: 1.4382 0.8201 sec/batch\n",
      "Epoch 20/20  Iteration 3405/3560 Training loss: 1.4377 0.8949 sec/batch\n",
      "Epoch 20/20  Iteration 3406/3560 Training loss: 1.4375 0.8008 sec/batch\n",
      "Epoch 20/20  Iteration 3407/3560 Training loss: 1.4373 0.7961 sec/batch\n",
      "Epoch 20/20  Iteration 3408/3560 Training loss: 1.4360 0.7840 sec/batch\n",
      "Epoch 20/20  Iteration 3409/3560 Training loss: 1.4350 0.8190 sec/batch\n",
      "Epoch 20/20  Iteration 3410/3560 Training loss: 1.4357 0.8275 sec/batch\n",
      "Epoch 20/20  Iteration 3411/3560 Training loss: 1.4358 0.8477 sec/batch\n",
      "Epoch 20/20  Iteration 3412/3560 Training loss: 1.4363 0.9340 sec/batch\n",
      "Epoch 20/20  Iteration 3413/3560 Training loss: 1.4360 0.8105 sec/batch\n",
      "Epoch 20/20  Iteration 3414/3560 Training loss: 1.4350 0.8756 sec/batch\n",
      "Epoch 20/20  Iteration 3415/3560 Training loss: 1.4355 0.8245 sec/batch\n",
      "Epoch 20/20  Iteration 3416/3560 Training loss: 1.4356 0.8020 sec/batch\n",
      "Epoch 20/20  Iteration 3417/3560 Training loss: 1.4356 0.7779 sec/batch\n",
      "Epoch 20/20  Iteration 3418/3560 Training loss: 1.4356 0.7697 sec/batch\n",
      "Epoch 20/20  Iteration 3419/3560 Training loss: 1.4350 0.8171 sec/batch\n",
      "Epoch 20/20  Iteration 3420/3560 Training loss: 1.4341 0.8168 sec/batch\n",
      "Epoch 20/20  Iteration 3421/3560 Training loss: 1.4327 0.7972 sec/batch\n",
      "Epoch 20/20  Iteration 3422/3560 Training loss: 1.4321 0.7741 sec/batch\n",
      "Epoch 20/20  Iteration 3423/3560 Training loss: 1.4317 0.7648 sec/batch\n",
      "Epoch 20/20  Iteration 3424/3560 Training loss: 1.4324 0.8489 sec/batch\n",
      "Epoch 20/20  Iteration 3425/3560 Training loss: 1.4322 0.8093 sec/batch\n",
      "Epoch 20/20  Iteration 3426/3560 Training loss: 1.4316 0.7958 sec/batch\n",
      "Epoch 20/20  Iteration 3427/3560 Training loss: 1.4318 0.7877 sec/batch\n",
      "Epoch 20/20  Iteration 3428/3560 Training loss: 1.4311 0.7888 sec/batch\n",
      "Epoch 20/20  Iteration 3429/3560 Training loss: 1.4308 0.7914 sec/batch\n",
      "Epoch 20/20  Iteration 3430/3560 Training loss: 1.4305 0.7594 sec/batch\n",
      "Epoch 20/20  Iteration 3431/3560 Training loss: 1.4304 0.7931 sec/batch\n",
      "Epoch 20/20  Iteration 3432/3560 Training loss: 1.4307 0.7851 sec/batch\n",
      "Epoch 20/20  Iteration 3433/3560 Training loss: 1.4301 0.7593 sec/batch\n",
      "Epoch 20/20  Iteration 3434/3560 Training loss: 1.4308 0.7589 sec/batch\n",
      "Epoch 20/20  Iteration 3435/3560 Training loss: 1.4308 0.7914 sec/batch\n",
      "Epoch 20/20  Iteration 3436/3560 Training loss: 1.4310 0.8400 sec/batch\n",
      "Epoch 20/20  Iteration 3437/3560 Training loss: 1.4309 0.8546 sec/batch\n",
      "Epoch 20/20  Iteration 3438/3560 Training loss: 1.4310 0.8460 sec/batch\n",
      "Epoch 20/20  Iteration 3439/3560 Training loss: 1.4314 0.7990 sec/batch\n",
      "Epoch 20/20  Iteration 3440/3560 Training loss: 1.4311 0.8570 sec/batch\n",
      "Epoch 20/20  Iteration 3441/3560 Training loss: 1.4306 0.9295 sec/batch\n",
      "Epoch 20/20  Iteration 3442/3560 Training loss: 1.4313 1.0064 sec/batch\n",
      "Epoch 20/20  Iteration 3443/3560 Training loss: 1.4312 0.9516 sec/batch\n",
      "Epoch 20/20  Iteration 3444/3560 Training loss: 1.4322 0.9371 sec/batch\n",
      "Epoch 20/20  Iteration 3445/3560 Training loss: 1.4325 0.9527 sec/batch\n",
      "Epoch 20/20  Iteration 3446/3560 Training loss: 1.4328 0.7791 sec/batch\n",
      "Epoch 20/20  Iteration 3447/3560 Training loss: 1.4326 0.9049 sec/batch\n",
      "Epoch 20/20  Iteration 3448/3560 Training loss: 1.4328 0.8455 sec/batch\n",
      "Epoch 20/20  Iteration 3449/3560 Training loss: 1.4332 0.8851 sec/batch\n",
      "Epoch 20/20  Iteration 3450/3560 Training loss: 1.4329 0.7940 sec/batch\n",
      "Epoch 20/20  Iteration 3451/3560 Training loss: 1.4330 1.1350 sec/batch\n",
      "Epoch 20/20  Iteration 3452/3560 Training loss: 1.4330 1.1533 sec/batch\n",
      "Epoch 20/20  Iteration 3453/3560 Training loss: 1.4336 0.8270 sec/batch\n",
      "Epoch 20/20  Iteration 3454/3560 Training loss: 1.4339 0.8821 sec/batch\n",
      "Epoch 20/20  Iteration 3455/3560 Training loss: 1.4344 0.9246 sec/batch\n",
      "Epoch 20/20  Iteration 3456/3560 Training loss: 1.4342 0.9947 sec/batch\n",
      "Epoch 20/20  Iteration 3457/3560 Training loss: 1.4342 0.9434 sec/batch\n",
      "Epoch 20/20  Iteration 3458/3560 Training loss: 1.4343 0.9829 sec/batch\n",
      "Epoch 20/20  Iteration 3459/3560 Training loss: 1.4342 0.8817 sec/batch\n",
      "Epoch 20/20  Iteration 3460/3560 Training loss: 1.4342 0.8398 sec/batch\n",
      "Epoch 20/20  Iteration 3461/3560 Training loss: 1.4336 0.8114 sec/batch\n",
      "Epoch 20/20  Iteration 3462/3560 Training loss: 1.4335 0.8228 sec/batch\n",
      "Epoch 20/20  Iteration 3463/3560 Training loss: 1.4329 0.8235 sec/batch\n",
      "Epoch 20/20  Iteration 3464/3560 Training loss: 1.4328 0.8431 sec/batch\n",
      "Epoch 20/20  Iteration 3465/3560 Training loss: 1.4322 0.8283 sec/batch\n",
      "Epoch 20/20  Iteration 3466/3560 Training loss: 1.4321 0.8321 sec/batch\n",
      "Epoch 20/20  Iteration 3467/3560 Training loss: 1.4319 0.7985 sec/batch\n",
      "Epoch 20/20  Iteration 3468/3560 Training loss: 1.4317 0.8064 sec/batch\n",
      "Epoch 20/20  Iteration 3469/3560 Training loss: 1.4314 0.8342 sec/batch\n",
      "Epoch 20/20  Iteration 3470/3560 Training loss: 1.4312 0.8392 sec/batch\n",
      "Epoch 20/20  Iteration 3471/3560 Training loss: 1.4308 0.8182 sec/batch\n",
      "Epoch 20/20  Iteration 3472/3560 Training loss: 1.4309 0.8630 sec/batch\n",
      "Epoch 20/20  Iteration 3473/3560 Training loss: 1.4308 0.8372 sec/batch\n",
      "Epoch 20/20  Iteration 3474/3560 Training loss: 1.4308 0.8027 sec/batch\n",
      "Epoch 20/20  Iteration 3475/3560 Training loss: 1.4304 0.8244 sec/batch\n",
      "Epoch 20/20  Iteration 3476/3560 Training loss: 1.4301 0.8174 sec/batch\n",
      "Epoch 20/20  Iteration 3477/3560 Training loss: 1.4298 0.7461 sec/batch\n",
      "Epoch 20/20  Iteration 3478/3560 Training loss: 1.4299 0.7521 sec/batch\n",
      "Epoch 20/20  Iteration 3479/3560 Training loss: 1.4299 0.7045 sec/batch\n",
      "Epoch 20/20  Iteration 3480/3560 Training loss: 1.4295 0.7849 sec/batch\n",
      "Epoch 20/20  Iteration 3481/3560 Training loss: 1.4292 0.7398 sec/batch\n",
      "Epoch 20/20  Iteration 3482/3560 Training loss: 1.4288 0.7198 sec/batch\n",
      "Epoch 20/20  Iteration 3483/3560 Training loss: 1.4289 0.7342 sec/batch\n",
      "Epoch 20/20  Iteration 3484/3560 Training loss: 1.4288 0.7617 sec/batch\n",
      "Epoch 20/20  Iteration 3485/3560 Training loss: 1.4287 0.7785 sec/batch\n",
      "Epoch 20/20  Iteration 3486/3560 Training loss: 1.4286 0.7283 sec/batch\n",
      "Epoch 20/20  Iteration 3487/3560 Training loss: 1.4284 0.7118 sec/batch\n",
      "Epoch 20/20  Iteration 3488/3560 Training loss: 1.4283 0.7083 sec/batch\n",
      "Epoch 20/20  Iteration 3489/3560 Training loss: 1.4282 0.7430 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3490/3560 Training loss: 1.4282 0.7164 sec/batch\n",
      "Epoch 20/20  Iteration 3491/3560 Training loss: 1.4280 0.7471 sec/batch\n",
      "Epoch 20/20  Iteration 3492/3560 Training loss: 1.4281 0.7385 sec/batch\n",
      "Epoch 20/20  Iteration 3493/3560 Training loss: 1.4280 0.7115 sec/batch\n",
      "Epoch 20/20  Iteration 3494/3560 Training loss: 1.4279 0.7627 sec/batch\n",
      "Epoch 20/20  Iteration 3495/3560 Training loss: 1.4278 0.7707 sec/batch\n",
      "Epoch 20/20  Iteration 3496/3560 Training loss: 1.4276 0.7582 sec/batch\n",
      "Epoch 20/20  Iteration 3497/3560 Training loss: 1.4275 0.7340 sec/batch\n",
      "Epoch 20/20  Iteration 3498/3560 Training loss: 1.4271 0.7682 sec/batch\n",
      "Epoch 20/20  Iteration 3499/3560 Training loss: 1.4272 0.7108 sec/batch\n",
      "Epoch 20/20  Iteration 3500/3560 Training loss: 1.4273 0.7208 sec/batch\n",
      "Epoch 20/20  Iteration 3501/3560 Training loss: 1.4272 0.7739 sec/batch\n",
      "Epoch 20/20  Iteration 3502/3560 Training loss: 1.4272 0.8223 sec/batch\n",
      "Epoch 20/20  Iteration 3503/3560 Training loss: 1.4272 0.8265 sec/batch\n",
      "Epoch 20/20  Iteration 3504/3560 Training loss: 1.4269 0.8290 sec/batch\n",
      "Epoch 20/20  Iteration 3505/3560 Training loss: 1.4266 0.7676 sec/batch\n",
      "Epoch 20/20  Iteration 3506/3560 Training loss: 1.4266 0.8087 sec/batch\n",
      "Epoch 20/20  Iteration 3507/3560 Training loss: 1.4265 0.7828 sec/batch\n",
      "Epoch 20/20  Iteration 3508/3560 Training loss: 1.4262 0.7773 sec/batch\n",
      "Epoch 20/20  Iteration 3509/3560 Training loss: 1.4262 0.8319 sec/batch\n",
      "Epoch 20/20  Iteration 3510/3560 Training loss: 1.4263 0.7667 sec/batch\n",
      "Epoch 20/20  Iteration 3511/3560 Training loss: 1.4261 0.9767 sec/batch\n",
      "Epoch 20/20  Iteration 3512/3560 Training loss: 1.4259 1.1527 sec/batch\n",
      "Epoch 20/20  Iteration 3513/3560 Training loss: 1.4256 1.0181 sec/batch\n",
      "Epoch 20/20  Iteration 3514/3560 Training loss: 1.4254 1.0201 sec/batch\n",
      "Epoch 20/20  Iteration 3515/3560 Training loss: 1.4256 0.9996 sec/batch\n",
      "Epoch 20/20  Iteration 3516/3560 Training loss: 1.4256 0.9028 sec/batch\n",
      "Epoch 20/20  Iteration 3517/3560 Training loss: 1.4255 0.9676 sec/batch\n",
      "Epoch 20/20  Iteration 3518/3560 Training loss: 1.4255 1.0030 sec/batch\n",
      "Epoch 20/20  Iteration 3519/3560 Training loss: 1.4257 0.8539 sec/batch\n",
      "Epoch 20/20  Iteration 3520/3560 Training loss: 1.4258 0.7372 sec/batch\n",
      "Epoch 20/20  Iteration 3521/3560 Training loss: 1.4259 0.7759 sec/batch\n",
      "Epoch 20/20  Iteration 3522/3560 Training loss: 1.4259 0.7825 sec/batch\n",
      "Epoch 20/20  Iteration 3523/3560 Training loss: 1.4263 0.8291 sec/batch\n",
      "Epoch 20/20  Iteration 3524/3560 Training loss: 1.4263 0.7348 sec/batch\n",
      "Epoch 20/20  Iteration 3525/3560 Training loss: 1.4263 0.7138 sec/batch\n",
      "Epoch 20/20  Iteration 3526/3560 Training loss: 1.4265 0.7516 sec/batch\n",
      "Epoch 20/20  Iteration 3527/3560 Training loss: 1.4264 0.8031 sec/batch\n",
      "Epoch 20/20  Iteration 3528/3560 Training loss: 1.4267 0.7492 sec/batch\n",
      "Epoch 20/20  Iteration 3529/3560 Training loss: 1.4267 0.7433 sec/batch\n",
      "Epoch 20/20  Iteration 3530/3560 Training loss: 1.4269 0.7092 sec/batch\n",
      "Epoch 20/20  Iteration 3531/3560 Training loss: 1.4271 0.7880 sec/batch\n",
      "Epoch 20/20  Iteration 3532/3560 Training loss: 1.4269 0.7565 sec/batch\n",
      "Epoch 20/20  Iteration 3533/3560 Training loss: 1.4267 0.7164 sec/batch\n",
      "Epoch 20/20  Iteration 3534/3560 Training loss: 1.4266 0.7503 sec/batch\n",
      "Epoch 20/20  Iteration 3535/3560 Training loss: 1.4267 0.7804 sec/batch\n",
      "Epoch 20/20  Iteration 3536/3560 Training loss: 1.4267 0.7120 sec/batch\n",
      "Epoch 20/20  Iteration 3537/3560 Training loss: 1.4267 0.7274 sec/batch\n",
      "Epoch 20/20  Iteration 3538/3560 Training loss: 1.4268 0.7506 sec/batch\n",
      "Epoch 20/20  Iteration 3539/3560 Training loss: 1.4268 0.7136 sec/batch\n",
      "Epoch 20/20  Iteration 3540/3560 Training loss: 1.4268 0.7353 sec/batch\n",
      "Epoch 20/20  Iteration 3541/3560 Training loss: 1.4266 0.7162 sec/batch\n",
      "Epoch 20/20  Iteration 3542/3560 Training loss: 1.4267 0.7125 sec/batch\n",
      "Epoch 20/20  Iteration 3543/3560 Training loss: 1.4269 0.7294 sec/batch\n",
      "Epoch 20/20  Iteration 3544/3560 Training loss: 1.4269 0.7089 sec/batch\n",
      "Epoch 20/20  Iteration 3545/3560 Training loss: 1.4269 0.7186 sec/batch\n",
      "Epoch 20/20  Iteration 3546/3560 Training loss: 1.4269 0.7020 sec/batch\n",
      "Epoch 20/20  Iteration 3547/3560 Training loss: 1.4269 0.7247 sec/batch\n",
      "Epoch 20/20  Iteration 3548/3560 Training loss: 1.4269 0.7105 sec/batch\n",
      "Epoch 20/20  Iteration 3549/3560 Training loss: 1.4271 0.7585 sec/batch\n",
      "Epoch 20/20  Iteration 3550/3560 Training loss: 1.4275 0.7040 sec/batch\n",
      "Epoch 20/20  Iteration 3551/3560 Training loss: 1.4275 0.7090 sec/batch\n",
      "Epoch 20/20  Iteration 3552/3560 Training loss: 1.4276 0.7307 sec/batch\n",
      "Epoch 20/20  Iteration 3553/3560 Training loss: 1.4275 0.7133 sec/batch\n",
      "Epoch 20/20  Iteration 3554/3560 Training loss: 1.4273 0.7061 sec/batch\n",
      "Epoch 20/20  Iteration 3555/3560 Training loss: 1.4275 0.7210 sec/batch\n",
      "Epoch 20/20  Iteration 3556/3560 Training loss: 1.4276 0.7333 sec/batch\n",
      "Epoch 20/20  Iteration 3557/3560 Training loss: 1.4276 0.7047 sec/batch\n",
      "Epoch 20/20  Iteration 3558/3560 Training loss: 1.4276 0.7529 sec/batch\n",
      "Epoch 20/20  Iteration 3559/3560 Training loss: 1.4275 0.7254 sec/batch\n",
      "Epoch 20/20  Iteration 3560/3560 Training loss: 1.4276 0.7022 sec/batch\n",
      "Epoch 1/20  Iteration 1/3560 Training loss: 4.4156 1.2688 sec/batch\n",
      "Epoch 1/20  Iteration 2/3560 Training loss: 4.3516 1.1587 sec/batch\n",
      "Epoch 1/20  Iteration 3/3560 Training loss: 4.3095 1.1592 sec/batch\n",
      "Epoch 1/20  Iteration 4/3560 Training loss: 4.1708 1.1585 sec/batch\n",
      "Epoch 1/20  Iteration 5/3560 Training loss: 4.0916 1.1537 sec/batch\n",
      "Epoch 1/20  Iteration 6/3560 Training loss: 4.0136 1.1490 sec/batch\n",
      "Epoch 1/20  Iteration 7/3560 Training loss: 3.9371 1.1472 sec/batch\n",
      "Epoch 1/20  Iteration 8/3560 Training loss: 3.8622 1.1711 sec/batch\n",
      "Epoch 1/20  Iteration 9/3560 Training loss: 3.7990 1.1452 sec/batch\n",
      "Epoch 1/20  Iteration 10/3560 Training loss: 3.7466 1.1796 sec/batch\n",
      "Epoch 1/20  Iteration 11/3560 Training loss: 3.7011 1.1486 sec/batch\n",
      "Epoch 1/20  Iteration 12/3560 Training loss: 3.6642 1.1513 sec/batch\n",
      "Epoch 1/20  Iteration 13/3560 Training loss: 3.6311 1.1632 sec/batch\n",
      "Epoch 1/20  Iteration 14/3560 Training loss: 3.6018 1.1534 sec/batch\n",
      "Epoch 1/20  Iteration 15/3560 Training loss: 3.5750 1.1622 sec/batch\n",
      "Epoch 1/20  Iteration 16/3560 Training loss: 3.5520 1.1528 sec/batch\n",
      "Epoch 1/20  Iteration 17/3560 Training loss: 3.5307 1.1839 sec/batch\n",
      "Epoch 1/20  Iteration 18/3560 Training loss: 3.5128 1.1924 sec/batch\n",
      "Epoch 1/20  Iteration 19/3560 Training loss: 3.4951 1.3224 sec/batch\n",
      "Epoch 1/20  Iteration 20/3560 Training loss: 3.4771 1.2779 sec/batch\n",
      "Epoch 1/20  Iteration 21/3560 Training loss: 3.4617 1.2727 sec/batch\n",
      "Epoch 1/20  Iteration 22/3560 Training loss: 3.4476 1.2949 sec/batch\n",
      "Epoch 1/20  Iteration 23/3560 Training loss: 3.4342 1.3356 sec/batch\n",
      "Epoch 1/20  Iteration 24/3560 Training loss: 3.4218 1.3212 sec/batch\n",
      "Epoch 1/20  Iteration 25/3560 Training loss: 3.4100 1.3052 sec/batch\n",
      "Epoch 1/20  Iteration 26/3560 Training loss: 3.3996 1.3949 sec/batch\n",
      "Epoch 1/20  Iteration 27/3560 Training loss: 3.3898 1.5966 sec/batch\n",
      "Epoch 1/20  Iteration 28/3560 Training loss: 3.3796 1.5363 sec/batch\n",
      "Epoch 1/20  Iteration 29/3560 Training loss: 3.3704 1.3103 sec/batch\n",
      "Epoch 1/20  Iteration 30/3560 Training loss: 3.3619 1.3129 sec/batch\n",
      "Epoch 1/20  Iteration 31/3560 Training loss: 3.3544 1.2985 sec/batch\n",
      "Epoch 1/20  Iteration 32/3560 Training loss: 3.3464 1.2550 sec/batch\n",
      "Epoch 1/20  Iteration 33/3560 Training loss: 3.3385 1.1808 sec/batch\n",
      "Epoch 1/20  Iteration 34/3560 Training loss: 3.3315 1.4358 sec/batch\n",
      "Epoch 1/20  Iteration 35/3560 Training loss: 3.3244 1.5283 sec/batch\n",
      "Epoch 1/20  Iteration 36/3560 Training loss: 3.3180 1.5924 sec/batch\n",
      "Epoch 1/20  Iteration 37/3560 Training loss: 3.3112 1.5008 sec/batch\n",
      "Epoch 1/20  Iteration 38/3560 Training loss: 3.3047 1.2437 sec/batch\n",
      "Epoch 1/20  Iteration 39/3560 Training loss: 3.2983 1.2782 sec/batch\n",
      "Epoch 1/20  Iteration 40/3560 Training loss: 3.2924 1.1769 sec/batch\n",
      "Epoch 1/20  Iteration 41/3560 Training loss: 3.2864 1.3424 sec/batch\n",
      "Epoch 1/20  Iteration 42/3560 Training loss: 3.2807 1.4130 sec/batch\n",
      "Epoch 1/20  Iteration 43/3560 Training loss: 3.2750 1.2199 sec/batch\n",
      "Epoch 1/20  Iteration 44/3560 Training loss: 3.2697 1.2387 sec/batch\n",
      "Epoch 1/20  Iteration 45/3560 Training loss: 3.2644 1.3635 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 46/3560 Training loss: 3.2595 1.1704 sec/batch\n",
      "Epoch 1/20  Iteration 47/3560 Training loss: 3.2548 1.1652 sec/batch\n",
      "Epoch 1/20  Iteration 48/3560 Training loss: 3.2502 1.1618 sec/batch\n",
      "Epoch 1/20  Iteration 49/3560 Training loss: 3.2458 1.4043 sec/batch\n",
      "Epoch 1/20  Iteration 50/3560 Training loss: 3.2415 1.4958 sec/batch\n",
      "Epoch 1/20  Iteration 51/3560 Training loss: 3.2369 1.2238 sec/batch\n",
      "Epoch 1/20  Iteration 52/3560 Training loss: 3.2324 1.1720 sec/batch\n",
      "Epoch 1/20  Iteration 53/3560 Training loss: 3.2283 1.1797 sec/batch\n",
      "Epoch 1/20  Iteration 54/3560 Training loss: 3.2238 1.1560 sec/batch\n",
      "Epoch 1/20  Iteration 55/3560 Training loss: 3.2196 1.1630 sec/batch\n",
      "Epoch 1/20  Iteration 56/3560 Training loss: 3.2153 1.1519 sec/batch\n",
      "Epoch 1/20  Iteration 57/3560 Training loss: 3.2111 1.1920 sec/batch\n",
      "Epoch 1/20  Iteration 58/3560 Training loss: 3.2069 1.1911 sec/batch\n",
      "Epoch 1/20  Iteration 59/3560 Training loss: 3.2026 1.1502 sec/batch\n",
      "Epoch 1/20  Iteration 60/3560 Training loss: 3.1985 1.1561 sec/batch\n",
      "Epoch 1/20  Iteration 61/3560 Training loss: 3.1943 1.1516 sec/batch\n",
      "Epoch 1/20  Iteration 62/3560 Training loss: 3.1904 1.1496 sec/batch\n",
      "Epoch 1/20  Iteration 63/3560 Training loss: 3.1866 1.1509 sec/batch\n",
      "Epoch 1/20  Iteration 64/3560 Training loss: 3.1822 1.1596 sec/batch\n",
      "Epoch 1/20  Iteration 65/3560 Training loss: 3.1776 1.1626 sec/batch\n",
      "Epoch 1/20  Iteration 66/3560 Training loss: 3.1736 1.1828 sec/batch\n",
      "Epoch 1/20  Iteration 67/3560 Training loss: 3.1693 1.1641 sec/batch\n",
      "Epoch 1/20  Iteration 68/3560 Training loss: 3.1644 1.1543 sec/batch\n",
      "Epoch 1/20  Iteration 69/3560 Training loss: 3.1596 1.1728 sec/batch\n",
      "Epoch 1/20  Iteration 70/3560 Training loss: 3.1551 1.1696 sec/batch\n",
      "Epoch 1/20  Iteration 71/3560 Training loss: 3.1504 1.1583 sec/batch\n",
      "Epoch 1/20  Iteration 72/3560 Training loss: 3.1457 1.1639 sec/batch\n",
      "Epoch 1/20  Iteration 73/3560 Training loss: 3.1408 1.1599 sec/batch\n",
      "Epoch 1/20  Iteration 74/3560 Training loss: 3.1358 1.1721 sec/batch\n",
      "Epoch 1/20  Iteration 75/3560 Training loss: 3.1314 1.2002 sec/batch\n",
      "Epoch 1/20  Iteration 76/3560 Training loss: 3.1273 1.1633 sec/batch\n",
      "Epoch 1/20  Iteration 77/3560 Training loss: 3.1223 1.1577 sec/batch\n",
      "Epoch 1/20  Iteration 78/3560 Training loss: 3.1177 1.1553 sec/batch\n",
      "Epoch 1/20  Iteration 79/3560 Training loss: 3.1128 1.1608 sec/batch\n",
      "Epoch 1/20  Iteration 80/3560 Training loss: 3.1078 1.1528 sec/batch\n",
      "Epoch 1/20  Iteration 81/3560 Training loss: 3.1027 1.1704 sec/batch\n",
      "Epoch 1/20  Iteration 82/3560 Training loss: 3.0978 1.1630 sec/batch\n",
      "Epoch 1/20  Iteration 83/3560 Training loss: 3.0926 1.1946 sec/batch\n",
      "Epoch 1/20  Iteration 84/3560 Training loss: 3.0874 1.1577 sec/batch\n",
      "Epoch 1/20  Iteration 85/3560 Training loss: 3.0819 1.1458 sec/batch\n",
      "Epoch 1/20  Iteration 86/3560 Training loss: 3.0765 1.1971 sec/batch\n",
      "Epoch 1/20  Iteration 87/3560 Training loss: 3.0712 1.1574 sec/batch\n",
      "Epoch 1/20  Iteration 88/3560 Training loss: 3.0659 1.1586 sec/batch\n",
      "Epoch 1/20  Iteration 89/3560 Training loss: 3.0605 1.1551 sec/batch\n",
      "Epoch 1/20  Iteration 90/3560 Training loss: 3.0553 1.1553 sec/batch\n",
      "Epoch 1/20  Iteration 91/3560 Training loss: 3.0499 1.1607 sec/batch\n",
      "Epoch 1/20  Iteration 92/3560 Training loss: 3.0447 1.1852 sec/batch\n",
      "Epoch 1/20  Iteration 93/3560 Training loss: 3.0396 1.1511 sec/batch\n",
      "Epoch 1/20  Iteration 94/3560 Training loss: 3.0345 1.1538 sec/batch\n",
      "Epoch 1/20  Iteration 95/3560 Training loss: 3.0291 1.1822 sec/batch\n",
      "Epoch 1/20  Iteration 96/3560 Training loss: 3.0239 1.1570 sec/batch\n",
      "Epoch 1/20  Iteration 97/3560 Training loss: 3.0187 1.1453 sec/batch\n",
      "Epoch 1/20  Iteration 98/3560 Training loss: 3.0136 1.1658 sec/batch\n",
      "Epoch 1/20  Iteration 99/3560 Training loss: 3.0086 1.1528 sec/batch\n",
      "Epoch 1/20  Iteration 100/3560 Training loss: 3.0035 1.1916 sec/batch\n",
      "Epoch 1/20  Iteration 101/3560 Training loss: 2.9986 1.1666 sec/batch\n",
      "Epoch 1/20  Iteration 102/3560 Training loss: 2.9936 1.1603 sec/batch\n",
      "Epoch 1/20  Iteration 103/3560 Training loss: 2.9885 1.1652 sec/batch\n",
      "Epoch 1/20  Iteration 104/3560 Training loss: 2.9834 1.1490 sec/batch\n",
      "Epoch 1/20  Iteration 105/3560 Training loss: 2.9783 1.1600 sec/batch\n",
      "Epoch 1/20  Iteration 106/3560 Training loss: 2.9734 1.1521 sec/batch\n",
      "Epoch 1/20  Iteration 107/3560 Training loss: 2.9683 1.1630 sec/batch\n",
      "Epoch 1/20  Iteration 108/3560 Training loss: 2.9637 1.1606 sec/batch\n",
      "Epoch 1/20  Iteration 109/3560 Training loss: 2.9589 1.1817 sec/batch\n",
      "Epoch 1/20  Iteration 110/3560 Training loss: 2.9538 1.1470 sec/batch\n",
      "Epoch 1/20  Iteration 111/3560 Training loss: 2.9492 1.1451 sec/batch\n",
      "Epoch 1/20  Iteration 112/3560 Training loss: 2.9447 1.1559 sec/batch\n",
      "Epoch 1/20  Iteration 113/3560 Training loss: 2.9400 1.1558 sec/batch\n",
      "Epoch 1/20  Iteration 114/3560 Training loss: 2.9352 1.1516 sec/batch\n",
      "Epoch 1/20  Iteration 115/3560 Training loss: 2.9305 1.1564 sec/batch\n",
      "Epoch 1/20  Iteration 116/3560 Training loss: 2.9256 1.1617 sec/batch\n",
      "Epoch 1/20  Iteration 117/3560 Training loss: 2.9210 1.3929 sec/batch\n",
      "Epoch 1/20  Iteration 118/3560 Training loss: 2.9164 1.4685 sec/batch\n",
      "Epoch 1/20  Iteration 119/3560 Training loss: 2.9121 1.4502 sec/batch\n",
      "Epoch 1/20  Iteration 120/3560 Training loss: 2.9076 1.4590 sec/batch\n",
      "Epoch 1/20  Iteration 121/3560 Training loss: 2.9034 1.4463 sec/batch\n",
      "Epoch 1/20  Iteration 122/3560 Training loss: 2.8992 1.4885 sec/batch\n",
      "Epoch 1/20  Iteration 123/3560 Training loss: 2.8950 1.5081 sec/batch\n",
      "Epoch 1/20  Iteration 124/3560 Training loss: 2.8907 1.4665 sec/batch\n",
      "Epoch 1/20  Iteration 125/3560 Training loss: 2.8865 1.3521 sec/batch\n",
      "Epoch 1/20  Iteration 126/3560 Training loss: 2.8822 1.1557 sec/batch\n",
      "Epoch 1/20  Iteration 127/3560 Training loss: 2.8782 1.1565 sec/batch\n",
      "Epoch 1/20  Iteration 128/3560 Training loss: 2.8743 1.1656 sec/batch\n",
      "Epoch 1/20  Iteration 129/3560 Training loss: 2.8701 1.1561 sec/batch\n",
      "Epoch 1/20  Iteration 130/3560 Training loss: 2.8661 1.1586 sec/batch\n",
      "Epoch 1/20  Iteration 131/3560 Training loss: 2.8620 1.1523 sec/batch\n",
      "Epoch 1/20  Iteration 132/3560 Training loss: 2.8579 1.1611 sec/batch\n",
      "Epoch 1/20  Iteration 133/3560 Training loss: 2.8539 1.1844 sec/batch\n",
      "Epoch 1/20  Iteration 134/3560 Training loss: 2.8501 1.1604 sec/batch\n",
      "Epoch 1/20  Iteration 135/3560 Training loss: 2.8461 1.1630 sec/batch\n",
      "Epoch 1/20  Iteration 136/3560 Training loss: 2.8423 1.1662 sec/batch\n",
      "Epoch 1/20  Iteration 137/3560 Training loss: 2.8384 1.1560 sec/batch\n",
      "Epoch 1/20  Iteration 138/3560 Training loss: 2.8346 1.1532 sec/batch\n",
      "Epoch 1/20  Iteration 139/3560 Training loss: 2.8310 1.1593 sec/batch\n",
      "Epoch 1/20  Iteration 140/3560 Training loss: 2.8272 1.1742 sec/batch\n",
      "Epoch 1/20  Iteration 141/3560 Training loss: 2.8237 1.1639 sec/batch\n",
      "Epoch 1/20  Iteration 142/3560 Training loss: 2.8200 1.1683 sec/batch\n",
      "Epoch 1/20  Iteration 143/3560 Training loss: 2.8165 1.1629 sec/batch\n",
      "Epoch 1/20  Iteration 144/3560 Training loss: 2.8129 1.1535 sec/batch\n",
      "Epoch 1/20  Iteration 145/3560 Training loss: 2.8093 1.1489 sec/batch\n",
      "Epoch 1/20  Iteration 146/3560 Training loss: 2.8060 1.1623 sec/batch\n",
      "Epoch 1/20  Iteration 147/3560 Training loss: 2.8024 1.1468 sec/batch\n",
      "Epoch 1/20  Iteration 148/3560 Training loss: 2.7991 1.1628 sec/batch\n",
      "Epoch 1/20  Iteration 149/3560 Training loss: 2.7955 1.1626 sec/batch\n",
      "Epoch 1/20  Iteration 150/3560 Training loss: 2.7920 1.1951 sec/batch\n",
      "Epoch 1/20  Iteration 151/3560 Training loss: 2.7886 1.1706 sec/batch\n",
      "Epoch 1/20  Iteration 152/3560 Training loss: 2.7855 1.1614 sec/batch\n",
      "Epoch 1/20  Iteration 153/3560 Training loss: 2.7822 1.1528 sec/batch\n",
      "Epoch 1/20  Iteration 154/3560 Training loss: 2.7790 1.1510 sec/batch\n",
      "Epoch 1/20  Iteration 155/3560 Training loss: 2.7757 1.1552 sec/batch\n",
      "Epoch 1/20  Iteration 156/3560 Training loss: 2.7724 1.1561 sec/batch\n",
      "Epoch 1/20  Iteration 157/3560 Training loss: 2.7691 1.1772 sec/batch\n",
      "Epoch 1/20  Iteration 158/3560 Training loss: 2.7658 1.1500 sec/batch\n",
      "Epoch 1/20  Iteration 159/3560 Training loss: 2.7624 1.1931 sec/batch\n",
      "Epoch 1/20  Iteration 160/3560 Training loss: 2.7594 1.1510 sec/batch\n",
      "Epoch 1/20  Iteration 161/3560 Training loss: 2.7563 1.1603 sec/batch\n",
      "Epoch 1/20  Iteration 162/3560 Training loss: 2.7531 1.1529 sec/batch\n",
      "Epoch 1/20  Iteration 163/3560 Training loss: 2.7500 1.1665 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 164/3560 Training loss: 2.7469 1.1484 sec/batch\n",
      "Epoch 1/20  Iteration 165/3560 Training loss: 2.7439 1.1572 sec/batch\n",
      "Epoch 1/20  Iteration 166/3560 Training loss: 2.7408 1.1743 sec/batch\n",
      "Epoch 1/20  Iteration 167/3560 Training loss: 2.7380 1.1573 sec/batch\n",
      "Epoch 1/20  Iteration 168/3560 Training loss: 2.7360 1.1864 sec/batch\n",
      "Epoch 1/20  Iteration 169/3560 Training loss: 2.7341 1.1679 sec/batch\n",
      "Epoch 1/20  Iteration 170/3560 Training loss: 2.7317 1.1516 sec/batch\n",
      "Epoch 1/20  Iteration 171/3560 Training loss: 2.7290 1.1489 sec/batch\n",
      "Epoch 1/20  Iteration 172/3560 Training loss: 2.7262 1.1564 sec/batch\n",
      "Epoch 1/20  Iteration 173/3560 Training loss: 2.7236 1.1524 sec/batch\n",
      "Epoch 1/20  Iteration 174/3560 Training loss: 2.7209 1.1573 sec/batch\n",
      "Epoch 1/20  Iteration 175/3560 Training loss: 2.7183 1.1666 sec/batch\n",
      "Epoch 1/20  Iteration 176/3560 Training loss: 2.7155 1.2095 sec/batch\n",
      "Epoch 1/20  Iteration 177/3560 Training loss: 2.7126 1.1883 sec/batch\n",
      "Epoch 1/20  Iteration 178/3560 Training loss: 2.7098 1.1612 sec/batch\n",
      "Epoch 2/20  Iteration 179/3560 Training loss: 2.2828 1.1604 sec/batch\n",
      "Epoch 2/20  Iteration 180/3560 Training loss: 2.2348 1.1574 sec/batch\n",
      "Epoch 2/20  Iteration 181/3560 Training loss: 2.2213 1.1609 sec/batch\n",
      "Epoch 2/20  Iteration 182/3560 Training loss: 2.2162 1.1550 sec/batch\n",
      "Epoch 2/20  Iteration 183/3560 Training loss: 2.2130 1.1583 sec/batch\n",
      "Epoch 2/20  Iteration 184/3560 Training loss: 2.2090 1.2136 sec/batch\n",
      "Epoch 2/20  Iteration 185/3560 Training loss: 2.2086 1.2243 sec/batch\n",
      "Epoch 2/20  Iteration 186/3560 Training loss: 2.2079 1.1617 sec/batch\n",
      "Epoch 2/20  Iteration 187/3560 Training loss: 2.2087 1.1512 sec/batch\n",
      "Epoch 2/20  Iteration 188/3560 Training loss: 2.2075 1.1548 sec/batch\n",
      "Epoch 2/20  Iteration 189/3560 Training loss: 2.2040 1.1772 sec/batch\n",
      "Epoch 2/20  Iteration 190/3560 Training loss: 2.2014 1.1480 sec/batch\n",
      "Epoch 2/20  Iteration 191/3560 Training loss: 2.1998 1.1552 sec/batch\n",
      "Epoch 2/20  Iteration 192/3560 Training loss: 2.2012 1.1795 sec/batch\n",
      "Epoch 2/20  Iteration 193/3560 Training loss: 2.1999 1.1632 sec/batch\n",
      "Epoch 2/20  Iteration 194/3560 Training loss: 2.1982 1.1793 sec/batch\n",
      "Epoch 2/20  Iteration 195/3560 Training loss: 2.1970 1.1515 sec/batch\n",
      "Epoch 2/20  Iteration 196/3560 Training loss: 2.1979 1.1652 sec/batch\n",
      "Epoch 2/20  Iteration 197/3560 Training loss: 2.1975 1.1640 sec/batch\n",
      "Epoch 2/20  Iteration 198/3560 Training loss: 2.1958 1.1493 sec/batch\n",
      "Epoch 2/20  Iteration 199/3560 Training loss: 2.1941 1.1575 sec/batch\n",
      "Epoch 2/20  Iteration 200/3560 Training loss: 2.1951 1.1583 sec/batch\n",
      "Epoch 2/20  Iteration 201/3560 Training loss: 2.1935 1.1575 sec/batch\n",
      "Epoch 2/20  Iteration 202/3560 Training loss: 2.1914 1.1917 sec/batch\n",
      "Epoch 2/20  Iteration 203/3560 Training loss: 2.1900 1.1600 sec/batch\n",
      "Epoch 2/20  Iteration 204/3560 Training loss: 2.1883 1.1639 sec/batch\n",
      "Epoch 2/20  Iteration 205/3560 Training loss: 2.1863 1.1498 sec/batch\n",
      "Epoch 2/20  Iteration 206/3560 Training loss: 2.1856 1.1631 sec/batch\n",
      "Epoch 2/20  Iteration 207/3560 Training loss: 2.1853 1.1552 sec/batch\n",
      "Epoch 2/20  Iteration 208/3560 Training loss: 2.1843 1.1570 sec/batch\n",
      "Epoch 2/20  Iteration 209/3560 Training loss: 2.1833 1.1592 sec/batch\n",
      "Epoch 2/20  Iteration 210/3560 Training loss: 2.1818 1.1541 sec/batch\n",
      "Epoch 2/20  Iteration 211/3560 Training loss: 2.1805 1.1912 sec/batch\n",
      "Epoch 2/20  Iteration 212/3560 Training loss: 2.1798 1.1521 sec/batch\n",
      "Epoch 2/20  Iteration 213/3560 Training loss: 2.1786 1.1513 sec/batch\n",
      "Epoch 2/20  Iteration 214/3560 Training loss: 2.1773 1.1542 sec/batch\n",
      "Epoch 2/20  Iteration 215/3560 Training loss: 2.1761 1.1562 sec/batch\n",
      "Epoch 2/20  Iteration 216/3560 Training loss: 2.1739 1.1521 sec/batch\n",
      "Epoch 2/20  Iteration 217/3560 Training loss: 2.1720 1.1508 sec/batch\n",
      "Epoch 2/20  Iteration 218/3560 Training loss: 2.1705 1.1587 sec/batch\n",
      "Epoch 2/20  Iteration 219/3560 Training loss: 2.1691 1.1759 sec/batch\n",
      "Epoch 2/20  Iteration 220/3560 Training loss: 2.1679 1.1863 sec/batch\n",
      "Epoch 2/20  Iteration 221/3560 Training loss: 2.1666 1.1435 sec/batch\n",
      "Epoch 2/20  Iteration 222/3560 Training loss: 2.1648 1.1697 sec/batch\n",
      "Epoch 2/20  Iteration 223/3560 Training loss: 2.1637 1.1535 sec/batch\n",
      "Epoch 2/20  Iteration 224/3560 Training loss: 2.1615 1.1822 sec/batch\n",
      "Epoch 2/20  Iteration 225/3560 Training loss: 2.1605 1.1998 sec/batch\n",
      "Epoch 2/20  Iteration 226/3560 Training loss: 2.1589 1.1880 sec/batch\n",
      "Epoch 2/20  Iteration 227/3560 Training loss: 2.1576 1.1996 sec/batch\n",
      "Epoch 2/20  Iteration 228/3560 Training loss: 2.1570 1.2434 sec/batch\n",
      "Epoch 2/20  Iteration 229/3560 Training loss: 2.1555 1.1695 sec/batch\n",
      "Epoch 2/20  Iteration 230/3560 Training loss: 2.1549 1.1736 sec/batch\n",
      "Epoch 2/20  Iteration 231/3560 Training loss: 2.1535 1.1497 sec/batch\n",
      "Epoch 2/20  Iteration 232/3560 Training loss: 2.1523 1.1665 sec/batch\n",
      "Epoch 2/20  Iteration 233/3560 Training loss: 2.1511 1.1533 sec/batch\n",
      "Epoch 2/20  Iteration 234/3560 Training loss: 2.1501 1.1485 sec/batch\n",
      "Epoch 2/20  Iteration 235/3560 Training loss: 2.1492 1.1720 sec/batch\n",
      "Epoch 2/20  Iteration 236/3560 Training loss: 2.1480 1.1681 sec/batch\n",
      "Epoch 2/20  Iteration 237/3560 Training loss: 2.1467 1.1974 sec/batch\n",
      "Epoch 2/20  Iteration 238/3560 Training loss: 2.1463 1.1558 sec/batch\n",
      "Epoch 2/20  Iteration 239/3560 Training loss: 2.1452 1.1587 sec/batch\n",
      "Epoch 2/20  Iteration 240/3560 Training loss: 2.1447 1.1584 sec/batch\n",
      "Epoch 2/20  Iteration 241/3560 Training loss: 2.1440 1.1495 sec/batch\n",
      "Epoch 2/20  Iteration 242/3560 Training loss: 2.1432 1.1593 sec/batch\n",
      "Epoch 2/20  Iteration 243/3560 Training loss: 2.1421 1.1463 sec/batch\n",
      "Epoch 2/20  Iteration 244/3560 Training loss: 2.1415 1.1718 sec/batch\n",
      "Epoch 2/20  Iteration 245/3560 Training loss: 2.1406 1.1637 sec/batch\n",
      "Epoch 2/20  Iteration 246/3560 Training loss: 2.1391 1.1874 sec/batch\n",
      "Epoch 2/20  Iteration 247/3560 Training loss: 2.1380 1.1592 sec/batch\n",
      "Epoch 2/20  Iteration 248/3560 Training loss: 2.1371 1.1501 sec/batch\n",
      "Epoch 2/20  Iteration 249/3560 Training loss: 2.1366 1.1531 sec/batch\n",
      "Epoch 2/20  Iteration 250/3560 Training loss: 2.1356 1.1613 sec/batch\n",
      "Epoch 2/20  Iteration 251/3560 Training loss: 2.1350 1.1607 sec/batch\n",
      "Epoch 2/20  Iteration 252/3560 Training loss: 2.1336 1.1591 sec/batch\n",
      "Epoch 2/20  Iteration 253/3560 Training loss: 2.1325 1.1507 sec/batch\n",
      "Epoch 2/20  Iteration 254/3560 Training loss: 2.1320 1.1852 sec/batch\n",
      "Epoch 2/20  Iteration 255/3560 Training loss: 2.1309 1.1783 sec/batch\n",
      "Epoch 2/20  Iteration 256/3560 Training loss: 2.1302 1.1486 sec/batch\n",
      "Epoch 2/20  Iteration 257/3560 Training loss: 2.1288 1.1492 sec/batch\n",
      "Epoch 2/20  Iteration 258/3560 Training loss: 2.1276 1.1579 sec/batch\n",
      "Epoch 2/20  Iteration 259/3560 Training loss: 2.1263 1.1547 sec/batch\n",
      "Epoch 2/20  Iteration 260/3560 Training loss: 2.1255 1.1461 sec/batch\n",
      "Epoch 2/20  Iteration 261/3560 Training loss: 2.1242 1.1616 sec/batch\n",
      "Epoch 2/20  Iteration 262/3560 Training loss: 2.1232 1.1539 sec/batch\n",
      "Epoch 2/20  Iteration 263/3560 Training loss: 2.1219 1.1955 sec/batch\n",
      "Epoch 2/20  Iteration 264/3560 Training loss: 2.1207 1.1500 sec/batch\n",
      "Epoch 2/20  Iteration 265/3560 Training loss: 2.1198 1.1464 sec/batch\n",
      "Epoch 2/20  Iteration 266/3560 Training loss: 2.1187 1.1555 sec/batch\n",
      "Epoch 2/20  Iteration 267/3560 Training loss: 2.1174 1.1593 sec/batch\n",
      "Epoch 2/20  Iteration 268/3560 Training loss: 2.1167 1.1599 sec/batch\n",
      "Epoch 2/20  Iteration 269/3560 Training loss: 2.1156 1.3059 sec/batch\n",
      "Epoch 2/20  Iteration 270/3560 Training loss: 2.1147 1.4931 sec/batch\n",
      "Epoch 2/20  Iteration 271/3560 Training loss: 2.1133 1.5375 sec/batch\n",
      "Epoch 2/20  Iteration 272/3560 Training loss: 2.1123 1.4758 sec/batch\n",
      "Epoch 2/20  Iteration 273/3560 Training loss: 2.1111 1.4085 sec/batch\n",
      "Epoch 2/20  Iteration 274/3560 Training loss: 2.1101 1.2669 sec/batch\n",
      "Epoch 2/20  Iteration 275/3560 Training loss: 2.1091 1.1494 sec/batch\n",
      "Epoch 2/20  Iteration 276/3560 Training loss: 2.1080 1.1583 sec/batch\n",
      "Epoch 2/20  Iteration 277/3560 Training loss: 2.1068 1.1620 sec/batch\n",
      "Epoch 2/20  Iteration 278/3560 Training loss: 2.1057 1.1547 sec/batch\n",
      "Epoch 2/20  Iteration 279/3560 Training loss: 2.1049 1.2402 sec/batch\n",
      "Epoch 2/20  Iteration 280/3560 Training loss: 2.1041 1.1517 sec/batch\n",
      "Epoch 2/20  Iteration 281/3560 Training loss: 2.1030 1.1490 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 282/3560 Training loss: 2.1021 1.1522 sec/batch\n",
      "Epoch 2/20  Iteration 283/3560 Training loss: 2.1011 1.1532 sec/batch\n",
      "Epoch 2/20  Iteration 284/3560 Training loss: 2.1002 1.1601 sec/batch\n",
      "Epoch 2/20  Iteration 285/3560 Training loss: 2.0993 1.1510 sec/batch\n",
      "Epoch 2/20  Iteration 286/3560 Training loss: 2.0986 1.1718 sec/batch\n",
      "Epoch 2/20  Iteration 287/3560 Training loss: 2.0978 1.1554 sec/batch\n",
      "Epoch 2/20  Iteration 288/3560 Training loss: 2.0970 1.1868 sec/batch\n",
      "Epoch 2/20  Iteration 289/3560 Training loss: 2.0962 1.1711 sec/batch\n",
      "Epoch 2/20  Iteration 290/3560 Training loss: 2.0954 1.1480 sec/batch\n",
      "Epoch 2/20  Iteration 291/3560 Training loss: 2.0945 1.1471 sec/batch\n",
      "Epoch 2/20  Iteration 292/3560 Training loss: 2.0935 1.1480 sec/batch\n",
      "Epoch 2/20  Iteration 293/3560 Training loss: 2.0925 1.1549 sec/batch\n",
      "Epoch 2/20  Iteration 294/3560 Training loss: 2.0913 1.1526 sec/batch\n",
      "Epoch 2/20  Iteration 295/3560 Training loss: 2.0905 1.1618 sec/batch\n",
      "Epoch 2/20  Iteration 296/3560 Training loss: 2.0896 1.1603 sec/batch\n",
      "Epoch 2/20  Iteration 297/3560 Training loss: 2.0888 1.1827 sec/batch\n",
      "Epoch 2/20  Iteration 298/3560 Training loss: 2.0880 1.1498 sec/batch\n",
      "Epoch 2/20  Iteration 299/3560 Training loss: 2.0873 1.1626 sec/batch\n",
      "Epoch 2/20  Iteration 300/3560 Training loss: 2.0862 1.1566 sec/batch\n",
      "Epoch 2/20  Iteration 301/3560 Training loss: 2.0852 1.1556 sec/batch\n",
      "Epoch 2/20  Iteration 302/3560 Training loss: 2.0845 1.1545 sec/batch\n",
      "Epoch 2/20  Iteration 303/3560 Training loss: 2.0837 1.1753 sec/batch\n",
      "Epoch 2/20  Iteration 304/3560 Training loss: 2.0827 1.1475 sec/batch\n",
      "Epoch 2/20  Iteration 305/3560 Training loss: 2.0820 1.1873 sec/batch\n",
      "Epoch 2/20  Iteration 306/3560 Training loss: 2.0812 1.1818 sec/batch\n",
      "Epoch 2/20  Iteration 307/3560 Training loss: 2.0804 1.1542 sec/batch\n",
      "Epoch 2/20  Iteration 308/3560 Training loss: 2.0796 1.1600 sec/batch\n",
      "Epoch 2/20  Iteration 309/3560 Training loss: 2.0786 1.1468 sec/batch\n",
      "Epoch 2/20  Iteration 310/3560 Training loss: 2.0775 1.1571 sec/batch\n",
      "Epoch 2/20  Iteration 311/3560 Training loss: 2.0768 1.1483 sec/batch\n",
      "Epoch 2/20  Iteration 312/3560 Training loss: 2.0761 1.1651 sec/batch\n",
      "Epoch 2/20  Iteration 313/3560 Training loss: 2.0754 1.1437 sec/batch\n",
      "Epoch 2/20  Iteration 314/3560 Training loss: 2.0746 1.2026 sec/batch\n",
      "Epoch 2/20  Iteration 315/3560 Training loss: 2.0739 1.1534 sec/batch\n",
      "Epoch 2/20  Iteration 316/3560 Training loss: 2.0732 1.1459 sec/batch\n",
      "Epoch 2/20  Iteration 317/3560 Training loss: 2.0729 1.1512 sec/batch\n",
      "Epoch 2/20  Iteration 318/3560 Training loss: 2.0721 1.1524 sec/batch\n",
      "Epoch 2/20  Iteration 319/3560 Training loss: 2.0716 1.1578 sec/batch\n",
      "Epoch 2/20  Iteration 320/3560 Training loss: 2.0709 1.1629 sec/batch\n",
      "Epoch 2/20  Iteration 321/3560 Training loss: 2.0702 1.1614 sec/batch\n",
      "Epoch 2/20  Iteration 322/3560 Training loss: 2.0695 1.1549 sec/batch\n",
      "Epoch 2/20  Iteration 323/3560 Training loss: 2.0687 1.1899 sec/batch\n",
      "Epoch 2/20  Iteration 324/3560 Training loss: 2.0681 1.1525 sec/batch\n",
      "Epoch 2/20  Iteration 325/3560 Training loss: 2.0675 1.1506 sec/batch\n",
      "Epoch 2/20  Iteration 326/3560 Training loss: 2.0670 1.1524 sec/batch\n",
      "Epoch 2/20  Iteration 327/3560 Training loss: 2.0662 1.1584 sec/batch\n",
      "Epoch 2/20  Iteration 328/3560 Training loss: 2.0654 1.1558 sec/batch\n",
      "Epoch 2/20  Iteration 329/3560 Training loss: 2.0647 1.1569 sec/batch\n",
      "Epoch 2/20  Iteration 330/3560 Training loss: 2.0642 1.1589 sec/batch\n",
      "Epoch 2/20  Iteration 331/3560 Training loss: 2.0635 1.1886 sec/batch\n",
      "Epoch 2/20  Iteration 332/3560 Training loss: 2.0629 1.1910 sec/batch\n",
      "Epoch 2/20  Iteration 333/3560 Training loss: 2.0622 1.1427 sec/batch\n",
      "Epoch 2/20  Iteration 334/3560 Training loss: 2.0615 1.1642 sec/batch\n",
      "Epoch 2/20  Iteration 335/3560 Training loss: 2.0607 1.1492 sec/batch\n",
      "Epoch 2/20  Iteration 336/3560 Training loss: 2.0600 1.1559 sec/batch\n",
      "Epoch 2/20  Iteration 337/3560 Training loss: 2.0591 1.1971 sec/batch\n",
      "Epoch 2/20  Iteration 338/3560 Training loss: 2.0587 1.1978 sec/batch\n",
      "Epoch 2/20  Iteration 339/3560 Training loss: 2.0581 1.1631 sec/batch\n",
      "Epoch 2/20  Iteration 340/3560 Training loss: 2.0574 1.1930 sec/batch\n",
      "Epoch 2/20  Iteration 341/3560 Training loss: 2.0568 1.1532 sec/batch\n",
      "Epoch 2/20  Iteration 342/3560 Training loss: 2.0561 1.1521 sec/batch\n",
      "Epoch 2/20  Iteration 343/3560 Training loss: 2.0554 1.1525 sec/batch\n",
      "Epoch 2/20  Iteration 344/3560 Training loss: 2.0546 1.1579 sec/batch\n",
      "Epoch 2/20  Iteration 345/3560 Training loss: 2.0540 1.1494 sec/batch\n",
      "Epoch 2/20  Iteration 346/3560 Training loss: 2.0536 1.1566 sec/batch\n",
      "Epoch 2/20  Iteration 347/3560 Training loss: 2.0529 1.1472 sec/batch\n",
      "Epoch 2/20  Iteration 348/3560 Training loss: 2.0522 1.1657 sec/batch\n",
      "Epoch 2/20  Iteration 349/3560 Training loss: 2.0514 1.1830 sec/batch\n",
      "Epoch 2/20  Iteration 350/3560 Training loss: 2.0506 1.1561 sec/batch\n",
      "Epoch 2/20  Iteration 351/3560 Training loss: 2.0501 1.1566 sec/batch\n",
      "Epoch 2/20  Iteration 352/3560 Training loss: 2.0495 1.1546 sec/batch\n",
      "Epoch 2/20  Iteration 353/3560 Training loss: 2.0488 1.1659 sec/batch\n",
      "Epoch 2/20  Iteration 354/3560 Training loss: 2.0481 1.1537 sec/batch\n",
      "Epoch 2/20  Iteration 355/3560 Training loss: 2.0473 1.1504 sec/batch\n",
      "Epoch 2/20  Iteration 356/3560 Training loss: 2.0467 1.1647 sec/batch\n",
      "Epoch 3/20  Iteration 357/3560 Training loss: 1.9932 1.1650 sec/batch\n",
      "Epoch 3/20  Iteration 358/3560 Training loss: 1.9480 1.1822 sec/batch\n",
      "Epoch 3/20  Iteration 359/3560 Training loss: 1.9368 1.1553 sec/batch\n",
      "Epoch 3/20  Iteration 360/3560 Training loss: 1.9295 1.1506 sec/batch\n",
      "Epoch 3/20  Iteration 361/3560 Training loss: 1.9263 1.1502 sec/batch\n",
      "Epoch 3/20  Iteration 362/3560 Training loss: 1.9172 1.1620 sec/batch\n",
      "Epoch 3/20  Iteration 363/3560 Training loss: 1.9166 1.1503 sec/batch\n",
      "Epoch 3/20  Iteration 364/3560 Training loss: 1.9150 1.1588 sec/batch\n",
      "Epoch 3/20  Iteration 365/3560 Training loss: 1.9175 1.1622 sec/batch\n",
      "Epoch 3/20  Iteration 366/3560 Training loss: 1.9168 1.1656 sec/batch\n",
      "Epoch 3/20  Iteration 367/3560 Training loss: 1.9140 1.1718 sec/batch\n",
      "Epoch 3/20  Iteration 368/3560 Training loss: 1.9111 1.1441 sec/batch\n",
      "Epoch 3/20  Iteration 369/3560 Training loss: 1.9105 1.1519 sec/batch\n",
      "Epoch 3/20  Iteration 370/3560 Training loss: 1.9124 1.1603 sec/batch\n",
      "Epoch 3/20  Iteration 371/3560 Training loss: 1.9112 1.1545 sec/batch\n",
      "Epoch 3/20  Iteration 372/3560 Training loss: 1.9090 1.1664 sec/batch\n",
      "Epoch 3/20  Iteration 373/3560 Training loss: 1.9079 1.1532 sec/batch\n",
      "Epoch 3/20  Iteration 374/3560 Training loss: 1.9096 1.1631 sec/batch\n",
      "Epoch 3/20  Iteration 375/3560 Training loss: 1.9090 1.1724 sec/batch\n",
      "Epoch 3/20  Iteration 376/3560 Training loss: 1.9084 1.1794 sec/batch\n",
      "Epoch 3/20  Iteration 377/3560 Training loss: 1.9074 1.1542 sec/batch\n",
      "Epoch 3/20  Iteration 378/3560 Training loss: 1.9081 1.1520 sec/batch\n",
      "Epoch 3/20  Iteration 379/3560 Training loss: 1.9068 1.1745 sec/batch\n",
      "Epoch 3/20  Iteration 380/3560 Training loss: 1.9061 1.1437 sec/batch\n",
      "Epoch 3/20  Iteration 381/3560 Training loss: 1.9056 1.1718 sec/batch\n",
      "Epoch 3/20  Iteration 382/3560 Training loss: 1.9037 1.1760 sec/batch\n",
      "Epoch 3/20  Iteration 383/3560 Training loss: 1.9022 1.1653 sec/batch\n",
      "Epoch 3/20  Iteration 384/3560 Training loss: 1.9023 1.2136 sec/batch\n",
      "Epoch 3/20  Iteration 385/3560 Training loss: 1.9026 1.1565 sec/batch\n",
      "Epoch 3/20  Iteration 386/3560 Training loss: 1.9023 1.1538 sec/batch\n",
      "Epoch 3/20  Iteration 387/3560 Training loss: 1.9017 1.1668 sec/batch\n",
      "Epoch 3/20  Iteration 388/3560 Training loss: 1.9003 1.1557 sec/batch\n",
      "Epoch 3/20  Iteration 389/3560 Training loss: 1.8998 1.1509 sec/batch\n",
      "Epoch 3/20  Iteration 390/3560 Training loss: 1.9000 1.1457 sec/batch\n",
      "Epoch 3/20  Iteration 391/3560 Training loss: 1.8988 1.1623 sec/batch\n",
      "Epoch 3/20  Iteration 392/3560 Training loss: 1.8981 1.1552 sec/batch\n",
      "Epoch 3/20  Iteration 393/3560 Training loss: 1.8974 1.1845 sec/batch\n",
      "Epoch 3/20  Iteration 394/3560 Training loss: 1.8958 1.1513 sec/batch\n",
      "Epoch 3/20  Iteration 395/3560 Training loss: 1.8942 1.1532 sec/batch\n",
      "Epoch 3/20  Iteration 396/3560 Training loss: 1.8929 1.1772 sec/batch\n",
      "Epoch 3/20  Iteration 397/3560 Training loss: 1.8918 1.1473 sec/batch\n",
      "Epoch 3/20  Iteration 398/3560 Training loss: 1.8916 1.1563 sec/batch\n",
      "Epoch 3/20  Iteration 399/3560 Training loss: 1.8906 1.1522 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 400/3560 Training loss: 1.8895 1.1643 sec/batch\n",
      "Epoch 3/20  Iteration 401/3560 Training loss: 1.8894 1.1611 sec/batch\n",
      "Epoch 3/20  Iteration 402/3560 Training loss: 1.8876 1.1874 sec/batch\n",
      "Epoch 3/20  Iteration 403/3560 Training loss: 1.8869 1.1493 sec/batch\n",
      "Epoch 3/20  Iteration 404/3560 Training loss: 1.8858 1.1564 sec/batch\n",
      "Epoch 3/20  Iteration 405/3560 Training loss: 1.8851 1.1612 sec/batch\n",
      "Epoch 3/20  Iteration 406/3560 Training loss: 1.8854 1.1416 sec/batch\n",
      "Epoch 3/20  Iteration 407/3560 Training loss: 1.8843 1.1529 sec/batch\n",
      "Epoch 3/20  Iteration 408/3560 Training loss: 1.8844 1.1703 sec/batch\n",
      "Epoch 3/20  Iteration 409/3560 Training loss: 1.8836 1.1529 sec/batch\n",
      "Epoch 3/20  Iteration 410/3560 Training loss: 1.8831 1.1943 sec/batch\n",
      "Epoch 3/20  Iteration 411/3560 Training loss: 1.8824 1.1491 sec/batch\n",
      "Epoch 3/20  Iteration 412/3560 Training loss: 1.8819 1.1645 sec/batch\n",
      "Epoch 3/20  Iteration 413/3560 Training loss: 1.8817 1.1514 sec/batch\n",
      "Epoch 3/20  Iteration 414/3560 Training loss: 1.8809 1.1609 sec/batch\n",
      "Epoch 3/20  Iteration 415/3560 Training loss: 1.8800 1.1586 sec/batch\n",
      "Epoch 3/20  Iteration 416/3560 Training loss: 1.8802 1.1477 sec/batch\n",
      "Epoch 3/20  Iteration 417/3560 Training loss: 1.8796 1.1635 sec/batch\n",
      "Epoch 3/20  Iteration 418/3560 Training loss: 1.8798 1.1589 sec/batch\n",
      "Epoch 3/20  Iteration 419/3560 Training loss: 1.8797 1.1831 sec/batch\n",
      "Epoch 3/20  Iteration 420/3560 Training loss: 1.8794 1.1522 sec/batch\n",
      "Epoch 3/20  Iteration 421/3560 Training loss: 1.8787 1.1456 sec/batch\n",
      "Epoch 3/20  Iteration 422/3560 Training loss: 1.8788 1.1609 sec/batch\n",
      "Epoch 3/20  Iteration 423/3560 Training loss: 1.8783 1.3817 sec/batch\n",
      "Epoch 3/20  Iteration 424/3560 Training loss: 1.8775 1.4099 sec/batch\n",
      "Epoch 3/20  Iteration 425/3560 Training loss: 1.8770 1.4548 sec/batch\n",
      "Epoch 3/20  Iteration 426/3560 Training loss: 1.8763 1.4610 sec/batch\n",
      "Epoch 3/20  Iteration 427/3560 Training loss: 1.8764 1.5206 sec/batch\n",
      "Epoch 3/20  Iteration 428/3560 Training loss: 1.8760 1.1698 sec/batch\n",
      "Epoch 3/20  Iteration 429/3560 Training loss: 1.8759 1.1453 sec/batch\n",
      "Epoch 3/20  Iteration 430/3560 Training loss: 1.8751 1.1658 sec/batch\n",
      "Epoch 3/20  Iteration 431/3560 Training loss: 1.8744 1.1558 sec/batch\n",
      "Epoch 3/20  Iteration 432/3560 Training loss: 1.8743 1.1649 sec/batch\n",
      "Epoch 3/20  Iteration 433/3560 Training loss: 1.8737 1.1748 sec/batch\n",
      "Epoch 3/20  Iteration 434/3560 Training loss: 1.8732 1.1663 sec/batch\n",
      "Epoch 3/20  Iteration 435/3560 Training loss: 1.8723 1.2068 sec/batch\n",
      "Epoch 3/20  Iteration 436/3560 Training loss: 1.8716 1.1868 sec/batch\n",
      "Epoch 3/20  Iteration 437/3560 Training loss: 1.8707 1.1628 sec/batch\n",
      "Epoch 3/20  Iteration 438/3560 Training loss: 1.8705 1.1510 sec/batch\n",
      "Epoch 3/20  Iteration 439/3560 Training loss: 1.8695 1.1613 sec/batch\n",
      "Epoch 3/20  Iteration 440/3560 Training loss: 1.8689 1.1615 sec/batch\n",
      "Epoch 3/20  Iteration 441/3560 Training loss: 1.8679 1.1445 sec/batch\n",
      "Epoch 3/20  Iteration 442/3560 Training loss: 1.8671 1.1668 sec/batch\n",
      "Epoch 3/20  Iteration 443/3560 Training loss: 1.8664 1.1676 sec/batch\n",
      "Epoch 3/20  Iteration 444/3560 Training loss: 1.8658 1.1772 sec/batch\n",
      "Epoch 3/20  Iteration 445/3560 Training loss: 1.8648 1.1547 sec/batch\n",
      "Epoch 3/20  Iteration 446/3560 Training loss: 1.8645 1.1535 sec/batch\n",
      "Epoch 3/20  Iteration 447/3560 Training loss: 1.8638 1.1518 sec/batch\n",
      "Epoch 3/20  Iteration 448/3560 Training loss: 1.8631 1.1457 sec/batch\n",
      "Epoch 3/20  Iteration 449/3560 Training loss: 1.8622 1.1546 sec/batch\n",
      "Epoch 3/20  Iteration 450/3560 Training loss: 1.8615 1.1540 sec/batch\n",
      "Epoch 3/20  Iteration 451/3560 Training loss: 1.8608 1.1508 sec/batch\n",
      "Epoch 3/20  Iteration 452/3560 Training loss: 1.8603 1.1732 sec/batch\n",
      "Epoch 3/20  Iteration 453/3560 Training loss: 1.8598 1.1699 sec/batch\n",
      "Epoch 3/20  Iteration 454/3560 Training loss: 1.8589 1.1552 sec/batch\n",
      "Epoch 3/20  Iteration 455/3560 Training loss: 1.8581 1.1435 sec/batch\n",
      "Epoch 3/20  Iteration 456/3560 Training loss: 1.8571 1.1593 sec/batch\n",
      "Epoch 3/20  Iteration 457/3560 Training loss: 1.8566 1.1549 sec/batch\n",
      "Epoch 3/20  Iteration 458/3560 Training loss: 1.8561 1.1430 sec/batch\n",
      "Epoch 3/20  Iteration 459/3560 Training loss: 1.8554 1.1690 sec/batch\n",
      "Epoch 3/20  Iteration 460/3560 Training loss: 1.8548 1.1571 sec/batch\n",
      "Epoch 3/20  Iteration 461/3560 Training loss: 1.8541 1.1948 sec/batch\n",
      "Epoch 3/20  Iteration 462/3560 Training loss: 1.8536 1.1531 sec/batch\n",
      "Epoch 3/20  Iteration 463/3560 Training loss: 1.8530 1.1532 sec/batch\n",
      "Epoch 3/20  Iteration 464/3560 Training loss: 1.8526 1.1539 sec/batch\n",
      "Epoch 3/20  Iteration 465/3560 Training loss: 1.8523 1.1620 sec/batch\n",
      "Epoch 3/20  Iteration 466/3560 Training loss: 1.8519 1.1607 sec/batch\n",
      "Epoch 3/20  Iteration 467/3560 Training loss: 1.8515 1.1539 sec/batch\n",
      "Epoch 3/20  Iteration 468/3560 Training loss: 1.8510 1.1751 sec/batch\n",
      "Epoch 3/20  Iteration 469/3560 Training loss: 1.8504 1.1619 sec/batch\n",
      "Epoch 3/20  Iteration 470/3560 Training loss: 1.8498 1.1871 sec/batch\n",
      "Epoch 3/20  Iteration 471/3560 Training loss: 1.8492 1.1667 sec/batch\n",
      "Epoch 3/20  Iteration 472/3560 Training loss: 1.8483 1.1555 sec/batch\n",
      "Epoch 3/20  Iteration 473/3560 Training loss: 1.8478 1.1607 sec/batch\n",
      "Epoch 3/20  Iteration 474/3560 Training loss: 1.8473 1.1492 sec/batch\n",
      "Epoch 3/20  Iteration 475/3560 Training loss: 1.8468 1.1495 sec/batch\n",
      "Epoch 3/20  Iteration 476/3560 Training loss: 1.8463 1.1557 sec/batch\n",
      "Epoch 3/20  Iteration 477/3560 Training loss: 1.8458 1.1599 sec/batch\n",
      "Epoch 3/20  Iteration 478/3560 Training loss: 1.8449 1.1607 sec/batch\n",
      "Epoch 3/20  Iteration 479/3560 Training loss: 1.8442 1.1774 sec/batch\n",
      "Epoch 3/20  Iteration 480/3560 Training loss: 1.8438 1.1591 sec/batch\n",
      "Epoch 3/20  Iteration 481/3560 Training loss: 1.8434 1.1527 sec/batch\n",
      "Epoch 3/20  Iteration 482/3560 Training loss: 1.8426 1.1571 sec/batch\n",
      "Epoch 3/20  Iteration 483/3560 Training loss: 1.8423 1.1821 sec/batch\n",
      "Epoch 3/20  Iteration 484/3560 Training loss: 1.8419 1.1590 sec/batch\n",
      "Epoch 3/20  Iteration 485/3560 Training loss: 1.8414 1.1583 sec/batch\n",
      "Epoch 3/20  Iteration 486/3560 Training loss: 1.8408 1.1569 sec/batch\n",
      "Epoch 3/20  Iteration 487/3560 Training loss: 1.8401 1.1716 sec/batch\n",
      "Epoch 3/20  Iteration 488/3560 Training loss: 1.8394 1.1935 sec/batch\n",
      "Epoch 3/20  Iteration 489/3560 Training loss: 1.8390 1.2821 sec/batch\n",
      "Epoch 3/20  Iteration 490/3560 Training loss: 1.8386 1.2338 sec/batch\n",
      "Epoch 3/20  Iteration 491/3560 Training loss: 1.8381 1.1758 sec/batch\n",
      "Epoch 3/20  Iteration 492/3560 Training loss: 1.8377 1.1672 sec/batch\n",
      "Epoch 3/20  Iteration 493/3560 Training loss: 1.8374 1.2410 sec/batch\n",
      "Epoch 3/20  Iteration 494/3560 Training loss: 1.8370 1.2219 sec/batch\n",
      "Epoch 3/20  Iteration 495/3560 Training loss: 1.8367 1.1769 sec/batch\n",
      "Epoch 3/20  Iteration 496/3560 Training loss: 1.8361 1.1754 sec/batch\n",
      "Epoch 3/20  Iteration 497/3560 Training loss: 1.8359 1.1515 sec/batch\n",
      "Epoch 3/20  Iteration 498/3560 Training loss: 1.8355 1.1482 sec/batch\n",
      "Epoch 3/20  Iteration 499/3560 Training loss: 1.8350 1.1557 sec/batch\n",
      "Epoch 3/20  Iteration 500/3560 Training loss: 1.8347 1.1475 sec/batch\n",
      "Epoch 3/20  Iteration 501/3560 Training loss: 1.8341 1.1462 sec/batch\n",
      "Epoch 3/20  Iteration 502/3560 Training loss: 1.8338 1.1536 sec/batch\n",
      "Epoch 3/20  Iteration 503/3560 Training loss: 1.8334 1.1602 sec/batch\n",
      "Epoch 3/20  Iteration 504/3560 Training loss: 1.8332 1.1493 sec/batch\n",
      "Epoch 3/20  Iteration 505/3560 Training loss: 1.8329 1.1891 sec/batch\n",
      "Epoch 3/20  Iteration 506/3560 Training loss: 1.8326 1.1461 sec/batch\n",
      "Epoch 3/20  Iteration 507/3560 Training loss: 1.8320 1.1519 sec/batch\n",
      "Epoch 3/20  Iteration 508/3560 Training loss: 1.8318 1.1519 sec/batch\n",
      "Epoch 3/20  Iteration 509/3560 Training loss: 1.8315 1.1502 sec/batch\n",
      "Epoch 3/20  Iteration 510/3560 Training loss: 1.8312 1.1576 sec/batch\n",
      "Epoch 3/20  Iteration 511/3560 Training loss: 1.8308 1.1587 sec/batch\n",
      "Epoch 3/20  Iteration 512/3560 Training loss: 1.8305 1.1533 sec/batch\n",
      "Epoch 3/20  Iteration 513/3560 Training loss: 1.8302 1.1897 sec/batch\n",
      "Epoch 3/20  Iteration 514/3560 Training loss: 1.8298 1.1565 sec/batch\n",
      "Epoch 3/20  Iteration 515/3560 Training loss: 1.8292 1.1443 sec/batch\n",
      "Epoch 3/20  Iteration 516/3560 Training loss: 1.8290 1.1543 sec/batch\n",
      "Epoch 3/20  Iteration 517/3560 Training loss: 1.8288 1.1446 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 518/3560 Training loss: 1.8283 1.1496 sec/batch\n",
      "Epoch 3/20  Iteration 519/3560 Training loss: 1.8280 1.1639 sec/batch\n",
      "Epoch 3/20  Iteration 520/3560 Training loss: 1.8277 1.1526 sec/batch\n",
      "Epoch 3/20  Iteration 521/3560 Training loss: 1.8273 1.1643 sec/batch\n",
      "Epoch 3/20  Iteration 522/3560 Training loss: 1.8269 1.1922 sec/batch\n",
      "Epoch 3/20  Iteration 523/3560 Training loss: 1.8266 1.1562 sec/batch\n",
      "Epoch 3/20  Iteration 524/3560 Training loss: 1.8266 1.1477 sec/batch\n",
      "Epoch 3/20  Iteration 525/3560 Training loss: 1.8261 1.1510 sec/batch\n",
      "Epoch 3/20  Iteration 526/3560 Training loss: 1.8258 1.1454 sec/batch\n",
      "Epoch 3/20  Iteration 527/3560 Training loss: 1.8253 1.1445 sec/batch\n",
      "Epoch 3/20  Iteration 528/3560 Training loss: 1.8247 1.1625 sec/batch\n",
      "Epoch 3/20  Iteration 529/3560 Training loss: 1.8245 1.1568 sec/batch\n",
      "Epoch 3/20  Iteration 530/3560 Training loss: 1.8241 1.1482 sec/batch\n",
      "Epoch 3/20  Iteration 531/3560 Training loss: 1.8237 1.1855 sec/batch\n",
      "Epoch 3/20  Iteration 532/3560 Training loss: 1.8232 1.1508 sec/batch\n",
      "Epoch 3/20  Iteration 533/3560 Training loss: 1.8226 1.1522 sec/batch\n",
      "Epoch 3/20  Iteration 534/3560 Training loss: 1.8223 1.1493 sec/batch\n",
      "Epoch 4/20  Iteration 535/3560 Training loss: 1.8278 1.1600 sec/batch\n",
      "Epoch 4/20  Iteration 536/3560 Training loss: 1.7851 1.1593 sec/batch\n",
      "Epoch 4/20  Iteration 537/3560 Training loss: 1.7689 1.1493 sec/batch\n",
      "Epoch 4/20  Iteration 538/3560 Training loss: 1.7605 1.1759 sec/batch\n",
      "Epoch 4/20  Iteration 539/3560 Training loss: 1.7537 1.1595 sec/batch\n",
      "Epoch 4/20  Iteration 540/3560 Training loss: 1.7440 1.2457 sec/batch\n",
      "Epoch 4/20  Iteration 541/3560 Training loss: 1.7438 1.1417 sec/batch\n",
      "Epoch 4/20  Iteration 542/3560 Training loss: 1.7417 1.1577 sec/batch\n",
      "Epoch 4/20  Iteration 543/3560 Training loss: 1.7436 1.1424 sec/batch\n",
      "Epoch 4/20  Iteration 544/3560 Training loss: 1.7425 1.1422 sec/batch\n",
      "Epoch 4/20  Iteration 545/3560 Training loss: 1.7393 1.1486 sec/batch\n",
      "Epoch 4/20  Iteration 546/3560 Training loss: 1.7367 1.1444 sec/batch\n",
      "Epoch 4/20  Iteration 547/3560 Training loss: 1.7375 1.1678 sec/batch\n",
      "Epoch 4/20  Iteration 548/3560 Training loss: 1.7394 1.1554 sec/batch\n",
      "Epoch 4/20  Iteration 549/3560 Training loss: 1.7379 1.1794 sec/batch\n",
      "Epoch 4/20  Iteration 550/3560 Training loss: 1.7362 1.1587 sec/batch\n",
      "Epoch 4/20  Iteration 551/3560 Training loss: 1.7358 1.1585 sec/batch\n",
      "Epoch 4/20  Iteration 552/3560 Training loss: 1.7375 1.1575 sec/batch\n",
      "Epoch 4/20  Iteration 553/3560 Training loss: 1.7369 1.1465 sec/batch\n",
      "Epoch 4/20  Iteration 554/3560 Training loss: 1.7374 1.1554 sec/batch\n",
      "Epoch 4/20  Iteration 555/3560 Training loss: 1.7365 1.1572 sec/batch\n",
      "Epoch 4/20  Iteration 556/3560 Training loss: 1.7372 1.1593 sec/batch\n",
      "Epoch 4/20  Iteration 557/3560 Training loss: 1.7361 1.1631 sec/batch\n",
      "Epoch 4/20  Iteration 558/3560 Training loss: 1.7354 1.1598 sec/batch\n",
      "Epoch 4/20  Iteration 559/3560 Training loss: 1.7353 1.1569 sec/batch\n",
      "Epoch 4/20  Iteration 560/3560 Training loss: 1.7336 1.1498 sec/batch\n",
      "Epoch 4/20  Iteration 561/3560 Training loss: 1.7324 1.1592 sec/batch\n",
      "Epoch 4/20  Iteration 562/3560 Training loss: 1.7323 1.1591 sec/batch\n",
      "Epoch 4/20  Iteration 563/3560 Training loss: 1.7324 1.1515 sec/batch\n",
      "Epoch 4/20  Iteration 564/3560 Training loss: 1.7323 1.1616 sec/batch\n",
      "Epoch 4/20  Iteration 565/3560 Training loss: 1.7319 1.1509 sec/batch\n",
      "Epoch 4/20  Iteration 566/3560 Training loss: 1.7306 1.1886 sec/batch\n",
      "Epoch 4/20  Iteration 567/3560 Training loss: 1.7305 1.1446 sec/batch\n",
      "Epoch 4/20  Iteration 568/3560 Training loss: 1.7309 1.1523 sec/batch\n",
      "Epoch 4/20  Iteration 569/3560 Training loss: 1.7305 1.1522 sec/batch\n",
      "Epoch 4/20  Iteration 570/3560 Training loss: 1.7297 1.1502 sec/batch\n",
      "Epoch 4/20  Iteration 571/3560 Training loss: 1.7289 1.1502 sec/batch\n",
      "Epoch 4/20  Iteration 572/3560 Training loss: 1.7277 1.1528 sec/batch\n",
      "Epoch 4/20  Iteration 573/3560 Training loss: 1.7261 1.1608 sec/batch\n",
      "Epoch 4/20  Iteration 574/3560 Training loss: 1.7250 1.1607 sec/batch\n",
      "Epoch 4/20  Iteration 575/3560 Training loss: 1.7241 1.1915 sec/batch\n",
      "Epoch 4/20  Iteration 576/3560 Training loss: 1.7243 1.2466 sec/batch\n",
      "Epoch 4/20  Iteration 577/3560 Training loss: 1.7236 1.4359 sec/batch\n",
      "Epoch 4/20  Iteration 578/3560 Training loss: 1.7228 1.4229 sec/batch\n",
      "Epoch 4/20  Iteration 579/3560 Training loss: 1.7229 1.4764 sec/batch\n",
      "Epoch 4/20  Iteration 580/3560 Training loss: 1.7216 1.4106 sec/batch\n",
      "Epoch 4/20  Iteration 581/3560 Training loss: 1.7211 1.3447 sec/batch\n",
      "Epoch 4/20  Iteration 582/3560 Training loss: 1.7203 1.1623 sec/batch\n",
      "Epoch 4/20  Iteration 583/3560 Training loss: 1.7197 1.1718 sec/batch\n",
      "Epoch 4/20  Iteration 584/3560 Training loss: 1.7202 1.1425 sec/batch\n",
      "Epoch 4/20  Iteration 585/3560 Training loss: 1.7193 1.1533 sec/batch\n",
      "Epoch 4/20  Iteration 586/3560 Training loss: 1.7198 1.1712 sec/batch\n",
      "Epoch 4/20  Iteration 587/3560 Training loss: 1.7194 1.1680 sec/batch\n",
      "Epoch 4/20  Iteration 588/3560 Training loss: 1.7192 1.1478 sec/batch\n",
      "Epoch 4/20  Iteration 589/3560 Training loss: 1.7186 1.1583 sec/batch\n",
      "Epoch 4/20  Iteration 590/3560 Training loss: 1.7185 1.1516 sec/batch\n",
      "Epoch 4/20  Iteration 591/3560 Training loss: 1.7184 1.2224 sec/batch\n",
      "Epoch 4/20  Iteration 592/3560 Training loss: 1.7177 1.1500 sec/batch\n",
      "Epoch 4/20  Iteration 593/3560 Training loss: 1.7168 1.1507 sec/batch\n",
      "Epoch 4/20  Iteration 594/3560 Training loss: 1.7173 1.1614 sec/batch\n",
      "Epoch 4/20  Iteration 595/3560 Training loss: 1.7166 1.1474 sec/batch\n",
      "Epoch 4/20  Iteration 596/3560 Training loss: 1.7170 1.1540 sec/batch\n",
      "Epoch 4/20  Iteration 597/3560 Training loss: 1.7172 1.1511 sec/batch\n",
      "Epoch 4/20  Iteration 598/3560 Training loss: 1.7173 1.1510 sec/batch\n",
      "Epoch 4/20  Iteration 599/3560 Training loss: 1.7167 1.1550 sec/batch\n",
      "Epoch 4/20  Iteration 600/3560 Training loss: 1.7167 1.1683 sec/batch\n",
      "Epoch 4/20  Iteration 601/3560 Training loss: 1.7166 1.1493 sec/batch\n",
      "Epoch 4/20  Iteration 602/3560 Training loss: 1.7158 1.1503 sec/batch\n",
      "Epoch 4/20  Iteration 603/3560 Training loss: 1.7154 1.1550 sec/batch\n",
      "Epoch 4/20  Iteration 604/3560 Training loss: 1.7151 1.1452 sec/batch\n",
      "Epoch 4/20  Iteration 605/3560 Training loss: 1.7153 1.1464 sec/batch\n",
      "Epoch 4/20  Iteration 606/3560 Training loss: 1.7153 1.1499 sec/batch\n",
      "Epoch 4/20  Iteration 607/3560 Training loss: 1.7156 1.1476 sec/batch\n",
      "Epoch 4/20  Iteration 608/3560 Training loss: 1.7151 1.1553 sec/batch\n",
      "Epoch 4/20  Iteration 609/3560 Training loss: 1.7146 1.1734 sec/batch\n",
      "Epoch 4/20  Iteration 610/3560 Training loss: 1.7146 1.1394 sec/batch\n",
      "Epoch 4/20  Iteration 611/3560 Training loss: 1.7142 1.1568 sec/batch\n",
      "Epoch 4/20  Iteration 612/3560 Training loss: 1.7140 1.1651 sec/batch\n",
      "Epoch 4/20  Iteration 613/3560 Training loss: 1.7131 1.1598 sec/batch\n",
      "Epoch 4/20  Iteration 614/3560 Training loss: 1.7127 1.1503 sec/batch\n",
      "Epoch 4/20  Iteration 615/3560 Training loss: 1.7119 1.1600 sec/batch\n",
      "Epoch 4/20  Iteration 616/3560 Training loss: 1.7118 1.1546 sec/batch\n",
      "Epoch 4/20  Iteration 617/3560 Training loss: 1.7110 1.1627 sec/batch\n",
      "Epoch 4/20  Iteration 618/3560 Training loss: 1.7108 1.1688 sec/batch\n",
      "Epoch 4/20  Iteration 619/3560 Training loss: 1.7101 1.1475 sec/batch\n",
      "Epoch 4/20  Iteration 620/3560 Training loss: 1.7095 1.1576 sec/batch\n",
      "Epoch 4/20  Iteration 621/3560 Training loss: 1.7091 1.1567 sec/batch\n",
      "Epoch 4/20  Iteration 622/3560 Training loss: 1.7085 1.1529 sec/batch\n",
      "Epoch 4/20  Iteration 623/3560 Training loss: 1.7077 1.1467 sec/batch\n",
      "Epoch 4/20  Iteration 624/3560 Training loss: 1.7075 1.1617 sec/batch\n",
      "Epoch 4/20  Iteration 625/3560 Training loss: 1.7070 1.1546 sec/batch\n",
      "Epoch 4/20  Iteration 626/3560 Training loss: 1.7065 1.1904 sec/batch\n",
      "Epoch 4/20  Iteration 627/3560 Training loss: 1.7059 1.1506 sec/batch\n",
      "Epoch 4/20  Iteration 628/3560 Training loss: 1.7053 1.1428 sec/batch\n",
      "Epoch 4/20  Iteration 629/3560 Training loss: 1.7047 1.1619 sec/batch\n",
      "Epoch 4/20  Iteration 630/3560 Training loss: 1.7045 1.1565 sec/batch\n",
      "Epoch 4/20  Iteration 631/3560 Training loss: 1.7041 1.1556 sec/batch\n",
      "Epoch 4/20  Iteration 632/3560 Training loss: 1.7036 1.1508 sec/batch\n",
      "Epoch 4/20  Iteration 633/3560 Training loss: 1.7031 1.1493 sec/batch\n",
      "Epoch 4/20  Iteration 634/3560 Training loss: 1.7023 1.1541 sec/batch\n",
      "Epoch 4/20  Iteration 635/3560 Training loss: 1.7020 1.1739 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 636/3560 Training loss: 1.7015 1.1455 sec/batch\n",
      "Epoch 4/20  Iteration 637/3560 Training loss: 1.7011 1.1530 sec/batch\n",
      "Epoch 4/20  Iteration 638/3560 Training loss: 1.7008 1.1693 sec/batch\n",
      "Epoch 4/20  Iteration 639/3560 Training loss: 1.7003 1.1592 sec/batch\n",
      "Epoch 4/20  Iteration 640/3560 Training loss: 1.7000 1.1464 sec/batch\n",
      "Epoch 4/20  Iteration 641/3560 Training loss: 1.6997 1.1538 sec/batch\n",
      "Epoch 4/20  Iteration 642/3560 Training loss: 1.6994 1.1569 sec/batch\n",
      "Epoch 4/20  Iteration 643/3560 Training loss: 1.6992 1.1668 sec/batch\n",
      "Epoch 4/20  Iteration 644/3560 Training loss: 1.6990 1.2594 sec/batch\n",
      "Epoch 4/20  Iteration 645/3560 Training loss: 1.6986 1.1841 sec/batch\n",
      "Epoch 4/20  Iteration 646/3560 Training loss: 1.6983 1.1592 sec/batch\n",
      "Epoch 4/20  Iteration 647/3560 Training loss: 1.6978 1.1470 sec/batch\n",
      "Epoch 4/20  Iteration 648/3560 Training loss: 1.6974 1.1507 sec/batch\n",
      "Epoch 4/20  Iteration 649/3560 Training loss: 1.6968 1.1439 sec/batch\n",
      "Epoch 4/20  Iteration 650/3560 Training loss: 1.6961 1.1641 sec/batch\n",
      "Epoch 4/20  Iteration 651/3560 Training loss: 1.6958 1.1446 sec/batch\n",
      "Epoch 4/20  Iteration 652/3560 Training loss: 1.6954 1.1606 sec/batch\n",
      "Epoch 4/20  Iteration 653/3560 Training loss: 1.6950 1.1726 sec/batch\n",
      "Epoch 4/20  Iteration 654/3560 Training loss: 1.6946 1.1427 sec/batch\n",
      "Epoch 4/20  Iteration 655/3560 Training loss: 1.6942 1.1535 sec/batch\n",
      "Epoch 4/20  Iteration 656/3560 Training loss: 1.6936 1.1470 sec/batch\n",
      "Epoch 4/20  Iteration 657/3560 Training loss: 1.6930 1.1515 sec/batch\n",
      "Epoch 4/20  Iteration 658/3560 Training loss: 1.6927 1.1433 sec/batch\n",
      "Epoch 4/20  Iteration 659/3560 Training loss: 1.6924 1.1568 sec/batch\n",
      "Epoch 4/20  Iteration 660/3560 Training loss: 1.6917 1.1528 sec/batch\n",
      "Epoch 4/20  Iteration 661/3560 Training loss: 1.6915 1.1896 sec/batch\n",
      "Epoch 4/20  Iteration 662/3560 Training loss: 1.6913 1.1638 sec/batch\n",
      "Epoch 4/20  Iteration 663/3560 Training loss: 1.6909 1.1528 sec/batch\n",
      "Epoch 4/20  Iteration 664/3560 Training loss: 1.6904 1.1707 sec/batch\n",
      "Epoch 4/20  Iteration 665/3560 Training loss: 1.6898 1.1566 sec/batch\n",
      "Epoch 4/20  Iteration 666/3560 Training loss: 1.6893 1.1446 sec/batch\n",
      "Epoch 4/20  Iteration 667/3560 Training loss: 1.6889 1.1452 sec/batch\n",
      "Epoch 4/20  Iteration 668/3560 Training loss: 1.6887 1.1552 sec/batch\n",
      "Epoch 4/20  Iteration 669/3560 Training loss: 1.6884 1.1583 sec/batch\n",
      "Epoch 4/20  Iteration 670/3560 Training loss: 1.6882 1.1934 sec/batch\n",
      "Epoch 4/20  Iteration 671/3560 Training loss: 1.6880 1.1521 sec/batch\n",
      "Epoch 4/20  Iteration 672/3560 Training loss: 1.6878 1.1507 sec/batch\n",
      "Epoch 4/20  Iteration 673/3560 Training loss: 1.6876 1.1491 sec/batch\n",
      "Epoch 4/20  Iteration 674/3560 Training loss: 1.6872 1.1549 sec/batch\n",
      "Epoch 4/20  Iteration 675/3560 Training loss: 1.6872 1.1424 sec/batch\n",
      "Epoch 4/20  Iteration 676/3560 Training loss: 1.6869 1.1555 sec/batch\n",
      "Epoch 4/20  Iteration 677/3560 Training loss: 1.6866 1.1521 sec/batch\n",
      "Epoch 4/20  Iteration 678/3560 Training loss: 1.6864 1.1446 sec/batch\n",
      "Epoch 4/20  Iteration 679/3560 Training loss: 1.6859 1.1783 sec/batch\n",
      "Epoch 4/20  Iteration 680/3560 Training loss: 1.6858 1.1511 sec/batch\n",
      "Epoch 4/20  Iteration 681/3560 Training loss: 1.6855 1.1594 sec/batch\n",
      "Epoch 4/20  Iteration 682/3560 Training loss: 1.6855 1.1399 sec/batch\n",
      "Epoch 4/20  Iteration 683/3560 Training loss: 1.6853 1.1537 sec/batch\n",
      "Epoch 4/20  Iteration 684/3560 Training loss: 1.6849 1.1470 sec/batch\n",
      "Epoch 4/20  Iteration 685/3560 Training loss: 1.6842 1.1397 sec/batch\n",
      "Epoch 4/20  Iteration 686/3560 Training loss: 1.6840 1.1545 sec/batch\n",
      "Epoch 4/20  Iteration 687/3560 Training loss: 1.6839 1.1468 sec/batch\n",
      "Epoch 4/20  Iteration 688/3560 Training loss: 1.6837 1.1750 sec/batch\n",
      "Epoch 4/20  Iteration 689/3560 Training loss: 1.6834 1.1552 sec/batch\n",
      "Epoch 4/20  Iteration 690/3560 Training loss: 1.6831 1.1674 sec/batch\n",
      "Epoch 4/20  Iteration 691/3560 Training loss: 1.6830 1.1451 sec/batch\n",
      "Epoch 4/20  Iteration 692/3560 Training loss: 1.6828 1.1468 sec/batch\n",
      "Epoch 4/20  Iteration 693/3560 Training loss: 1.6823 1.1510 sec/batch\n",
      "Epoch 4/20  Iteration 694/3560 Training loss: 1.6822 1.1563 sec/batch\n",
      "Epoch 4/20  Iteration 695/3560 Training loss: 1.6821 1.1589 sec/batch\n",
      "Epoch 4/20  Iteration 696/3560 Training loss: 1.6818 1.1738 sec/batch\n",
      "Epoch 4/20  Iteration 697/3560 Training loss: 1.6816 1.1827 sec/batch\n",
      "Epoch 4/20  Iteration 698/3560 Training loss: 1.6814 1.1658 sec/batch\n",
      "Epoch 4/20  Iteration 699/3560 Training loss: 1.6811 1.1509 sec/batch\n",
      "Epoch 4/20  Iteration 700/3560 Training loss: 1.6808 1.1461 sec/batch\n",
      "Epoch 4/20  Iteration 701/3560 Training loss: 1.6807 1.1506 sec/batch\n",
      "Epoch 4/20  Iteration 702/3560 Training loss: 1.6808 1.1561 sec/batch\n",
      "Epoch 4/20  Iteration 703/3560 Training loss: 1.6804 1.1610 sec/batch\n",
      "Epoch 4/20  Iteration 704/3560 Training loss: 1.6801 1.1419 sec/batch\n",
      "Epoch 4/20  Iteration 705/3560 Training loss: 1.6797 1.1966 sec/batch\n",
      "Epoch 4/20  Iteration 706/3560 Training loss: 1.6793 1.1482 sec/batch\n",
      "Epoch 4/20  Iteration 707/3560 Training loss: 1.6792 1.1546 sec/batch\n",
      "Epoch 4/20  Iteration 708/3560 Training loss: 1.6789 1.1610 sec/batch\n",
      "Epoch 4/20  Iteration 709/3560 Training loss: 1.6788 1.1483 sec/batch\n",
      "Epoch 4/20  Iteration 710/3560 Training loss: 1.6784 1.1463 sec/batch\n",
      "Epoch 4/20  Iteration 711/3560 Training loss: 1.6779 1.1401 sec/batch\n",
      "Epoch 4/20  Iteration 712/3560 Training loss: 1.6777 1.1584 sec/batch\n",
      "Epoch 5/20  Iteration 713/3560 Training loss: 1.7104 1.1594 sec/batch\n",
      "Epoch 5/20  Iteration 714/3560 Training loss: 1.6678 1.1784 sec/batch\n",
      "Epoch 5/20  Iteration 715/3560 Training loss: 1.6510 1.1501 sec/batch\n",
      "Epoch 5/20  Iteration 716/3560 Training loss: 1.6454 1.1674 sec/batch\n",
      "Epoch 5/20  Iteration 717/3560 Training loss: 1.6380 1.1504 sec/batch\n",
      "Epoch 5/20  Iteration 718/3560 Training loss: 1.6284 1.1459 sec/batch\n",
      "Epoch 5/20  Iteration 719/3560 Training loss: 1.6271 1.1403 sec/batch\n",
      "Epoch 5/20  Iteration 720/3560 Training loss: 1.6248 1.1498 sec/batch\n",
      "Epoch 5/20  Iteration 721/3560 Training loss: 1.6254 1.1699 sec/batch\n",
      "Epoch 5/20  Iteration 722/3560 Training loss: 1.6241 1.1471 sec/batch\n",
      "Epoch 5/20  Iteration 723/3560 Training loss: 1.6209 1.1800 sec/batch\n",
      "Epoch 5/20  Iteration 724/3560 Training loss: 1.6198 1.1477 sec/batch\n",
      "Epoch 5/20  Iteration 725/3560 Training loss: 1.6196 1.1535 sec/batch\n",
      "Epoch 5/20  Iteration 726/3560 Training loss: 1.6211 1.1645 sec/batch\n",
      "Epoch 5/20  Iteration 727/3560 Training loss: 1.6194 1.1431 sec/batch\n",
      "Epoch 5/20  Iteration 728/3560 Training loss: 1.6176 1.4141 sec/batch\n",
      "Epoch 5/20  Iteration 729/3560 Training loss: 1.6174 1.2268 sec/batch\n",
      "Epoch 5/20  Iteration 730/3560 Training loss: 1.6185 1.4359 sec/batch\n",
      "Epoch 5/20  Iteration 731/3560 Training loss: 1.6187 1.6476 sec/batch\n",
      "Epoch 5/20  Iteration 732/3560 Training loss: 1.6197 1.5483 sec/batch\n",
      "Epoch 5/20  Iteration 733/3560 Training loss: 1.6189 1.6543 sec/batch\n",
      "Epoch 5/20  Iteration 734/3560 Training loss: 1.6191 1.5417 sec/batch\n",
      "Epoch 5/20  Iteration 735/3560 Training loss: 1.6183 1.4364 sec/batch\n",
      "Epoch 5/20  Iteration 736/3560 Training loss: 1.6179 1.2253 sec/batch\n",
      "Epoch 5/20  Iteration 737/3560 Training loss: 1.6180 1.2139 sec/batch\n",
      "Epoch 5/20  Iteration 738/3560 Training loss: 1.6166 1.2431 sec/batch\n",
      "Epoch 5/20  Iteration 739/3560 Training loss: 1.6148 1.2216 sec/batch\n",
      "Epoch 5/20  Iteration 740/3560 Training loss: 1.6152 1.2245 sec/batch\n",
      "Epoch 5/20  Iteration 741/3560 Training loss: 1.6155 1.2187 sec/batch\n",
      "Epoch 5/20  Iteration 742/3560 Training loss: 1.6156 1.2470 sec/batch\n",
      "Epoch 5/20  Iteration 743/3560 Training loss: 1.6152 1.2646 sec/batch\n",
      "Epoch 5/20  Iteration 744/3560 Training loss: 1.6142 1.2237 sec/batch\n",
      "Epoch 5/20  Iteration 745/3560 Training loss: 1.6142 1.2078 sec/batch\n",
      "Epoch 5/20  Iteration 746/3560 Training loss: 1.6145 1.2827 sec/batch\n",
      "Epoch 5/20  Iteration 747/3560 Training loss: 1.6143 1.2118 sec/batch\n",
      "Epoch 5/20  Iteration 748/3560 Training loss: 1.6137 1.2061 sec/batch\n",
      "Epoch 5/20  Iteration 749/3560 Training loss: 1.6127 1.2100 sec/batch\n",
      "Epoch 5/20  Iteration 750/3560 Training loss: 1.6113 1.2104 sec/batch\n",
      "Epoch 5/20  Iteration 751/3560 Training loss: 1.6098 1.2150 sec/batch\n",
      "Epoch 5/20  Iteration 752/3560 Training loss: 1.6092 1.3022 sec/batch\n",
      "Epoch 5/20  Iteration 753/3560 Training loss: 1.6085 1.2284 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 754/3560 Training loss: 1.6090 1.2541 sec/batch\n",
      "Epoch 5/20  Iteration 755/3560 Training loss: 1.6082 1.2079 sec/batch\n",
      "Epoch 5/20  Iteration 756/3560 Training loss: 1.6073 1.2088 sec/batch\n",
      "Epoch 5/20  Iteration 757/3560 Training loss: 1.6076 1.2181 sec/batch\n",
      "Epoch 5/20  Iteration 758/3560 Training loss: 1.6064 1.2030 sec/batch\n",
      "Epoch 5/20  Iteration 759/3560 Training loss: 1.6059 1.2181 sec/batch\n",
      "Epoch 5/20  Iteration 760/3560 Training loss: 1.6052 1.2886 sec/batch\n",
      "Epoch 5/20  Iteration 761/3560 Training loss: 1.6048 1.2396 sec/batch\n",
      "Epoch 5/20  Iteration 762/3560 Training loss: 1.6053 1.2349 sec/batch\n",
      "Epoch 5/20  Iteration 763/3560 Training loss: 1.6047 1.2255 sec/batch\n",
      "Epoch 5/20  Iteration 764/3560 Training loss: 1.6053 1.2248 sec/batch\n",
      "Epoch 5/20  Iteration 765/3560 Training loss: 1.6050 1.2110 sec/batch\n",
      "Epoch 5/20  Iteration 766/3560 Training loss: 1.6050 1.2039 sec/batch\n",
      "Epoch 5/20  Iteration 767/3560 Training loss: 1.6045 1.2210 sec/batch\n",
      "Epoch 5/20  Iteration 768/3560 Training loss: 1.6045 1.2257 sec/batch\n",
      "Epoch 5/20  Iteration 769/3560 Training loss: 1.6047 1.2177 sec/batch\n",
      "Epoch 5/20  Iteration 770/3560 Training loss: 1.6041 1.3151 sec/batch\n",
      "Epoch 5/20  Iteration 771/3560 Training loss: 1.6032 1.2281 sec/batch\n",
      "Epoch 5/20  Iteration 772/3560 Training loss: 1.6037 1.2245 sec/batch\n",
      "Epoch 5/20  Iteration 773/3560 Training loss: 1.6035 1.2200 sec/batch\n",
      "Epoch 5/20  Iteration 774/3560 Training loss: 1.6043 1.2098 sec/batch\n",
      "Epoch 5/20  Iteration 775/3560 Training loss: 1.6045 1.2269 sec/batch\n",
      "Epoch 5/20  Iteration 776/3560 Training loss: 1.6046 1.2079 sec/batch\n",
      "Epoch 5/20  Iteration 777/3560 Training loss: 1.6043 1.2235 sec/batch\n",
      "Epoch 5/20  Iteration 778/3560 Training loss: 1.6043 1.2150 sec/batch\n",
      "Epoch 5/20  Iteration 779/3560 Training loss: 1.6043 1.2935 sec/batch\n",
      "Epoch 5/20  Iteration 780/3560 Training loss: 1.6037 1.2060 sec/batch\n",
      "Epoch 5/20  Iteration 781/3560 Training loss: 1.6034 1.2053 sec/batch\n",
      "Epoch 5/20  Iteration 782/3560 Training loss: 1.6031 1.2103 sec/batch\n",
      "Epoch 5/20  Iteration 783/3560 Training loss: 1.6035 1.2123 sec/batch\n",
      "Epoch 5/20  Iteration 784/3560 Training loss: 1.6034 1.2010 sec/batch\n",
      "Epoch 5/20  Iteration 785/3560 Training loss: 1.6037 1.2336 sec/batch\n",
      "Epoch 5/20  Iteration 786/3560 Training loss: 1.6033 1.3112 sec/batch\n",
      "Epoch 5/20  Iteration 787/3560 Training loss: 1.6030 1.2573 sec/batch\n",
      "Epoch 5/20  Iteration 788/3560 Training loss: 1.6031 1.2318 sec/batch\n",
      "Epoch 5/20  Iteration 789/3560 Training loss: 1.6027 1.2143 sec/batch\n",
      "Epoch 5/20  Iteration 790/3560 Training loss: 1.6025 1.2153 sec/batch\n",
      "Epoch 5/20  Iteration 791/3560 Training loss: 1.6017 1.2135 sec/batch\n",
      "Epoch 5/20  Iteration 792/3560 Training loss: 1.6014 1.2076 sec/batch\n",
      "Epoch 5/20  Iteration 793/3560 Training loss: 1.6007 1.2211 sec/batch\n",
      "Epoch 5/20  Iteration 794/3560 Training loss: 1.6005 1.2722 sec/batch\n",
      "Epoch 5/20  Iteration 795/3560 Training loss: 1.5999 1.2844 sec/batch\n",
      "Epoch 5/20  Iteration 796/3560 Training loss: 1.5997 1.3123 sec/batch\n",
      "Epoch 5/20  Iteration 797/3560 Training loss: 1.5992 1.2347 sec/batch\n",
      "Epoch 5/20  Iteration 798/3560 Training loss: 1.5986 1.2217 sec/batch\n",
      "Epoch 5/20  Iteration 799/3560 Training loss: 1.5981 1.2131 sec/batch\n",
      "Epoch 5/20  Iteration 800/3560 Training loss: 1.5975 1.2118 sec/batch\n",
      "Epoch 5/20  Iteration 801/3560 Training loss: 1.5969 1.2695 sec/batch\n",
      "Epoch 5/20  Iteration 802/3560 Training loss: 1.5968 1.2810 sec/batch\n",
      "Epoch 5/20  Iteration 803/3560 Training loss: 1.5964 1.2539 sec/batch\n",
      "Epoch 5/20  Iteration 804/3560 Training loss: 1.5960 1.2228 sec/batch\n",
      "Epoch 5/20  Iteration 805/3560 Training loss: 1.5954 1.2136 sec/batch\n",
      "Epoch 5/20  Iteration 806/3560 Training loss: 1.5948 1.2191 sec/batch\n",
      "Epoch 5/20  Iteration 807/3560 Training loss: 1.5943 1.2134 sec/batch\n",
      "Epoch 5/20  Iteration 808/3560 Training loss: 1.5941 1.2069 sec/batch\n",
      "Epoch 5/20  Iteration 809/3560 Training loss: 1.5938 1.2676 sec/batch\n",
      "Epoch 5/20  Iteration 810/3560 Training loss: 1.5933 1.2192 sec/batch\n",
      "Epoch 5/20  Iteration 811/3560 Training loss: 1.5927 1.2312 sec/batch\n",
      "Epoch 5/20  Iteration 812/3560 Training loss: 1.5920 1.2521 sec/batch\n",
      "Epoch 5/20  Iteration 813/3560 Training loss: 1.5917 1.2117 sec/batch\n",
      "Epoch 5/20  Iteration 814/3560 Training loss: 1.5913 1.2203 sec/batch\n",
      "Epoch 5/20  Iteration 815/3560 Training loss: 1.5909 1.2269 sec/batch\n",
      "Epoch 5/20  Iteration 816/3560 Training loss: 1.5906 1.2582 sec/batch\n",
      "Epoch 5/20  Iteration 817/3560 Training loss: 1.5902 1.2140 sec/batch\n",
      "Epoch 5/20  Iteration 818/3560 Training loss: 1.5898 1.2191 sec/batch\n",
      "Epoch 5/20  Iteration 819/3560 Training loss: 1.5896 1.2072 sec/batch\n",
      "Epoch 5/20  Iteration 820/3560 Training loss: 1.5894 1.2621 sec/batch\n",
      "Epoch 5/20  Iteration 821/3560 Training loss: 1.5890 1.2156 sec/batch\n",
      "Epoch 5/20  Iteration 822/3560 Training loss: 1.5889 1.2252 sec/batch\n",
      "Epoch 5/20  Iteration 823/3560 Training loss: 1.5885 1.2060 sec/batch\n",
      "Epoch 5/20  Iteration 824/3560 Training loss: 1.5882 1.2158 sec/batch\n",
      "Epoch 5/20  Iteration 825/3560 Training loss: 1.5879 1.2947 sec/batch\n",
      "Epoch 5/20  Iteration 826/3560 Training loss: 1.5876 1.2192 sec/batch\n",
      "Epoch 5/20  Iteration 827/3560 Training loss: 1.5871 1.2162 sec/batch\n",
      "Epoch 5/20  Iteration 828/3560 Training loss: 1.5865 1.2507 sec/batch\n",
      "Epoch 5/20  Iteration 829/3560 Training loss: 1.5863 1.2150 sec/batch\n",
      "Epoch 5/20  Iteration 830/3560 Training loss: 1.5860 1.2170 sec/batch\n",
      "Epoch 5/20  Iteration 831/3560 Training loss: 1.5857 1.2153 sec/batch\n",
      "Epoch 5/20  Iteration 832/3560 Training loss: 1.5855 1.2080 sec/batch\n",
      "Epoch 5/20  Iteration 833/3560 Training loss: 1.5853 1.2215 sec/batch\n",
      "Epoch 5/20  Iteration 834/3560 Training loss: 1.5848 1.2243 sec/batch\n",
      "Epoch 5/20  Iteration 835/3560 Training loss: 1.5842 1.2875 sec/batch\n",
      "Epoch 5/20  Iteration 836/3560 Training loss: 1.5841 1.2621 sec/batch\n",
      "Epoch 5/20  Iteration 837/3560 Training loss: 1.5840 1.2199 sec/batch\n",
      "Epoch 5/20  Iteration 838/3560 Training loss: 1.5834 1.2085 sec/batch\n",
      "Epoch 5/20  Iteration 839/3560 Training loss: 1.5833 1.2107 sec/batch\n",
      "Epoch 5/20  Iteration 840/3560 Training loss: 1.5832 1.2088 sec/batch\n",
      "Epoch 5/20  Iteration 841/3560 Training loss: 1.5829 1.2165 sec/batch\n",
      "Epoch 5/20  Iteration 842/3560 Training loss: 1.5824 1.2365 sec/batch\n",
      "Epoch 5/20  Iteration 843/3560 Training loss: 1.5819 1.3403 sec/batch\n",
      "Epoch 5/20  Iteration 844/3560 Training loss: 1.5816 1.2369 sec/batch\n",
      "Epoch 5/20  Iteration 845/3560 Training loss: 1.5814 1.2547 sec/batch\n",
      "Epoch 5/20  Iteration 846/3560 Training loss: 1.5813 1.2856 sec/batch\n",
      "Epoch 5/20  Iteration 847/3560 Training loss: 1.5811 1.2306 sec/batch\n",
      "Epoch 5/20  Iteration 848/3560 Training loss: 1.5809 1.2231 sec/batch\n",
      "Epoch 5/20  Iteration 849/3560 Training loss: 1.5809 1.2300 sec/batch\n",
      "Epoch 5/20  Iteration 850/3560 Training loss: 1.5808 1.2290 sec/batch\n",
      "Epoch 5/20  Iteration 851/3560 Training loss: 1.5807 1.2378 sec/batch\n",
      "Epoch 5/20  Iteration 852/3560 Training loss: 1.5804 1.2821 sec/batch\n",
      "Epoch 5/20  Iteration 853/3560 Training loss: 1.5807 1.2705 sec/batch\n",
      "Epoch 5/20  Iteration 854/3560 Training loss: 1.5804 1.2320 sec/batch\n",
      "Epoch 5/20  Iteration 855/3560 Training loss: 1.5802 1.2193 sec/batch\n",
      "Epoch 5/20  Iteration 856/3560 Training loss: 1.5801 1.2145 sec/batch\n",
      "Epoch 5/20  Iteration 857/3560 Training loss: 1.5798 1.2093 sec/batch\n",
      "Epoch 5/20  Iteration 858/3560 Training loss: 1.5797 1.2174 sec/batch\n",
      "Epoch 5/20  Iteration 859/3560 Training loss: 1.5796 1.2206 sec/batch\n",
      "Epoch 5/20  Iteration 860/3560 Training loss: 1.5796 1.2015 sec/batch\n",
      "Epoch 5/20  Iteration 861/3560 Training loss: 1.5795 1.3335 sec/batch\n",
      "Epoch 5/20  Iteration 862/3560 Training loss: 1.5792 1.2122 sec/batch\n",
      "Epoch 5/20  Iteration 863/3560 Training loss: 1.5788 1.2153 sec/batch\n",
      "Epoch 5/20  Iteration 864/3560 Training loss: 1.5785 1.2141 sec/batch\n",
      "Epoch 5/20  Iteration 865/3560 Training loss: 1.5785 1.2076 sec/batch\n",
      "Epoch 5/20  Iteration 866/3560 Training loss: 1.5783 1.2246 sec/batch\n",
      "Epoch 5/20  Iteration 867/3560 Training loss: 1.5782 1.2252 sec/batch\n",
      "Epoch 5/20  Iteration 868/3560 Training loss: 1.5780 1.2331 sec/batch\n",
      "Epoch 5/20  Iteration 869/3560 Training loss: 1.5779 1.2504 sec/batch\n",
      "Epoch 5/20  Iteration 870/3560 Training loss: 1.5777 1.2388 sec/batch\n",
      "Epoch 5/20  Iteration 871/3560 Training loss: 1.5773 1.2902 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 872/3560 Training loss: 1.5772 1.2185 sec/batch\n",
      "Epoch 5/20  Iteration 873/3560 Training loss: 1.5772 1.2146 sec/batch\n",
      "Epoch 5/20  Iteration 874/3560 Training loss: 1.5770 1.2428 sec/batch\n",
      "Epoch 5/20  Iteration 875/3560 Training loss: 1.5768 1.5177 sec/batch\n",
      "Epoch 5/20  Iteration 876/3560 Training loss: 1.5766 1.5113 sec/batch\n",
      "Epoch 5/20  Iteration 877/3560 Training loss: 1.5764 1.5896 sec/batch\n",
      "Epoch 5/20  Iteration 878/3560 Training loss: 1.5762 1.5098 sec/batch\n",
      "Epoch 5/20  Iteration 879/3560 Training loss: 1.5761 1.6910 sec/batch\n",
      "Epoch 5/20  Iteration 880/3560 Training loss: 1.5764 1.2571 sec/batch\n",
      "Epoch 5/20  Iteration 881/3560 Training loss: 1.5761 1.2248 sec/batch\n",
      "Epoch 5/20  Iteration 882/3560 Training loss: 1.5759 1.2210 sec/batch\n",
      "Epoch 5/20  Iteration 883/3560 Training loss: 1.5756 1.2059 sec/batch\n",
      "Epoch 5/20  Iteration 884/3560 Training loss: 1.5753 1.2808 sec/batch\n",
      "Epoch 5/20  Iteration 885/3560 Training loss: 1.5753 1.2126 sec/batch\n",
      "Epoch 5/20  Iteration 886/3560 Training loss: 1.5751 1.2110 sec/batch\n",
      "Epoch 5/20  Iteration 887/3560 Training loss: 1.5750 1.2179 sec/batch\n",
      "Epoch 5/20  Iteration 888/3560 Training loss: 1.5747 1.2155 sec/batch\n",
      "Epoch 5/20  Iteration 889/3560 Training loss: 1.5744 1.2986 sec/batch\n",
      "Epoch 5/20  Iteration 890/3560 Training loss: 1.5744 1.2191 sec/batch\n",
      "Epoch 6/20  Iteration 891/3560 Training loss: 1.6199 1.2187 sec/batch\n",
      "Epoch 6/20  Iteration 892/3560 Training loss: 1.5822 1.2941 sec/batch\n",
      "Epoch 6/20  Iteration 893/3560 Training loss: 1.5672 1.2124 sec/batch\n",
      "Epoch 6/20  Iteration 894/3560 Training loss: 1.5614 1.2117 sec/batch\n",
      "Epoch 6/20  Iteration 895/3560 Training loss: 1.5544 1.2102 sec/batch\n",
      "Epoch 6/20  Iteration 896/3560 Training loss: 1.5445 1.2068 sec/batch\n",
      "Epoch 6/20  Iteration 897/3560 Training loss: 1.5447 1.2142 sec/batch\n",
      "Epoch 6/20  Iteration 898/3560 Training loss: 1.5409 1.2203 sec/batch\n",
      "Epoch 6/20  Iteration 899/3560 Training loss: 1.5405 1.2758 sec/batch\n",
      "Epoch 6/20  Iteration 900/3560 Training loss: 1.5392 1.2610 sec/batch\n",
      "Epoch 6/20  Iteration 901/3560 Training loss: 1.5353 1.2101 sec/batch\n",
      "Epoch 6/20  Iteration 902/3560 Training loss: 1.5340 1.2088 sec/batch\n",
      "Epoch 6/20  Iteration 903/3560 Training loss: 1.5347 1.2202 sec/batch\n",
      "Epoch 6/20  Iteration 904/3560 Training loss: 1.5368 1.2043 sec/batch\n",
      "Epoch 6/20  Iteration 905/3560 Training loss: 1.5356 1.2110 sec/batch\n",
      "Epoch 6/20  Iteration 906/3560 Training loss: 1.5340 1.2016 sec/batch\n",
      "Epoch 6/20  Iteration 907/3560 Training loss: 1.5342 1.2286 sec/batch\n",
      "Epoch 6/20  Iteration 908/3560 Training loss: 1.5353 1.2698 sec/batch\n",
      "Epoch 6/20  Iteration 909/3560 Training loss: 1.5350 1.2450 sec/batch\n",
      "Epoch 6/20  Iteration 910/3560 Training loss: 1.5358 1.2119 sec/batch\n",
      "Epoch 6/20  Iteration 911/3560 Training loss: 1.5352 1.2065 sec/batch\n",
      "Epoch 6/20  Iteration 912/3560 Training loss: 1.5354 1.2111 sec/batch\n",
      "Epoch 6/20  Iteration 913/3560 Training loss: 1.5347 1.2122 sec/batch\n",
      "Epoch 6/20  Iteration 914/3560 Training loss: 1.5343 1.2344 sec/batch\n",
      "Epoch 6/20  Iteration 915/3560 Training loss: 1.5344 1.2625 sec/batch\n",
      "Epoch 6/20  Iteration 916/3560 Training loss: 1.5327 1.1995 sec/batch\n",
      "Epoch 6/20  Iteration 917/3560 Training loss: 1.5313 1.2487 sec/batch\n",
      "Epoch 6/20  Iteration 918/3560 Training loss: 1.5317 1.2124 sec/batch\n",
      "Epoch 6/20  Iteration 919/3560 Training loss: 1.5320 1.2119 sec/batch\n",
      "Epoch 6/20  Iteration 920/3560 Training loss: 1.5319 1.2123 sec/batch\n",
      "Epoch 6/20  Iteration 921/3560 Training loss: 1.5315 1.2107 sec/batch\n",
      "Epoch 6/20  Iteration 922/3560 Training loss: 1.5306 1.2148 sec/batch\n",
      "Epoch 6/20  Iteration 923/3560 Training loss: 1.5309 1.2243 sec/batch\n",
      "Epoch 6/20  Iteration 924/3560 Training loss: 1.5314 1.2626 sec/batch\n",
      "Epoch 6/20  Iteration 925/3560 Training loss: 1.5310 1.2494 sec/batch\n",
      "Epoch 6/20  Iteration 926/3560 Training loss: 1.5310 1.2107 sec/batch\n",
      "Epoch 6/20  Iteration 927/3560 Training loss: 1.5301 1.2107 sec/batch\n",
      "Epoch 6/20  Iteration 928/3560 Training loss: 1.5287 1.2172 sec/batch\n",
      "Epoch 6/20  Iteration 929/3560 Training loss: 1.5271 1.2054 sec/batch\n",
      "Epoch 6/20  Iteration 930/3560 Training loss: 1.5264 1.2253 sec/batch\n",
      "Epoch 6/20  Iteration 931/3560 Training loss: 1.5257 1.2043 sec/batch\n",
      "Epoch 6/20  Iteration 932/3560 Training loss: 1.5263 1.2176 sec/batch\n",
      "Epoch 6/20  Iteration 933/3560 Training loss: 1.5257 1.2788 sec/batch\n",
      "Epoch 6/20  Iteration 934/3560 Training loss: 1.5250 1.2445 sec/batch\n",
      "Epoch 6/20  Iteration 935/3560 Training loss: 1.5251 1.2347 sec/batch\n",
      "Epoch 6/20  Iteration 936/3560 Training loss: 1.5241 1.2194 sec/batch\n",
      "Epoch 6/20  Iteration 937/3560 Training loss: 1.5238 1.2032 sec/batch\n",
      "Epoch 6/20  Iteration 938/3560 Training loss: 1.5232 1.2064 sec/batch\n",
      "Epoch 6/20  Iteration 939/3560 Training loss: 1.5230 1.2734 sec/batch\n",
      "Epoch 6/20  Iteration 940/3560 Training loss: 1.5235 1.3226 sec/batch\n",
      "Epoch 6/20  Iteration 941/3560 Training loss: 1.5229 1.2429 sec/batch\n",
      "Epoch 6/20  Iteration 942/3560 Training loss: 1.5235 1.2664 sec/batch\n",
      "Epoch 6/20  Iteration 943/3560 Training loss: 1.5232 1.2300 sec/batch\n",
      "Epoch 6/20  Iteration 944/3560 Training loss: 1.5233 1.2337 sec/batch\n",
      "Epoch 6/20  Iteration 945/3560 Training loss: 1.5228 1.2123 sec/batch\n",
      "Epoch 6/20  Iteration 946/3560 Training loss: 1.5228 1.2195 sec/batch\n",
      "Epoch 6/20  Iteration 947/3560 Training loss: 1.5230 1.2027 sec/batch\n",
      "Epoch 6/20  Iteration 948/3560 Training loss: 1.5224 1.2876 sec/batch\n",
      "Epoch 6/20  Iteration 949/3560 Training loss: 1.5215 1.3904 sec/batch\n",
      "Epoch 6/20  Iteration 950/3560 Training loss: 1.5220 1.3494 sec/batch\n",
      "Epoch 6/20  Iteration 951/3560 Training loss: 1.5217 1.2342 sec/batch\n",
      "Epoch 6/20  Iteration 952/3560 Training loss: 1.5225 1.2297 sec/batch\n",
      "Epoch 6/20  Iteration 953/3560 Training loss: 1.5226 1.2210 sec/batch\n",
      "Epoch 6/20  Iteration 954/3560 Training loss: 1.5227 1.2344 sec/batch\n",
      "Epoch 6/20  Iteration 955/3560 Training loss: 1.5224 1.2084 sec/batch\n",
      "Epoch 6/20  Iteration 956/3560 Training loss: 1.5223 1.2321 sec/batch\n",
      "Epoch 6/20  Iteration 957/3560 Training loss: 1.5222 1.2277 sec/batch\n",
      "Epoch 6/20  Iteration 958/3560 Training loss: 1.5218 1.2679 sec/batch\n",
      "Epoch 6/20  Iteration 959/3560 Training loss: 1.5215 1.2690 sec/batch\n",
      "Epoch 6/20  Iteration 960/3560 Training loss: 1.5212 1.2265 sec/batch\n",
      "Epoch 6/20  Iteration 961/3560 Training loss: 1.5216 1.2124 sec/batch\n",
      "Epoch 6/20  Iteration 962/3560 Training loss: 1.5216 1.2309 sec/batch\n",
      "Epoch 6/20  Iteration 963/3560 Training loss: 1.5219 1.2081 sec/batch\n",
      "Epoch 6/20  Iteration 964/3560 Training loss: 1.5214 1.2448 sec/batch\n",
      "Epoch 6/20  Iteration 965/3560 Training loss: 1.5212 1.2205 sec/batch\n",
      "Epoch 6/20  Iteration 966/3560 Training loss: 1.5211 1.2519 sec/batch\n",
      "Epoch 6/20  Iteration 967/3560 Training loss: 1.5208 1.2658 sec/batch\n",
      "Epoch 6/20  Iteration 968/3560 Training loss: 1.5204 1.2337 sec/batch\n",
      "Epoch 6/20  Iteration 969/3560 Training loss: 1.5197 1.2165 sec/batch\n",
      "Epoch 6/20  Iteration 970/3560 Training loss: 1.5195 1.2281 sec/batch\n",
      "Epoch 6/20  Iteration 971/3560 Training loss: 1.5189 1.2195 sec/batch\n",
      "Epoch 6/20  Iteration 972/3560 Training loss: 1.5187 1.2511 sec/batch\n",
      "Epoch 6/20  Iteration 973/3560 Training loss: 1.5180 1.2396 sec/batch\n",
      "Epoch 6/20  Iteration 974/3560 Training loss: 1.5179 1.2656 sec/batch\n",
      "Epoch 6/20  Iteration 975/3560 Training loss: 1.5174 1.2811 sec/batch\n",
      "Epoch 6/20  Iteration 976/3560 Training loss: 1.5169 1.2724 sec/batch\n",
      "Epoch 6/20  Iteration 977/3560 Training loss: 1.5165 1.2374 sec/batch\n",
      "Epoch 6/20  Iteration 978/3560 Training loss: 1.5160 1.2279 sec/batch\n",
      "Epoch 6/20  Iteration 979/3560 Training loss: 1.5154 1.2208 sec/batch\n",
      "Epoch 6/20  Iteration 980/3560 Training loss: 1.5153 1.2375 sec/batch\n",
      "Epoch 6/20  Iteration 981/3560 Training loss: 1.5149 1.2300 sec/batch\n",
      "Epoch 6/20  Iteration 982/3560 Training loss: 1.5147 1.2391 sec/batch\n",
      "Epoch 6/20  Iteration 983/3560 Training loss: 1.5142 1.2334 sec/batch\n",
      "Epoch 6/20  Iteration 984/3560 Training loss: 1.5136 1.3010 sec/batch\n",
      "Epoch 6/20  Iteration 985/3560 Training loss: 1.5132 1.2291 sec/batch\n",
      "Epoch 6/20  Iteration 986/3560 Training loss: 1.5131 1.2229 sec/batch\n",
      "Epoch 6/20  Iteration 987/3560 Training loss: 1.5129 1.2661 sec/batch\n",
      "Epoch 6/20  Iteration 988/3560 Training loss: 1.5124 1.2285 sec/batch\n",
      "Epoch 6/20  Iteration 989/3560 Training loss: 1.5120 1.2317 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 990/3560 Training loss: 1.5114 1.2291 sec/batch\n",
      "Epoch 6/20  Iteration 991/3560 Training loss: 1.5113 1.2792 sec/batch\n",
      "Epoch 6/20  Iteration 992/3560 Training loss: 1.5109 1.2052 sec/batch\n",
      "Epoch 6/20  Iteration 993/3560 Training loss: 1.5106 1.2655 sec/batch\n",
      "Epoch 6/20  Iteration 994/3560 Training loss: 1.5103 1.2757 sec/batch\n",
      "Epoch 6/20  Iteration 995/3560 Training loss: 1.5100 1.2195 sec/batch\n",
      "Epoch 6/20  Iteration 996/3560 Training loss: 1.5096 1.2154 sec/batch\n",
      "Epoch 6/20  Iteration 997/3560 Training loss: 1.5094 1.2350 sec/batch\n",
      "Epoch 6/20  Iteration 998/3560 Training loss: 1.5092 1.2243 sec/batch\n",
      "Epoch 6/20  Iteration 999/3560 Training loss: 1.5089 1.2556 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3560 Training loss: 1.5088 1.2334 sec/batch\n",
      "Epoch 6/20  Iteration 1001/3560 Training loss: 1.5085 1.2154 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3560 Training loss: 1.5082 1.2079 sec/batch\n",
      "Epoch 6/20  Iteration 1003/3560 Training loss: 1.5080 1.3200 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3560 Training loss: 1.5077 1.2314 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3560 Training loss: 1.5074 1.2258 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3560 Training loss: 1.5069 1.2200 sec/batch\n",
      "Epoch 6/20  Iteration 1007/3560 Training loss: 1.5067 1.2460 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3560 Training loss: 1.5065 1.2035 sec/batch\n",
      "Epoch 6/20  Iteration 1009/3560 Training loss: 1.5063 1.2106 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3560 Training loss: 1.5061 1.2444 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3560 Training loss: 1.5059 1.3396 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3560 Training loss: 1.5055 1.2325 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3560 Training loss: 1.5050 1.2306 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3560 Training loss: 1.5049 1.2272 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3560 Training loss: 1.5048 1.2707 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3560 Training loss: 1.5043 1.2041 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3560 Training loss: 1.5043 1.2360 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3560 Training loss: 1.5044 1.2173 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3560 Training loss: 1.5041 1.4222 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3560 Training loss: 1.5038 1.6158 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3560 Training loss: 1.5033 1.5299 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3560 Training loss: 1.5030 1.9754 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3560 Training loss: 1.5029 1.8678 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3560 Training loss: 1.5028 1.4240 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3560 Training loss: 1.5027 1.5532 sec/batch\n",
      "Epoch 6/20  Iteration 1026/3560 Training loss: 1.5027 1.7305 sec/batch\n",
      "Epoch 6/20  Iteration 1027/3560 Training loss: 1.5027 1.4588 sec/batch\n",
      "Epoch 6/20  Iteration 1028/3560 Training loss: 1.5026 1.3810 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3560 Training loss: 1.5025 1.6210 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3560 Training loss: 1.5024 1.4508 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3560 Training loss: 1.5026 1.2747 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3560 Training loss: 1.5025 1.2486 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3560 Training loss: 1.5023 1.2873 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3560 Training loss: 1.5023 1.2772 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3560 Training loss: 1.5020 1.3189 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3560 Training loss: 1.5021 1.2920 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3560 Training loss: 1.5020 1.3759 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3560 Training loss: 1.5021 1.2844 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3560 Training loss: 1.5021 1.3180 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3560 Training loss: 1.5019 1.2567 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3560 Training loss: 1.5014 1.2925 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3560 Training loss: 1.5012 1.3024 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3560 Training loss: 1.5012 1.2670 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3560 Training loss: 1.5011 1.3098 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3560 Training loss: 1.5010 1.3838 sec/batch\n",
      "Epoch 6/20  Iteration 1046/3560 Training loss: 1.5008 1.2998 sec/batch\n",
      "Epoch 6/20  Iteration 1047/3560 Training loss: 1.5008 1.2672 sec/batch\n",
      "Epoch 6/20  Iteration 1048/3560 Training loss: 1.5007 1.2521 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3560 Training loss: 1.5004 1.2343 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3560 Training loss: 1.5004 1.2659 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3560 Training loss: 1.5004 1.2133 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3560 Training loss: 1.5003 1.2642 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3560 Training loss: 1.5002 1.2294 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3560 Training loss: 1.5000 1.2163 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3560 Training loss: 1.4999 1.2502 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3560 Training loss: 1.4997 1.2577 sec/batch\n",
      "Epoch 6/20  Iteration 1057/3560 Training loss: 1.4998 1.2625 sec/batch\n",
      "Epoch 6/20  Iteration 1058/3560 Training loss: 1.5001 1.2807 sec/batch\n",
      "Epoch 6/20  Iteration 1059/3560 Training loss: 1.5000 1.2584 sec/batch\n",
      "Epoch 6/20  Iteration 1060/3560 Training loss: 1.4999 1.2703 sec/batch\n",
      "Epoch 6/20  Iteration 1061/3560 Training loss: 1.4997 1.2296 sec/batch\n",
      "Epoch 6/20  Iteration 1062/3560 Training loss: 1.4994 1.2222 sec/batch\n",
      "Epoch 6/20  Iteration 1063/3560 Training loss: 1.4994 1.2701 sec/batch\n",
      "Epoch 6/20  Iteration 1064/3560 Training loss: 1.4993 1.2697 sec/batch\n",
      "Epoch 6/20  Iteration 1065/3560 Training loss: 1.4993 1.3059 sec/batch\n",
      "Epoch 6/20  Iteration 1066/3560 Training loss: 1.4990 1.2683 sec/batch\n",
      "Epoch 6/20  Iteration 1067/3560 Training loss: 1.4987 1.2446 sec/batch\n",
      "Epoch 6/20  Iteration 1068/3560 Training loss: 1.4987 1.2894 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3560 Training loss: 1.5523 1.2417 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3560 Training loss: 1.5169 1.2519 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3560 Training loss: 1.5049 1.5157 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3560 Training loss: 1.4978 1.5206 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3560 Training loss: 1.4896 1.4720 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3560 Training loss: 1.4808 1.5082 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3560 Training loss: 1.4800 1.7335 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3560 Training loss: 1.4769 1.5068 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3560 Training loss: 1.4777 1.4644 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3560 Training loss: 1.4764 1.5939 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3560 Training loss: 1.4722 1.5293 sec/batch\n",
      "Epoch 7/20  Iteration 1080/3560 Training loss: 1.4713 1.4880 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3560 Training loss: 1.4713 1.3754 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3560 Training loss: 1.4724 1.3873 sec/batch\n",
      "Epoch 7/20  Iteration 1083/3560 Training loss: 1.4707 1.3203 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3560 Training loss: 1.4688 1.3741 sec/batch\n",
      "Epoch 7/20  Iteration 1085/3560 Training loss: 1.4686 1.3195 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3560 Training loss: 1.4699 1.3764 sec/batch\n",
      "Epoch 7/20  Iteration 1087/3560 Training loss: 1.4700 1.3358 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3560 Training loss: 1.4709 1.3451 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3560 Training loss: 1.4698 1.3706 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3560 Training loss: 1.4701 1.3840 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3560 Training loss: 1.4689 1.3400 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3560 Training loss: 1.4688 1.3523 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3560 Training loss: 1.4690 1.3637 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3560 Training loss: 1.4671 1.2803 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3560 Training loss: 1.4658 1.2705 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3560 Training loss: 1.4662 1.2337 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3560 Training loss: 1.4663 1.3448 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3560 Training loss: 1.4660 1.2480 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3560 Training loss: 1.4655 1.2400 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3560 Training loss: 1.4643 1.2631 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3560 Training loss: 1.4644 1.2999 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3560 Training loss: 1.4647 1.2607 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3560 Training loss: 1.4644 1.2433 sec/batch\n",
      "Epoch 7/20  Iteration 1104/3560 Training loss: 1.4641 1.2427 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3560 Training loss: 1.4633 1.2908 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1106/3560 Training loss: 1.4619 1.2994 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3560 Training loss: 1.4605 1.2403 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3560 Training loss: 1.4599 1.2411 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3560 Training loss: 1.4593 1.2249 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3560 Training loss: 1.4600 1.2713 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3560 Training loss: 1.4595 1.2475 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3560 Training loss: 1.4589 1.2398 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3560 Training loss: 1.4592 1.2770 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3560 Training loss: 1.4582 1.3258 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3560 Training loss: 1.4576 1.2434 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3560 Training loss: 1.4572 1.2413 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3560 Training loss: 1.4571 1.2314 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3560 Training loss: 1.4576 1.2438 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3560 Training loss: 1.4571 1.2442 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3560 Training loss: 1.4577 1.2493 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3560 Training loss: 1.4576 1.2625 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3560 Training loss: 1.4578 1.3174 sec/batch\n",
      "Epoch 7/20  Iteration 1123/3560 Training loss: 1.4574 1.2358 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3560 Training loss: 1.4575 1.2323 sec/batch\n",
      "Epoch 7/20  Iteration 1125/3560 Training loss: 1.4578 1.2321 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3560 Training loss: 1.4572 1.2350 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3560 Training loss: 1.4566 1.2493 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3560 Training loss: 1.4570 1.2471 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3560 Training loss: 1.4569 1.3485 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3560 Training loss: 1.4577 1.2672 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3560 Training loss: 1.4579 1.2433 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3560 Training loss: 1.4580 1.2385 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3560 Training loss: 1.4577 1.2263 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3560 Training loss: 1.4577 1.2504 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3560 Training loss: 1.4578 1.2436 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3560 Training loss: 1.4575 1.2873 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3560 Training loss: 1.4575 1.3130 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3560 Training loss: 1.4571 1.2650 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3560 Training loss: 1.4577 1.2378 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3560 Training loss: 1.4578 1.2446 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3560 Training loss: 1.4580 1.2633 sec/batch\n",
      "Epoch 7/20  Iteration 1142/3560 Training loss: 1.4576 1.2356 sec/batch\n",
      "Epoch 7/20  Iteration 1143/3560 Training loss: 1.4574 1.2545 sec/batch\n",
      "Epoch 7/20  Iteration 1144/3560 Training loss: 1.4574 1.2407 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3560 Training loss: 1.4572 1.3010 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3560 Training loss: 1.4570 1.3185 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3560 Training loss: 1.4563 1.2481 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3560 Training loss: 1.4562 1.2395 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3560 Training loss: 1.4556 1.2788 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3560 Training loss: 1.4555 1.2509 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3560 Training loss: 1.4549 1.2570 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3560 Training loss: 1.4547 1.2431 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3560 Training loss: 1.4544 1.2743 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3560 Training loss: 1.4540 1.2681 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3560 Training loss: 1.4536 1.4486 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3560 Training loss: 1.4533 1.5566 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3560 Training loss: 1.4527 1.5735 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3560 Training loss: 1.4527 1.5900 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3560 Training loss: 1.4522 1.5063 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3560 Training loss: 1.4520 1.5673 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3560 Training loss: 1.4516 1.2839 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3560 Training loss: 1.4511 1.2779 sec/batch\n",
      "Epoch 7/20  Iteration 1163/3560 Training loss: 1.4506 1.2479 sec/batch\n",
      "Epoch 7/20  Iteration 1164/3560 Training loss: 1.4506 1.2519 sec/batch\n",
      "Epoch 7/20  Iteration 1165/3560 Training loss: 1.4504 1.2491 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3560 Training loss: 1.4499 1.2508 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3560 Training loss: 1.4495 1.2345 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3560 Training loss: 1.4489 1.2850 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3560 Training loss: 1.4488 1.2588 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3560 Training loss: 1.4485 1.2426 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3560 Training loss: 1.4482 1.3626 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3560 Training loss: 1.4481 1.2375 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3560 Training loss: 1.4479 1.2531 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3560 Training loss: 1.4476 1.2481 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3560 Training loss: 1.4474 1.2424 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3560 Training loss: 1.4473 1.2998 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3560 Training loss: 1.4471 1.2452 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3560 Training loss: 1.4470 1.2283 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3560 Training loss: 1.4468 1.2614 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3560 Training loss: 1.4466 1.2641 sec/batch\n",
      "Epoch 7/20  Iteration 1181/3560 Training loss: 1.4464 1.2622 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3560 Training loss: 1.4461 1.2620 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3560 Training loss: 1.4457 1.2244 sec/batch\n",
      "Epoch 7/20  Iteration 1184/3560 Training loss: 1.4452 1.2965 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3560 Training loss: 1.4451 1.2795 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3560 Training loss: 1.4451 1.2602 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3560 Training loss: 1.4448 1.2586 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3560 Training loss: 1.4448 1.2798 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3560 Training loss: 1.4447 1.3097 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3560 Training loss: 1.4443 1.2542 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3560 Training loss: 1.4438 1.2447 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3560 Training loss: 1.4437 1.2879 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3560 Training loss: 1.4436 1.2400 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3560 Training loss: 1.4431 1.2353 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3560 Training loss: 1.4431 1.2328 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3560 Training loss: 1.4431 1.2300 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3560 Training loss: 1.4428 1.2479 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3560 Training loss: 1.4425 1.2904 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3560 Training loss: 1.4420 1.2373 sec/batch\n",
      "Epoch 7/20  Iteration 1200/3560 Training loss: 1.4417 1.2904 sec/batch\n",
      "Epoch 7/20  Iteration 1201/3560 Training loss: 1.4417 1.2476 sec/batch\n",
      "Epoch 7/20  Iteration 1202/3560 Training loss: 1.4416 1.2478 sec/batch\n",
      "Epoch 7/20  Iteration 1203/3560 Training loss: 1.4416 1.2517 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3560 Training loss: 1.4416 1.2380 sec/batch\n",
      "Epoch 7/20  Iteration 1205/3560 Training loss: 1.4416 1.2458 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3560 Training loss: 1.4416 1.2413 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3560 Training loss: 1.4415 1.2450 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3560 Training loss: 1.4413 1.3231 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3560 Training loss: 1.4416 1.2936 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3560 Training loss: 1.4415 1.2242 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3560 Training loss: 1.4414 1.2480 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3560 Training loss: 1.4415 1.2715 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3560 Training loss: 1.4412 1.2819 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3560 Training loss: 1.4412 1.2702 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3560 Training loss: 1.4412 1.2757 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3560 Training loss: 1.4414 1.2764 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3560 Training loss: 1.4414 1.3410 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3560 Training loss: 1.4411 1.3434 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3560 Training loss: 1.4407 1.2350 sec/batch\n",
      "Epoch 7/20  Iteration 1220/3560 Training loss: 1.4406 1.2346 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3560 Training loss: 1.4406 1.2398 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1222/3560 Training loss: 1.4404 1.2459 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3560 Training loss: 1.4403 1.2302 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3560 Training loss: 1.4403 1.2938 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3560 Training loss: 1.4403 1.2390 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3560 Training loss: 1.4402 1.3043 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3560 Training loss: 1.4399 1.2302 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3560 Training loss: 1.4400 1.2558 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3560 Training loss: 1.4400 1.2348 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3560 Training loss: 1.4399 1.2536 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3560 Training loss: 1.4398 1.2222 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3560 Training loss: 1.4397 1.2827 sec/batch\n",
      "Epoch 7/20  Iteration 1233/3560 Training loss: 1.4396 1.3121 sec/batch\n",
      "Epoch 7/20  Iteration 1234/3560 Training loss: 1.4394 1.2267 sec/batch\n",
      "Epoch 7/20  Iteration 1235/3560 Training loss: 1.4395 1.2605 sec/batch\n",
      "Epoch 7/20  Iteration 1236/3560 Training loss: 1.4398 1.2402 sec/batch\n",
      "Epoch 7/20  Iteration 1237/3560 Training loss: 1.4396 1.2378 sec/batch\n",
      "Epoch 7/20  Iteration 1238/3560 Training loss: 1.4396 1.2340 sec/batch\n",
      "Epoch 7/20  Iteration 1239/3560 Training loss: 1.4394 1.2378 sec/batch\n",
      "Epoch 7/20  Iteration 1240/3560 Training loss: 1.4392 1.3103 sec/batch\n",
      "Epoch 7/20  Iteration 1241/3560 Training loss: 1.4392 1.2980 sec/batch\n",
      "Epoch 7/20  Iteration 1242/3560 Training loss: 1.4391 1.2583 sec/batch\n",
      "Epoch 7/20  Iteration 1243/3560 Training loss: 1.4390 1.2348 sec/batch\n",
      "Epoch 7/20  Iteration 1244/3560 Training loss: 1.4388 1.2450 sec/batch\n",
      "Epoch 7/20  Iteration 1245/3560 Training loss: 1.4386 1.2555 sec/batch\n",
      "Epoch 7/20  Iteration 1246/3560 Training loss: 1.4387 1.2460 sec/batch\n",
      "Epoch 8/20  Iteration 1247/3560 Training loss: 1.5116 1.2306 sec/batch\n",
      "Epoch 8/20  Iteration 1248/3560 Training loss: 1.4704 1.2754 sec/batch\n",
      "Epoch 8/20  Iteration 1249/3560 Training loss: 1.4538 1.2513 sec/batch\n",
      "Epoch 8/20  Iteration 1250/3560 Training loss: 1.4471 1.3290 sec/batch\n",
      "Epoch 8/20  Iteration 1251/3560 Training loss: 1.4371 1.2309 sec/batch\n",
      "Epoch 8/20  Iteration 1252/3560 Training loss: 1.4271 1.2441 sec/batch\n",
      "Epoch 8/20  Iteration 1253/3560 Training loss: 1.4249 1.2349 sec/batch\n",
      "Epoch 8/20  Iteration 1254/3560 Training loss: 1.4222 1.2456 sec/batch\n",
      "Epoch 8/20  Iteration 1255/3560 Training loss: 1.4221 1.2343 sec/batch\n",
      "Epoch 8/20  Iteration 1256/3560 Training loss: 1.4212 1.3004 sec/batch\n",
      "Epoch 8/20  Iteration 1257/3560 Training loss: 1.4168 1.3416 sec/batch\n",
      "Epoch 8/20  Iteration 1258/3560 Training loss: 1.4160 1.2286 sec/batch\n",
      "Epoch 8/20  Iteration 1259/3560 Training loss: 1.4157 1.2522 sec/batch\n",
      "Epoch 8/20  Iteration 1260/3560 Training loss: 1.4169 1.2459 sec/batch\n",
      "Epoch 8/20  Iteration 1261/3560 Training loss: 1.4158 1.2515 sec/batch\n",
      "Epoch 8/20  Iteration 1262/3560 Training loss: 1.4143 1.2369 sec/batch\n",
      "Epoch 8/20  Iteration 1263/3560 Training loss: 1.4145 1.2226 sec/batch\n",
      "Epoch 8/20  Iteration 1264/3560 Training loss: 1.4154 1.2497 sec/batch\n",
      "Epoch 8/20  Iteration 1265/3560 Training loss: 1.4157 1.2604 sec/batch\n",
      "Epoch 8/20  Iteration 1266/3560 Training loss: 1.4170 1.3150 sec/batch\n",
      "Epoch 8/20  Iteration 1267/3560 Training loss: 1.4163 1.2600 sec/batch\n",
      "Epoch 8/20  Iteration 1268/3560 Training loss: 1.4168 1.2584 sec/batch\n",
      "Epoch 8/20  Iteration 1269/3560 Training loss: 1.4161 1.2633 sec/batch\n",
      "Epoch 8/20  Iteration 1270/3560 Training loss: 1.4159 1.2276 sec/batch\n",
      "Epoch 8/20  Iteration 1271/3560 Training loss: 1.4156 1.2704 sec/batch\n",
      "Epoch 8/20  Iteration 1272/3560 Training loss: 1.4134 1.2662 sec/batch\n",
      "Epoch 8/20  Iteration 1273/3560 Training loss: 1.4120 1.3101 sec/batch\n",
      "Epoch 8/20  Iteration 1274/3560 Training loss: 1.4123 1.3418 sec/batch\n",
      "Epoch 8/20  Iteration 1275/3560 Training loss: 1.4124 1.2500 sec/batch\n",
      "Epoch 8/20  Iteration 1276/3560 Training loss: 1.4124 1.2695 sec/batch\n",
      "Epoch 8/20  Iteration 1277/3560 Training loss: 1.4117 1.2566 sec/batch\n",
      "Epoch 8/20  Iteration 1278/3560 Training loss: 1.4106 1.2629 sec/batch\n",
      "Epoch 8/20  Iteration 1279/3560 Training loss: 1.4106 1.2695 sec/batch\n",
      "Epoch 8/20  Iteration 1280/3560 Training loss: 1.4105 1.2471 sec/batch\n",
      "Epoch 8/20  Iteration 1281/3560 Training loss: 1.4102 1.2724 sec/batch\n",
      "Epoch 8/20  Iteration 1282/3560 Training loss: 1.4098 1.2268 sec/batch\n",
      "Epoch 8/20  Iteration 1283/3560 Training loss: 1.4089 1.2845 sec/batch\n",
      "Epoch 8/20  Iteration 1284/3560 Training loss: 1.4073 1.3074 sec/batch\n",
      "Epoch 8/20  Iteration 1285/3560 Training loss: 1.4059 1.2693 sec/batch\n",
      "Epoch 8/20  Iteration 1286/3560 Training loss: 1.4051 1.2791 sec/batch\n",
      "Epoch 8/20  Iteration 1287/3560 Training loss: 1.4044 1.2496 sec/batch\n",
      "Epoch 8/20  Iteration 1288/3560 Training loss: 1.4050 1.2878 sec/batch\n",
      "Epoch 8/20  Iteration 1289/3560 Training loss: 1.4048 1.2608 sec/batch\n",
      "Epoch 8/20  Iteration 1290/3560 Training loss: 1.4041 1.2518 sec/batch\n",
      "Epoch 8/20  Iteration 1291/3560 Training loss: 1.4042 1.2215 sec/batch\n",
      "Epoch 8/20  Iteration 1292/3560 Training loss: 1.4033 1.2568 sec/batch\n",
      "Epoch 8/20  Iteration 1293/3560 Training loss: 1.4029 1.3585 sec/batch\n",
      "Epoch 8/20  Iteration 1294/3560 Training loss: 1.4023 1.2552 sec/batch\n",
      "Epoch 8/20  Iteration 1295/3560 Training loss: 1.4019 1.2531 sec/batch\n",
      "Epoch 8/20  Iteration 1296/3560 Training loss: 1.4022 1.3812 sec/batch\n",
      "Epoch 8/20  Iteration 1297/3560 Training loss: 1.4015 1.5905 sec/batch\n",
      "Epoch 8/20  Iteration 1298/3560 Training loss: 1.4019 1.5830 sec/batch\n",
      "Epoch 8/20  Iteration 1299/3560 Training loss: 1.4016 1.5100 sec/batch\n",
      "Epoch 8/20  Iteration 1300/3560 Training loss: 1.4016 1.6146 sec/batch\n",
      "Epoch 8/20  Iteration 1301/3560 Training loss: 1.4014 1.6231 sec/batch\n",
      "Epoch 8/20  Iteration 1302/3560 Training loss: 1.4017 1.2426 sec/batch\n",
      "Epoch 8/20  Iteration 1303/3560 Training loss: 1.4020 1.2811 sec/batch\n",
      "Epoch 8/20  Iteration 1304/3560 Training loss: 1.4015 1.2292 sec/batch\n",
      "Epoch 8/20  Iteration 1305/3560 Training loss: 1.4008 1.2743 sec/batch\n",
      "Epoch 8/20  Iteration 1306/3560 Training loss: 1.4012 1.2468 sec/batch\n",
      "Epoch 8/20  Iteration 1307/3560 Training loss: 1.4010 1.2434 sec/batch\n",
      "Epoch 8/20  Iteration 1308/3560 Training loss: 1.4017 1.2194 sec/batch\n",
      "Epoch 8/20  Iteration 1309/3560 Training loss: 1.4018 1.2449 sec/batch\n",
      "Epoch 8/20  Iteration 1310/3560 Training loss: 1.4018 1.2907 sec/batch\n",
      "Epoch 8/20  Iteration 1311/3560 Training loss: 1.4016 1.2789 sec/batch\n",
      "Epoch 8/20  Iteration 1312/3560 Training loss: 1.4017 1.2318 sec/batch\n",
      "Epoch 8/20  Iteration 1313/3560 Training loss: 1.4018 1.2302 sec/batch\n",
      "Epoch 8/20  Iteration 1314/3560 Training loss: 1.4015 1.2591 sec/batch\n",
      "Epoch 8/20  Iteration 1315/3560 Training loss: 1.4014 1.2458 sec/batch\n",
      "Epoch 8/20  Iteration 1316/3560 Training loss: 1.4012 1.2307 sec/batch\n",
      "Epoch 8/20  Iteration 1317/3560 Training loss: 1.4016 1.2353 sec/batch\n",
      "Epoch 8/20  Iteration 1318/3560 Training loss: 1.4017 1.2554 sec/batch\n",
      "Epoch 8/20  Iteration 1319/3560 Training loss: 1.4019 1.3643 sec/batch\n",
      "Epoch 8/20  Iteration 1320/3560 Training loss: 1.4016 1.2286 sec/batch\n",
      "Epoch 8/20  Iteration 1321/3560 Training loss: 1.4015 1.2276 sec/batch\n",
      "Epoch 8/20  Iteration 1322/3560 Training loss: 1.4014 1.2278 sec/batch\n",
      "Epoch 8/20  Iteration 1323/3560 Training loss: 1.4012 1.2455 sec/batch\n",
      "Epoch 8/20  Iteration 1324/3560 Training loss: 1.4010 1.2332 sec/batch\n",
      "Epoch 8/20  Iteration 1325/3560 Training loss: 1.4002 1.2564 sec/batch\n",
      "Epoch 8/20  Iteration 1326/3560 Training loss: 1.4001 1.2980 sec/batch\n",
      "Epoch 8/20  Iteration 1327/3560 Training loss: 1.3996 1.3743 sec/batch\n",
      "Epoch 8/20  Iteration 1328/3560 Training loss: 1.3994 1.3973 sec/batch\n",
      "Epoch 8/20  Iteration 1329/3560 Training loss: 1.3987 1.2803 sec/batch\n",
      "Epoch 8/20  Iteration 1330/3560 Training loss: 1.3986 1.2742 sec/batch\n",
      "Epoch 8/20  Iteration 1331/3560 Training loss: 1.3981 1.2361 sec/batch\n",
      "Epoch 8/20  Iteration 1332/3560 Training loss: 1.3977 1.2391 sec/batch\n",
      "Epoch 8/20  Iteration 1333/3560 Training loss: 1.3972 1.2451 sec/batch\n",
      "Epoch 8/20  Iteration 1334/3560 Training loss: 1.3968 1.2257 sec/batch\n",
      "Epoch 8/20  Iteration 1335/3560 Training loss: 1.3962 1.2764 sec/batch\n",
      "Epoch 8/20  Iteration 1336/3560 Training loss: 1.3962 1.2374 sec/batch\n",
      "Epoch 8/20  Iteration 1337/3560 Training loss: 1.3958 1.2594 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1338/3560 Training loss: 1.3956 1.3176 sec/batch\n",
      "Epoch 8/20  Iteration 1339/3560 Training loss: 1.3951 1.2390 sec/batch\n",
      "Epoch 8/20  Iteration 1340/3560 Training loss: 1.3945 1.2350 sec/batch\n",
      "Epoch 8/20  Iteration 1341/3560 Training loss: 1.3942 1.2368 sec/batch\n",
      "Epoch 8/20  Iteration 1342/3560 Training loss: 1.3942 1.2246 sec/batch\n",
      "Epoch 8/20  Iteration 1343/3560 Training loss: 1.3940 1.2803 sec/batch\n",
      "Epoch 8/20  Iteration 1344/3560 Training loss: 1.3936 1.2423 sec/batch\n",
      "Epoch 8/20  Iteration 1345/3560 Training loss: 1.3931 1.3230 sec/batch\n",
      "Epoch 8/20  Iteration 1346/3560 Training loss: 1.3926 1.3816 sec/batch\n",
      "Epoch 8/20  Iteration 1347/3560 Training loss: 1.3923 1.3298 sec/batch\n",
      "Epoch 8/20  Iteration 1348/3560 Training loss: 1.3918 1.2624 sec/batch\n",
      "Epoch 8/20  Iteration 1349/3560 Training loss: 1.3916 1.3245 sec/batch\n",
      "Epoch 8/20  Iteration 1350/3560 Training loss: 1.3913 1.2809 sec/batch\n",
      "Epoch 8/20  Iteration 1351/3560 Training loss: 1.3910 1.3043 sec/batch\n",
      "Epoch 8/20  Iteration 1352/3560 Training loss: 1.3908 1.2660 sec/batch\n",
      "Epoch 8/20  Iteration 1353/3560 Training loss: 1.3906 1.2864 sec/batch\n",
      "Epoch 8/20  Iteration 1354/3560 Training loss: 1.3906 1.2743 sec/batch\n",
      "Epoch 8/20  Iteration 1355/3560 Training loss: 1.3903 1.2980 sec/batch\n",
      "Epoch 8/20  Iteration 1356/3560 Training loss: 1.3903 1.3768 sec/batch\n",
      "Epoch 8/20  Iteration 1357/3560 Training loss: 1.3900 1.2696 sec/batch\n",
      "Epoch 8/20  Iteration 1358/3560 Training loss: 1.3900 1.3555 sec/batch\n",
      "Epoch 8/20  Iteration 1359/3560 Training loss: 1.3897 1.3193 sec/batch\n",
      "Epoch 8/20  Iteration 1360/3560 Training loss: 1.3895 1.2561 sec/batch\n",
      "Epoch 8/20  Iteration 1361/3560 Training loss: 1.3891 1.2765 sec/batch\n",
      "Epoch 8/20  Iteration 1362/3560 Training loss: 1.3886 1.2420 sec/batch\n",
      "Epoch 8/20  Iteration 1363/3560 Training loss: 1.3885 1.2385 sec/batch\n",
      "Epoch 8/20  Iteration 1364/3560 Training loss: 1.3884 1.2433 sec/batch\n",
      "Epoch 8/20  Iteration 1365/3560 Training loss: 1.3882 1.3969 sec/batch\n",
      "Epoch 8/20  Iteration 1366/3560 Training loss: 1.3880 1.4157 sec/batch\n",
      "Epoch 8/20  Iteration 1367/3560 Training loss: 1.3878 1.2759 sec/batch\n",
      "Epoch 8/20  Iteration 1368/3560 Training loss: 1.3873 1.2715 sec/batch\n",
      "Epoch 8/20  Iteration 1369/3560 Training loss: 1.3868 1.2533 sec/batch\n",
      "Epoch 8/20  Iteration 1370/3560 Training loss: 1.3867 1.2360 sec/batch\n",
      "Epoch 8/20  Iteration 1371/3560 Training loss: 1.3865 1.2378 sec/batch\n",
      "Epoch 8/20  Iteration 1372/3560 Training loss: 1.3861 1.2366 sec/batch\n",
      "Epoch 8/20  Iteration 1373/3560 Training loss: 1.3860 1.2474 sec/batch\n",
      "Epoch 8/20  Iteration 1374/3560 Training loss: 1.3859 1.3133 sec/batch\n",
      "Epoch 8/20  Iteration 1375/3560 Training loss: 1.3856 1.2910 sec/batch\n",
      "Epoch 8/20  Iteration 1376/3560 Training loss: 1.3851 1.2456 sec/batch\n",
      "Epoch 8/20  Iteration 1377/3560 Training loss: 1.3846 1.2696 sec/batch\n",
      "Epoch 8/20  Iteration 1378/3560 Training loss: 1.3843 1.2267 sec/batch\n",
      "Epoch 8/20  Iteration 1379/3560 Training loss: 1.3842 1.2372 sec/batch\n",
      "Epoch 8/20  Iteration 1380/3560 Training loss: 1.3841 1.2935 sec/batch\n",
      "Epoch 8/20  Iteration 1381/3560 Training loss: 1.3840 1.3108 sec/batch\n",
      "Epoch 8/20  Iteration 1382/3560 Training loss: 1.3840 1.2721 sec/batch\n",
      "Epoch 8/20  Iteration 1383/3560 Training loss: 1.3840 1.2412 sec/batch\n",
      "Epoch 8/20  Iteration 1384/3560 Training loss: 1.3840 1.2364 sec/batch\n",
      "Epoch 8/20  Iteration 1385/3560 Training loss: 1.3839 1.2551 sec/batch\n",
      "Epoch 8/20  Iteration 1386/3560 Training loss: 1.3838 1.2420 sec/batch\n",
      "Epoch 8/20  Iteration 1387/3560 Training loss: 1.3841 1.2489 sec/batch\n",
      "Epoch 8/20  Iteration 1388/3560 Training loss: 1.3841 1.2423 sec/batch\n",
      "Epoch 8/20  Iteration 1389/3560 Training loss: 1.3839 1.2368 sec/batch\n",
      "Epoch 8/20  Iteration 1390/3560 Training loss: 1.3840 1.3615 sec/batch\n",
      "Epoch 8/20  Iteration 1391/3560 Training loss: 1.3838 1.2882 sec/batch\n",
      "Epoch 8/20  Iteration 1392/3560 Training loss: 1.3838 1.2414 sec/batch\n",
      "Epoch 8/20  Iteration 1393/3560 Training loss: 1.3838 1.2615 sec/batch\n",
      "Epoch 8/20  Iteration 1394/3560 Training loss: 1.3839 1.2358 sec/batch\n",
      "Epoch 8/20  Iteration 1395/3560 Training loss: 1.3840 1.2258 sec/batch\n",
      "Epoch 8/20  Iteration 1396/3560 Training loss: 1.3838 1.2512 sec/batch\n",
      "Epoch 8/20  Iteration 1397/3560 Training loss: 1.3834 1.2534 sec/batch\n",
      "Epoch 8/20  Iteration 1398/3560 Training loss: 1.3832 1.2675 sec/batch\n",
      "Epoch 8/20  Iteration 1399/3560 Training loss: 1.3832 1.2909 sec/batch\n",
      "Epoch 8/20  Iteration 1400/3560 Training loss: 1.3831 1.2977 sec/batch\n",
      "Epoch 8/20  Iteration 1401/3560 Training loss: 1.3830 1.2601 sec/batch\n",
      "Epoch 8/20  Iteration 1402/3560 Training loss: 1.3829 1.2668 sec/batch\n",
      "Epoch 8/20  Iteration 1403/3560 Training loss: 1.3829 1.2470 sec/batch\n",
      "Epoch 8/20  Iteration 1404/3560 Training loss: 1.3827 1.2872 sec/batch\n",
      "Epoch 8/20  Iteration 1405/3560 Training loss: 1.3824 1.2495 sec/batch\n",
      "Epoch 8/20  Iteration 1406/3560 Training loss: 1.3825 1.2836 sec/batch\n",
      "Epoch 8/20  Iteration 1407/3560 Training loss: 1.3825 1.2341 sec/batch\n",
      "Epoch 8/20  Iteration 1408/3560 Training loss: 1.3824 1.2263 sec/batch\n",
      "Epoch 8/20  Iteration 1409/3560 Training loss: 1.3823 1.2854 sec/batch\n",
      "Epoch 8/20  Iteration 1410/3560 Training loss: 1.3822 1.3132 sec/batch\n",
      "Epoch 8/20  Iteration 1411/3560 Training loss: 1.3821 1.2502 sec/batch\n",
      "Epoch 8/20  Iteration 1412/3560 Training loss: 1.3820 1.2651 sec/batch\n",
      "Epoch 8/20  Iteration 1413/3560 Training loss: 1.3821 1.2647 sec/batch\n",
      "Epoch 8/20  Iteration 1414/3560 Training loss: 1.3824 1.3002 sec/batch\n",
      "Epoch 8/20  Iteration 1415/3560 Training loss: 1.3823 1.2484 sec/batch\n",
      "Epoch 8/20  Iteration 1416/3560 Training loss: 1.3822 1.2242 sec/batch\n",
      "Epoch 8/20  Iteration 1417/3560 Training loss: 1.3820 1.2430 sec/batch\n",
      "Epoch 8/20  Iteration 1418/3560 Training loss: 1.3817 1.2289 sec/batch\n",
      "Epoch 8/20  Iteration 1419/3560 Training loss: 1.3818 1.3228 sec/batch\n",
      "Epoch 8/20  Iteration 1420/3560 Training loss: 1.3817 1.2635 sec/batch\n",
      "Epoch 8/20  Iteration 1421/3560 Training loss: 1.3816 1.2493 sec/batch\n",
      "Epoch 8/20  Iteration 1422/3560 Training loss: 1.3814 1.3116 sec/batch\n",
      "Epoch 8/20  Iteration 1423/3560 Training loss: 1.3812 1.2787 sec/batch\n",
      "Epoch 8/20  Iteration 1424/3560 Training loss: 1.3812 1.2505 sec/batch\n",
      "Epoch 9/20  Iteration 1425/3560 Training loss: 1.5270 1.2580 sec/batch\n",
      "Epoch 9/20  Iteration 1426/3560 Training loss: 1.4588 1.2467 sec/batch\n",
      "Epoch 9/20  Iteration 1427/3560 Training loss: 1.4350 1.2225 sec/batch\n",
      "Epoch 9/20  Iteration 1428/3560 Training loss: 1.4266 1.2988 sec/batch\n",
      "Epoch 9/20  Iteration 1429/3560 Training loss: 1.4109 1.3244 sec/batch\n",
      "Epoch 9/20  Iteration 1430/3560 Training loss: 1.3986 1.3180 sec/batch\n",
      "Epoch 9/20  Iteration 1431/3560 Training loss: 1.3960 1.2772 sec/batch\n",
      "Epoch 9/20  Iteration 1432/3560 Training loss: 1.3918 1.2503 sec/batch\n",
      "Epoch 9/20  Iteration 1433/3560 Training loss: 1.3888 1.2376 sec/batch\n",
      "Epoch 9/20  Iteration 1434/3560 Training loss: 1.3851 1.2427 sec/batch\n",
      "Epoch 9/20  Iteration 1435/3560 Training loss: 1.3797 1.2597 sec/batch\n",
      "Epoch 9/20  Iteration 1436/3560 Training loss: 1.3787 1.3371 sec/batch\n",
      "Epoch 9/20  Iteration 1437/3560 Training loss: 1.3785 1.6172 sec/batch\n",
      "Epoch 9/20  Iteration 1438/3560 Training loss: 1.3790 1.6263 sec/batch\n",
      "Epoch 9/20  Iteration 1439/3560 Training loss: 1.3767 1.5723 sec/batch\n",
      "Epoch 9/20  Iteration 1440/3560 Training loss: 1.3748 1.6235 sec/batch\n",
      "Epoch 9/20  Iteration 1441/3560 Training loss: 1.3742 1.5164 sec/batch\n",
      "Epoch 9/20  Iteration 1442/3560 Training loss: 1.3745 1.3161 sec/batch\n",
      "Epoch 9/20  Iteration 1443/3560 Training loss: 1.3741 1.2739 sec/batch\n",
      "Epoch 9/20  Iteration 1444/3560 Training loss: 1.3752 1.3098 sec/batch\n",
      "Epoch 9/20  Iteration 1445/3560 Training loss: 1.3738 1.2789 sec/batch\n",
      "Epoch 9/20  Iteration 1446/3560 Training loss: 1.3738 1.2581 sec/batch\n",
      "Epoch 9/20  Iteration 1447/3560 Training loss: 1.3726 1.2596 sec/batch\n",
      "Epoch 9/20  Iteration 1448/3560 Training loss: 1.3719 1.2360 sec/batch\n",
      "Epoch 9/20  Iteration 1449/3560 Training loss: 1.3714 1.2343 sec/batch\n",
      "Epoch 9/20  Iteration 1450/3560 Training loss: 1.3691 1.2528 sec/batch\n",
      "Epoch 9/20  Iteration 1451/3560 Training loss: 1.3676 1.2586 sec/batch\n",
      "Epoch 9/20  Iteration 1452/3560 Training loss: 1.3679 1.2386 sec/batch\n",
      "Epoch 9/20  Iteration 1453/3560 Training loss: 1.3676 1.4171 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1454/3560 Training loss: 1.3676 1.2844 sec/batch\n",
      "Epoch 9/20  Iteration 1455/3560 Training loss: 1.3668 1.2346 sec/batch\n",
      "Epoch 9/20  Iteration 1456/3560 Training loss: 1.3654 1.2349 sec/batch\n",
      "Epoch 9/20  Iteration 1457/3560 Training loss: 1.3653 1.2304 sec/batch\n",
      "Epoch 9/20  Iteration 1458/3560 Training loss: 1.3651 1.2306 sec/batch\n",
      "Epoch 9/20  Iteration 1459/3560 Training loss: 1.3647 1.2532 sec/batch\n",
      "Epoch 9/20  Iteration 1460/3560 Training loss: 1.3641 1.2359 sec/batch\n",
      "Epoch 9/20  Iteration 1461/3560 Training loss: 1.3630 1.3182 sec/batch\n",
      "Epoch 9/20  Iteration 1462/3560 Training loss: 1.3616 1.2735 sec/batch\n",
      "Epoch 9/20  Iteration 1463/3560 Training loss: 1.3601 1.3244 sec/batch\n",
      "Epoch 9/20  Iteration 1464/3560 Training loss: 1.3595 1.2716 sec/batch\n",
      "Epoch 9/20  Iteration 1465/3560 Training loss: 1.3588 1.2523 sec/batch\n",
      "Epoch 9/20  Iteration 1466/3560 Training loss: 1.3595 1.2594 sec/batch\n",
      "Epoch 9/20  Iteration 1467/3560 Training loss: 1.3592 1.2588 sec/batch\n",
      "Epoch 9/20  Iteration 1468/3560 Training loss: 1.3581 1.2378 sec/batch\n",
      "Epoch 9/20  Iteration 1469/3560 Training loss: 1.3581 1.3026 sec/batch\n",
      "Epoch 9/20  Iteration 1470/3560 Training loss: 1.3573 1.2805 sec/batch\n",
      "Epoch 9/20  Iteration 1471/3560 Training loss: 1.3568 1.2421 sec/batch\n",
      "Epoch 9/20  Iteration 1472/3560 Training loss: 1.3563 1.3441 sec/batch\n",
      "Epoch 9/20  Iteration 1473/3560 Training loss: 1.3561 1.2605 sec/batch\n",
      "Epoch 9/20  Iteration 1474/3560 Training loss: 1.3563 1.2405 sec/batch\n",
      "Epoch 9/20  Iteration 1475/3560 Training loss: 1.3555 1.2533 sec/batch\n",
      "Epoch 9/20  Iteration 1476/3560 Training loss: 1.3561 1.2399 sec/batch\n",
      "Epoch 9/20  Iteration 1477/3560 Training loss: 1.3557 1.2951 sec/batch\n",
      "Epoch 9/20  Iteration 1478/3560 Training loss: 1.3559 1.2485 sec/batch\n",
      "Epoch 9/20  Iteration 1479/3560 Training loss: 1.3555 1.2649 sec/batch\n",
      "Epoch 9/20  Iteration 1480/3560 Training loss: 1.3555 1.2532 sec/batch\n",
      "Epoch 9/20  Iteration 1481/3560 Training loss: 1.3558 1.3142 sec/batch\n",
      "Epoch 9/20  Iteration 1482/3560 Training loss: 1.3553 1.3258 sec/batch\n",
      "Epoch 9/20  Iteration 1483/3560 Training loss: 1.3546 1.2751 sec/batch\n",
      "Epoch 9/20  Iteration 1484/3560 Training loss: 1.3551 1.2597 sec/batch\n",
      "Epoch 9/20  Iteration 1485/3560 Training loss: 1.3549 1.2686 sec/batch\n",
      "Epoch 9/20  Iteration 1486/3560 Training loss: 1.3556 1.2520 sec/batch\n",
      "Epoch 9/20  Iteration 1487/3560 Training loss: 1.3557 1.2247 sec/batch\n",
      "Epoch 9/20  Iteration 1488/3560 Training loss: 1.3556 1.2443 sec/batch\n",
      "Epoch 9/20  Iteration 1489/3560 Training loss: 1.3554 1.2357 sec/batch\n",
      "Epoch 9/20  Iteration 1490/3560 Training loss: 1.3555 1.2516 sec/batch\n",
      "Epoch 9/20  Iteration 1491/3560 Training loss: 1.3555 1.3519 sec/batch\n",
      "Epoch 9/20  Iteration 1492/3560 Training loss: 1.3551 1.2571 sec/batch\n",
      "Epoch 9/20  Iteration 1493/3560 Training loss: 1.3550 1.3140 sec/batch\n",
      "Epoch 9/20  Iteration 1494/3560 Training loss: 1.3548 1.2598 sec/batch\n",
      "Epoch 9/20  Iteration 1495/3560 Training loss: 1.3552 1.2539 sec/batch\n",
      "Epoch 9/20  Iteration 1496/3560 Training loss: 1.3553 1.2734 sec/batch\n",
      "Epoch 9/20  Iteration 1497/3560 Training loss: 1.3556 1.3335 sec/batch\n",
      "Epoch 9/20  Iteration 1498/3560 Training loss: 1.3552 1.2409 sec/batch\n",
      "Epoch 9/20  Iteration 1499/3560 Training loss: 1.3551 1.3361 sec/batch\n",
      "Epoch 9/20  Iteration 1500/3560 Training loss: 1.3550 1.3149 sec/batch\n",
      "Epoch 9/20  Iteration 1501/3560 Training loss: 1.3549 1.2775 sec/batch\n",
      "Epoch 9/20  Iteration 1502/3560 Training loss: 1.3545 1.2583 sec/batch\n",
      "Epoch 9/20  Iteration 1503/3560 Training loss: 1.3537 1.2438 sec/batch\n",
      "Epoch 9/20  Iteration 1504/3560 Training loss: 1.3535 1.2973 sec/batch\n",
      "Epoch 9/20  Iteration 1505/3560 Training loss: 1.3529 1.2560 sec/batch\n",
      "Epoch 9/20  Iteration 1506/3560 Training loss: 1.3528 1.2566 sec/batch\n",
      "Epoch 9/20  Iteration 1507/3560 Training loss: 1.3522 1.3636 sec/batch\n",
      "Epoch 9/20  Iteration 1508/3560 Training loss: 1.3519 1.3567 sec/batch\n",
      "Epoch 9/20  Iteration 1509/3560 Training loss: 1.3516 1.3192 sec/batch\n",
      "Epoch 9/20  Iteration 1510/3560 Training loss: 1.3513 1.2616 sec/batch\n",
      "Epoch 9/20  Iteration 1511/3560 Training loss: 1.3509 1.2554 sec/batch\n",
      "Epoch 9/20  Iteration 1512/3560 Training loss: 1.3505 1.2926 sec/batch\n",
      "Epoch 9/20  Iteration 1513/3560 Training loss: 1.3500 1.3187 sec/batch\n",
      "Epoch 9/20  Iteration 1514/3560 Training loss: 1.3500 1.2573 sec/batch\n",
      "Epoch 9/20  Iteration 1515/3560 Training loss: 1.3496 1.2860 sec/batch\n",
      "Epoch 9/20  Iteration 1516/3560 Training loss: 1.3494 1.3104 sec/batch\n",
      "Epoch 9/20  Iteration 1517/3560 Training loss: 1.3489 1.2409 sec/batch\n",
      "Epoch 9/20  Iteration 1518/3560 Training loss: 1.3485 1.2527 sec/batch\n",
      "Epoch 9/20  Iteration 1519/3560 Training loss: 1.3481 1.2379 sec/batch\n",
      "Epoch 9/20  Iteration 1520/3560 Training loss: 1.3482 1.2538 sec/batch\n",
      "Epoch 9/20  Iteration 1521/3560 Training loss: 1.3480 1.2901 sec/batch\n",
      "Epoch 9/20  Iteration 1522/3560 Training loss: 1.3476 1.2713 sec/batch\n",
      "Epoch 9/20  Iteration 1523/3560 Training loss: 1.3471 1.2574 sec/batch\n",
      "Epoch 9/20  Iteration 1524/3560 Training loss: 1.3466 1.2970 sec/batch\n",
      "Epoch 9/20  Iteration 1525/3560 Training loss: 1.3464 1.2392 sec/batch\n",
      "Epoch 9/20  Iteration 1526/3560 Training loss: 1.3461 1.2383 sec/batch\n",
      "Epoch 9/20  Iteration 1527/3560 Training loss: 1.3458 1.2353 sec/batch\n",
      "Epoch 9/20  Iteration 1528/3560 Training loss: 1.3457 1.2224 sec/batch\n",
      "Epoch 9/20  Iteration 1529/3560 Training loss: 1.3454 1.2372 sec/batch\n",
      "Epoch 9/20  Iteration 1530/3560 Training loss: 1.3452 1.2879 sec/batch\n",
      "Epoch 9/20  Iteration 1531/3560 Training loss: 1.3450 1.2350 sec/batch\n",
      "Epoch 9/20  Iteration 1532/3560 Training loss: 1.3450 1.2924 sec/batch\n",
      "Epoch 9/20  Iteration 1533/3560 Training loss: 1.3447 1.2438 sec/batch\n",
      "Epoch 9/20  Iteration 1534/3560 Training loss: 1.3446 1.2394 sec/batch\n",
      "Epoch 9/20  Iteration 1535/3560 Training loss: 1.3443 1.2368 sec/batch\n",
      "Epoch 9/20  Iteration 1536/3560 Training loss: 1.3441 1.2575 sec/batch\n",
      "Epoch 9/20  Iteration 1537/3560 Training loss: 1.3438 1.2429 sec/batch\n",
      "Epoch 9/20  Iteration 1538/3560 Training loss: 1.3436 1.2555 sec/batch\n",
      "Epoch 9/20  Iteration 1539/3560 Training loss: 1.3434 1.2502 sec/batch\n",
      "Epoch 9/20  Iteration 1540/3560 Training loss: 1.3429 1.3356 sec/batch\n",
      "Epoch 9/20  Iteration 1541/3560 Training loss: 1.3428 1.2617 sec/batch\n",
      "Epoch 9/20  Iteration 1542/3560 Training loss: 1.3427 1.2493 sec/batch\n",
      "Epoch 9/20  Iteration 1543/3560 Training loss: 1.3425 1.2549 sec/batch\n",
      "Epoch 9/20  Iteration 1544/3560 Training loss: 1.3423 1.2392 sec/batch\n",
      "Epoch 9/20  Iteration 1545/3560 Training loss: 1.3422 1.2460 sec/batch\n",
      "Epoch 9/20  Iteration 1546/3560 Training loss: 1.3418 1.2511 sec/batch\n",
      "Epoch 9/20  Iteration 1547/3560 Training loss: 1.3412 1.2466 sec/batch\n",
      "Epoch 9/20  Iteration 1548/3560 Training loss: 1.3411 1.2874 sec/batch\n",
      "Epoch 9/20  Iteration 1549/3560 Training loss: 1.3409 1.3013 sec/batch\n",
      "Epoch 9/20  Iteration 1550/3560 Training loss: 1.3405 1.2714 sec/batch\n",
      "Epoch 9/20  Iteration 1551/3560 Training loss: 1.3404 1.2479 sec/batch\n",
      "Epoch 9/20  Iteration 1552/3560 Training loss: 1.3404 1.2489 sec/batch\n",
      "Epoch 9/20  Iteration 1553/3560 Training loss: 1.3402 1.2622 sec/batch\n",
      "Epoch 9/20  Iteration 1554/3560 Training loss: 1.3397 1.2528 sec/batch\n",
      "Epoch 9/20  Iteration 1555/3560 Training loss: 1.3393 1.2474 sec/batch\n",
      "Epoch 9/20  Iteration 1556/3560 Training loss: 1.3390 1.2838 sec/batch\n",
      "Epoch 9/20  Iteration 1557/3560 Training loss: 1.3390 1.3021 sec/batch\n",
      "Epoch 9/20  Iteration 1558/3560 Training loss: 1.3390 1.2380 sec/batch\n",
      "Epoch 9/20  Iteration 1559/3560 Training loss: 1.3389 1.2309 sec/batch\n",
      "Epoch 9/20  Iteration 1560/3560 Training loss: 1.3389 1.2525 sec/batch\n",
      "Epoch 9/20  Iteration 1561/3560 Training loss: 1.3390 1.2421 sec/batch\n",
      "Epoch 9/20  Iteration 1562/3560 Training loss: 1.3390 1.2790 sec/batch\n",
      "Epoch 9/20  Iteration 1563/3560 Training loss: 1.3389 1.2265 sec/batch\n",
      "Epoch 9/20  Iteration 1564/3560 Training loss: 1.3389 1.2919 sec/batch\n",
      "Epoch 9/20  Iteration 1565/3560 Training loss: 1.3392 1.2420 sec/batch\n",
      "Epoch 9/20  Iteration 1566/3560 Training loss: 1.3392 1.2352 sec/batch\n",
      "Epoch 9/20  Iteration 1567/3560 Training loss: 1.3390 1.2336 sec/batch\n",
      "Epoch 9/20  Iteration 1568/3560 Training loss: 1.3391 1.2433 sec/batch\n",
      "Epoch 9/20  Iteration 1569/3560 Training loss: 1.3390 1.2629 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1570/3560 Training loss: 1.3391 1.2502 sec/batch\n",
      "Epoch 9/20  Iteration 1571/3560 Training loss: 1.3391 1.2449 sec/batch\n",
      "Epoch 9/20  Iteration 1572/3560 Training loss: 1.3393 1.2653 sec/batch\n",
      "Epoch 9/20  Iteration 1573/3560 Training loss: 1.3394 1.2929 sec/batch\n",
      "Epoch 9/20  Iteration 1574/3560 Training loss: 1.3392 1.2299 sec/batch\n",
      "Epoch 9/20  Iteration 1575/3560 Training loss: 1.3388 1.2366 sec/batch\n",
      "Epoch 9/20  Iteration 1576/3560 Training loss: 1.3386 1.2258 sec/batch\n",
      "Epoch 9/20  Iteration 1577/3560 Training loss: 1.3387 1.3405 sec/batch\n",
      "Epoch 9/20  Iteration 1578/3560 Training loss: 1.3386 1.5998 sec/batch\n",
      "Epoch 9/20  Iteration 1579/3560 Training loss: 1.3386 1.5856 sec/batch\n",
      "Epoch 9/20  Iteration 1580/3560 Training loss: 1.3385 1.5852 sec/batch\n",
      "Epoch 9/20  Iteration 1581/3560 Training loss: 1.3386 1.6586 sec/batch\n",
      "Epoch 9/20  Iteration 1582/3560 Training loss: 1.3384 1.5013 sec/batch\n",
      "Epoch 9/20  Iteration 1583/3560 Training loss: 1.3381 1.2278 sec/batch\n",
      "Epoch 9/20  Iteration 1584/3560 Training loss: 1.3382 1.2539 sec/batch\n",
      "Epoch 9/20  Iteration 1585/3560 Training loss: 1.3382 1.2475 sec/batch\n",
      "Epoch 9/20  Iteration 1586/3560 Training loss: 1.3381 1.2367 sec/batch\n",
      "Epoch 9/20  Iteration 1587/3560 Training loss: 1.3379 1.2872 sec/batch\n",
      "Epoch 9/20  Iteration 1588/3560 Training loss: 1.3379 1.2636 sec/batch\n",
      "Epoch 9/20  Iteration 1589/3560 Training loss: 1.3378 1.2350 sec/batch\n",
      "Epoch 9/20  Iteration 1590/3560 Training loss: 1.3377 1.2357 sec/batch\n",
      "Epoch 9/20  Iteration 1591/3560 Training loss: 1.3378 1.2318 sec/batch\n",
      "Epoch 9/20  Iteration 1592/3560 Training loss: 1.3381 1.2401 sec/batch\n",
      "Epoch 9/20  Iteration 1593/3560 Training loss: 1.3381 1.2474 sec/batch\n",
      "Epoch 9/20  Iteration 1594/3560 Training loss: 1.3381 1.2313 sec/batch\n",
      "Epoch 9/20  Iteration 1595/3560 Training loss: 1.3380 1.3502 sec/batch\n",
      "Epoch 9/20  Iteration 1596/3560 Training loss: 1.3378 1.2850 sec/batch\n",
      "Epoch 9/20  Iteration 1597/3560 Training loss: 1.3378 1.2578 sec/batch\n",
      "Epoch 9/20  Iteration 1598/3560 Training loss: 1.3377 1.2417 sec/batch\n",
      "Epoch 9/20  Iteration 1599/3560 Training loss: 1.3377 1.2396 sec/batch\n",
      "Epoch 9/20  Iteration 1600/3560 Training loss: 1.3375 1.2385 sec/batch\n",
      "Epoch 9/20  Iteration 1601/3560 Training loss: 1.3372 1.2281 sec/batch\n",
      "Epoch 9/20  Iteration 1602/3560 Training loss: 1.3374 1.2372 sec/batch\n",
      "Epoch 10/20  Iteration 1603/3560 Training loss: 1.4915 1.3153 sec/batch\n",
      "Epoch 10/20  Iteration 1604/3560 Training loss: 1.4292 1.2612 sec/batch\n",
      "Epoch 10/20  Iteration 1605/3560 Training loss: 1.4129 1.2209 sec/batch\n",
      "Epoch 10/20  Iteration 1606/3560 Training loss: 1.4067 1.2459 sec/batch\n",
      "Epoch 10/20  Iteration 1607/3560 Training loss: 1.3942 1.2758 sec/batch\n",
      "Epoch 10/20  Iteration 1608/3560 Training loss: 1.3808 1.2421 sec/batch\n",
      "Epoch 10/20  Iteration 1609/3560 Training loss: 1.3772 1.2314 sec/batch\n",
      "Epoch 10/20  Iteration 1610/3560 Training loss: 1.3729 1.2387 sec/batch\n",
      "Epoch 10/20  Iteration 1611/3560 Training loss: 1.3714 1.2802 sec/batch\n",
      "Epoch 10/20  Iteration 1612/3560 Training loss: 1.3692 1.2629 sec/batch\n",
      "Epoch 10/20  Iteration 1613/3560 Training loss: 1.3648 1.2255 sec/batch\n",
      "Epoch 10/20  Iteration 1614/3560 Training loss: 1.3637 1.2268 sec/batch\n",
      "Epoch 10/20  Iteration 1615/3560 Training loss: 1.3624 1.2244 sec/batch\n",
      "Epoch 10/20  Iteration 1616/3560 Training loss: 1.3626 1.2521 sec/batch\n",
      "Epoch 10/20  Iteration 1617/3560 Training loss: 1.3603 1.2364 sec/batch\n",
      "Epoch 10/20  Iteration 1618/3560 Training loss: 1.3580 1.2351 sec/batch\n",
      "Epoch 10/20  Iteration 1619/3560 Training loss: 1.3577 1.2539 sec/batch\n",
      "Epoch 10/20  Iteration 1620/3560 Training loss: 1.3583 1.2805 sec/batch\n",
      "Epoch 10/20  Iteration 1621/3560 Training loss: 1.3582 1.2323 sec/batch\n",
      "Epoch 10/20  Iteration 1622/3560 Training loss: 1.3587 1.2366 sec/batch\n",
      "Epoch 10/20  Iteration 1623/3560 Training loss: 1.3574 1.3549 sec/batch\n",
      "Epoch 10/20  Iteration 1624/3560 Training loss: 1.3575 1.2580 sec/batch\n",
      "Epoch 10/20  Iteration 1625/3560 Training loss: 1.3567 1.2780 sec/batch\n",
      "Epoch 10/20  Iteration 1626/3560 Training loss: 1.3566 1.2515 sec/batch\n",
      "Epoch 10/20  Iteration 1627/3560 Training loss: 1.3561 1.2894 sec/batch\n",
      "Epoch 10/20  Iteration 1628/3560 Training loss: 1.3539 1.2389 sec/batch\n",
      "Epoch 10/20  Iteration 1629/3560 Training loss: 1.3524 1.2250 sec/batch\n",
      "Epoch 10/20  Iteration 1630/3560 Training loss: 1.3527 1.2315 sec/batch\n",
      "Epoch 10/20  Iteration 1631/3560 Training loss: 1.3522 1.2307 sec/batch\n",
      "Epoch 10/20  Iteration 1632/3560 Training loss: 1.3519 1.2494 sec/batch\n",
      "Epoch 10/20  Iteration 1633/3560 Training loss: 1.3508 1.2466 sec/batch\n",
      "Epoch 10/20  Iteration 1634/3560 Training loss: 1.3492 1.2566 sec/batch\n",
      "Epoch 10/20  Iteration 1635/3560 Training loss: 1.3489 1.2665 sec/batch\n",
      "Epoch 10/20  Iteration 1636/3560 Training loss: 1.3486 1.2631 sec/batch\n",
      "Epoch 10/20  Iteration 1637/3560 Training loss: 1.3480 1.2339 sec/batch\n",
      "Epoch 10/20  Iteration 1638/3560 Training loss: 1.3477 1.2382 sec/batch\n",
      "Epoch 10/20  Iteration 1639/3560 Training loss: 1.3464 1.2392 sec/batch\n",
      "Epoch 10/20  Iteration 1640/3560 Training loss: 1.3448 1.2987 sec/batch\n",
      "Epoch 10/20  Iteration 1641/3560 Training loss: 1.3430 1.2867 sec/batch\n",
      "Epoch 10/20  Iteration 1642/3560 Training loss: 1.3423 1.2529 sec/batch\n",
      "Epoch 10/20  Iteration 1643/3560 Training loss: 1.3412 1.2946 sec/batch\n",
      "Epoch 10/20  Iteration 1644/3560 Training loss: 1.3416 1.2617 sec/batch\n",
      "Epoch 10/20  Iteration 1645/3560 Training loss: 1.3410 1.2795 sec/batch\n",
      "Epoch 10/20  Iteration 1646/3560 Training loss: 1.3399 1.2595 sec/batch\n",
      "Epoch 10/20  Iteration 1647/3560 Training loss: 1.3398 1.2547 sec/batch\n",
      "Epoch 10/20  Iteration 1648/3560 Training loss: 1.3387 1.2385 sec/batch\n",
      "Epoch 10/20  Iteration 1649/3560 Training loss: 1.3381 1.2373 sec/batch\n",
      "Epoch 10/20  Iteration 1650/3560 Training loss: 1.3373 1.2611 sec/batch\n",
      "Epoch 10/20  Iteration 1651/3560 Training loss: 1.3372 1.2612 sec/batch\n",
      "Epoch 10/20  Iteration 1652/3560 Training loss: 1.3372 1.2903 sec/batch\n",
      "Epoch 10/20  Iteration 1653/3560 Training loss: 1.3363 1.2522 sec/batch\n",
      "Epoch 10/20  Iteration 1654/3560 Training loss: 1.3365 1.2427 sec/batch\n",
      "Epoch 10/20  Iteration 1655/3560 Training loss: 1.3363 1.2583 sec/batch\n",
      "Epoch 10/20  Iteration 1656/3560 Training loss: 1.3360 1.2252 sec/batch\n",
      "Epoch 10/20  Iteration 1657/3560 Training loss: 1.3357 1.2461 sec/batch\n",
      "Epoch 10/20  Iteration 1658/3560 Training loss: 1.3354 1.2695 sec/batch\n",
      "Epoch 10/20  Iteration 1659/3560 Training loss: 1.3355 1.2468 sec/batch\n",
      "Epoch 10/20  Iteration 1660/3560 Training loss: 1.3348 1.2681 sec/batch\n",
      "Epoch 10/20  Iteration 1661/3560 Training loss: 1.3340 1.2517 sec/batch\n",
      "Epoch 10/20  Iteration 1662/3560 Training loss: 1.3344 1.2533 sec/batch\n",
      "Epoch 10/20  Iteration 1663/3560 Training loss: 1.3341 1.2584 sec/batch\n",
      "Epoch 10/20  Iteration 1664/3560 Training loss: 1.3347 1.2367 sec/batch\n",
      "Epoch 10/20  Iteration 1665/3560 Training loss: 1.3347 1.2406 sec/batch\n",
      "Epoch 10/20  Iteration 1666/3560 Training loss: 1.3345 1.2590 sec/batch\n",
      "Epoch 10/20  Iteration 1667/3560 Training loss: 1.3342 1.2721 sec/batch\n",
      "Epoch 10/20  Iteration 1668/3560 Training loss: 1.3343 1.2668 sec/batch\n",
      "Epoch 10/20  Iteration 1669/3560 Training loss: 1.3343 1.2331 sec/batch\n",
      "Epoch 10/20  Iteration 1670/3560 Training loss: 1.3338 1.2356 sec/batch\n",
      "Epoch 10/20  Iteration 1671/3560 Training loss: 1.3336 1.2375 sec/batch\n",
      "Epoch 10/20  Iteration 1672/3560 Training loss: 1.3332 1.2349 sec/batch\n",
      "Epoch 10/20  Iteration 1673/3560 Training loss: 1.3337 1.2346 sec/batch\n",
      "Epoch 10/20  Iteration 1674/3560 Training loss: 1.3337 1.3763 sec/batch\n",
      "Epoch 10/20  Iteration 1675/3560 Training loss: 1.3339 1.3174 sec/batch\n",
      "Epoch 10/20  Iteration 1676/3560 Training loss: 1.3335 1.3123 sec/batch\n",
      "Epoch 10/20  Iteration 1677/3560 Training loss: 1.3333 1.2730 sec/batch\n",
      "Epoch 10/20  Iteration 1678/3560 Training loss: 1.3332 1.2464 sec/batch\n",
      "Epoch 10/20  Iteration 1679/3560 Training loss: 1.3329 1.2962 sec/batch\n",
      "Epoch 10/20  Iteration 1680/3560 Training loss: 1.3327 1.2813 sec/batch\n",
      "Epoch 10/20  Iteration 1681/3560 Training loss: 1.3317 1.2909 sec/batch\n",
      "Epoch 10/20  Iteration 1682/3560 Training loss: 1.3315 1.3033 sec/batch\n",
      "Epoch 10/20  Iteration 1683/3560 Training loss: 1.3310 1.2897 sec/batch\n",
      "Epoch 10/20  Iteration 1684/3560 Training loss: 1.3308 1.2826 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1685/3560 Training loss: 1.3301 1.2480 sec/batch\n",
      "Epoch 10/20  Iteration 1686/3560 Training loss: 1.3299 1.2543 sec/batch\n",
      "Epoch 10/20  Iteration 1687/3560 Training loss: 1.3294 1.2713 sec/batch\n",
      "Epoch 10/20  Iteration 1688/3560 Training loss: 1.3291 1.2716 sec/batch\n",
      "Epoch 10/20  Iteration 1689/3560 Training loss: 1.3286 1.2447 sec/batch\n",
      "Epoch 10/20  Iteration 1690/3560 Training loss: 1.3282 1.2591 sec/batch\n",
      "Epoch 10/20  Iteration 1691/3560 Training loss: 1.3276 1.2911 sec/batch\n",
      "Epoch 10/20  Iteration 1692/3560 Training loss: 1.3276 1.2590 sec/batch\n",
      "Epoch 10/20  Iteration 1693/3560 Training loss: 1.3271 1.2612 sec/batch\n",
      "Epoch 10/20  Iteration 1694/3560 Training loss: 1.3269 1.2399 sec/batch\n",
      "Epoch 10/20  Iteration 1695/3560 Training loss: 1.3263 1.2534 sec/batch\n",
      "Epoch 10/20  Iteration 1696/3560 Training loss: 1.3258 1.2458 sec/batch\n",
      "Epoch 10/20  Iteration 1697/3560 Training loss: 1.3254 1.2591 sec/batch\n",
      "Epoch 10/20  Iteration 1698/3560 Training loss: 1.3252 1.2686 sec/batch\n",
      "Epoch 10/20  Iteration 1699/3560 Training loss: 1.3251 1.2828 sec/batch\n",
      "Epoch 10/20  Iteration 1700/3560 Training loss: 1.3246 1.2944 sec/batch\n",
      "Epoch 10/20  Iteration 1701/3560 Training loss: 1.3242 1.2772 sec/batch\n",
      "Epoch 10/20  Iteration 1702/3560 Training loss: 1.3237 1.2599 sec/batch\n",
      "Epoch 10/20  Iteration 1703/3560 Training loss: 1.3234 1.2486 sec/batch\n",
      "Epoch 10/20  Iteration 1704/3560 Training loss: 1.3231 1.2443 sec/batch\n",
      "Epoch 10/20  Iteration 1705/3560 Training loss: 1.3228 1.2394 sec/batch\n",
      "Epoch 10/20  Iteration 1706/3560 Training loss: 1.3226 1.2712 sec/batch\n",
      "Epoch 10/20  Iteration 1707/3560 Training loss: 1.3222 1.2770 sec/batch\n",
      "Epoch 10/20  Iteration 1708/3560 Training loss: 1.3219 1.2537 sec/batch\n",
      "Epoch 10/20  Iteration 1709/3560 Training loss: 1.3217 1.3968 sec/batch\n",
      "Epoch 10/20  Iteration 1710/3560 Training loss: 1.3216 1.2718 sec/batch\n",
      "Epoch 10/20  Iteration 1711/3560 Training loss: 1.3213 1.3100 sec/batch\n",
      "Epoch 10/20  Iteration 1712/3560 Training loss: 1.3213 1.2884 sec/batch\n",
      "Epoch 10/20  Iteration 1713/3560 Training loss: 1.3209 1.2889 sec/batch\n",
      "Epoch 10/20  Iteration 1714/3560 Training loss: 1.3207 1.2440 sec/batch\n",
      "Epoch 10/20  Iteration 1715/3560 Training loss: 1.3204 1.3160 sec/batch\n",
      "Epoch 10/20  Iteration 1716/3560 Training loss: 1.3201 1.2459 sec/batch\n",
      "Epoch 10/20  Iteration 1717/3560 Training loss: 1.3197 1.2290 sec/batch\n",
      "Epoch 10/20  Iteration 1718/3560 Training loss: 1.3192 1.2513 sec/batch\n",
      "Epoch 10/20  Iteration 1719/3560 Training loss: 1.3191 1.5127 sec/batch\n",
      "Epoch 10/20  Iteration 1720/3560 Training loss: 1.3190 1.5701 sec/batch\n",
      "Epoch 10/20  Iteration 1721/3560 Training loss: 1.3188 1.7278 sec/batch\n",
      "Epoch 10/20  Iteration 1722/3560 Training loss: 1.3186 1.6623 sec/batch\n",
      "Epoch 10/20  Iteration 1723/3560 Training loss: 1.3185 1.5652 sec/batch\n",
      "Epoch 10/20  Iteration 1724/3560 Training loss: 1.3181 1.3354 sec/batch\n",
      "Epoch 10/20  Iteration 1725/3560 Training loss: 1.3175 1.2338 sec/batch\n",
      "Epoch 10/20  Iteration 1726/3560 Training loss: 1.3174 1.2343 sec/batch\n",
      "Epoch 10/20  Iteration 1727/3560 Training loss: 1.3172 1.2463 sec/batch\n",
      "Epoch 10/20  Iteration 1728/3560 Training loss: 1.3167 1.2476 sec/batch\n",
      "Epoch 10/20  Iteration 1729/3560 Training loss: 1.3167 1.2505 sec/batch\n",
      "Epoch 10/20  Iteration 1730/3560 Training loss: 1.3165 1.2820 sec/batch\n",
      "Epoch 10/20  Iteration 1731/3560 Training loss: 1.3162 1.2329 sec/batch\n",
      "Epoch 10/20  Iteration 1732/3560 Training loss: 1.3157 1.2500 sec/batch\n",
      "Epoch 10/20  Iteration 1733/3560 Training loss: 1.3151 1.2894 sec/batch\n",
      "Epoch 10/20  Iteration 1734/3560 Training loss: 1.3147 1.3472 sec/batch\n",
      "Epoch 10/20  Iteration 1735/3560 Training loss: 1.3147 1.2805 sec/batch\n",
      "Epoch 10/20  Iteration 1736/3560 Training loss: 1.3147 1.2396 sec/batch\n",
      "Epoch 10/20  Iteration 1737/3560 Training loss: 1.3146 1.2609 sec/batch\n",
      "Epoch 10/20  Iteration 1738/3560 Training loss: 1.3145 1.2865 sec/batch\n",
      "Epoch 10/20  Iteration 1739/3560 Training loss: 1.3146 1.2547 sec/batch\n",
      "Epoch 10/20  Iteration 1740/3560 Training loss: 1.3146 1.2481 sec/batch\n",
      "Epoch 10/20  Iteration 1741/3560 Training loss: 1.3144 1.2512 sec/batch\n",
      "Epoch 10/20  Iteration 1742/3560 Training loss: 1.3143 1.2583 sec/batch\n",
      "Epoch 10/20  Iteration 1743/3560 Training loss: 1.3145 1.2451 sec/batch\n",
      "Epoch 10/20  Iteration 1744/3560 Training loss: 1.3145 1.2391 sec/batch\n",
      "Epoch 10/20  Iteration 1745/3560 Training loss: 1.3144 1.2565 sec/batch\n",
      "Epoch 10/20  Iteration 1746/3560 Training loss: 1.3146 1.3221 sec/batch\n",
      "Epoch 10/20  Iteration 1747/3560 Training loss: 1.3143 1.2365 sec/batch\n",
      "Epoch 10/20  Iteration 1748/3560 Training loss: 1.3145 1.2579 sec/batch\n",
      "Epoch 10/20  Iteration 1749/3560 Training loss: 1.3144 1.2330 sec/batch\n",
      "Epoch 10/20  Iteration 1750/3560 Training loss: 1.3146 1.3137 sec/batch\n",
      "Epoch 10/20  Iteration 1751/3560 Training loss: 1.3146 1.5060 sec/batch\n",
      "Epoch 10/20  Iteration 1752/3560 Training loss: 1.3145 1.4416 sec/batch\n",
      "Epoch 10/20  Iteration 1753/3560 Training loss: 1.3141 1.3892 sec/batch\n",
      "Epoch 10/20  Iteration 1754/3560 Training loss: 1.3138 1.3265 sec/batch\n",
      "Epoch 10/20  Iteration 1755/3560 Training loss: 1.3138 1.3949 sec/batch\n",
      "Epoch 10/20  Iteration 1756/3560 Training loss: 1.3137 1.4583 sec/batch\n",
      "Epoch 10/20  Iteration 1757/3560 Training loss: 1.3136 1.3752 sec/batch\n",
      "Epoch 10/20  Iteration 1758/3560 Training loss: 1.3135 1.2238 sec/batch\n",
      "Epoch 10/20  Iteration 1759/3560 Training loss: 1.3135 1.2586 sec/batch\n",
      "Epoch 10/20  Iteration 1760/3560 Training loss: 1.3134 1.2889 sec/batch\n",
      "Epoch 10/20  Iteration 1761/3560 Training loss: 1.3131 1.2635 sec/batch\n",
      "Epoch 10/20  Iteration 1762/3560 Training loss: 1.3131 1.2796 sec/batch\n",
      "Epoch 10/20  Iteration 1763/3560 Training loss: 1.3131 1.5239 sec/batch\n",
      "Epoch 10/20  Iteration 1764/3560 Training loss: 1.3130 1.5482 sec/batch\n",
      "Epoch 10/20  Iteration 1765/3560 Training loss: 1.3129 1.5657 sec/batch\n",
      "Epoch 10/20  Iteration 1766/3560 Training loss: 1.3128 1.6043 sec/batch\n",
      "Epoch 10/20  Iteration 1767/3560 Training loss: 1.3128 1.6010 sec/batch\n",
      "Epoch 10/20  Iteration 1768/3560 Training loss: 1.3126 1.5731 sec/batch\n",
      "Epoch 10/20  Iteration 1769/3560 Training loss: 1.3127 1.5757 sec/batch\n",
      "Epoch 10/20  Iteration 1770/3560 Training loss: 1.3130 1.4106 sec/batch\n",
      "Epoch 10/20  Iteration 1771/3560 Training loss: 1.3129 1.4364 sec/batch\n",
      "Epoch 10/20  Iteration 1772/3560 Training loss: 1.3128 1.4310 sec/batch\n",
      "Epoch 10/20  Iteration 1773/3560 Training loss: 1.3127 1.4233 sec/batch\n",
      "Epoch 10/20  Iteration 1774/3560 Training loss: 1.3125 1.4654 sec/batch\n",
      "Epoch 10/20  Iteration 1775/3560 Training loss: 1.3126 1.6624 sec/batch\n",
      "Epoch 10/20  Iteration 1776/3560 Training loss: 1.3126 1.4006 sec/batch\n",
      "Epoch 10/20  Iteration 1777/3560 Training loss: 1.3126 1.4824 sec/batch\n",
      "Epoch 10/20  Iteration 1778/3560 Training loss: 1.3123 1.3836 sec/batch\n",
      "Epoch 10/20  Iteration 1779/3560 Training loss: 1.3121 1.2675 sec/batch\n",
      "Epoch 10/20  Iteration 1780/3560 Training loss: 1.3122 1.2482 sec/batch\n",
      "Epoch 11/20  Iteration 1781/3560 Training loss: 1.4275 1.2488 sec/batch\n",
      "Epoch 11/20  Iteration 1782/3560 Training loss: 1.3744 1.2714 sec/batch\n",
      "Epoch 11/20  Iteration 1783/3560 Training loss: 1.3496 1.2516 sec/batch\n",
      "Epoch 11/20  Iteration 1784/3560 Training loss: 1.3427 1.2508 sec/batch\n",
      "Epoch 11/20  Iteration 1785/3560 Training loss: 1.3299 1.2985 sec/batch\n",
      "Epoch 11/20  Iteration 1786/3560 Training loss: 1.3185 1.2470 sec/batch\n",
      "Epoch 11/20  Iteration 1787/3560 Training loss: 1.3160 1.2494 sec/batch\n",
      "Epoch 11/20  Iteration 1788/3560 Training loss: 1.3125 1.2507 sec/batch\n",
      "Epoch 11/20  Iteration 1789/3560 Training loss: 1.3111 1.2500 sec/batch\n",
      "Epoch 11/20  Iteration 1790/3560 Training loss: 1.3102 1.3066 sec/batch\n",
      "Epoch 11/20  Iteration 1791/3560 Training loss: 1.3059 1.2455 sec/batch\n",
      "Epoch 11/20  Iteration 1792/3560 Training loss: 1.3048 1.2562 sec/batch\n",
      "Epoch 11/20  Iteration 1793/3560 Training loss: 1.3036 1.2222 sec/batch\n",
      "Epoch 11/20  Iteration 1794/3560 Training loss: 1.3038 1.2430 sec/batch\n",
      "Epoch 11/20  Iteration 1795/3560 Training loss: 1.3018 1.3237 sec/batch\n",
      "Epoch 11/20  Iteration 1796/3560 Training loss: 1.2996 1.2634 sec/batch\n",
      "Epoch 11/20  Iteration 1797/3560 Training loss: 1.2988 1.2475 sec/batch\n",
      "Epoch 11/20  Iteration 1798/3560 Training loss: 1.2998 1.2732 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1799/3560 Training loss: 1.3001 1.2439 sec/batch\n",
      "Epoch 11/20  Iteration 1800/3560 Training loss: 1.3011 1.2500 sec/batch\n",
      "Epoch 11/20  Iteration 1801/3560 Training loss: 1.3004 1.2314 sec/batch\n",
      "Epoch 11/20  Iteration 1802/3560 Training loss: 1.3006 1.2448 sec/batch\n",
      "Epoch 11/20  Iteration 1803/3560 Training loss: 1.3000 1.2388 sec/batch\n",
      "Epoch 11/20  Iteration 1804/3560 Training loss: 1.2998 1.2944 sec/batch\n",
      "Epoch 11/20  Iteration 1805/3560 Training loss: 1.2997 1.2530 sec/batch\n",
      "Epoch 11/20  Iteration 1806/3560 Training loss: 1.2977 1.2741 sec/batch\n",
      "Epoch 11/20  Iteration 1807/3560 Training loss: 1.2964 1.2479 sec/batch\n",
      "Epoch 11/20  Iteration 1808/3560 Training loss: 1.2969 1.2379 sec/batch\n",
      "Epoch 11/20  Iteration 1809/3560 Training loss: 1.2967 1.2352 sec/batch\n",
      "Epoch 11/20  Iteration 1810/3560 Training loss: 1.2968 1.2374 sec/batch\n",
      "Epoch 11/20  Iteration 1811/3560 Training loss: 1.2960 1.2561 sec/batch\n",
      "Epoch 11/20  Iteration 1812/3560 Training loss: 1.2950 1.2489 sec/batch\n",
      "Epoch 11/20  Iteration 1813/3560 Training loss: 1.2949 1.2715 sec/batch\n",
      "Epoch 11/20  Iteration 1814/3560 Training loss: 1.2948 1.3392 sec/batch\n",
      "Epoch 11/20  Iteration 1815/3560 Training loss: 1.2943 1.2508 sec/batch\n",
      "Epoch 11/20  Iteration 1816/3560 Training loss: 1.2941 1.2441 sec/batch\n",
      "Epoch 11/20  Iteration 1817/3560 Training loss: 1.2931 1.2362 sec/batch\n",
      "Epoch 11/20  Iteration 1818/3560 Training loss: 1.2919 1.2341 sec/batch\n",
      "Epoch 11/20  Iteration 1819/3560 Training loss: 1.2906 1.2703 sec/batch\n",
      "Epoch 11/20  Iteration 1820/3560 Training loss: 1.2901 1.2776 sec/batch\n",
      "Epoch 11/20  Iteration 1821/3560 Training loss: 1.2893 1.3008 sec/batch\n",
      "Epoch 11/20  Iteration 1822/3560 Training loss: 1.2902 1.2613 sec/batch\n",
      "Epoch 11/20  Iteration 1823/3560 Training loss: 1.2900 1.2799 sec/batch\n",
      "Epoch 11/20  Iteration 1824/3560 Training loss: 1.2893 1.3123 sec/batch\n",
      "Epoch 11/20  Iteration 1825/3560 Training loss: 1.2895 1.2337 sec/batch\n",
      "Epoch 11/20  Iteration 1826/3560 Training loss: 1.2887 1.2342 sec/batch\n",
      "Epoch 11/20  Iteration 1827/3560 Training loss: 1.2885 1.2396 sec/batch\n",
      "Epoch 11/20  Iteration 1828/3560 Training loss: 1.2878 1.2564 sec/batch\n",
      "Epoch 11/20  Iteration 1829/3560 Training loss: 1.2877 1.2562 sec/batch\n",
      "Epoch 11/20  Iteration 1830/3560 Training loss: 1.2880 1.2910 sec/batch\n",
      "Epoch 11/20  Iteration 1831/3560 Training loss: 1.2874 1.2452 sec/batch\n",
      "Epoch 11/20  Iteration 1832/3560 Training loss: 1.2879 1.2433 sec/batch\n",
      "Epoch 11/20  Iteration 1833/3560 Training loss: 1.2876 1.3062 sec/batch\n",
      "Epoch 11/20  Iteration 1834/3560 Training loss: 1.2877 1.2281 sec/batch\n",
      "Epoch 11/20  Iteration 1835/3560 Training loss: 1.2874 1.2467 sec/batch\n",
      "Epoch 11/20  Iteration 1836/3560 Training loss: 1.2876 1.2410 sec/batch\n",
      "Epoch 11/20  Iteration 1837/3560 Training loss: 1.2878 1.2392 sec/batch\n",
      "Epoch 11/20  Iteration 1838/3560 Training loss: 1.2875 1.3151 sec/batch\n",
      "Epoch 11/20  Iteration 1839/3560 Training loss: 1.2869 1.2450 sec/batch\n",
      "Epoch 11/20  Iteration 1840/3560 Training loss: 1.2873 1.2463 sec/batch\n",
      "Epoch 11/20  Iteration 1841/3560 Training loss: 1.2872 1.2361 sec/batch\n",
      "Epoch 11/20  Iteration 1842/3560 Training loss: 1.2879 1.2672 sec/batch\n",
      "Epoch 11/20  Iteration 1843/3560 Training loss: 1.2881 1.2870 sec/batch\n",
      "Epoch 11/20  Iteration 1844/3560 Training loss: 1.2882 1.2485 sec/batch\n",
      "Epoch 11/20  Iteration 1845/3560 Training loss: 1.2882 1.2360 sec/batch\n",
      "Epoch 11/20  Iteration 1846/3560 Training loss: 1.2883 1.2742 sec/batch\n",
      "Epoch 11/20  Iteration 1847/3560 Training loss: 1.2885 1.2390 sec/batch\n",
      "Epoch 11/20  Iteration 1848/3560 Training loss: 1.2882 1.2455 sec/batch\n",
      "Epoch 11/20  Iteration 1849/3560 Training loss: 1.2881 1.2291 sec/batch\n",
      "Epoch 11/20  Iteration 1850/3560 Training loss: 1.2880 1.2428 sec/batch\n",
      "Epoch 11/20  Iteration 1851/3560 Training loss: 1.2885 1.2325 sec/batch\n",
      "Epoch 11/20  Iteration 1852/3560 Training loss: 1.2887 1.3045 sec/batch\n",
      "Epoch 11/20  Iteration 1853/3560 Training loss: 1.2892 1.2314 sec/batch\n",
      "Epoch 11/20  Iteration 1854/3560 Training loss: 1.2888 1.2563 sec/batch\n",
      "Epoch 11/20  Iteration 1855/3560 Training loss: 1.2887 1.2719 sec/batch\n",
      "Epoch 11/20  Iteration 1856/3560 Training loss: 1.2888 1.4016 sec/batch\n",
      "Epoch 11/20  Iteration 1857/3560 Training loss: 1.2887 1.5823 sec/batch\n",
      "Epoch 11/20  Iteration 1858/3560 Training loss: 1.2885 1.5656 sec/batch\n",
      "Epoch 11/20  Iteration 1859/3560 Training loss: 1.2878 1.6153 sec/batch\n",
      "Epoch 11/20  Iteration 1860/3560 Training loss: 1.2876 1.6817 sec/batch\n",
      "Epoch 11/20  Iteration 1861/3560 Training loss: 1.2872 1.4423 sec/batch\n",
      "Epoch 11/20  Iteration 1862/3560 Training loss: 1.2871 1.2374 sec/batch\n",
      "Epoch 11/20  Iteration 1863/3560 Training loss: 1.2866 1.2542 sec/batch\n",
      "Epoch 11/20  Iteration 1864/3560 Training loss: 1.2865 1.2430 sec/batch\n",
      "Epoch 11/20  Iteration 1865/3560 Training loss: 1.2862 1.2466 sec/batch\n",
      "Epoch 11/20  Iteration 1866/3560 Training loss: 1.2861 1.2672 sec/batch\n",
      "Epoch 11/20  Iteration 1867/3560 Training loss: 1.2858 1.2389 sec/batch\n",
      "Epoch 11/20  Iteration 1868/3560 Training loss: 1.2854 1.2348 sec/batch\n",
      "Epoch 11/20  Iteration 1869/3560 Training loss: 1.2851 1.2748 sec/batch\n",
      "Epoch 11/20  Iteration 1870/3560 Training loss: 1.2851 1.3049 sec/batch\n",
      "Epoch 11/20  Iteration 1871/3560 Training loss: 1.2848 1.2395 sec/batch\n",
      "Epoch 11/20  Iteration 1872/3560 Training loss: 1.2847 1.2338 sec/batch\n",
      "Epoch 11/20  Iteration 1873/3560 Training loss: 1.2844 1.2271 sec/batch\n",
      "Epoch 11/20  Iteration 1874/3560 Training loss: 1.2838 1.2501 sec/batch\n",
      "Epoch 11/20  Iteration 1875/3560 Training loss: 1.2836 1.2631 sec/batch\n",
      "Epoch 11/20  Iteration 1876/3560 Training loss: 1.2836 1.2419 sec/batch\n",
      "Epoch 11/20  Iteration 1877/3560 Training loss: 1.2835 1.2764 sec/batch\n",
      "Epoch 11/20  Iteration 1878/3560 Training loss: 1.2831 1.2407 sec/batch\n",
      "Epoch 11/20  Iteration 1879/3560 Training loss: 1.2828 1.2564 sec/batch\n",
      "Epoch 11/20  Iteration 1880/3560 Training loss: 1.2825 1.2864 sec/batch\n",
      "Epoch 11/20  Iteration 1881/3560 Training loss: 1.2823 1.2386 sec/batch\n",
      "Epoch 11/20  Iteration 1882/3560 Training loss: 1.2822 1.2522 sec/batch\n",
      "Epoch 11/20  Iteration 1883/3560 Training loss: 1.2820 1.2420 sec/batch\n",
      "Epoch 11/20  Iteration 1884/3560 Training loss: 1.2819 1.2421 sec/batch\n",
      "Epoch 11/20  Iteration 1885/3560 Training loss: 1.2816 1.3108 sec/batch\n",
      "Epoch 11/20  Iteration 1886/3560 Training loss: 1.2814 1.2520 sec/batch\n",
      "Epoch 11/20  Iteration 1887/3560 Training loss: 1.2813 1.2444 sec/batch\n",
      "Epoch 11/20  Iteration 1888/3560 Training loss: 1.2812 1.2348 sec/batch\n",
      "Epoch 11/20  Iteration 1889/3560 Training loss: 1.2810 1.3153 sec/batch\n",
      "Epoch 11/20  Iteration 1890/3560 Training loss: 1.2810 1.2687 sec/batch\n",
      "Epoch 11/20  Iteration 1891/3560 Training loss: 1.2809 1.2522 sec/batch\n",
      "Epoch 11/20  Iteration 1892/3560 Training loss: 1.2808 1.2619 sec/batch\n",
      "Epoch 11/20  Iteration 1893/3560 Training loss: 1.2805 1.3226 sec/batch\n",
      "Epoch 11/20  Iteration 1894/3560 Training loss: 1.2804 1.2405 sec/batch\n",
      "Epoch 11/20  Iteration 1895/3560 Training loss: 1.2800 1.2376 sec/batch\n",
      "Epoch 11/20  Iteration 1896/3560 Training loss: 1.2797 1.2436 sec/batch\n",
      "Epoch 11/20  Iteration 1897/3560 Training loss: 1.2797 1.2365 sec/batch\n",
      "Epoch 11/20  Iteration 1898/3560 Training loss: 1.2798 1.2659 sec/batch\n",
      "Epoch 11/20  Iteration 1899/3560 Training loss: 1.2796 1.2982 sec/batch\n",
      "Epoch 11/20  Iteration 1900/3560 Training loss: 1.2795 1.2580 sec/batch\n",
      "Epoch 11/20  Iteration 1901/3560 Training loss: 1.2794 1.3066 sec/batch\n",
      "Epoch 11/20  Iteration 1902/3560 Training loss: 1.2791 1.2522 sec/batch\n",
      "Epoch 11/20  Iteration 1903/3560 Training loss: 1.2786 1.2515 sec/batch\n",
      "Epoch 11/20  Iteration 1904/3560 Training loss: 1.2785 1.2534 sec/batch\n",
      "Epoch 11/20  Iteration 1905/3560 Training loss: 1.2783 1.2345 sec/batch\n",
      "Epoch 11/20  Iteration 1906/3560 Training loss: 1.2779 1.2488 sec/batch\n",
      "Epoch 11/20  Iteration 1907/3560 Training loss: 1.2779 1.2462 sec/batch\n",
      "Epoch 11/20  Iteration 1908/3560 Training loss: 1.2778 1.2729 sec/batch\n",
      "Epoch 11/20  Iteration 1909/3560 Training loss: 1.2776 1.3111 sec/batch\n",
      "Epoch 11/20  Iteration 1910/3560 Training loss: 1.2772 1.2431 sec/batch\n",
      "Epoch 11/20  Iteration 1911/3560 Training loss: 1.2767 1.2355 sec/batch\n",
      "Epoch 11/20  Iteration 1912/3560 Training loss: 1.2764 1.2450 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1913/3560 Training loss: 1.2765 1.2405 sec/batch\n",
      "Epoch 11/20  Iteration 1914/3560 Training loss: 1.2766 1.2620 sec/batch\n",
      "Epoch 11/20  Iteration 1915/3560 Training loss: 1.2765 1.2512 sec/batch\n",
      "Epoch 11/20  Iteration 1916/3560 Training loss: 1.2765 1.2311 sec/batch\n",
      "Epoch 11/20  Iteration 1917/3560 Training loss: 1.2766 1.3103 sec/batch\n",
      "Epoch 11/20  Iteration 1918/3560 Training loss: 1.2767 1.3043 sec/batch\n",
      "Epoch 11/20  Iteration 1919/3560 Training loss: 1.2766 1.3439 sec/batch\n",
      "Epoch 11/20  Iteration 1920/3560 Training loss: 1.2765 1.3412 sec/batch\n",
      "Epoch 11/20  Iteration 1921/3560 Training loss: 1.2769 1.2734 sec/batch\n",
      "Epoch 11/20  Iteration 1922/3560 Training loss: 1.2769 1.2274 sec/batch\n",
      "Epoch 11/20  Iteration 1923/3560 Training loss: 1.2769 1.2728 sec/batch\n",
      "Epoch 11/20  Iteration 1924/3560 Training loss: 1.2771 1.2598 sec/batch\n",
      "Epoch 11/20  Iteration 1925/3560 Training loss: 1.2770 1.2953 sec/batch\n",
      "Epoch 11/20  Iteration 1926/3560 Training loss: 1.2771 1.2362 sec/batch\n",
      "Epoch 11/20  Iteration 1927/3560 Training loss: 1.2771 1.2746 sec/batch\n",
      "Epoch 11/20  Iteration 1928/3560 Training loss: 1.2774 1.2740 sec/batch\n",
      "Epoch 11/20  Iteration 1929/3560 Training loss: 1.2775 1.2573 sec/batch\n",
      "Epoch 11/20  Iteration 1930/3560 Training loss: 1.2774 1.2389 sec/batch\n",
      "Epoch 11/20  Iteration 1931/3560 Training loss: 1.2771 1.2763 sec/batch\n",
      "Epoch 11/20  Iteration 1932/3560 Training loss: 1.2770 1.2323 sec/batch\n",
      "Epoch 11/20  Iteration 1933/3560 Training loss: 1.2770 1.3199 sec/batch\n",
      "Epoch 11/20  Iteration 1934/3560 Training loss: 1.2770 1.2429 sec/batch\n",
      "Epoch 11/20  Iteration 1935/3560 Training loss: 1.2769 1.2452 sec/batch\n",
      "Epoch 11/20  Iteration 1936/3560 Training loss: 1.2769 1.2608 sec/batch\n",
      "Epoch 11/20  Iteration 1937/3560 Training loss: 1.2770 1.2783 sec/batch\n",
      "Epoch 11/20  Iteration 1938/3560 Training loss: 1.2769 1.2866 sec/batch\n",
      "Epoch 11/20  Iteration 1939/3560 Training loss: 1.2767 1.2724 sec/batch\n",
      "Epoch 11/20  Iteration 1940/3560 Training loss: 1.2768 1.2561 sec/batch\n",
      "Epoch 11/20  Iteration 1941/3560 Training loss: 1.2769 1.3045 sec/batch\n",
      "Epoch 11/20  Iteration 1942/3560 Training loss: 1.2768 1.2574 sec/batch\n",
      "Epoch 11/20  Iteration 1943/3560 Training loss: 1.2767 1.2403 sec/batch\n",
      "Epoch 11/20  Iteration 1944/3560 Training loss: 1.2767 1.2636 sec/batch\n",
      "Epoch 11/20  Iteration 1945/3560 Training loss: 1.2766 1.3254 sec/batch\n",
      "Epoch 11/20  Iteration 1946/3560 Training loss: 1.2765 1.2775 sec/batch\n",
      "Epoch 11/20  Iteration 1947/3560 Training loss: 1.2767 1.2409 sec/batch\n",
      "Epoch 11/20  Iteration 1948/3560 Training loss: 1.2770 1.2385 sec/batch\n",
      "Epoch 11/20  Iteration 1949/3560 Training loss: 1.2770 1.2845 sec/batch\n",
      "Epoch 11/20  Iteration 1950/3560 Training loss: 1.2770 1.2323 sec/batch\n",
      "Epoch 11/20  Iteration 1951/3560 Training loss: 1.2769 1.2353 sec/batch\n",
      "Epoch 11/20  Iteration 1952/3560 Training loss: 1.2767 1.2415 sec/batch\n",
      "Epoch 11/20  Iteration 1953/3560 Training loss: 1.2768 1.2533 sec/batch\n",
      "Epoch 11/20  Iteration 1954/3560 Training loss: 1.2768 1.2686 sec/batch\n",
      "Epoch 11/20  Iteration 1955/3560 Training loss: 1.2768 1.2568 sec/batch\n",
      "Epoch 11/20  Iteration 1956/3560 Training loss: 1.2766 1.2403 sec/batch\n",
      "Epoch 11/20  Iteration 1957/3560 Training loss: 1.2764 1.2777 sec/batch\n",
      "Epoch 11/20  Iteration 1958/3560 Training loss: 1.2766 1.2393 sec/batch\n",
      "Epoch 12/20  Iteration 1959/3560 Training loss: 1.4155 1.2341 sec/batch\n",
      "Epoch 12/20  Iteration 1960/3560 Training loss: 1.3556 1.2685 sec/batch\n",
      "Epoch 12/20  Iteration 1961/3560 Training loss: 1.3335 1.3012 sec/batch\n",
      "Epoch 12/20  Iteration 1962/3560 Training loss: 1.3262 1.2407 sec/batch\n",
      "Epoch 12/20  Iteration 1963/3560 Training loss: 1.3141 1.2325 sec/batch\n",
      "Epoch 12/20  Iteration 1964/3560 Training loss: 1.3016 1.2787 sec/batch\n",
      "Epoch 12/20  Iteration 1965/3560 Training loss: 1.3002 1.2864 sec/batch\n",
      "Epoch 12/20  Iteration 1966/3560 Training loss: 1.2962 1.2413 sec/batch\n",
      "Epoch 12/20  Iteration 1967/3560 Training loss: 1.2937 1.2451 sec/batch\n",
      "Epoch 12/20  Iteration 1968/3560 Training loss: 1.2906 1.2388 sec/batch\n",
      "Epoch 12/20  Iteration 1969/3560 Training loss: 1.2861 1.2578 sec/batch\n",
      "Epoch 12/20  Iteration 1970/3560 Training loss: 1.2855 1.2919 sec/batch\n",
      "Epoch 12/20  Iteration 1971/3560 Training loss: 1.2855 1.2383 sec/batch\n",
      "Epoch 12/20  Iteration 1972/3560 Training loss: 1.2850 1.2401 sec/batch\n",
      "Epoch 12/20  Iteration 1973/3560 Training loss: 1.2837 1.3170 sec/batch\n",
      "Epoch 12/20  Iteration 1974/3560 Training loss: 1.2808 1.2404 sec/batch\n",
      "Epoch 12/20  Iteration 1975/3560 Training loss: 1.2799 1.2394 sec/batch\n",
      "Epoch 12/20  Iteration 1976/3560 Training loss: 1.2802 1.2368 sec/batch\n",
      "Epoch 12/20  Iteration 1977/3560 Training loss: 1.2800 1.2646 sec/batch\n",
      "Epoch 12/20  Iteration 1978/3560 Training loss: 1.2810 1.2361 sec/batch\n",
      "Epoch 12/20  Iteration 1979/3560 Training loss: 1.2802 1.2688 sec/batch\n",
      "Epoch 12/20  Iteration 1980/3560 Training loss: 1.2799 1.2918 sec/batch\n",
      "Epoch 12/20  Iteration 1981/3560 Training loss: 1.2790 1.3172 sec/batch\n",
      "Epoch 12/20  Iteration 1982/3560 Training loss: 1.2789 1.2352 sec/batch\n",
      "Epoch 12/20  Iteration 1983/3560 Training loss: 1.2783 1.2456 sec/batch\n",
      "Epoch 12/20  Iteration 1984/3560 Training loss: 1.2759 1.2490 sec/batch\n",
      "Epoch 12/20  Iteration 1985/3560 Training loss: 1.2744 1.2575 sec/batch\n",
      "Epoch 12/20  Iteration 1986/3560 Training loss: 1.2746 1.2332 sec/batch\n",
      "Epoch 12/20  Iteration 1987/3560 Training loss: 1.2745 1.2456 sec/batch\n",
      "Epoch 12/20  Iteration 1988/3560 Training loss: 1.2742 1.2273 sec/batch\n",
      "Epoch 12/20  Iteration 1989/3560 Training loss: 1.2732 1.3474 sec/batch\n",
      "Epoch 12/20  Iteration 1990/3560 Training loss: 1.2720 1.2839 sec/batch\n",
      "Epoch 12/20  Iteration 1991/3560 Training loss: 1.2719 1.2516 sec/batch\n",
      "Epoch 12/20  Iteration 1992/3560 Training loss: 1.2721 1.2442 sec/batch\n",
      "Epoch 12/20  Iteration 1993/3560 Training loss: 1.2715 1.2746 sec/batch\n",
      "Epoch 12/20  Iteration 1994/3560 Training loss: 1.2712 1.2488 sec/batch\n",
      "Epoch 12/20  Iteration 1995/3560 Training loss: 1.2701 1.2543 sec/batch\n",
      "Epoch 12/20  Iteration 1996/3560 Training loss: 1.2689 1.2675 sec/batch\n",
      "Epoch 12/20  Iteration 1997/3560 Training loss: 1.2676 1.2971 sec/batch\n",
      "Epoch 12/20  Iteration 1998/3560 Training loss: 1.2669 1.6340 sec/batch\n",
      "Epoch 12/20  Iteration 1999/3560 Training loss: 1.2665 1.5668 sec/batch\n",
      "Epoch 12/20  Iteration 2000/3560 Training loss: 1.2674 1.6012 sec/batch\n",
      "Epoch 12/20  Iteration 2001/3560 Training loss: 1.2671 1.6219 sec/batch\n",
      "Epoch 12/20  Iteration 2002/3560 Training loss: 1.2665 1.6084 sec/batch\n",
      "Epoch 12/20  Iteration 2003/3560 Training loss: 1.2669 1.2273 sec/batch\n",
      "Epoch 12/20  Iteration 2004/3560 Training loss: 1.2661 1.2698 sec/batch\n",
      "Epoch 12/20  Iteration 2005/3560 Training loss: 1.2658 1.2261 sec/batch\n",
      "Epoch 12/20  Iteration 2006/3560 Training loss: 1.2652 1.2465 sec/batch\n",
      "Epoch 12/20  Iteration 2007/3560 Training loss: 1.2652 1.3001 sec/batch\n",
      "Epoch 12/20  Iteration 2008/3560 Training loss: 1.2653 1.2757 sec/batch\n",
      "Epoch 12/20  Iteration 2009/3560 Training loss: 1.2648 1.2427 sec/batch\n",
      "Epoch 12/20  Iteration 2010/3560 Training loss: 1.2653 1.2279 sec/batch\n",
      "Epoch 12/20  Iteration 2011/3560 Training loss: 1.2651 1.2295 sec/batch\n",
      "Epoch 12/20  Iteration 2012/3560 Training loss: 1.2653 1.2713 sec/batch\n",
      "Epoch 12/20  Iteration 2013/3560 Training loss: 1.2650 1.2303 sec/batch\n",
      "Epoch 12/20  Iteration 2014/3560 Training loss: 1.2651 1.2414 sec/batch\n",
      "Epoch 12/20  Iteration 2015/3560 Training loss: 1.2651 1.2301 sec/batch\n",
      "Epoch 12/20  Iteration 2016/3560 Training loss: 1.2649 1.2653 sec/batch\n",
      "Epoch 12/20  Iteration 2017/3560 Training loss: 1.2644 1.2933 sec/batch\n",
      "Epoch 12/20  Iteration 2018/3560 Training loss: 1.2649 1.2443 sec/batch\n",
      "Epoch 12/20  Iteration 2019/3560 Training loss: 1.2647 1.2438 sec/batch\n",
      "Epoch 12/20  Iteration 2020/3560 Training loss: 1.2654 1.2837 sec/batch\n",
      "Epoch 12/20  Iteration 2021/3560 Training loss: 1.2654 1.2392 sec/batch\n",
      "Epoch 12/20  Iteration 2022/3560 Training loss: 1.2655 1.2542 sec/batch\n",
      "Epoch 12/20  Iteration 2023/3560 Training loss: 1.2655 1.2437 sec/batch\n",
      "Epoch 12/20  Iteration 2024/3560 Training loss: 1.2656 1.2702 sec/batch\n",
      "Epoch 12/20  Iteration 2025/3560 Training loss: 1.2657 1.2752 sec/batch\n",
      "Epoch 12/20  Iteration 2026/3560 Training loss: 1.2655 1.2729 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 2027/3560 Training loss: 1.2655 1.2604 sec/batch\n",
      "Epoch 12/20  Iteration 2028/3560 Training loss: 1.2654 1.3202 sec/batch\n",
      "Epoch 12/20  Iteration 2029/3560 Training loss: 1.2657 1.2375 sec/batch\n",
      "Epoch 12/20  Iteration 2030/3560 Training loss: 1.2659 1.2371 sec/batch\n",
      "Epoch 12/20  Iteration 2031/3560 Training loss: 1.2662 1.2546 sec/batch\n",
      "Epoch 12/20  Iteration 2032/3560 Training loss: 1.2658 1.2483 sec/batch\n",
      "Epoch 12/20  Iteration 2033/3560 Training loss: 1.2657 1.2679 sec/batch\n",
      "Epoch 12/20  Iteration 2034/3560 Training loss: 1.2658 1.2447 sec/batch\n",
      "Epoch 12/20  Iteration 2035/3560 Training loss: 1.2657 1.2551 sec/batch\n",
      "Epoch 12/20  Iteration 2036/3560 Training loss: 1.2655 1.3380 sec/batch\n",
      "Epoch 12/20  Iteration 2037/3560 Training loss: 1.2649 1.2297 sec/batch\n",
      "Epoch 12/20  Iteration 2038/3560 Training loss: 1.2647 1.2395 sec/batch\n",
      "Epoch 12/20  Iteration 2039/3560 Training loss: 1.2644 1.2392 sec/batch\n",
      "Epoch 12/20  Iteration 2040/3560 Training loss: 1.2643 1.2465 sec/batch\n",
      "Epoch 12/20  Iteration 2041/3560 Training loss: 1.2639 1.2642 sec/batch\n",
      "Epoch 12/20  Iteration 2042/3560 Training loss: 1.2638 1.2387 sec/batch\n",
      "Epoch 12/20  Iteration 2043/3560 Training loss: 1.2635 1.2358 sec/batch\n",
      "Epoch 12/20  Iteration 2044/3560 Training loss: 1.2634 1.2881 sec/batch\n",
      "Epoch 12/20  Iteration 2045/3560 Training loss: 1.2630 1.2883 sec/batch\n",
      "Epoch 12/20  Iteration 2046/3560 Training loss: 1.2626 1.2624 sec/batch\n",
      "Epoch 12/20  Iteration 2047/3560 Training loss: 1.2622 1.2355 sec/batch\n",
      "Epoch 12/20  Iteration 2048/3560 Training loss: 1.2623 1.2366 sec/batch\n",
      "Epoch 12/20  Iteration 2049/3560 Training loss: 1.2620 1.2458 sec/batch\n",
      "Epoch 12/20  Iteration 2050/3560 Training loss: 1.2620 1.2402 sec/batch\n",
      "Epoch 12/20  Iteration 2051/3560 Training loss: 1.2616 1.2389 sec/batch\n",
      "Epoch 12/20  Iteration 2052/3560 Training loss: 1.2613 1.3084 sec/batch\n",
      "Epoch 12/20  Iteration 2053/3560 Training loss: 1.2609 1.2236 sec/batch\n",
      "Epoch 12/20  Iteration 2054/3560 Training loss: 1.2607 1.2649 sec/batch\n",
      "Epoch 12/20  Iteration 2055/3560 Training loss: 1.2607 1.2845 sec/batch\n",
      "Epoch 12/20  Iteration 2056/3560 Training loss: 1.2603 1.2438 sec/batch\n",
      "Epoch 12/20  Iteration 2057/3560 Training loss: 1.2599 1.2514 sec/batch\n",
      "Epoch 12/20  Iteration 2058/3560 Training loss: 1.2596 1.2396 sec/batch\n",
      "Epoch 12/20  Iteration 2059/3560 Training loss: 1.2594 1.2331 sec/batch\n",
      "Epoch 12/20  Iteration 2060/3560 Training loss: 1.2592 1.3894 sec/batch\n",
      "Epoch 12/20  Iteration 2061/3560 Training loss: 1.2590 1.2771 sec/batch\n",
      "Epoch 12/20  Iteration 2062/3560 Training loss: 1.2589 1.2637 sec/batch\n",
      "Epoch 12/20  Iteration 2063/3560 Training loss: 1.2587 1.2605 sec/batch\n",
      "Epoch 12/20  Iteration 2064/3560 Training loss: 1.2584 1.3350 sec/batch\n",
      "Epoch 12/20  Iteration 2065/3560 Training loss: 1.2584 1.2608 sec/batch\n",
      "Epoch 12/20  Iteration 2066/3560 Training loss: 1.2583 1.2664 sec/batch\n",
      "Epoch 12/20  Iteration 2067/3560 Training loss: 1.2581 1.2563 sec/batch\n",
      "Epoch 12/20  Iteration 2068/3560 Training loss: 1.2581 1.2903 sec/batch\n",
      "Epoch 12/20  Iteration 2069/3560 Training loss: 1.2578 1.2274 sec/batch\n",
      "Epoch 12/20  Iteration 2070/3560 Training loss: 1.2577 1.2431 sec/batch\n",
      "Epoch 12/20  Iteration 2071/3560 Training loss: 1.2575 1.2463 sec/batch\n",
      "Epoch 12/20  Iteration 2072/3560 Training loss: 1.2572 1.2746 sec/batch\n",
      "Epoch 12/20  Iteration 2073/3560 Training loss: 1.2569 1.2582 sec/batch\n",
      "Epoch 12/20  Iteration 2074/3560 Training loss: 1.2565 1.2929 sec/batch\n",
      "Epoch 12/20  Iteration 2075/3560 Training loss: 1.2565 1.2581 sec/batch\n",
      "Epoch 12/20  Iteration 2076/3560 Training loss: 1.2564 1.3261 sec/batch\n",
      "Epoch 12/20  Iteration 2077/3560 Training loss: 1.2563 1.2539 sec/batch\n",
      "Epoch 12/20  Iteration 2078/3560 Training loss: 1.2562 1.2733 sec/batch\n",
      "Epoch 12/20  Iteration 2079/3560 Training loss: 1.2562 1.2483 sec/batch\n",
      "Epoch 12/20  Iteration 2080/3560 Training loss: 1.2557 1.2567 sec/batch\n",
      "Epoch 12/20  Iteration 2081/3560 Training loss: 1.2552 1.2852 sec/batch\n",
      "Epoch 12/20  Iteration 2082/3560 Training loss: 1.2551 1.3311 sec/batch\n",
      "Epoch 12/20  Iteration 2083/3560 Training loss: 1.2550 1.2514 sec/batch\n",
      "Epoch 12/20  Iteration 2084/3560 Training loss: 1.2546 1.2755 sec/batch\n",
      "Epoch 12/20  Iteration 2085/3560 Training loss: 1.2546 1.2380 sec/batch\n",
      "Epoch 12/20  Iteration 2086/3560 Training loss: 1.2544 1.2273 sec/batch\n",
      "Epoch 12/20  Iteration 2087/3560 Training loss: 1.2541 1.2620 sec/batch\n",
      "Epoch 12/20  Iteration 2088/3560 Training loss: 1.2538 1.2384 sec/batch\n",
      "Epoch 12/20  Iteration 2089/3560 Training loss: 1.2533 1.2469 sec/batch\n",
      "Epoch 12/20  Iteration 2090/3560 Training loss: 1.2531 1.2495 sec/batch\n",
      "Epoch 12/20  Iteration 2091/3560 Training loss: 1.2531 1.2883 sec/batch\n",
      "Epoch 12/20  Iteration 2092/3560 Training loss: 1.2530 1.2951 sec/batch\n",
      "Epoch 12/20  Iteration 2093/3560 Training loss: 1.2530 1.2473 sec/batch\n",
      "Epoch 12/20  Iteration 2094/3560 Training loss: 1.2530 1.2433 sec/batch\n",
      "Epoch 12/20  Iteration 2095/3560 Training loss: 1.2531 1.2456 sec/batch\n",
      "Epoch 12/20  Iteration 2096/3560 Training loss: 1.2532 1.2410 sec/batch\n",
      "Epoch 12/20  Iteration 2097/3560 Training loss: 1.2532 1.2554 sec/batch\n",
      "Epoch 12/20  Iteration 2098/3560 Training loss: 1.2531 1.2516 sec/batch\n",
      "Epoch 12/20  Iteration 2099/3560 Training loss: 1.2534 1.2366 sec/batch\n",
      "Epoch 12/20  Iteration 2100/3560 Training loss: 1.2534 1.3440 sec/batch\n",
      "Epoch 12/20  Iteration 2101/3560 Training loss: 1.2533 1.2573 sec/batch\n",
      "Epoch 12/20  Iteration 2102/3560 Training loss: 1.2535 1.2348 sec/batch\n",
      "Epoch 12/20  Iteration 2103/3560 Training loss: 1.2534 1.2498 sec/batch\n",
      "Epoch 12/20  Iteration 2104/3560 Training loss: 1.2535 1.2402 sec/batch\n",
      "Epoch 12/20  Iteration 2105/3560 Training loss: 1.2535 1.2378 sec/batch\n",
      "Epoch 12/20  Iteration 2106/3560 Training loss: 1.2537 1.2567 sec/batch\n",
      "Epoch 12/20  Iteration 2107/3560 Training loss: 1.2538 1.2290 sec/batch\n",
      "Epoch 12/20  Iteration 2108/3560 Training loss: 1.2537 1.3068 sec/batch\n",
      "Epoch 12/20  Iteration 2109/3560 Training loss: 1.2534 1.2422 sec/batch\n",
      "Epoch 12/20  Iteration 2110/3560 Training loss: 1.2532 1.2813 sec/batch\n",
      "Epoch 12/20  Iteration 2111/3560 Training loss: 1.2533 1.2541 sec/batch\n",
      "Epoch 12/20  Iteration 2112/3560 Training loss: 1.2532 1.2412 sec/batch\n",
      "Epoch 12/20  Iteration 2113/3560 Training loss: 1.2532 1.2438 sec/batch\n",
      "Epoch 12/20  Iteration 2114/3560 Training loss: 1.2531 1.2462 sec/batch\n",
      "Epoch 12/20  Iteration 2115/3560 Training loss: 1.2531 1.2383 sec/batch\n",
      "Epoch 12/20  Iteration 2116/3560 Training loss: 1.2529 1.3303 sec/batch\n",
      "Epoch 12/20  Iteration 2117/3560 Training loss: 1.2528 1.2386 sec/batch\n",
      "Epoch 12/20  Iteration 2118/3560 Training loss: 1.2529 1.2336 sec/batch\n",
      "Epoch 12/20  Iteration 2119/3560 Training loss: 1.2531 1.2807 sec/batch\n",
      "Epoch 12/20  Iteration 2120/3560 Training loss: 1.2531 1.2856 sec/batch\n",
      "Epoch 12/20  Iteration 2121/3560 Training loss: 1.2531 1.2610 sec/batch\n",
      "Epoch 12/20  Iteration 2122/3560 Training loss: 1.2531 1.2486 sec/batch\n",
      "Epoch 12/20  Iteration 2123/3560 Training loss: 1.2531 1.2478 sec/batch\n",
      "Epoch 12/20  Iteration 2124/3560 Training loss: 1.2530 1.3129 sec/batch\n",
      "Epoch 12/20  Iteration 2125/3560 Training loss: 1.2532 1.2481 sec/batch\n",
      "Epoch 12/20  Iteration 2126/3560 Training loss: 1.2536 1.2406 sec/batch\n",
      "Epoch 12/20  Iteration 2127/3560 Training loss: 1.2536 1.2400 sec/batch\n",
      "Epoch 12/20  Iteration 2128/3560 Training loss: 1.2537 1.2557 sec/batch\n",
      "Epoch 12/20  Iteration 2129/3560 Training loss: 1.2537 1.3180 sec/batch\n",
      "Epoch 12/20  Iteration 2130/3560 Training loss: 1.2536 1.2660 sec/batch\n",
      "Epoch 12/20  Iteration 2131/3560 Training loss: 1.2537 1.2472 sec/batch\n",
      "Epoch 12/20  Iteration 2132/3560 Training loss: 1.2538 1.2959 sec/batch\n",
      "Epoch 12/20  Iteration 2133/3560 Training loss: 1.2538 1.2484 sec/batch\n",
      "Epoch 12/20  Iteration 2134/3560 Training loss: 1.2536 1.2474 sec/batch\n",
      "Epoch 12/20  Iteration 2135/3560 Training loss: 1.2535 1.2648 sec/batch\n",
      "Epoch 12/20  Iteration 2136/3560 Training loss: 1.2537 1.2433 sec/batch\n",
      "Epoch 13/20  Iteration 2137/3560 Training loss: 1.3944 1.2621 sec/batch\n",
      "Epoch 13/20  Iteration 2138/3560 Training loss: 1.3369 1.3157 sec/batch\n",
      "Epoch 13/20  Iteration 2139/3560 Training loss: 1.3114 1.5166 sec/batch\n",
      "Epoch 13/20  Iteration 2140/3560 Training loss: 1.3028 1.6601 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2141/3560 Training loss: 1.2900 1.6275 sec/batch\n",
      "Epoch 13/20  Iteration 2142/3560 Training loss: 1.2778 1.6022 sec/batch\n",
      "Epoch 13/20  Iteration 2143/3560 Training loss: 1.2747 1.6071 sec/batch\n",
      "Epoch 13/20  Iteration 2144/3560 Training loss: 1.2720 1.3570 sec/batch\n",
      "Epoch 13/20  Iteration 2145/3560 Training loss: 1.2705 1.2451 sec/batch\n",
      "Epoch 13/20  Iteration 2146/3560 Training loss: 1.2676 1.2603 sec/batch\n",
      "Epoch 13/20  Iteration 2147/3560 Training loss: 1.2626 1.3188 sec/batch\n",
      "Epoch 13/20  Iteration 2148/3560 Training loss: 1.2617 1.2419 sec/batch\n",
      "Epoch 13/20  Iteration 2149/3560 Training loss: 1.2607 1.2516 sec/batch\n",
      "Epoch 13/20  Iteration 2150/3560 Training loss: 1.2608 1.2411 sec/batch\n",
      "Epoch 13/20  Iteration 2151/3560 Training loss: 1.2587 1.2467 sec/batch\n",
      "Epoch 13/20  Iteration 2152/3560 Training loss: 1.2570 1.2400 sec/batch\n",
      "Epoch 13/20  Iteration 2153/3560 Training loss: 1.2563 1.2340 sec/batch\n",
      "Epoch 13/20  Iteration 2154/3560 Training loss: 1.2572 1.2392 sec/batch\n",
      "Epoch 13/20  Iteration 2155/3560 Training loss: 1.2570 1.2630 sec/batch\n",
      "Epoch 13/20  Iteration 2156/3560 Training loss: 1.2576 1.3080 sec/batch\n",
      "Epoch 13/20  Iteration 2157/3560 Training loss: 1.2568 1.2383 sec/batch\n",
      "Epoch 13/20  Iteration 2158/3560 Training loss: 1.2573 1.2452 sec/batch\n",
      "Epoch 13/20  Iteration 2159/3560 Training loss: 1.2566 1.2637 sec/batch\n",
      "Epoch 13/20  Iteration 2160/3560 Training loss: 1.2565 1.2434 sec/batch\n",
      "Epoch 13/20  Iteration 2161/3560 Training loss: 1.2562 1.2409 sec/batch\n",
      "Epoch 13/20  Iteration 2162/3560 Training loss: 1.2539 1.2504 sec/batch\n",
      "Epoch 13/20  Iteration 2163/3560 Training loss: 1.2526 1.2694 sec/batch\n",
      "Epoch 13/20  Iteration 2164/3560 Training loss: 1.2530 1.2442 sec/batch\n",
      "Epoch 13/20  Iteration 2165/3560 Training loss: 1.2526 1.2552 sec/batch\n",
      "Epoch 13/20  Iteration 2166/3560 Training loss: 1.2525 1.2752 sec/batch\n",
      "Epoch 13/20  Iteration 2167/3560 Training loss: 1.2515 1.2752 sec/batch\n",
      "Epoch 13/20  Iteration 2168/3560 Training loss: 1.2504 1.2373 sec/batch\n",
      "Epoch 13/20  Iteration 2169/3560 Training loss: 1.2502 1.2426 sec/batch\n",
      "Epoch 13/20  Iteration 2170/3560 Training loss: 1.2504 1.2424 sec/batch\n",
      "Epoch 13/20  Iteration 2171/3560 Training loss: 1.2498 1.2904 sec/batch\n",
      "Epoch 13/20  Iteration 2172/3560 Training loss: 1.2498 1.2451 sec/batch\n",
      "Epoch 13/20  Iteration 2173/3560 Training loss: 1.2489 1.2338 sec/batch\n",
      "Epoch 13/20  Iteration 2174/3560 Training loss: 1.2475 1.2323 sec/batch\n",
      "Epoch 13/20  Iteration 2175/3560 Training loss: 1.2463 1.2845 sec/batch\n",
      "Epoch 13/20  Iteration 2176/3560 Training loss: 1.2459 1.2529 sec/batch\n",
      "Epoch 13/20  Iteration 2177/3560 Training loss: 1.2451 1.2466 sec/batch\n",
      "Epoch 13/20  Iteration 2178/3560 Training loss: 1.2458 1.2422 sec/batch\n",
      "Epoch 13/20  Iteration 2179/3560 Training loss: 1.2454 1.2577 sec/batch\n",
      "Epoch 13/20  Iteration 2180/3560 Training loss: 1.2447 1.2319 sec/batch\n",
      "Epoch 13/20  Iteration 2181/3560 Training loss: 1.2451 1.2338 sec/batch\n",
      "Epoch 13/20  Iteration 2182/3560 Training loss: 1.2442 1.2236 sec/batch\n",
      "Epoch 13/20  Iteration 2183/3560 Training loss: 1.2438 1.2312 sec/batch\n",
      "Epoch 13/20  Iteration 2184/3560 Training loss: 1.2433 1.2308 sec/batch\n",
      "Epoch 13/20  Iteration 2185/3560 Training loss: 1.2433 1.3396 sec/batch\n",
      "Epoch 13/20  Iteration 2186/3560 Training loss: 1.2432 1.2508 sec/batch\n",
      "Epoch 13/20  Iteration 2187/3560 Training loss: 1.2426 1.2749 sec/batch\n",
      "Epoch 13/20  Iteration 2188/3560 Training loss: 1.2430 1.2398 sec/batch\n",
      "Epoch 13/20  Iteration 2189/3560 Training loss: 1.2428 1.2349 sec/batch\n",
      "Epoch 13/20  Iteration 2190/3560 Training loss: 1.2428 1.2315 sec/batch\n",
      "Epoch 13/20  Iteration 2191/3560 Training loss: 1.2426 1.2577 sec/batch\n",
      "Epoch 13/20  Iteration 2192/3560 Training loss: 1.2427 1.2345 sec/batch\n",
      "Epoch 13/20  Iteration 2193/3560 Training loss: 1.2430 1.2426 sec/batch\n",
      "Epoch 13/20  Iteration 2194/3560 Training loss: 1.2427 1.2842 sec/batch\n",
      "Epoch 13/20  Iteration 2195/3560 Training loss: 1.2422 1.3184 sec/batch\n",
      "Epoch 13/20  Iteration 2196/3560 Training loss: 1.2427 1.2800 sec/batch\n",
      "Epoch 13/20  Iteration 2197/3560 Training loss: 1.2426 1.2722 sec/batch\n",
      "Epoch 13/20  Iteration 2198/3560 Training loss: 1.2432 1.2516 sec/batch\n",
      "Epoch 13/20  Iteration 2199/3560 Training loss: 1.2432 1.2662 sec/batch\n",
      "Epoch 13/20  Iteration 2200/3560 Training loss: 1.2432 1.2509 sec/batch\n",
      "Epoch 13/20  Iteration 2201/3560 Training loss: 1.2432 1.2671 sec/batch\n",
      "Epoch 13/20  Iteration 2202/3560 Training loss: 1.2434 1.3261 sec/batch\n",
      "Epoch 13/20  Iteration 2203/3560 Training loss: 1.2436 1.3175 sec/batch\n",
      "Epoch 13/20  Iteration 2204/3560 Training loss: 1.2434 1.3276 sec/batch\n",
      "Epoch 13/20  Iteration 2205/3560 Training loss: 1.2436 1.2526 sec/batch\n",
      "Epoch 13/20  Iteration 2206/3560 Training loss: 1.2436 1.2827 sec/batch\n",
      "Epoch 13/20  Iteration 2207/3560 Training loss: 1.2442 1.2363 sec/batch\n",
      "Epoch 13/20  Iteration 2208/3560 Training loss: 1.2444 1.2496 sec/batch\n",
      "Epoch 13/20  Iteration 2209/3560 Training loss: 1.2448 1.2831 sec/batch\n",
      "Epoch 13/20  Iteration 2210/3560 Training loss: 1.2444 1.2757 sec/batch\n",
      "Epoch 13/20  Iteration 2211/3560 Training loss: 1.2444 1.3030 sec/batch\n",
      "Epoch 13/20  Iteration 2212/3560 Training loss: 1.2447 1.2425 sec/batch\n",
      "Epoch 13/20  Iteration 2213/3560 Training loss: 1.2446 1.3161 sec/batch\n",
      "Epoch 13/20  Iteration 2214/3560 Training loss: 1.2445 1.2660 sec/batch\n",
      "Epoch 13/20  Iteration 2215/3560 Training loss: 1.2438 1.2507 sec/batch\n",
      "Epoch 13/20  Iteration 2216/3560 Training loss: 1.2438 1.2492 sec/batch\n",
      "Epoch 13/20  Iteration 2217/3560 Training loss: 1.2434 1.2563 sec/batch\n",
      "Epoch 13/20  Iteration 2218/3560 Training loss: 1.2432 1.2561 sec/batch\n",
      "Epoch 13/20  Iteration 2219/3560 Training loss: 1.2428 1.3432 sec/batch\n",
      "Epoch 13/20  Iteration 2220/3560 Training loss: 1.2428 1.2420 sec/batch\n",
      "Epoch 13/20  Iteration 2221/3560 Training loss: 1.2424 1.2443 sec/batch\n",
      "Epoch 13/20  Iteration 2222/3560 Training loss: 1.2424 1.2642 sec/batch\n",
      "Epoch 13/20  Iteration 2223/3560 Training loss: 1.2422 1.2976 sec/batch\n",
      "Epoch 13/20  Iteration 2224/3560 Training loss: 1.2418 1.2629 sec/batch\n",
      "Epoch 13/20  Iteration 2225/3560 Training loss: 1.2413 1.2608 sec/batch\n",
      "Epoch 13/20  Iteration 2226/3560 Training loss: 1.2414 1.2560 sec/batch\n",
      "Epoch 13/20  Iteration 2227/3560 Training loss: 1.2412 1.2985 sec/batch\n",
      "Epoch 13/20  Iteration 2228/3560 Training loss: 1.2410 1.2405 sec/batch\n",
      "Epoch 13/20  Iteration 2229/3560 Training loss: 1.2406 1.2658 sec/batch\n",
      "Epoch 13/20  Iteration 2230/3560 Training loss: 1.2401 1.2340 sec/batch\n",
      "Epoch 13/20  Iteration 2231/3560 Training loss: 1.2398 1.2605 sec/batch\n",
      "Epoch 13/20  Iteration 2232/3560 Training loss: 1.2398 1.3240 sec/batch\n",
      "Epoch 13/20  Iteration 2233/3560 Training loss: 1.2398 1.2633 sec/batch\n",
      "Epoch 13/20  Iteration 2234/3560 Training loss: 1.2394 1.2593 sec/batch\n",
      "Epoch 13/20  Iteration 2235/3560 Training loss: 1.2391 1.3071 sec/batch\n",
      "Epoch 13/20  Iteration 2236/3560 Training loss: 1.2387 1.2426 sec/batch\n",
      "Epoch 13/20  Iteration 2237/3560 Training loss: 1.2385 1.2796 sec/batch\n",
      "Epoch 13/20  Iteration 2238/3560 Training loss: 1.2383 1.2640 sec/batch\n",
      "Epoch 13/20  Iteration 2239/3560 Training loss: 1.2381 1.2357 sec/batch\n",
      "Epoch 13/20  Iteration 2240/3560 Training loss: 1.2379 1.2405 sec/batch\n",
      "Epoch 13/20  Iteration 2241/3560 Training loss: 1.2377 1.2751 sec/batch\n",
      "Epoch 13/20  Iteration 2242/3560 Training loss: 1.2375 1.2733 sec/batch\n",
      "Epoch 13/20  Iteration 2243/3560 Training loss: 1.2374 1.2951 sec/batch\n",
      "Epoch 13/20  Iteration 2244/3560 Training loss: 1.2374 1.2255 sec/batch\n",
      "Epoch 13/20  Iteration 2245/3560 Training loss: 1.2372 1.2535 sec/batch\n",
      "Epoch 13/20  Iteration 2246/3560 Training loss: 1.2372 1.2309 sec/batch\n",
      "Epoch 13/20  Iteration 2247/3560 Training loss: 1.2369 1.2405 sec/batch\n",
      "Epoch 13/20  Iteration 2248/3560 Training loss: 1.2367 1.2322 sec/batch\n",
      "Epoch 13/20  Iteration 2249/3560 Training loss: 1.2366 1.2375 sec/batch\n",
      "Epoch 13/20  Iteration 2250/3560 Training loss: 1.2364 1.2331 sec/batch\n",
      "Epoch 13/20  Iteration 2251/3560 Training loss: 1.2361 1.3710 sec/batch\n",
      "Epoch 13/20  Iteration 2252/3560 Training loss: 1.2358 1.3060 sec/batch\n",
      "Epoch 13/20  Iteration 2253/3560 Training loss: 1.2357 1.2722 sec/batch\n",
      "Epoch 13/20  Iteration 2254/3560 Training loss: 1.2357 1.2470 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2255/3560 Training loss: 1.2355 1.2460 sec/batch\n",
      "Epoch 13/20  Iteration 2256/3560 Training loss: 1.2354 1.2660 sec/batch\n",
      "Epoch 13/20  Iteration 2257/3560 Training loss: 1.2353 1.2564 sec/batch\n",
      "Epoch 13/20  Iteration 2258/3560 Training loss: 1.2350 1.2460 sec/batch\n",
      "Epoch 13/20  Iteration 2259/3560 Training loss: 1.2346 1.3053 sec/batch\n",
      "Epoch 13/20  Iteration 2260/3560 Training loss: 1.2346 1.2444 sec/batch\n",
      "Epoch 13/20  Iteration 2261/3560 Training loss: 1.2344 1.3024 sec/batch\n",
      "Epoch 13/20  Iteration 2262/3560 Training loss: 1.2340 1.2363 sec/batch\n",
      "Epoch 13/20  Iteration 2263/3560 Training loss: 1.2340 1.2395 sec/batch\n",
      "Epoch 13/20  Iteration 2264/3560 Training loss: 1.2339 1.2313 sec/batch\n",
      "Epoch 13/20  Iteration 2265/3560 Training loss: 1.2337 1.2548 sec/batch\n",
      "Epoch 13/20  Iteration 2266/3560 Training loss: 1.2333 1.2568 sec/batch\n",
      "Epoch 13/20  Iteration 2267/3560 Training loss: 1.2329 1.3308 sec/batch\n",
      "Epoch 13/20  Iteration 2268/3560 Training loss: 1.2326 1.2328 sec/batch\n",
      "Epoch 13/20  Iteration 2269/3560 Training loss: 1.2327 1.2414 sec/batch\n",
      "Epoch 13/20  Iteration 2270/3560 Training loss: 1.2326 1.3422 sec/batch\n",
      "Epoch 13/20  Iteration 2271/3560 Training loss: 1.2325 1.2413 sec/batch\n",
      "Epoch 13/20  Iteration 2272/3560 Training loss: 1.2325 1.2432 sec/batch\n",
      "Epoch 13/20  Iteration 2273/3560 Training loss: 1.2325 1.2593 sec/batch\n",
      "Epoch 13/20  Iteration 2274/3560 Training loss: 1.2326 1.2569 sec/batch\n",
      "Epoch 13/20  Iteration 2275/3560 Training loss: 1.2326 1.2925 sec/batch\n",
      "Epoch 13/20  Iteration 2276/3560 Training loss: 1.2325 1.2185 sec/batch\n",
      "Epoch 13/20  Iteration 2277/3560 Training loss: 1.2328 1.2477 sec/batch\n",
      "Epoch 13/20  Iteration 2278/3560 Training loss: 1.2328 1.2433 sec/batch\n",
      "Epoch 13/20  Iteration 2279/3560 Training loss: 1.2327 1.2543 sec/batch\n",
      "Epoch 13/20  Iteration 2280/3560 Training loss: 1.2328 1.4373 sec/batch\n",
      "Epoch 13/20  Iteration 2281/3560 Training loss: 1.2327 1.6531 sec/batch\n",
      "Epoch 13/20  Iteration 2282/3560 Training loss: 1.2328 1.6087 sec/batch\n",
      "Epoch 13/20  Iteration 2283/3560 Training loss: 1.2327 1.6109 sec/batch\n",
      "Epoch 13/20  Iteration 2284/3560 Training loss: 1.2329 1.6199 sec/batch\n",
      "Epoch 13/20  Iteration 2285/3560 Training loss: 1.2331 1.3824 sec/batch\n",
      "Epoch 13/20  Iteration 2286/3560 Training loss: 1.2329 1.2374 sec/batch\n",
      "Epoch 13/20  Iteration 2287/3560 Training loss: 1.2326 1.2373 sec/batch\n",
      "Epoch 13/20  Iteration 2288/3560 Training loss: 1.2324 1.3053 sec/batch\n",
      "Epoch 13/20  Iteration 2289/3560 Training loss: 1.2325 1.2415 sec/batch\n",
      "Epoch 13/20  Iteration 2290/3560 Training loss: 1.2324 1.2749 sec/batch\n",
      "Epoch 13/20  Iteration 2291/3560 Training loss: 1.2324 1.2581 sec/batch\n",
      "Epoch 13/20  Iteration 2292/3560 Training loss: 1.2323 1.2278 sec/batch\n",
      "Epoch 13/20  Iteration 2293/3560 Training loss: 1.2323 1.2335 sec/batch\n",
      "Epoch 13/20  Iteration 2294/3560 Training loss: 1.2322 1.2508 sec/batch\n",
      "Epoch 13/20  Iteration 2295/3560 Training loss: 1.2320 1.2618 sec/batch\n",
      "Epoch 13/20  Iteration 2296/3560 Training loss: 1.2321 1.2475 sec/batch\n",
      "Epoch 13/20  Iteration 2297/3560 Training loss: 1.2322 1.2890 sec/batch\n",
      "Epoch 13/20  Iteration 2298/3560 Training loss: 1.2322 1.2751 sec/batch\n",
      "Epoch 13/20  Iteration 2299/3560 Training loss: 1.2321 1.2342 sec/batch\n",
      "Epoch 13/20  Iteration 2300/3560 Training loss: 1.2321 1.2182 sec/batch\n",
      "Epoch 13/20  Iteration 2301/3560 Training loss: 1.2320 1.2358 sec/batch\n",
      "Epoch 13/20  Iteration 2302/3560 Training loss: 1.2319 1.2463 sec/batch\n",
      "Epoch 13/20  Iteration 2303/3560 Training loss: 1.2321 1.2378 sec/batch\n",
      "Epoch 13/20  Iteration 2304/3560 Training loss: 1.2325 1.2425 sec/batch\n",
      "Epoch 13/20  Iteration 2305/3560 Training loss: 1.2325 1.2300 sec/batch\n",
      "Epoch 13/20  Iteration 2306/3560 Training loss: 1.2325 1.2780 sec/batch\n",
      "Epoch 13/20  Iteration 2307/3560 Training loss: 1.2325 1.2983 sec/batch\n",
      "Epoch 13/20  Iteration 2308/3560 Training loss: 1.2324 1.2311 sec/batch\n",
      "Epoch 13/20  Iteration 2309/3560 Training loss: 1.2325 1.2279 sec/batch\n",
      "Epoch 13/20  Iteration 2310/3560 Training loss: 1.2325 1.2362 sec/batch\n",
      "Epoch 13/20  Iteration 2311/3560 Training loss: 1.2326 1.2281 sec/batch\n",
      "Epoch 13/20  Iteration 2312/3560 Training loss: 1.2324 1.2446 sec/batch\n",
      "Epoch 13/20  Iteration 2313/3560 Training loss: 1.2323 1.2436 sec/batch\n",
      "Epoch 13/20  Iteration 2314/3560 Training loss: 1.2325 1.3046 sec/batch\n",
      "Epoch 14/20  Iteration 2315/3560 Training loss: 1.3704 1.2418 sec/batch\n",
      "Epoch 14/20  Iteration 2316/3560 Training loss: 1.3089 1.2451 sec/batch\n",
      "Epoch 14/20  Iteration 2317/3560 Training loss: 1.2841 1.3017 sec/batch\n",
      "Epoch 14/20  Iteration 2318/3560 Training loss: 1.2757 1.2464 sec/batch\n",
      "Epoch 14/20  Iteration 2319/3560 Training loss: 1.2615 1.2366 sec/batch\n",
      "Epoch 14/20  Iteration 2320/3560 Training loss: 1.2483 1.2349 sec/batch\n",
      "Epoch 14/20  Iteration 2321/3560 Training loss: 1.2472 1.2285 sec/batch\n",
      "Epoch 14/20  Iteration 2322/3560 Training loss: 1.2440 1.2864 sec/batch\n",
      "Epoch 14/20  Iteration 2323/3560 Training loss: 1.2426 1.2620 sec/batch\n",
      "Epoch 14/20  Iteration 2324/3560 Training loss: 1.2404 1.2259 sec/batch\n",
      "Epoch 14/20  Iteration 2325/3560 Training loss: 1.2351 1.2424 sec/batch\n",
      "Epoch 14/20  Iteration 2326/3560 Training loss: 1.2350 1.2957 sec/batch\n",
      "Epoch 14/20  Iteration 2327/3560 Training loss: 1.2350 1.2649 sec/batch\n",
      "Epoch 14/20  Iteration 2328/3560 Training loss: 1.2354 1.2435 sec/batch\n",
      "Epoch 14/20  Iteration 2329/3560 Training loss: 1.2336 1.2271 sec/batch\n",
      "Epoch 14/20  Iteration 2330/3560 Training loss: 1.2313 1.2715 sec/batch\n",
      "Epoch 14/20  Iteration 2331/3560 Training loss: 1.2309 1.2362 sec/batch\n",
      "Epoch 14/20  Iteration 2332/3560 Training loss: 1.2321 1.2309 sec/batch\n",
      "Epoch 14/20  Iteration 2333/3560 Training loss: 1.2323 1.2362 sec/batch\n",
      "Epoch 14/20  Iteration 2334/3560 Training loss: 1.2332 1.2420 sec/batch\n",
      "Epoch 14/20  Iteration 2335/3560 Training loss: 1.2330 1.2440 sec/batch\n",
      "Epoch 14/20  Iteration 2336/3560 Training loss: 1.2333 1.3236 sec/batch\n",
      "Epoch 14/20  Iteration 2337/3560 Training loss: 1.2320 1.2630 sec/batch\n",
      "Epoch 14/20  Iteration 2338/3560 Training loss: 1.2325 1.2805 sec/batch\n",
      "Epoch 14/20  Iteration 2339/3560 Training loss: 1.2322 1.2831 sec/batch\n",
      "Epoch 14/20  Iteration 2340/3560 Training loss: 1.2301 1.2578 sec/batch\n",
      "Epoch 14/20  Iteration 2341/3560 Training loss: 1.2285 1.2568 sec/batch\n",
      "Epoch 14/20  Iteration 2342/3560 Training loss: 1.2287 1.2317 sec/batch\n",
      "Epoch 14/20  Iteration 2343/3560 Training loss: 1.2285 1.3397 sec/batch\n",
      "Epoch 14/20  Iteration 2344/3560 Training loss: 1.2286 1.2989 sec/batch\n",
      "Epoch 14/20  Iteration 2345/3560 Training loss: 1.2276 1.3310 sec/batch\n",
      "Epoch 14/20  Iteration 2346/3560 Training loss: 1.2267 1.2893 sec/batch\n",
      "Epoch 14/20  Iteration 2347/3560 Training loss: 1.2267 1.2945 sec/batch\n",
      "Epoch 14/20  Iteration 2348/3560 Training loss: 1.2268 1.2305 sec/batch\n",
      "Epoch 14/20  Iteration 2349/3560 Training loss: 1.2263 1.2338 sec/batch\n",
      "Epoch 14/20  Iteration 2350/3560 Training loss: 1.2262 1.2502 sec/batch\n",
      "Epoch 14/20  Iteration 2351/3560 Training loss: 1.2254 1.2361 sec/batch\n",
      "Epoch 14/20  Iteration 2352/3560 Training loss: 1.2240 1.2414 sec/batch\n",
      "Epoch 14/20  Iteration 2353/3560 Training loss: 1.2229 1.2340 sec/batch\n",
      "Epoch 14/20  Iteration 2354/3560 Training loss: 1.2226 1.2575 sec/batch\n",
      "Epoch 14/20  Iteration 2355/3560 Training loss: 1.2218 1.3489 sec/batch\n",
      "Epoch 14/20  Iteration 2356/3560 Training loss: 1.2227 1.2472 sec/batch\n",
      "Epoch 14/20  Iteration 2357/3560 Training loss: 1.2222 1.2559 sec/batch\n",
      "Epoch 14/20  Iteration 2358/3560 Training loss: 1.2214 1.2713 sec/batch\n",
      "Epoch 14/20  Iteration 2359/3560 Training loss: 1.2217 1.2391 sec/batch\n",
      "Epoch 14/20  Iteration 2360/3560 Training loss: 1.2212 1.2392 sec/batch\n",
      "Epoch 14/20  Iteration 2361/3560 Training loss: 1.2209 1.2751 sec/batch\n",
      "Epoch 14/20  Iteration 2362/3560 Training loss: 1.2205 1.2728 sec/batch\n",
      "Epoch 14/20  Iteration 2363/3560 Training loss: 1.2204 1.2875 sec/batch\n",
      "Epoch 14/20  Iteration 2364/3560 Training loss: 1.2207 1.2924 sec/batch\n",
      "Epoch 14/20  Iteration 2365/3560 Training loss: 1.2200 1.2497 sec/batch\n",
      "Epoch 14/20  Iteration 2366/3560 Training loss: 1.2205 1.2508 sec/batch\n",
      "Epoch 14/20  Iteration 2367/3560 Training loss: 1.2204 1.2341 sec/batch\n",
      "Epoch 14/20  Iteration 2368/3560 Training loss: 1.2203 1.2499 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2369/3560 Training loss: 1.2202 1.2492 sec/batch\n",
      "Epoch 14/20  Iteration 2370/3560 Training loss: 1.2203 1.2741 sec/batch\n",
      "Epoch 14/20  Iteration 2371/3560 Training loss: 1.2205 1.2609 sec/batch\n",
      "Epoch 14/20  Iteration 2372/3560 Training loss: 1.2202 1.3395 sec/batch\n",
      "Epoch 14/20  Iteration 2373/3560 Training loss: 1.2196 1.2424 sec/batch\n",
      "Epoch 14/20  Iteration 2374/3560 Training loss: 1.2202 1.2647 sec/batch\n",
      "Epoch 14/20  Iteration 2375/3560 Training loss: 1.2200 1.2433 sec/batch\n",
      "Epoch 14/20  Iteration 2376/3560 Training loss: 1.2207 1.2731 sec/batch\n",
      "Epoch 14/20  Iteration 2377/3560 Training loss: 1.2208 1.2410 sec/batch\n",
      "Epoch 14/20  Iteration 2378/3560 Training loss: 1.2208 1.2649 sec/batch\n",
      "Epoch 14/20  Iteration 2379/3560 Training loss: 1.2209 1.2534 sec/batch\n",
      "Epoch 14/20  Iteration 2380/3560 Training loss: 1.2211 1.2467 sec/batch\n",
      "Epoch 14/20  Iteration 2381/3560 Training loss: 1.2214 1.2707 sec/batch\n",
      "Epoch 14/20  Iteration 2382/3560 Training loss: 1.2212 1.2899 sec/batch\n",
      "Epoch 14/20  Iteration 2383/3560 Training loss: 1.2213 1.2567 sec/batch\n",
      "Epoch 14/20  Iteration 2384/3560 Training loss: 1.2212 1.2657 sec/batch\n",
      "Epoch 14/20  Iteration 2385/3560 Training loss: 1.2217 1.2724 sec/batch\n",
      "Epoch 14/20  Iteration 2386/3560 Training loss: 1.2221 1.3068 sec/batch\n",
      "Epoch 14/20  Iteration 2387/3560 Training loss: 1.2224 1.2449 sec/batch\n",
      "Epoch 14/20  Iteration 2388/3560 Training loss: 1.2221 1.3141 sec/batch\n",
      "Epoch 14/20  Iteration 2389/3560 Training loss: 1.2219 1.2434 sec/batch\n",
      "Epoch 14/20  Iteration 2390/3560 Training loss: 1.2220 1.2684 sec/batch\n",
      "Epoch 14/20  Iteration 2391/3560 Training loss: 1.2219 1.2403 sec/batch\n",
      "Epoch 14/20  Iteration 2392/3560 Training loss: 1.2218 1.2584 sec/batch\n",
      "Epoch 14/20  Iteration 2393/3560 Training loss: 1.2211 1.2539 sec/batch\n",
      "Epoch 14/20  Iteration 2394/3560 Training loss: 1.2209 1.2804 sec/batch\n",
      "Epoch 14/20  Iteration 2395/3560 Training loss: 1.2206 1.2506 sec/batch\n",
      "Epoch 14/20  Iteration 2396/3560 Training loss: 1.2205 1.2607 sec/batch\n",
      "Epoch 14/20  Iteration 2397/3560 Training loss: 1.2200 1.2621 sec/batch\n",
      "Epoch 14/20  Iteration 2398/3560 Training loss: 1.2200 1.3068 sec/batch\n",
      "Epoch 14/20  Iteration 2399/3560 Training loss: 1.2196 1.2575 sec/batch\n",
      "Epoch 14/20  Iteration 2400/3560 Training loss: 1.2196 1.2604 sec/batch\n",
      "Epoch 14/20  Iteration 2401/3560 Training loss: 1.2194 1.2484 sec/batch\n",
      "Epoch 14/20  Iteration 2402/3560 Training loss: 1.2189 1.3017 sec/batch\n",
      "Epoch 14/20  Iteration 2403/3560 Training loss: 1.2185 1.2566 sec/batch\n",
      "Epoch 14/20  Iteration 2404/3560 Training loss: 1.2186 1.2432 sec/batch\n",
      "Epoch 14/20  Iteration 2405/3560 Training loss: 1.2183 1.2254 sec/batch\n",
      "Epoch 14/20  Iteration 2406/3560 Training loss: 1.2183 1.2833 sec/batch\n",
      "Epoch 14/20  Iteration 2407/3560 Training loss: 1.2179 1.2884 sec/batch\n",
      "Epoch 14/20  Iteration 2408/3560 Training loss: 1.2174 1.2688 sec/batch\n",
      "Epoch 14/20  Iteration 2409/3560 Training loss: 1.2172 1.2517 sec/batch\n",
      "Epoch 14/20  Iteration 2410/3560 Training loss: 1.2173 1.3120 sec/batch\n",
      "Epoch 14/20  Iteration 2411/3560 Training loss: 1.2173 1.2674 sec/batch\n",
      "Epoch 14/20  Iteration 2412/3560 Training loss: 1.2169 1.2611 sec/batch\n",
      "Epoch 14/20  Iteration 2413/3560 Training loss: 1.2166 1.2483 sec/batch\n",
      "Epoch 14/20  Iteration 2414/3560 Training loss: 1.2163 1.2397 sec/batch\n",
      "Epoch 14/20  Iteration 2415/3560 Training loss: 1.2162 1.2379 sec/batch\n",
      "Epoch 14/20  Iteration 2416/3560 Training loss: 1.2160 1.3079 sec/batch\n",
      "Epoch 14/20  Iteration 2417/3560 Training loss: 1.2159 1.2662 sec/batch\n",
      "Epoch 14/20  Iteration 2418/3560 Training loss: 1.2158 1.3049 sec/batch\n",
      "Epoch 14/20  Iteration 2419/3560 Training loss: 1.2157 1.2870 sec/batch\n",
      "Epoch 14/20  Iteration 2420/3560 Training loss: 1.2154 1.2933 sec/batch\n",
      "Epoch 14/20  Iteration 2421/3560 Training loss: 1.2154 1.3411 sec/batch\n",
      "Epoch 14/20  Iteration 2422/3560 Training loss: 1.2154 1.5582 sec/batch\n",
      "Epoch 14/20  Iteration 2423/3560 Training loss: 1.2152 1.5333 sec/batch\n",
      "Epoch 14/20  Iteration 2424/3560 Training loss: 1.2153 1.5951 sec/batch\n",
      "Epoch 14/20  Iteration 2425/3560 Training loss: 1.2150 1.7497 sec/batch\n",
      "Epoch 14/20  Iteration 2426/3560 Training loss: 1.2148 1.5632 sec/batch\n",
      "Epoch 14/20  Iteration 2427/3560 Training loss: 1.2147 1.3027 sec/batch\n",
      "Epoch 14/20  Iteration 2428/3560 Training loss: 1.2145 1.2400 sec/batch\n",
      "Epoch 14/20  Iteration 2429/3560 Training loss: 1.2143 1.2350 sec/batch\n",
      "Epoch 14/20  Iteration 2430/3560 Training loss: 1.2139 1.2553 sec/batch\n",
      "Epoch 14/20  Iteration 2431/3560 Training loss: 1.2140 1.3085 sec/batch\n",
      "Epoch 14/20  Iteration 2432/3560 Training loss: 1.2140 1.2774 sec/batch\n",
      "Epoch 14/20  Iteration 2433/3560 Training loss: 1.2139 1.2851 sec/batch\n",
      "Epoch 14/20  Iteration 2434/3560 Training loss: 1.2139 1.2423 sec/batch\n",
      "Epoch 14/20  Iteration 2435/3560 Training loss: 1.2138 1.2365 sec/batch\n",
      "Epoch 14/20  Iteration 2436/3560 Training loss: 1.2135 1.2279 sec/batch\n",
      "Epoch 14/20  Iteration 2437/3560 Training loss: 1.2131 1.2414 sec/batch\n",
      "Epoch 14/20  Iteration 2438/3560 Training loss: 1.2131 1.2334 sec/batch\n",
      "Epoch 14/20  Iteration 2439/3560 Training loss: 1.2129 1.2389 sec/batch\n",
      "Epoch 14/20  Iteration 2440/3560 Training loss: 1.2125 1.2370 sec/batch\n",
      "Epoch 14/20  Iteration 2441/3560 Training loss: 1.2125 1.3726 sec/batch\n",
      "Epoch 14/20  Iteration 2442/3560 Training loss: 1.2124 1.2363 sec/batch\n",
      "Epoch 14/20  Iteration 2443/3560 Training loss: 1.2121 1.2461 sec/batch\n",
      "Epoch 14/20  Iteration 2444/3560 Training loss: 1.2118 1.2336 sec/batch\n",
      "Epoch 14/20  Iteration 2445/3560 Training loss: 1.2113 1.2283 sec/batch\n",
      "Epoch 14/20  Iteration 2446/3560 Training loss: 1.2112 1.2429 sec/batch\n",
      "Epoch 14/20  Iteration 2447/3560 Training loss: 1.2112 1.2474 sec/batch\n",
      "Epoch 14/20  Iteration 2448/3560 Training loss: 1.2111 1.2253 sec/batch\n",
      "Epoch 14/20  Iteration 2449/3560 Training loss: 1.2111 1.2797 sec/batch\n",
      "Epoch 14/20  Iteration 2450/3560 Training loss: 1.2111 1.2722 sec/batch\n",
      "Epoch 14/20  Iteration 2451/3560 Training loss: 1.2112 1.3181 sec/batch\n",
      "Epoch 14/20  Iteration 2452/3560 Training loss: 1.2113 1.2405 sec/batch\n",
      "Epoch 14/20  Iteration 2453/3560 Training loss: 1.2113 1.2558 sec/batch\n",
      "Epoch 14/20  Iteration 2454/3560 Training loss: 1.2113 1.2432 sec/batch\n",
      "Epoch 14/20  Iteration 2455/3560 Training loss: 1.2117 1.2511 sec/batch\n",
      "Epoch 14/20  Iteration 2456/3560 Training loss: 1.2117 1.2434 sec/batch\n",
      "Epoch 14/20  Iteration 2457/3560 Training loss: 1.2116 1.3250 sec/batch\n",
      "Epoch 14/20  Iteration 2458/3560 Training loss: 1.2118 1.2254 sec/batch\n",
      "Epoch 14/20  Iteration 2459/3560 Training loss: 1.2117 1.2443 sec/batch\n",
      "Epoch 14/20  Iteration 2460/3560 Training loss: 1.2118 1.3280 sec/batch\n",
      "Epoch 14/20  Iteration 2461/3560 Training loss: 1.2118 1.2482 sec/batch\n",
      "Epoch 14/20  Iteration 2462/3560 Training loss: 1.2121 1.2625 sec/batch\n",
      "Epoch 14/20  Iteration 2463/3560 Training loss: 1.2122 1.2602 sec/batch\n",
      "Epoch 14/20  Iteration 2464/3560 Training loss: 1.2121 1.2522 sec/batch\n",
      "Epoch 14/20  Iteration 2465/3560 Training loss: 1.2118 1.2653 sec/batch\n",
      "Epoch 14/20  Iteration 2466/3560 Training loss: 1.2116 1.2654 sec/batch\n",
      "Epoch 14/20  Iteration 2467/3560 Training loss: 1.2117 1.2436 sec/batch\n",
      "Epoch 14/20  Iteration 2468/3560 Training loss: 1.2117 1.2578 sec/batch\n",
      "Epoch 14/20  Iteration 2469/3560 Training loss: 1.2116 1.2761 sec/batch\n",
      "Epoch 14/20  Iteration 2470/3560 Training loss: 1.2116 1.3105 sec/batch\n",
      "Epoch 14/20  Iteration 2471/3560 Training loss: 1.2116 1.2604 sec/batch\n",
      "Epoch 14/20  Iteration 2472/3560 Training loss: 1.2116 1.2824 sec/batch\n",
      "Epoch 14/20  Iteration 2473/3560 Training loss: 1.2114 1.2555 sec/batch\n",
      "Epoch 14/20  Iteration 2474/3560 Training loss: 1.2115 1.2492 sec/batch\n",
      "Epoch 14/20  Iteration 2475/3560 Training loss: 1.2116 1.2317 sec/batch\n",
      "Epoch 14/20  Iteration 2476/3560 Training loss: 1.2116 1.2293 sec/batch\n",
      "Epoch 14/20  Iteration 2477/3560 Training loss: 1.2115 1.2393 sec/batch\n",
      "Epoch 14/20  Iteration 2478/3560 Training loss: 1.2115 1.2470 sec/batch\n",
      "Epoch 14/20  Iteration 2479/3560 Training loss: 1.2114 1.3152 sec/batch\n",
      "Epoch 14/20  Iteration 2480/3560 Training loss: 1.2113 1.2401 sec/batch\n",
      "Epoch 14/20  Iteration 2481/3560 Training loss: 1.2114 1.2725 sec/batch\n",
      "Epoch 14/20  Iteration 2482/3560 Training loss: 1.2118 1.2524 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2483/3560 Training loss: 1.2118 1.2351 sec/batch\n",
      "Epoch 14/20  Iteration 2484/3560 Training loss: 1.2119 1.3255 sec/batch\n",
      "Epoch 14/20  Iteration 2485/3560 Training loss: 1.2119 1.2936 sec/batch\n",
      "Epoch 14/20  Iteration 2486/3560 Training loss: 1.2118 1.2289 sec/batch\n",
      "Epoch 14/20  Iteration 2487/3560 Training loss: 1.2120 1.2498 sec/batch\n",
      "Epoch 14/20  Iteration 2488/3560 Training loss: 1.2120 1.2754 sec/batch\n",
      "Epoch 14/20  Iteration 2489/3560 Training loss: 1.2120 1.3263 sec/batch\n",
      "Epoch 14/20  Iteration 2490/3560 Training loss: 1.2119 1.2682 sec/batch\n",
      "Epoch 14/20  Iteration 2491/3560 Training loss: 1.2119 1.2358 sec/batch\n",
      "Epoch 14/20  Iteration 2492/3560 Training loss: 1.2121 1.2501 sec/batch\n",
      "Epoch 15/20  Iteration 2493/3560 Training loss: 1.3427 1.2520 sec/batch\n",
      "Epoch 15/20  Iteration 2494/3560 Training loss: 1.2889 1.2252 sec/batch\n",
      "Epoch 15/20  Iteration 2495/3560 Training loss: 1.2655 1.2588 sec/batch\n",
      "Epoch 15/20  Iteration 2496/3560 Training loss: 1.2576 1.2565 sec/batch\n",
      "Epoch 15/20  Iteration 2497/3560 Training loss: 1.2454 1.2907 sec/batch\n",
      "Epoch 15/20  Iteration 2498/3560 Training loss: 1.2332 1.3104 sec/batch\n",
      "Epoch 15/20  Iteration 2499/3560 Training loss: 1.2314 1.2855 sec/batch\n",
      "Epoch 15/20  Iteration 2500/3560 Training loss: 1.2274 1.2439 sec/batch\n",
      "Epoch 15/20  Iteration 2501/3560 Training loss: 1.2257 1.2394 sec/batch\n",
      "Epoch 15/20  Iteration 2502/3560 Training loss: 1.2237 1.2491 sec/batch\n",
      "Epoch 15/20  Iteration 2503/3560 Training loss: 1.2198 1.2460 sec/batch\n",
      "Epoch 15/20  Iteration 2504/3560 Training loss: 1.2205 1.2530 sec/batch\n",
      "Epoch 15/20  Iteration 2505/3560 Training loss: 1.2207 1.3004 sec/batch\n",
      "Epoch 15/20  Iteration 2506/3560 Training loss: 1.2203 1.2588 sec/batch\n",
      "Epoch 15/20  Iteration 2507/3560 Training loss: 1.2186 1.2789 sec/batch\n",
      "Epoch 15/20  Iteration 2508/3560 Training loss: 1.2165 1.2868 sec/batch\n",
      "Epoch 15/20  Iteration 2509/3560 Training loss: 1.2158 1.2626 sec/batch\n",
      "Epoch 15/20  Iteration 2510/3560 Training loss: 1.2169 1.2525 sec/batch\n",
      "Epoch 15/20  Iteration 2511/3560 Training loss: 1.2166 1.2535 sec/batch\n",
      "Epoch 15/20  Iteration 2512/3560 Training loss: 1.2172 1.2683 sec/batch\n",
      "Epoch 15/20  Iteration 2513/3560 Training loss: 1.2167 1.2717 sec/batch\n",
      "Epoch 15/20  Iteration 2514/3560 Training loss: 1.2169 1.2454 sec/batch\n",
      "Epoch 15/20  Iteration 2515/3560 Training loss: 1.2159 1.2299 sec/batch\n",
      "Epoch 15/20  Iteration 2516/3560 Training loss: 1.2162 1.2417 sec/batch\n",
      "Epoch 15/20  Iteration 2517/3560 Training loss: 1.2161 1.3129 sec/batch\n",
      "Epoch 15/20  Iteration 2518/3560 Training loss: 1.2139 1.2713 sec/batch\n",
      "Epoch 15/20  Iteration 2519/3560 Training loss: 1.2124 1.2831 sec/batch\n",
      "Epoch 15/20  Iteration 2520/3560 Training loss: 1.2126 1.2721 sec/batch\n",
      "Epoch 15/20  Iteration 2521/3560 Training loss: 1.2124 1.2677 sec/batch\n",
      "Epoch 15/20  Iteration 2522/3560 Training loss: 1.2126 1.2388 sec/batch\n",
      "Epoch 15/20  Iteration 2523/3560 Training loss: 1.2114 1.2471 sec/batch\n",
      "Epoch 15/20  Iteration 2524/3560 Training loss: 1.2104 1.2556 sec/batch\n",
      "Epoch 15/20  Iteration 2525/3560 Training loss: 1.2100 1.2593 sec/batch\n",
      "Epoch 15/20  Iteration 2526/3560 Training loss: 1.2097 1.2627 sec/batch\n",
      "Epoch 15/20  Iteration 2527/3560 Training loss: 1.2091 1.2748 sec/batch\n",
      "Epoch 15/20  Iteration 2528/3560 Training loss: 1.2090 1.2641 sec/batch\n",
      "Epoch 15/20  Iteration 2529/3560 Training loss: 1.2081 1.3061 sec/batch\n",
      "Epoch 15/20  Iteration 2530/3560 Training loss: 1.2068 1.2334 sec/batch\n",
      "Epoch 15/20  Iteration 2531/3560 Training loss: 1.2055 1.2657 sec/batch\n",
      "Epoch 15/20  Iteration 2532/3560 Training loss: 1.2052 1.2541 sec/batch\n",
      "Epoch 15/20  Iteration 2533/3560 Training loss: 1.2046 1.2277 sec/batch\n",
      "Epoch 15/20  Iteration 2534/3560 Training loss: 1.2054 1.2274 sec/batch\n",
      "Epoch 15/20  Iteration 2535/3560 Training loss: 1.2052 1.2451 sec/batch\n",
      "Epoch 15/20  Iteration 2536/3560 Training loss: 1.2044 1.3352 sec/batch\n",
      "Epoch 15/20  Iteration 2537/3560 Training loss: 1.2046 1.2828 sec/batch\n",
      "Epoch 15/20  Iteration 2538/3560 Training loss: 1.2039 1.2550 sec/batch\n",
      "Epoch 15/20  Iteration 2539/3560 Training loss: 1.2035 1.2455 sec/batch\n",
      "Epoch 15/20  Iteration 2540/3560 Training loss: 1.2030 1.2566 sec/batch\n",
      "Epoch 15/20  Iteration 2541/3560 Training loss: 1.2029 1.2616 sec/batch\n",
      "Epoch 15/20  Iteration 2542/3560 Training loss: 1.2032 1.2485 sec/batch\n",
      "Epoch 15/20  Iteration 2543/3560 Training loss: 1.2025 1.2412 sec/batch\n",
      "Epoch 15/20  Iteration 2544/3560 Training loss: 1.2031 1.2386 sec/batch\n",
      "Epoch 15/20  Iteration 2545/3560 Training loss: 1.2029 1.3399 sec/batch\n",
      "Epoch 15/20  Iteration 2546/3560 Training loss: 1.2029 1.3029 sec/batch\n",
      "Epoch 15/20  Iteration 2547/3560 Training loss: 1.2027 1.2452 sec/batch\n",
      "Epoch 15/20  Iteration 2548/3560 Training loss: 1.2028 1.2312 sec/batch\n",
      "Epoch 15/20  Iteration 2549/3560 Training loss: 1.2027 1.2522 sec/batch\n",
      "Epoch 15/20  Iteration 2550/3560 Training loss: 1.2025 1.2389 sec/batch\n",
      "Epoch 15/20  Iteration 2551/3560 Training loss: 1.2020 1.2485 sec/batch\n",
      "Epoch 15/20  Iteration 2552/3560 Training loss: 1.2025 1.2500 sec/batch\n",
      "Epoch 15/20  Iteration 2553/3560 Training loss: 1.2024 1.2731 sec/batch\n",
      "Epoch 15/20  Iteration 2554/3560 Training loss: 1.2030 1.2924 sec/batch\n",
      "Epoch 15/20  Iteration 2555/3560 Training loss: 1.2032 1.3125 sec/batch\n",
      "Epoch 15/20  Iteration 2556/3560 Training loss: 1.2032 1.2513 sec/batch\n",
      "Epoch 15/20  Iteration 2557/3560 Training loss: 1.2031 1.2510 sec/batch\n",
      "Epoch 15/20  Iteration 2558/3560 Training loss: 1.2034 1.2538 sec/batch\n",
      "Epoch 15/20  Iteration 2559/3560 Training loss: 1.2037 1.2422 sec/batch\n",
      "Epoch 15/20  Iteration 2560/3560 Training loss: 1.2035 1.2565 sec/batch\n",
      "Epoch 15/20  Iteration 2561/3560 Training loss: 1.2036 1.2593 sec/batch\n",
      "Epoch 15/20  Iteration 2562/3560 Training loss: 1.2035 1.2525 sec/batch\n",
      "Epoch 15/20  Iteration 2563/3560 Training loss: 1.2041 1.6002 sec/batch\n",
      "Epoch 15/20  Iteration 2564/3560 Training loss: 1.2044 1.6798 sec/batch\n",
      "Epoch 15/20  Iteration 2565/3560 Training loss: 1.2049 1.5507 sec/batch\n",
      "Epoch 15/20  Iteration 2566/3560 Training loss: 1.2047 1.5366 sec/batch\n",
      "Epoch 15/20  Iteration 2567/3560 Training loss: 1.2046 1.5929 sec/batch\n",
      "Epoch 15/20  Iteration 2568/3560 Training loss: 1.2048 1.3275 sec/batch\n",
      "Epoch 15/20  Iteration 2569/3560 Training loss: 1.2047 1.2241 sec/batch\n",
      "Epoch 15/20  Iteration 2570/3560 Training loss: 1.2046 1.2248 sec/batch\n",
      "Epoch 15/20  Iteration 2571/3560 Training loss: 1.2041 1.2313 sec/batch\n",
      "Epoch 15/20  Iteration 2572/3560 Training loss: 1.2040 1.2428 sec/batch\n",
      "Epoch 15/20  Iteration 2573/3560 Training loss: 1.2037 1.3061 sec/batch\n",
      "Epoch 15/20  Iteration 2574/3560 Training loss: 1.2036 1.2568 sec/batch\n",
      "Epoch 15/20  Iteration 2575/3560 Training loss: 1.2032 1.2528 sec/batch\n",
      "Epoch 15/20  Iteration 2576/3560 Training loss: 1.2032 1.2838 sec/batch\n",
      "Epoch 15/20  Iteration 2577/3560 Training loss: 1.2029 1.2322 sec/batch\n",
      "Epoch 15/20  Iteration 2578/3560 Training loss: 1.2028 1.2393 sec/batch\n",
      "Epoch 15/20  Iteration 2579/3560 Training loss: 1.2026 1.2407 sec/batch\n",
      "Epoch 15/20  Iteration 2580/3560 Training loss: 1.2023 1.2369 sec/batch\n",
      "Epoch 15/20  Iteration 2581/3560 Training loss: 1.2018 1.2231 sec/batch\n",
      "Epoch 15/20  Iteration 2582/3560 Training loss: 1.2019 1.2744 sec/batch\n",
      "Epoch 15/20  Iteration 2583/3560 Training loss: 1.2016 1.3137 sec/batch\n",
      "Epoch 15/20  Iteration 2584/3560 Training loss: 1.2015 1.2900 sec/batch\n",
      "Epoch 15/20  Iteration 2585/3560 Training loss: 1.2011 1.2450 sec/batch\n",
      "Epoch 15/20  Iteration 2586/3560 Training loss: 1.2007 1.2850 sec/batch\n",
      "Epoch 15/20  Iteration 2587/3560 Training loss: 1.2004 1.2482 sec/batch\n",
      "Epoch 15/20  Iteration 2588/3560 Training loss: 1.2004 1.2335 sec/batch\n",
      "Epoch 15/20  Iteration 2589/3560 Training loss: 1.2004 1.2514 sec/batch\n",
      "Epoch 15/20  Iteration 2590/3560 Training loss: 1.1999 1.2453 sec/batch\n",
      "Epoch 15/20  Iteration 2591/3560 Training loss: 1.1996 1.2426 sec/batch\n",
      "Epoch 15/20  Iteration 2592/3560 Training loss: 1.1992 1.3750 sec/batch\n",
      "Epoch 15/20  Iteration 2593/3560 Training loss: 1.1993 1.2348 sec/batch\n",
      "Epoch 15/20  Iteration 2594/3560 Training loss: 1.1991 1.2414 sec/batch\n",
      "Epoch 15/20  Iteration 2595/3560 Training loss: 1.1990 1.2383 sec/batch\n",
      "Epoch 15/20  Iteration 2596/3560 Training loss: 1.1989 1.2462 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2597/3560 Training loss: 1.1987 1.2408 sec/batch\n",
      "Epoch 15/20  Iteration 2598/3560 Training loss: 1.1985 1.2443 sec/batch\n",
      "Epoch 15/20  Iteration 2599/3560 Training loss: 1.1984 1.2410 sec/batch\n",
      "Epoch 15/20  Iteration 2600/3560 Training loss: 1.1984 1.3173 sec/batch\n",
      "Epoch 15/20  Iteration 2601/3560 Training loss: 1.1981 1.2807 sec/batch\n",
      "Epoch 15/20  Iteration 2602/3560 Training loss: 1.1981 1.3067 sec/batch\n",
      "Epoch 15/20  Iteration 2603/3560 Training loss: 1.1980 1.2355 sec/batch\n",
      "Epoch 15/20  Iteration 2604/3560 Training loss: 1.1979 1.2334 sec/batch\n",
      "Epoch 15/20  Iteration 2605/3560 Training loss: 1.1978 1.2359 sec/batch\n",
      "Epoch 15/20  Iteration 2606/3560 Training loss: 1.1976 1.2483 sec/batch\n",
      "Epoch 15/20  Iteration 2607/3560 Training loss: 1.1973 1.2379 sec/batch\n",
      "Epoch 15/20  Iteration 2608/3560 Training loss: 1.1970 1.2863 sec/batch\n",
      "Epoch 15/20  Iteration 2609/3560 Training loss: 1.1971 1.2334 sec/batch\n",
      "Epoch 15/20  Iteration 2610/3560 Training loss: 1.1971 1.2457 sec/batch\n",
      "Epoch 15/20  Iteration 2611/3560 Training loss: 1.1970 1.3183 sec/batch\n",
      "Epoch 15/20  Iteration 2612/3560 Training loss: 1.1969 1.2463 sec/batch\n",
      "Epoch 15/20  Iteration 2613/3560 Training loss: 1.1969 1.2541 sec/batch\n",
      "Epoch 15/20  Iteration 2614/3560 Training loss: 1.1967 1.2588 sec/batch\n",
      "Epoch 15/20  Iteration 2615/3560 Training loss: 1.1963 1.2469 sec/batch\n",
      "Epoch 15/20  Iteration 2616/3560 Training loss: 1.1962 1.2957 sec/batch\n",
      "Epoch 15/20  Iteration 2617/3560 Training loss: 1.1961 1.2345 sec/batch\n",
      "Epoch 15/20  Iteration 2618/3560 Training loss: 1.1958 1.2377 sec/batch\n",
      "Epoch 15/20  Iteration 2619/3560 Training loss: 1.1958 1.2588 sec/batch\n",
      "Epoch 15/20  Iteration 2620/3560 Training loss: 1.1957 1.2583 sec/batch\n",
      "Epoch 15/20  Iteration 2621/3560 Training loss: 1.1955 1.3048 sec/batch\n",
      "Epoch 15/20  Iteration 2622/3560 Training loss: 1.1951 1.2660 sec/batch\n",
      "Epoch 15/20  Iteration 2623/3560 Training loss: 1.1947 1.2653 sec/batch\n",
      "Epoch 15/20  Iteration 2624/3560 Training loss: 1.1946 1.2970 sec/batch\n",
      "Epoch 15/20  Iteration 2625/3560 Training loss: 1.1946 1.3310 sec/batch\n",
      "Epoch 15/20  Iteration 2626/3560 Training loss: 1.1946 1.2766 sec/batch\n",
      "Epoch 15/20  Iteration 2627/3560 Training loss: 1.1946 1.2659 sec/batch\n",
      "Epoch 15/20  Iteration 2628/3560 Training loss: 1.1945 1.2424 sec/batch\n",
      "Epoch 15/20  Iteration 2629/3560 Training loss: 1.1947 1.2624 sec/batch\n",
      "Epoch 15/20  Iteration 2630/3560 Training loss: 1.1948 1.3437 sec/batch\n",
      "Epoch 15/20  Iteration 2631/3560 Training loss: 1.1949 1.2583 sec/batch\n",
      "Epoch 15/20  Iteration 2632/3560 Training loss: 1.1949 1.2990 sec/batch\n",
      "Epoch 15/20  Iteration 2633/3560 Training loss: 1.1952 1.2488 sec/batch\n",
      "Epoch 15/20  Iteration 2634/3560 Training loss: 1.1953 1.2494 sec/batch\n",
      "Epoch 15/20  Iteration 2635/3560 Training loss: 1.1952 1.2694 sec/batch\n",
      "Epoch 15/20  Iteration 2636/3560 Training loss: 1.1954 1.2480 sec/batch\n",
      "Epoch 15/20  Iteration 2637/3560 Training loss: 1.1953 1.2520 sec/batch\n",
      "Epoch 15/20  Iteration 2638/3560 Training loss: 1.1955 1.2555 sec/batch\n",
      "Epoch 15/20  Iteration 2639/3560 Training loss: 1.1955 1.2824 sec/batch\n",
      "Epoch 15/20  Iteration 2640/3560 Training loss: 1.1957 1.3317 sec/batch\n",
      "Epoch 15/20  Iteration 2641/3560 Training loss: 1.1958 1.2358 sec/batch\n",
      "Epoch 15/20  Iteration 2642/3560 Training loss: 1.1957 1.2400 sec/batch\n",
      "Epoch 15/20  Iteration 2643/3560 Training loss: 1.1954 1.2521 sec/batch\n",
      "Epoch 15/20  Iteration 2644/3560 Training loss: 1.1952 1.2304 sec/batch\n",
      "Epoch 15/20  Iteration 2645/3560 Training loss: 1.1952 1.2315 sec/batch\n",
      "Epoch 15/20  Iteration 2646/3560 Training loss: 1.1952 1.2657 sec/batch\n",
      "Epoch 15/20  Iteration 2647/3560 Training loss: 1.1952 1.2567 sec/batch\n",
      "Epoch 15/20  Iteration 2648/3560 Training loss: 1.1952 1.3155 sec/batch\n",
      "Epoch 15/20  Iteration 2649/3560 Training loss: 1.1951 1.3132 sec/batch\n",
      "Epoch 15/20  Iteration 2650/3560 Training loss: 1.1950 1.2342 sec/batch\n",
      "Epoch 15/20  Iteration 2651/3560 Training loss: 1.1948 1.2471 sec/batch\n",
      "Epoch 15/20  Iteration 2652/3560 Training loss: 1.1949 1.2571 sec/batch\n",
      "Epoch 15/20  Iteration 2653/3560 Training loss: 1.1951 1.2315 sec/batch\n",
      "Epoch 15/20  Iteration 2654/3560 Training loss: 1.1950 1.2392 sec/batch\n",
      "Epoch 15/20  Iteration 2655/3560 Training loss: 1.1950 1.2372 sec/batch\n",
      "Epoch 15/20  Iteration 2656/3560 Training loss: 1.1950 1.2800 sec/batch\n",
      "Epoch 15/20  Iteration 2657/3560 Training loss: 1.1951 1.2247 sec/batch\n",
      "Epoch 15/20  Iteration 2658/3560 Training loss: 1.1950 1.2728 sec/batch\n",
      "Epoch 15/20  Iteration 2659/3560 Training loss: 1.1952 1.2848 sec/batch\n",
      "Epoch 15/20  Iteration 2660/3560 Training loss: 1.1956 1.2334 sec/batch\n",
      "Epoch 15/20  Iteration 2661/3560 Training loss: 1.1957 1.2401 sec/batch\n",
      "Epoch 15/20  Iteration 2662/3560 Training loss: 1.1958 1.2442 sec/batch\n",
      "Epoch 15/20  Iteration 2663/3560 Training loss: 1.1958 1.2486 sec/batch\n",
      "Epoch 15/20  Iteration 2664/3560 Training loss: 1.1957 1.2868 sec/batch\n",
      "Epoch 15/20  Iteration 2665/3560 Training loss: 1.1959 1.2430 sec/batch\n",
      "Epoch 15/20  Iteration 2666/3560 Training loss: 1.1959 1.2494 sec/batch\n",
      "Epoch 15/20  Iteration 2667/3560 Training loss: 1.1959 1.2508 sec/batch\n",
      "Epoch 15/20  Iteration 2668/3560 Training loss: 1.1958 1.3125 sec/batch\n",
      "Epoch 15/20  Iteration 2669/3560 Training loss: 1.1957 1.2762 sec/batch\n",
      "Epoch 15/20  Iteration 2670/3560 Training loss: 1.1959 1.2708 sec/batch\n",
      "Epoch 16/20  Iteration 2671/3560 Training loss: 1.3181 1.2714 sec/batch\n",
      "Epoch 16/20  Iteration 2672/3560 Training loss: 1.2705 1.2893 sec/batch\n",
      "Epoch 16/20  Iteration 2673/3560 Training loss: 1.2498 1.2552 sec/batch\n",
      "Epoch 16/20  Iteration 2674/3560 Training loss: 1.2431 1.2336 sec/batch\n",
      "Epoch 16/20  Iteration 2675/3560 Training loss: 1.2302 1.2247 sec/batch\n",
      "Epoch 16/20  Iteration 2676/3560 Training loss: 1.2200 1.2234 sec/batch\n",
      "Epoch 16/20  Iteration 2677/3560 Training loss: 1.2172 1.2859 sec/batch\n",
      "Epoch 16/20  Iteration 2678/3560 Training loss: 1.2139 1.3309 sec/batch\n",
      "Epoch 16/20  Iteration 2679/3560 Training loss: 1.2125 1.2514 sec/batch\n",
      "Epoch 16/20  Iteration 2680/3560 Training loss: 1.2106 1.2899 sec/batch\n",
      "Epoch 16/20  Iteration 2681/3560 Training loss: 1.2066 1.2454 sec/batch\n",
      "Epoch 16/20  Iteration 2682/3560 Training loss: 1.2064 1.2419 sec/batch\n",
      "Epoch 16/20  Iteration 2683/3560 Training loss: 1.2061 1.2518 sec/batch\n",
      "Epoch 16/20  Iteration 2684/3560 Training loss: 1.2061 1.2508 sec/batch\n",
      "Epoch 16/20  Iteration 2685/3560 Training loss: 1.2034 1.2582 sec/batch\n",
      "Epoch 16/20  Iteration 2686/3560 Training loss: 1.2010 1.2616 sec/batch\n",
      "Epoch 16/20  Iteration 2687/3560 Training loss: 1.2011 1.3144 sec/batch\n",
      "Epoch 16/20  Iteration 2688/3560 Training loss: 1.2021 1.3008 sec/batch\n",
      "Epoch 16/20  Iteration 2689/3560 Training loss: 1.2018 1.2751 sec/batch\n",
      "Epoch 16/20  Iteration 2690/3560 Training loss: 1.2026 1.2586 sec/batch\n",
      "Epoch 16/20  Iteration 2691/3560 Training loss: 1.2020 1.2733 sec/batch\n",
      "Epoch 16/20  Iteration 2692/3560 Training loss: 1.2025 1.2343 sec/batch\n",
      "Epoch 16/20  Iteration 2693/3560 Training loss: 1.2017 1.2442 sec/batch\n",
      "Epoch 16/20  Iteration 2694/3560 Training loss: 1.2020 1.2491 sec/batch\n",
      "Epoch 16/20  Iteration 2695/3560 Training loss: 1.2017 1.2381 sec/batch\n",
      "Epoch 16/20  Iteration 2696/3560 Training loss: 1.1997 1.3906 sec/batch\n",
      "Epoch 16/20  Iteration 2697/3560 Training loss: 1.1981 1.2602 sec/batch\n",
      "Epoch 16/20  Iteration 2698/3560 Training loss: 1.1985 1.2451 sec/batch\n",
      "Epoch 16/20  Iteration 2699/3560 Training loss: 1.1982 1.2542 sec/batch\n",
      "Epoch 16/20  Iteration 2700/3560 Training loss: 1.1982 1.2465 sec/batch\n",
      "Epoch 16/20  Iteration 2701/3560 Training loss: 1.1974 1.2474 sec/batch\n",
      "Epoch 16/20  Iteration 2702/3560 Training loss: 1.1961 1.2640 sec/batch\n",
      "Epoch 16/20  Iteration 2703/3560 Training loss: 1.1961 1.2405 sec/batch\n",
      "Epoch 16/20  Iteration 2704/3560 Training loss: 1.1960 1.5159 sec/batch\n",
      "Epoch 16/20  Iteration 2705/3560 Training loss: 1.1953 1.6510 sec/batch\n",
      "Epoch 16/20  Iteration 2706/3560 Training loss: 1.1950 1.5797 sec/batch\n",
      "Epoch 16/20  Iteration 2707/3560 Training loss: 1.1942 1.5590 sec/batch\n",
      "Epoch 16/20  Iteration 2708/3560 Training loss: 1.1927 1.6006 sec/batch\n",
      "Epoch 16/20  Iteration 2709/3560 Training loss: 1.1917 1.3830 sec/batch\n",
      "Epoch 16/20  Iteration 2710/3560 Training loss: 1.1915 1.2359 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2711/3560 Training loss: 1.1909 1.2797 sec/batch\n",
      "Epoch 16/20  Iteration 2712/3560 Training loss: 1.1917 1.2313 sec/batch\n",
      "Epoch 16/20  Iteration 2713/3560 Training loss: 1.1914 1.2352 sec/batch\n",
      "Epoch 16/20  Iteration 2714/3560 Training loss: 1.1906 1.2895 sec/batch\n",
      "Epoch 16/20  Iteration 2715/3560 Training loss: 1.1907 1.3080 sec/batch\n",
      "Epoch 16/20  Iteration 2716/3560 Training loss: 1.1902 1.2449 sec/batch\n",
      "Epoch 16/20  Iteration 2717/3560 Training loss: 1.1899 1.2920 sec/batch\n",
      "Epoch 16/20  Iteration 2718/3560 Training loss: 1.1893 1.2417 sec/batch\n",
      "Epoch 16/20  Iteration 2719/3560 Training loss: 1.1893 1.2574 sec/batch\n",
      "Epoch 16/20  Iteration 2720/3560 Training loss: 1.1895 1.2279 sec/batch\n",
      "Epoch 16/20  Iteration 2721/3560 Training loss: 1.1888 1.2346 sec/batch\n",
      "Epoch 16/20  Iteration 2722/3560 Training loss: 1.1894 1.2444 sec/batch\n",
      "Epoch 16/20  Iteration 2723/3560 Training loss: 1.1894 1.2410 sec/batch\n",
      "Epoch 16/20  Iteration 2724/3560 Training loss: 1.1896 1.3096 sec/batch\n",
      "Epoch 16/20  Iteration 2725/3560 Training loss: 1.1894 1.2561 sec/batch\n",
      "Epoch 16/20  Iteration 2726/3560 Training loss: 1.1894 1.2378 sec/batch\n",
      "Epoch 16/20  Iteration 2727/3560 Training loss: 1.1895 1.2632 sec/batch\n",
      "Epoch 16/20  Iteration 2728/3560 Training loss: 1.1893 1.2325 sec/batch\n",
      "Epoch 16/20  Iteration 2729/3560 Training loss: 1.1888 1.2334 sec/batch\n",
      "Epoch 16/20  Iteration 2730/3560 Training loss: 1.1894 1.2399 sec/batch\n",
      "Epoch 16/20  Iteration 2731/3560 Training loss: 1.1893 1.2314 sec/batch\n",
      "Epoch 16/20  Iteration 2732/3560 Training loss: 1.1899 1.2345 sec/batch\n",
      "Epoch 16/20  Iteration 2733/3560 Training loss: 1.1901 1.2835 sec/batch\n",
      "Epoch 16/20  Iteration 2734/3560 Training loss: 1.1901 1.3341 sec/batch\n",
      "Epoch 16/20  Iteration 2735/3560 Training loss: 1.1900 1.2715 sec/batch\n",
      "Epoch 16/20  Iteration 2736/3560 Training loss: 1.1902 1.2378 sec/batch\n",
      "Epoch 16/20  Iteration 2737/3560 Training loss: 1.1904 1.2449 sec/batch\n",
      "Epoch 16/20  Iteration 2738/3560 Training loss: 1.1902 1.2317 sec/batch\n",
      "Epoch 16/20  Iteration 2739/3560 Training loss: 1.1903 1.2498 sec/batch\n",
      "Epoch 16/20  Iteration 2740/3560 Training loss: 1.1901 1.2398 sec/batch\n",
      "Epoch 16/20  Iteration 2741/3560 Training loss: 1.1906 1.2510 sec/batch\n",
      "Epoch 16/20  Iteration 2742/3560 Training loss: 1.1909 1.2599 sec/batch\n",
      "Epoch 16/20  Iteration 2743/3560 Training loss: 1.1913 1.3929 sec/batch\n",
      "Epoch 16/20  Iteration 2744/3560 Training loss: 1.1911 1.2649 sec/batch\n",
      "Epoch 16/20  Iteration 2745/3560 Training loss: 1.1910 1.2496 sec/batch\n",
      "Epoch 16/20  Iteration 2746/3560 Training loss: 1.1911 1.2515 sec/batch\n",
      "Epoch 16/20  Iteration 2747/3560 Training loss: 1.1911 1.2506 sec/batch\n",
      "Epoch 16/20  Iteration 2748/3560 Training loss: 1.1910 1.2325 sec/batch\n",
      "Epoch 16/20  Iteration 2749/3560 Training loss: 1.1903 1.2711 sec/batch\n",
      "Epoch 16/20  Iteration 2750/3560 Training loss: 1.1902 1.2612 sec/batch\n",
      "Epoch 16/20  Iteration 2751/3560 Training loss: 1.1899 1.2634 sec/batch\n",
      "Epoch 16/20  Iteration 2752/3560 Training loss: 1.1898 1.2668 sec/batch\n",
      "Epoch 16/20  Iteration 2753/3560 Training loss: 1.1894 1.3481 sec/batch\n",
      "Epoch 16/20  Iteration 2754/3560 Training loss: 1.1896 1.2449 sec/batch\n",
      "Epoch 16/20  Iteration 2755/3560 Training loss: 1.1892 1.2323 sec/batch\n",
      "Epoch 16/20  Iteration 2756/3560 Training loss: 1.1890 1.2389 sec/batch\n",
      "Epoch 16/20  Iteration 2757/3560 Training loss: 1.1889 1.2426 sec/batch\n",
      "Epoch 16/20  Iteration 2758/3560 Training loss: 1.1886 1.2469 sec/batch\n",
      "Epoch 16/20  Iteration 2759/3560 Training loss: 1.1882 1.2672 sec/batch\n",
      "Epoch 16/20  Iteration 2760/3560 Training loss: 1.1882 1.2402 sec/batch\n",
      "Epoch 16/20  Iteration 2761/3560 Training loss: 1.1880 1.2471 sec/batch\n",
      "Epoch 16/20  Iteration 2762/3560 Training loss: 1.1880 1.2902 sec/batch\n",
      "Epoch 16/20  Iteration 2763/3560 Training loss: 1.1876 1.2758 sec/batch\n",
      "Epoch 16/20  Iteration 2764/3560 Training loss: 1.1873 1.2443 sec/batch\n",
      "Epoch 16/20  Iteration 2765/3560 Training loss: 1.1870 1.2583 sec/batch\n",
      "Epoch 16/20  Iteration 2766/3560 Training loss: 1.1871 1.3279 sec/batch\n",
      "Epoch 16/20  Iteration 2767/3560 Training loss: 1.1871 1.3233 sec/batch\n",
      "Epoch 16/20  Iteration 2768/3560 Training loss: 1.1867 1.2770 sec/batch\n",
      "Epoch 16/20  Iteration 2769/3560 Training loss: 1.1864 1.3271 sec/batch\n",
      "Epoch 16/20  Iteration 2770/3560 Training loss: 1.1860 1.3181 sec/batch\n",
      "Epoch 16/20  Iteration 2771/3560 Training loss: 1.1860 1.2533 sec/batch\n",
      "Epoch 16/20  Iteration 2772/3560 Training loss: 1.1859 1.2641 sec/batch\n",
      "Epoch 16/20  Iteration 2773/3560 Training loss: 1.1858 1.2592 sec/batch\n",
      "Epoch 16/20  Iteration 2774/3560 Training loss: 1.1856 1.2533 sec/batch\n",
      "Epoch 16/20  Iteration 2775/3560 Training loss: 1.1854 1.2715 sec/batch\n",
      "Epoch 16/20  Iteration 2776/3560 Training loss: 1.1852 1.2526 sec/batch\n",
      "Epoch 16/20  Iteration 2777/3560 Training loss: 1.1850 1.2645 sec/batch\n",
      "Epoch 16/20  Iteration 2778/3560 Training loss: 1.1851 1.3027 sec/batch\n",
      "Epoch 16/20  Iteration 2779/3560 Training loss: 1.1850 1.3023 sec/batch\n",
      "Epoch 16/20  Iteration 2780/3560 Training loss: 1.1850 1.2499 sec/batch\n",
      "Epoch 16/20  Iteration 2781/3560 Training loss: 1.1849 1.2532 sec/batch\n",
      "Epoch 16/20  Iteration 2782/3560 Training loss: 1.1848 1.2627 sec/batch\n",
      "Epoch 16/20  Iteration 2783/3560 Training loss: 1.1846 1.2681 sec/batch\n",
      "Epoch 16/20  Iteration 2784/3560 Training loss: 1.1843 1.2725 sec/batch\n",
      "Epoch 16/20  Iteration 2785/3560 Training loss: 1.1841 1.3078 sec/batch\n",
      "Epoch 16/20  Iteration 2786/3560 Training loss: 1.1838 1.2525 sec/batch\n",
      "Epoch 16/20  Iteration 2787/3560 Training loss: 1.1838 1.2432 sec/batch\n",
      "Epoch 16/20  Iteration 2788/3560 Training loss: 1.1838 1.2576 sec/batch\n",
      "Epoch 16/20  Iteration 2789/3560 Training loss: 1.1838 1.2581 sec/batch\n",
      "Epoch 16/20  Iteration 2790/3560 Training loss: 1.1837 1.2624 sec/batch\n",
      "Epoch 16/20  Iteration 2791/3560 Training loss: 1.1837 1.3175 sec/batch\n",
      "Epoch 16/20  Iteration 2792/3560 Training loss: 1.1834 1.2326 sec/batch\n",
      "Epoch 16/20  Iteration 2793/3560 Training loss: 1.1830 1.2398 sec/batch\n",
      "Epoch 16/20  Iteration 2794/3560 Training loss: 1.1830 1.2631 sec/batch\n",
      "Epoch 16/20  Iteration 2795/3560 Training loss: 1.1829 1.2478 sec/batch\n",
      "Epoch 16/20  Iteration 2796/3560 Training loss: 1.1825 1.2389 sec/batch\n",
      "Epoch 16/20  Iteration 2797/3560 Training loss: 1.1825 1.2355 sec/batch\n",
      "Epoch 16/20  Iteration 2798/3560 Training loss: 1.1825 1.2490 sec/batch\n",
      "Epoch 16/20  Iteration 2799/3560 Training loss: 1.1824 1.2551 sec/batch\n",
      "Epoch 16/20  Iteration 2800/3560 Training loss: 1.1820 1.2517 sec/batch\n",
      "Epoch 16/20  Iteration 2801/3560 Training loss: 1.1816 1.2155 sec/batch\n",
      "Epoch 16/20  Iteration 2802/3560 Training loss: 1.1815 1.2448 sec/batch\n",
      "Epoch 16/20  Iteration 2803/3560 Training loss: 1.1816 1.2975 sec/batch\n",
      "Epoch 16/20  Iteration 2804/3560 Training loss: 1.1815 1.2423 sec/batch\n",
      "Epoch 16/20  Iteration 2805/3560 Training loss: 1.1815 1.2421 sec/batch\n",
      "Epoch 16/20  Iteration 2806/3560 Training loss: 1.1815 1.2627 sec/batch\n",
      "Epoch 16/20  Iteration 2807/3560 Training loss: 1.1816 1.2760 sec/batch\n",
      "Epoch 16/20  Iteration 2808/3560 Training loss: 1.1818 1.2473 sec/batch\n",
      "Epoch 16/20  Iteration 2809/3560 Training loss: 1.1817 1.2502 sec/batch\n",
      "Epoch 16/20  Iteration 2810/3560 Training loss: 1.1817 1.2586 sec/batch\n",
      "Epoch 16/20  Iteration 2811/3560 Training loss: 1.1820 1.2559 sec/batch\n",
      "Epoch 16/20  Iteration 2812/3560 Training loss: 1.1821 1.2463 sec/batch\n",
      "Epoch 16/20  Iteration 2813/3560 Training loss: 1.1820 1.2437 sec/batch\n",
      "Epoch 16/20  Iteration 2814/3560 Training loss: 1.1822 1.2425 sec/batch\n",
      "Epoch 16/20  Iteration 2815/3560 Training loss: 1.1821 1.2903 sec/batch\n",
      "Epoch 16/20  Iteration 2816/3560 Training loss: 1.1824 1.2285 sec/batch\n",
      "Epoch 16/20  Iteration 2817/3560 Training loss: 1.1824 1.2683 sec/batch\n",
      "Epoch 16/20  Iteration 2818/3560 Training loss: 1.1826 1.2477 sec/batch\n",
      "Epoch 16/20  Iteration 2819/3560 Training loss: 1.1827 1.2300 sec/batch\n",
      "Epoch 16/20  Iteration 2820/3560 Training loss: 1.1826 1.2484 sec/batch\n",
      "Epoch 16/20  Iteration 2821/3560 Training loss: 1.1823 1.2459 sec/batch\n",
      "Epoch 16/20  Iteration 2822/3560 Training loss: 1.1821 1.2416 sec/batch\n",
      "Epoch 16/20  Iteration 2823/3560 Training loss: 1.1822 1.2940 sec/batch\n",
      "Epoch 16/20  Iteration 2824/3560 Training loss: 1.1822 1.2365 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2825/3560 Training loss: 1.1822 1.2417 sec/batch\n",
      "Epoch 16/20  Iteration 2826/3560 Training loss: 1.1822 1.2389 sec/batch\n",
      "Epoch 16/20  Iteration 2827/3560 Training loss: 1.1822 1.2560 sec/batch\n",
      "Epoch 16/20  Iteration 2828/3560 Training loss: 1.1821 1.2544 sec/batch\n",
      "Epoch 16/20  Iteration 2829/3560 Training loss: 1.1819 1.2377 sec/batch\n",
      "Epoch 16/20  Iteration 2830/3560 Training loss: 1.1820 1.2517 sec/batch\n",
      "Epoch 16/20  Iteration 2831/3560 Training loss: 1.1821 1.2881 sec/batch\n",
      "Epoch 16/20  Iteration 2832/3560 Training loss: 1.1821 1.2505 sec/batch\n",
      "Epoch 16/20  Iteration 2833/3560 Training loss: 1.1821 1.2319 sec/batch\n",
      "Epoch 16/20  Iteration 2834/3560 Training loss: 1.1820 1.2313 sec/batch\n",
      "Epoch 16/20  Iteration 2835/3560 Training loss: 1.1820 1.2539 sec/batch\n",
      "Epoch 16/20  Iteration 2836/3560 Training loss: 1.1819 1.2365 sec/batch\n",
      "Epoch 16/20  Iteration 2837/3560 Training loss: 1.1820 1.2390 sec/batch\n",
      "Epoch 16/20  Iteration 2838/3560 Training loss: 1.1823 1.2405 sec/batch\n",
      "Epoch 16/20  Iteration 2839/3560 Training loss: 1.1824 1.2996 sec/batch\n",
      "Epoch 16/20  Iteration 2840/3560 Training loss: 1.1825 1.2343 sec/batch\n",
      "Epoch 16/20  Iteration 2841/3560 Training loss: 1.1824 1.2423 sec/batch\n",
      "Epoch 16/20  Iteration 2842/3560 Training loss: 1.1823 1.2516 sec/batch\n",
      "Epoch 16/20  Iteration 2843/3560 Training loss: 1.1825 1.2410 sec/batch\n",
      "Epoch 16/20  Iteration 2844/3560 Training loss: 1.1825 1.2394 sec/batch\n",
      "Epoch 16/20  Iteration 2845/3560 Training loss: 1.1826 1.2537 sec/batch\n",
      "Epoch 16/20  Iteration 2846/3560 Training loss: 1.1824 1.6315 sec/batch\n",
      "Epoch 16/20  Iteration 2847/3560 Training loss: 1.1823 1.6002 sec/batch\n",
      "Epoch 16/20  Iteration 2848/3560 Training loss: 1.1824 1.5851 sec/batch\n",
      "Epoch 17/20  Iteration 2849/3560 Training loss: 1.3099 1.5496 sec/batch\n",
      "Epoch 17/20  Iteration 2850/3560 Training loss: 1.2560 1.5297 sec/batch\n",
      "Epoch 17/20  Iteration 2851/3560 Training loss: 1.2342 1.2312 sec/batch\n",
      "Epoch 17/20  Iteration 2852/3560 Training loss: 1.2277 1.2400 sec/batch\n",
      "Epoch 17/20  Iteration 2853/3560 Training loss: 1.2164 1.2440 sec/batch\n",
      "Epoch 17/20  Iteration 2854/3560 Training loss: 1.2063 1.2839 sec/batch\n",
      "Epoch 17/20  Iteration 2855/3560 Training loss: 1.2046 1.2309 sec/batch\n",
      "Epoch 17/20  Iteration 2856/3560 Training loss: 1.2017 1.2337 sec/batch\n",
      "Epoch 17/20  Iteration 2857/3560 Training loss: 1.2003 1.2331 sec/batch\n",
      "Epoch 17/20  Iteration 2858/3560 Training loss: 1.1981 1.2265 sec/batch\n",
      "Epoch 17/20  Iteration 2859/3560 Training loss: 1.1944 1.2463 sec/batch\n",
      "Epoch 17/20  Iteration 2860/3560 Training loss: 1.1941 1.2379 sec/batch\n",
      "Epoch 17/20  Iteration 2861/3560 Training loss: 1.1936 1.2517 sec/batch\n",
      "Epoch 17/20  Iteration 2862/3560 Training loss: 1.1932 1.2854 sec/batch\n",
      "Epoch 17/20  Iteration 2863/3560 Training loss: 1.1907 1.2251 sec/batch\n",
      "Epoch 17/20  Iteration 2864/3560 Training loss: 1.1888 1.2330 sec/batch\n",
      "Epoch 17/20  Iteration 2865/3560 Training loss: 1.1886 1.2529 sec/batch\n",
      "Epoch 17/20  Iteration 2866/3560 Training loss: 1.1891 1.2221 sec/batch\n",
      "Epoch 17/20  Iteration 2867/3560 Training loss: 1.1887 1.2382 sec/batch\n",
      "Epoch 17/20  Iteration 2868/3560 Training loss: 1.1901 1.2333 sec/batch\n",
      "Epoch 17/20  Iteration 2869/3560 Training loss: 1.1890 1.2509 sec/batch\n",
      "Epoch 17/20  Iteration 2870/3560 Training loss: 1.1893 1.2482 sec/batch\n",
      "Epoch 17/20  Iteration 2871/3560 Training loss: 1.1885 1.2517 sec/batch\n",
      "Epoch 17/20  Iteration 2872/3560 Training loss: 1.1888 1.2255 sec/batch\n",
      "Epoch 17/20  Iteration 2873/3560 Training loss: 1.1885 1.2517 sec/batch\n",
      "Epoch 17/20  Iteration 2874/3560 Training loss: 1.1861 1.2374 sec/batch\n",
      "Epoch 17/20  Iteration 2875/3560 Training loss: 1.1847 1.2366 sec/batch\n",
      "Epoch 17/20  Iteration 2876/3560 Training loss: 1.1845 1.2276 sec/batch\n",
      "Epoch 17/20  Iteration 2877/3560 Training loss: 1.1842 1.2566 sec/batch\n",
      "Epoch 17/20  Iteration 2878/3560 Training loss: 1.1841 1.2416 sec/batch\n",
      "Epoch 17/20  Iteration 2879/3560 Training loss: 1.1832 1.2599 sec/batch\n",
      "Epoch 17/20  Iteration 2880/3560 Training loss: 1.1821 1.2638 sec/batch\n",
      "Epoch 17/20  Iteration 2881/3560 Training loss: 1.1819 1.2449 sec/batch\n",
      "Epoch 17/20  Iteration 2882/3560 Training loss: 1.1818 1.2387 sec/batch\n",
      "Epoch 17/20  Iteration 2883/3560 Training loss: 1.1812 1.2327 sec/batch\n",
      "Epoch 17/20  Iteration 2884/3560 Training loss: 1.1810 1.2216 sec/batch\n",
      "Epoch 17/20  Iteration 2885/3560 Training loss: 1.1800 1.2471 sec/batch\n",
      "Epoch 17/20  Iteration 2886/3560 Training loss: 1.1787 1.2500 sec/batch\n",
      "Epoch 17/20  Iteration 2887/3560 Training loss: 1.1776 1.2910 sec/batch\n",
      "Epoch 17/20  Iteration 2888/3560 Training loss: 1.1772 1.2389 sec/batch\n",
      "Epoch 17/20  Iteration 2889/3560 Training loss: 1.1765 1.2297 sec/batch\n",
      "Epoch 17/20  Iteration 2890/3560 Training loss: 1.1772 1.2433 sec/batch\n",
      "Epoch 17/20  Iteration 2891/3560 Training loss: 1.1768 1.2408 sec/batch\n",
      "Epoch 17/20  Iteration 2892/3560 Training loss: 1.1760 1.2278 sec/batch\n",
      "Epoch 17/20  Iteration 2893/3560 Training loss: 1.1762 1.2883 sec/batch\n",
      "Epoch 17/20  Iteration 2894/3560 Training loss: 1.1758 1.3173 sec/batch\n",
      "Epoch 17/20  Iteration 2895/3560 Training loss: 1.1755 1.3933 sec/batch\n",
      "Epoch 17/20  Iteration 2896/3560 Training loss: 1.1750 1.3599 sec/batch\n",
      "Epoch 17/20  Iteration 2897/3560 Training loss: 1.1749 1.2856 sec/batch\n",
      "Epoch 17/20  Iteration 2898/3560 Training loss: 1.1750 1.2491 sec/batch\n",
      "Epoch 17/20  Iteration 2899/3560 Training loss: 1.1745 1.2466 sec/batch\n",
      "Epoch 17/20  Iteration 2900/3560 Training loss: 1.1751 1.3339 sec/batch\n",
      "Epoch 17/20  Iteration 2901/3560 Training loss: 1.1750 1.2932 sec/batch\n",
      "Epoch 17/20  Iteration 2902/3560 Training loss: 1.1750 1.3174 sec/batch\n",
      "Epoch 17/20  Iteration 2903/3560 Training loss: 1.1748 1.2813 sec/batch\n",
      "Epoch 17/20  Iteration 2904/3560 Training loss: 1.1748 1.2680 sec/batch\n",
      "Epoch 17/20  Iteration 2905/3560 Training loss: 1.1751 1.2687 sec/batch\n",
      "Epoch 17/20  Iteration 2906/3560 Training loss: 1.1747 1.2614 sec/batch\n",
      "Epoch 17/20  Iteration 2907/3560 Training loss: 1.1741 1.2254 sec/batch\n",
      "Epoch 17/20  Iteration 2908/3560 Training loss: 1.1747 1.3401 sec/batch\n",
      "Epoch 17/20  Iteration 2909/3560 Training loss: 1.1744 1.2651 sec/batch\n",
      "Epoch 17/20  Iteration 2910/3560 Training loss: 1.1751 1.2866 sec/batch\n",
      "Epoch 17/20  Iteration 2911/3560 Training loss: 1.1753 1.2779 sec/batch\n",
      "Epoch 17/20  Iteration 2912/3560 Training loss: 1.1753 1.2533 sec/batch\n",
      "Epoch 17/20  Iteration 2913/3560 Training loss: 1.1752 1.2405 sec/batch\n",
      "Epoch 17/20  Iteration 2914/3560 Training loss: 1.1754 1.2458 sec/batch\n",
      "Epoch 17/20  Iteration 2915/3560 Training loss: 1.1757 1.3069 sec/batch\n",
      "Epoch 17/20  Iteration 2916/3560 Training loss: 1.1756 1.2537 sec/batch\n",
      "Epoch 17/20  Iteration 2917/3560 Training loss: 1.1756 1.2695 sec/batch\n",
      "Epoch 17/20  Iteration 2918/3560 Training loss: 1.1755 1.2605 sec/batch\n",
      "Epoch 17/20  Iteration 2919/3560 Training loss: 1.1761 1.3328 sec/batch\n",
      "Epoch 17/20  Iteration 2920/3560 Training loss: 1.1764 1.3841 sec/batch\n",
      "Epoch 17/20  Iteration 2921/3560 Training loss: 1.1768 1.5361 sec/batch\n",
      "Epoch 17/20  Iteration 2922/3560 Training loss: 1.1765 1.4909 sec/batch\n",
      "Epoch 17/20  Iteration 2923/3560 Training loss: 1.1765 1.5031 sec/batch\n",
      "Epoch 17/20  Iteration 2924/3560 Training loss: 1.1766 1.6875 sec/batch\n",
      "Epoch 17/20  Iteration 2925/3560 Training loss: 1.1765 1.4792 sec/batch\n",
      "Epoch 17/20  Iteration 2926/3560 Training loss: 1.1764 1.4102 sec/batch\n",
      "Epoch 17/20  Iteration 2927/3560 Training loss: 1.1758 1.4269 sec/batch\n",
      "Epoch 17/20  Iteration 2928/3560 Training loss: 1.1757 1.3352 sec/batch\n",
      "Epoch 17/20  Iteration 2929/3560 Training loss: 1.1754 1.3730 sec/batch\n",
      "Epoch 17/20  Iteration 2930/3560 Training loss: 1.1753 1.3433 sec/batch\n",
      "Epoch 17/20  Iteration 2931/3560 Training loss: 1.1749 1.3328 sec/batch\n",
      "Epoch 17/20  Iteration 2932/3560 Training loss: 1.1749 1.3615 sec/batch\n",
      "Epoch 17/20  Iteration 2933/3560 Training loss: 1.1746 1.4113 sec/batch\n",
      "Epoch 17/20  Iteration 2934/3560 Training loss: 1.1745 1.3227 sec/batch\n",
      "Epoch 17/20  Iteration 2935/3560 Training loss: 1.1742 1.3681 sec/batch\n",
      "Epoch 17/20  Iteration 2936/3560 Training loss: 1.1738 1.3432 sec/batch\n",
      "Epoch 17/20  Iteration 2937/3560 Training loss: 1.1734 1.3516 sec/batch\n",
      "Epoch 17/20  Iteration 2938/3560 Training loss: 1.1734 1.3909 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 2939/3560 Training loss: 1.1732 1.3457 sec/batch\n",
      "Epoch 17/20  Iteration 2940/3560 Training loss: 1.1732 1.3849 sec/batch\n",
      "Epoch 17/20  Iteration 2941/3560 Training loss: 1.1728 1.3399 sec/batch\n",
      "Epoch 17/20  Iteration 2942/3560 Training loss: 1.1724 1.3421 sec/batch\n",
      "Epoch 17/20  Iteration 2943/3560 Training loss: 1.1721 1.3608 sec/batch\n",
      "Epoch 17/20  Iteration 2944/3560 Training loss: 1.1721 1.3317 sec/batch\n",
      "Epoch 17/20  Iteration 2945/3560 Training loss: 1.1720 1.3485 sec/batch\n",
      "Epoch 17/20  Iteration 2946/3560 Training loss: 1.1716 1.3759 sec/batch\n",
      "Epoch 17/20  Iteration 2947/3560 Training loss: 1.1713 1.3776 sec/batch\n",
      "Epoch 17/20  Iteration 2948/3560 Training loss: 1.1710 1.3197 sec/batch\n",
      "Epoch 17/20  Iteration 2949/3560 Training loss: 1.1709 1.2516 sec/batch\n",
      "Epoch 17/20  Iteration 2950/3560 Training loss: 1.1707 1.2749 sec/batch\n",
      "Epoch 17/20  Iteration 2951/3560 Training loss: 1.1707 1.2476 sec/batch\n",
      "Epoch 17/20  Iteration 2952/3560 Training loss: 1.1706 1.2464 sec/batch\n",
      "Epoch 17/20  Iteration 2953/3560 Training loss: 1.1704 1.2448 sec/batch\n",
      "Epoch 17/20  Iteration 2954/3560 Training loss: 1.1702 1.3064 sec/batch\n",
      "Epoch 17/20  Iteration 2955/3560 Training loss: 1.1701 1.2878 sec/batch\n",
      "Epoch 17/20  Iteration 2956/3560 Training loss: 1.1702 1.3365 sec/batch\n",
      "Epoch 17/20  Iteration 2957/3560 Training loss: 1.1700 1.2669 sec/batch\n",
      "Epoch 17/20  Iteration 2958/3560 Training loss: 1.1700 1.3053 sec/batch\n",
      "Epoch 17/20  Iteration 2959/3560 Training loss: 1.1698 1.2337 sec/batch\n",
      "Epoch 17/20  Iteration 2960/3560 Training loss: 1.1698 1.2405 sec/batch\n",
      "Epoch 17/20  Iteration 2961/3560 Training loss: 1.1696 1.2409 sec/batch\n",
      "Epoch 17/20  Iteration 2962/3560 Training loss: 1.1694 1.2517 sec/batch\n",
      "Epoch 17/20  Iteration 2963/3560 Training loss: 1.1691 1.3233 sec/batch\n",
      "Epoch 17/20  Iteration 2964/3560 Training loss: 1.1688 1.3545 sec/batch\n",
      "Epoch 17/20  Iteration 2965/3560 Training loss: 1.1688 1.2401 sec/batch\n",
      "Epoch 17/20  Iteration 2966/3560 Training loss: 1.1688 1.2481 sec/batch\n",
      "Epoch 17/20  Iteration 2967/3560 Training loss: 1.1687 1.2454 sec/batch\n",
      "Epoch 17/20  Iteration 2968/3560 Training loss: 1.1687 1.2402 sec/batch\n",
      "Epoch 17/20  Iteration 2969/3560 Training loss: 1.1687 1.2408 sec/batch\n",
      "Epoch 17/20  Iteration 2970/3560 Training loss: 1.1684 1.2426 sec/batch\n",
      "Epoch 17/20  Iteration 2971/3560 Training loss: 1.1680 1.2737 sec/batch\n",
      "Epoch 17/20  Iteration 2972/3560 Training loss: 1.1679 1.2600 sec/batch\n",
      "Epoch 17/20  Iteration 2973/3560 Training loss: 1.1678 1.3388 sec/batch\n",
      "Epoch 17/20  Iteration 2974/3560 Training loss: 1.1675 1.2723 sec/batch\n",
      "Epoch 17/20  Iteration 2975/3560 Training loss: 1.1676 1.2477 sec/batch\n",
      "Epoch 17/20  Iteration 2976/3560 Training loss: 1.1675 1.2334 sec/batch\n",
      "Epoch 17/20  Iteration 2977/3560 Training loss: 1.1673 1.2371 sec/batch\n",
      "Epoch 17/20  Iteration 2978/3560 Training loss: 1.1670 1.2652 sec/batch\n",
      "Epoch 17/20  Iteration 2979/3560 Training loss: 1.1666 1.2765 sec/batch\n",
      "Epoch 17/20  Iteration 2980/3560 Training loss: 1.1664 1.2706 sec/batch\n",
      "Epoch 17/20  Iteration 2981/3560 Training loss: 1.1665 1.2213 sec/batch\n",
      "Epoch 17/20  Iteration 2982/3560 Training loss: 1.1665 1.2509 sec/batch\n",
      "Epoch 17/20  Iteration 2983/3560 Training loss: 1.1665 1.3442 sec/batch\n",
      "Epoch 17/20  Iteration 2984/3560 Training loss: 1.1664 1.5162 sec/batch\n",
      "Epoch 17/20  Iteration 2985/3560 Training loss: 1.1665 1.5868 sec/batch\n",
      "Epoch 17/20  Iteration 2986/3560 Training loss: 1.1666 1.5608 sec/batch\n",
      "Epoch 17/20  Iteration 2987/3560 Training loss: 1.1667 1.6071 sec/batch\n",
      "Epoch 17/20  Iteration 2988/3560 Training loss: 1.1666 1.5641 sec/batch\n",
      "Epoch 17/20  Iteration 2989/3560 Training loss: 1.1669 1.4261 sec/batch\n",
      "Epoch 17/20  Iteration 2990/3560 Training loss: 1.1671 1.2453 sec/batch\n",
      "Epoch 17/20  Iteration 2991/3560 Training loss: 1.1670 1.3250 sec/batch\n",
      "Epoch 17/20  Iteration 2992/3560 Training loss: 1.1673 1.2444 sec/batch\n",
      "Epoch 17/20  Iteration 2993/3560 Training loss: 1.1672 1.2424 sec/batch\n",
      "Epoch 17/20  Iteration 2994/3560 Training loss: 1.1674 1.2823 sec/batch\n",
      "Epoch 17/20  Iteration 2995/3560 Training loss: 1.1675 1.2351 sec/batch\n",
      "Epoch 17/20  Iteration 2996/3560 Training loss: 1.1676 1.2325 sec/batch\n",
      "Epoch 17/20  Iteration 2997/3560 Training loss: 1.1678 1.2313 sec/batch\n",
      "Epoch 17/20  Iteration 2998/3560 Training loss: 1.1677 1.2331 sec/batch\n",
      "Epoch 17/20  Iteration 2999/3560 Training loss: 1.1674 1.2380 sec/batch\n",
      "Epoch 17/20  Iteration 3000/3560 Training loss: 1.1672 1.3053 sec/batch\n",
      "Epoch 17/20  Iteration 3001/3560 Training loss: 1.1674 1.2707 sec/batch\n",
      "Epoch 17/20  Iteration 3002/3560 Training loss: 1.1674 1.2844 sec/batch\n",
      "Epoch 17/20  Iteration 3003/3560 Training loss: 1.1673 1.2390 sec/batch\n",
      "Epoch 17/20  Iteration 3004/3560 Training loss: 1.1673 1.2374 sec/batch\n",
      "Epoch 17/20  Iteration 3005/3560 Training loss: 1.1672 1.2486 sec/batch\n",
      "Epoch 17/20  Iteration 3006/3560 Training loss: 1.1672 1.2415 sec/batch\n",
      "Epoch 17/20  Iteration 3007/3560 Training loss: 1.1670 1.3090 sec/batch\n",
      "Epoch 17/20  Iteration 3008/3560 Training loss: 1.1672 1.3168 sec/batch\n",
      "Epoch 17/20  Iteration 3009/3560 Training loss: 1.1673 1.2464 sec/batch\n",
      "Epoch 17/20  Iteration 3010/3560 Training loss: 1.1672 1.3586 sec/batch\n",
      "Epoch 17/20  Iteration 3011/3560 Training loss: 1.1672 1.2472 sec/batch\n",
      "Epoch 17/20  Iteration 3012/3560 Training loss: 1.1672 1.2460 sec/batch\n",
      "Epoch 17/20  Iteration 3013/3560 Training loss: 1.1672 1.2553 sec/batch\n",
      "Epoch 17/20  Iteration 3014/3560 Training loss: 1.1671 1.2399 sec/batch\n",
      "Epoch 17/20  Iteration 3015/3560 Training loss: 1.1672 1.2534 sec/batch\n",
      "Epoch 17/20  Iteration 3016/3560 Training loss: 1.1676 1.2420 sec/batch\n",
      "Epoch 17/20  Iteration 3017/3560 Training loss: 1.1677 1.2475 sec/batch\n",
      "Epoch 17/20  Iteration 3018/3560 Training loss: 1.1677 1.2785 sec/batch\n",
      "Epoch 17/20  Iteration 3019/3560 Training loss: 1.1677 1.3018 sec/batch\n",
      "Epoch 17/20  Iteration 3020/3560 Training loss: 1.1676 1.2498 sec/batch\n",
      "Epoch 17/20  Iteration 3021/3560 Training loss: 1.1678 1.2534 sec/batch\n",
      "Epoch 17/20  Iteration 3022/3560 Training loss: 1.1678 1.2369 sec/batch\n",
      "Epoch 17/20  Iteration 3023/3560 Training loss: 1.1678 1.2463 sec/batch\n",
      "Epoch 17/20  Iteration 3024/3560 Training loss: 1.1677 1.2452 sec/batch\n",
      "Epoch 17/20  Iteration 3025/3560 Training loss: 1.1676 1.2446 sec/batch\n",
      "Epoch 17/20  Iteration 3026/3560 Training loss: 1.1678 1.3115 sec/batch\n",
      "Epoch 18/20  Iteration 3027/3560 Training loss: 1.2916 1.2356 sec/batch\n",
      "Epoch 18/20  Iteration 3028/3560 Training loss: 1.2405 1.2413 sec/batch\n",
      "Epoch 18/20  Iteration 3029/3560 Training loss: 1.2202 1.3649 sec/batch\n",
      "Epoch 18/20  Iteration 3030/3560 Training loss: 1.2158 1.2397 sec/batch\n",
      "Epoch 18/20  Iteration 3031/3560 Training loss: 1.2008 1.2338 sec/batch\n",
      "Epoch 18/20  Iteration 3032/3560 Training loss: 1.1885 1.2457 sec/batch\n",
      "Epoch 18/20  Iteration 3033/3560 Training loss: 1.1864 1.2385 sec/batch\n",
      "Epoch 18/20  Iteration 3034/3560 Training loss: 1.1825 1.2782 sec/batch\n",
      "Epoch 18/20  Iteration 3035/3560 Training loss: 1.1806 1.2277 sec/batch\n",
      "Epoch 18/20  Iteration 3036/3560 Training loss: 1.1787 1.2347 sec/batch\n",
      "Epoch 18/20  Iteration 3037/3560 Training loss: 1.1744 1.2414 sec/batch\n",
      "Epoch 18/20  Iteration 3038/3560 Training loss: 1.1744 1.2802 sec/batch\n",
      "Epoch 18/20  Iteration 3039/3560 Training loss: 1.1737 1.2892 sec/batch\n",
      "Epoch 18/20  Iteration 3040/3560 Training loss: 1.1733 1.2597 sec/batch\n",
      "Epoch 18/20  Iteration 3041/3560 Training loss: 1.1718 1.2510 sec/batch\n",
      "Epoch 18/20  Iteration 3042/3560 Training loss: 1.1700 1.2891 sec/batch\n",
      "Epoch 18/20  Iteration 3043/3560 Training loss: 1.1699 1.2312 sec/batch\n",
      "Epoch 18/20  Iteration 3044/3560 Training loss: 1.1706 1.2393 sec/batch\n",
      "Epoch 18/20  Iteration 3045/3560 Training loss: 1.1706 1.2647 sec/batch\n",
      "Epoch 18/20  Iteration 3046/3560 Training loss: 1.1720 1.3293 sec/batch\n",
      "Epoch 18/20  Iteration 3047/3560 Training loss: 1.1714 1.2970 sec/batch\n",
      "Epoch 18/20  Iteration 3048/3560 Training loss: 1.1718 1.3442 sec/batch\n",
      "Epoch 18/20  Iteration 3049/3560 Training loss: 1.1713 1.2631 sec/batch\n",
      "Epoch 18/20  Iteration 3050/3560 Training loss: 1.1716 1.2665 sec/batch\n",
      "Epoch 18/20  Iteration 3051/3560 Training loss: 1.1715 1.2423 sec/batch\n",
      "Epoch 18/20  Iteration 3052/3560 Training loss: 1.1695 1.2398 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3053/3560 Training loss: 1.1681 1.2490 sec/batch\n",
      "Epoch 18/20  Iteration 3054/3560 Training loss: 1.1683 1.2351 sec/batch\n",
      "Epoch 18/20  Iteration 3055/3560 Training loss: 1.1679 1.2268 sec/batch\n",
      "Epoch 18/20  Iteration 3056/3560 Training loss: 1.1682 1.2448 sec/batch\n",
      "Epoch 18/20  Iteration 3057/3560 Training loss: 1.1674 1.2978 sec/batch\n",
      "Epoch 18/20  Iteration 3058/3560 Training loss: 1.1661 1.2907 sec/batch\n",
      "Epoch 18/20  Iteration 3059/3560 Training loss: 1.1659 1.2429 sec/batch\n",
      "Epoch 18/20  Iteration 3060/3560 Training loss: 1.1660 1.2346 sec/batch\n",
      "Epoch 18/20  Iteration 3061/3560 Training loss: 1.1653 1.2521 sec/batch\n",
      "Epoch 18/20  Iteration 3062/3560 Training loss: 1.1652 1.2479 sec/batch\n",
      "Epoch 18/20  Iteration 3063/3560 Training loss: 1.1647 1.2394 sec/batch\n",
      "Epoch 18/20  Iteration 3064/3560 Training loss: 1.1636 1.2634 sec/batch\n",
      "Epoch 18/20  Iteration 3065/3560 Training loss: 1.1627 1.2644 sec/batch\n",
      "Epoch 18/20  Iteration 3066/3560 Training loss: 1.1624 1.2659 sec/batch\n",
      "Epoch 18/20  Iteration 3067/3560 Training loss: 1.1617 1.3897 sec/batch\n",
      "Epoch 18/20  Iteration 3068/3560 Training loss: 1.1623 1.2630 sec/batch\n",
      "Epoch 18/20  Iteration 3069/3560 Training loss: 1.1621 1.2820 sec/batch\n",
      "Epoch 18/20  Iteration 3070/3560 Training loss: 1.1615 1.4746 sec/batch\n",
      "Epoch 18/20  Iteration 3071/3560 Training loss: 1.1618 1.3047 sec/batch\n",
      "Epoch 18/20  Iteration 3072/3560 Training loss: 1.1613 1.7583 sec/batch\n",
      "Epoch 18/20  Iteration 3073/3560 Training loss: 1.1610 1.8325 sec/batch\n",
      "Epoch 18/20  Iteration 3074/3560 Training loss: 1.1605 1.6912 sec/batch\n",
      "Epoch 18/20  Iteration 3075/3560 Training loss: 1.1606 1.5797 sec/batch\n",
      "Epoch 18/20  Iteration 3076/3560 Training loss: 1.1608 1.3350 sec/batch\n",
      "Epoch 18/20  Iteration 3077/3560 Training loss: 1.1601 1.4350 sec/batch\n",
      "Epoch 18/20  Iteration 3078/3560 Training loss: 1.1608 1.5173 sec/batch\n",
      "Epoch 18/20  Iteration 3079/3560 Training loss: 1.1606 1.8922 sec/batch\n",
      "Epoch 18/20  Iteration 3080/3560 Training loss: 1.1607 1.6935 sec/batch\n",
      "Epoch 18/20  Iteration 3081/3560 Training loss: 1.1604 1.5333 sec/batch\n",
      "Epoch 18/20  Iteration 3082/3560 Training loss: 1.1604 2.3140 sec/batch\n",
      "Epoch 18/20  Iteration 3083/3560 Training loss: 1.1606 2.9635 sec/batch\n",
      "Epoch 18/20  Iteration 3084/3560 Training loss: 1.1603 1.5765 sec/batch\n",
      "Epoch 18/20  Iteration 3085/3560 Training loss: 1.1597 1.5407 sec/batch\n",
      "Epoch 18/20  Iteration 3086/3560 Training loss: 1.1604 1.6878 sec/batch\n",
      "Epoch 18/20  Iteration 3087/3560 Training loss: 1.1603 1.4978 sec/batch\n",
      "Epoch 18/20  Iteration 3088/3560 Training loss: 1.1609 1.4141 sec/batch\n",
      "Epoch 18/20  Iteration 3089/3560 Training loss: 1.1611 1.3789 sec/batch\n",
      "Epoch 18/20  Iteration 3090/3560 Training loss: 1.1612 1.2065 sec/batch\n",
      "Epoch 18/20  Iteration 3091/3560 Training loss: 1.1613 1.2858 sec/batch\n",
      "Epoch 18/20  Iteration 3092/3560 Training loss: 1.1615 1.2418 sec/batch\n",
      "Epoch 18/20  Iteration 3093/3560 Training loss: 1.1617 1.2508 sec/batch\n",
      "Epoch 18/20  Iteration 3094/3560 Training loss: 1.1616 1.2444 sec/batch\n",
      "Epoch 18/20  Iteration 3095/3560 Training loss: 1.1618 1.2692 sec/batch\n",
      "Epoch 18/20  Iteration 3096/3560 Training loss: 1.1616 1.2013 sec/batch\n",
      "Epoch 18/20  Iteration 3097/3560 Training loss: 1.1621 1.2540 sec/batch\n",
      "Epoch 18/20  Iteration 3098/3560 Training loss: 1.1624 1.2977 sec/batch\n",
      "Epoch 18/20  Iteration 3099/3560 Training loss: 1.1630 1.2716 sec/batch\n",
      "Epoch 18/20  Iteration 3100/3560 Training loss: 1.1627 1.3091 sec/batch\n",
      "Epoch 18/20  Iteration 3101/3560 Training loss: 1.1626 1.3747 sec/batch\n",
      "Epoch 18/20  Iteration 3102/3560 Training loss: 1.1627 1.3320 sec/batch\n",
      "Epoch 18/20  Iteration 3103/3560 Training loss: 1.1626 1.1850 sec/batch\n",
      "Epoch 18/20  Iteration 3104/3560 Training loss: 1.1625 1.2164 sec/batch\n",
      "Epoch 18/20  Iteration 3105/3560 Training loss: 1.1618 1.7401 sec/batch\n",
      "Epoch 18/20  Iteration 3106/3560 Training loss: 1.1618 1.6707 sec/batch\n",
      "Epoch 18/20  Iteration 3107/3560 Training loss: 1.1615 1.7446 sec/batch\n",
      "Epoch 18/20  Iteration 3108/3560 Training loss: 1.1614 1.9743 sec/batch\n",
      "Epoch 18/20  Iteration 3109/3560 Training loss: 1.1610 1.6207 sec/batch\n",
      "Epoch 18/20  Iteration 3110/3560 Training loss: 1.1610 1.5528 sec/batch\n",
      "Epoch 18/20  Iteration 3111/3560 Training loss: 1.1606 1.5737 sec/batch\n",
      "Epoch 18/20  Iteration 3112/3560 Training loss: 1.1605 1.8538 sec/batch\n",
      "Epoch 18/20  Iteration 3113/3560 Training loss: 1.1604 1.7352 sec/batch\n",
      "Epoch 18/20  Iteration 3114/3560 Training loss: 1.1602 1.7206 sec/batch\n",
      "Epoch 18/20  Iteration 3115/3560 Training loss: 1.1598 2.1953 sec/batch\n",
      "Epoch 18/20  Iteration 3116/3560 Training loss: 1.1599 2.4533 sec/batch\n",
      "Epoch 18/20  Iteration 3117/3560 Training loss: 1.1597 2.1280 sec/batch\n",
      "Epoch 18/20  Iteration 3118/3560 Training loss: 1.1597 2.0128 sec/batch\n",
      "Epoch 18/20  Iteration 3119/3560 Training loss: 1.1593 2.2346 sec/batch\n",
      "Epoch 18/20  Iteration 3120/3560 Training loss: 1.1590 1.7248 sec/batch\n",
      "Epoch 18/20  Iteration 3121/3560 Training loss: 1.1587 1.5548 sec/batch\n",
      "Epoch 18/20  Iteration 3122/3560 Training loss: 1.1588 1.5383 sec/batch\n",
      "Epoch 18/20  Iteration 3123/3560 Training loss: 1.1588 1.5064 sec/batch\n",
      "Epoch 18/20  Iteration 3124/3560 Training loss: 1.1584 1.6782 sec/batch\n",
      "Epoch 18/20  Iteration 3125/3560 Training loss: 1.1581 1.5492 sec/batch\n",
      "Epoch 18/20  Iteration 3126/3560 Training loss: 1.1578 1.4305 sec/batch\n",
      "Epoch 18/20  Iteration 3127/3560 Training loss: 1.1577 1.5202 sec/batch\n",
      "Epoch 18/20  Iteration 3128/3560 Training loss: 1.1576 1.5729 sec/batch\n",
      "Epoch 18/20  Iteration 3129/3560 Training loss: 1.1575 1.5080 sec/batch\n",
      "Epoch 18/20  Iteration 3130/3560 Training loss: 1.1574 1.4896 sec/batch\n",
      "Epoch 18/20  Iteration 3131/3560 Training loss: 1.1573 1.4200 sec/batch\n",
      "Epoch 18/20  Iteration 3132/3560 Training loss: 1.1571 1.5288 sec/batch\n",
      "Epoch 18/20  Iteration 3133/3560 Training loss: 1.1571 1.4494 sec/batch\n",
      "Epoch 18/20  Iteration 3134/3560 Training loss: 1.1570 1.3445 sec/batch\n",
      "Epoch 18/20  Iteration 3135/3560 Training loss: 1.1569 1.4399 sec/batch\n",
      "Epoch 18/20  Iteration 3136/3560 Training loss: 1.1569 1.4496 sec/batch\n",
      "Epoch 18/20  Iteration 3137/3560 Training loss: 1.1568 1.3885 sec/batch\n",
      "Epoch 18/20  Iteration 3138/3560 Training loss: 1.1568 1.3728 sec/batch\n",
      "Epoch 18/20  Iteration 3139/3560 Training loss: 1.1567 1.3503 sec/batch\n",
      "Epoch 18/20  Iteration 3140/3560 Training loss: 1.1565 1.3771 sec/batch\n",
      "Epoch 18/20  Iteration 3141/3560 Training loss: 1.1562 1.3828 sec/batch\n",
      "Epoch 18/20  Iteration 3142/3560 Training loss: 1.1559 1.3959 sec/batch\n",
      "Epoch 18/20  Iteration 3143/3560 Training loss: 1.1559 1.4678 sec/batch\n",
      "Epoch 18/20  Iteration 3144/3560 Training loss: 1.1560 1.5971 sec/batch\n",
      "Epoch 18/20  Iteration 3145/3560 Training loss: 1.1559 1.4010 sec/batch\n",
      "Epoch 18/20  Iteration 3146/3560 Training loss: 1.1558 1.3972 sec/batch\n",
      "Epoch 18/20  Iteration 3147/3560 Training loss: 1.1558 1.4603 sec/batch\n",
      "Epoch 18/20  Iteration 3148/3560 Training loss: 1.1555 1.3351 sec/batch\n",
      "Epoch 18/20  Iteration 3149/3560 Training loss: 1.1551 1.3278 sec/batch\n",
      "Epoch 18/20  Iteration 3150/3560 Training loss: 1.1550 1.3356 sec/batch\n",
      "Epoch 18/20  Iteration 3151/3560 Training loss: 1.1549 1.4191 sec/batch\n",
      "Epoch 18/20  Iteration 3152/3560 Training loss: 1.1546 1.5488 sec/batch\n",
      "Epoch 18/20  Iteration 3153/3560 Training loss: 1.1546 1.4918 sec/batch\n",
      "Epoch 18/20  Iteration 3154/3560 Training loss: 1.1546 1.5156 sec/batch\n",
      "Epoch 18/20  Iteration 3155/3560 Training loss: 1.1544 1.3800 sec/batch\n",
      "Epoch 18/20  Iteration 3156/3560 Training loss: 1.1541 1.4006 sec/batch\n",
      "Epoch 18/20  Iteration 3157/3560 Training loss: 1.1537 1.3382 sec/batch\n",
      "Epoch 18/20  Iteration 3158/3560 Training loss: 1.1536 1.4084 sec/batch\n",
      "Epoch 18/20  Iteration 3159/3560 Training loss: 1.1536 1.3719 sec/batch\n",
      "Epoch 18/20  Iteration 3160/3560 Training loss: 1.1537 1.4462 sec/batch\n",
      "Epoch 18/20  Iteration 3161/3560 Training loss: 1.1537 1.4352 sec/batch\n",
      "Epoch 18/20  Iteration 3162/3560 Training loss: 1.1537 1.4726 sec/batch\n",
      "Epoch 18/20  Iteration 3163/3560 Training loss: 1.1538 1.4132 sec/batch\n",
      "Epoch 18/20  Iteration 3164/3560 Training loss: 1.1538 1.4084 sec/batch\n",
      "Epoch 18/20  Iteration 3165/3560 Training loss: 1.1539 1.3932 sec/batch\n",
      "Epoch 18/20  Iteration 3166/3560 Training loss: 1.1539 1.3689 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3167/3560 Training loss: 1.1543 1.4140 sec/batch\n",
      "Epoch 18/20  Iteration 3168/3560 Training loss: 1.1544 1.6617 sec/batch\n",
      "Epoch 18/20  Iteration 3169/3560 Training loss: 1.1544 1.5189 sec/batch\n",
      "Epoch 18/20  Iteration 3170/3560 Training loss: 1.1546 1.5004 sec/batch\n",
      "Epoch 18/20  Iteration 3171/3560 Training loss: 1.1546 1.4465 sec/batch\n",
      "Epoch 18/20  Iteration 3172/3560 Training loss: 1.1548 1.4753 sec/batch\n",
      "Epoch 18/20  Iteration 3173/3560 Training loss: 1.1549 1.4882 sec/batch\n",
      "Epoch 18/20  Iteration 3174/3560 Training loss: 1.1551 1.4590 sec/batch\n",
      "Epoch 18/20  Iteration 3175/3560 Training loss: 1.1552 1.4258 sec/batch\n",
      "Epoch 18/20  Iteration 3176/3560 Training loss: 1.1552 1.4443 sec/batch\n",
      "Epoch 18/20  Iteration 3177/3560 Training loss: 1.1549 1.3965 sec/batch\n",
      "Epoch 18/20  Iteration 3178/3560 Training loss: 1.1547 1.3888 sec/batch\n",
      "Epoch 18/20  Iteration 3179/3560 Training loss: 1.1548 1.4349 sec/batch\n",
      "Epoch 18/20  Iteration 3180/3560 Training loss: 1.1548 1.3218 sec/batch\n",
      "Epoch 18/20  Iteration 3181/3560 Training loss: 1.1548 1.3880 sec/batch\n",
      "Epoch 18/20  Iteration 3182/3560 Training loss: 1.1548 1.4392 sec/batch\n",
      "Epoch 18/20  Iteration 3183/3560 Training loss: 1.1548 1.3497 sec/batch\n",
      "Epoch 18/20  Iteration 3184/3560 Training loss: 1.1547 1.3173 sec/batch\n",
      "Epoch 18/20  Iteration 3185/3560 Training loss: 1.1546 1.3257 sec/batch\n",
      "Epoch 18/20  Iteration 3186/3560 Training loss: 1.1547 1.2854 sec/batch\n",
      "Epoch 18/20  Iteration 3187/3560 Training loss: 1.1549 1.5062 sec/batch\n",
      "Epoch 18/20  Iteration 3188/3560 Training loss: 1.1548 1.4541 sec/batch\n",
      "Epoch 18/20  Iteration 3189/3560 Training loss: 1.1548 1.4179 sec/batch\n",
      "Epoch 18/20  Iteration 3190/3560 Training loss: 1.1548 1.4771 sec/batch\n",
      "Epoch 18/20  Iteration 3191/3560 Training loss: 1.1548 1.4832 sec/batch\n",
      "Epoch 18/20  Iteration 3192/3560 Training loss: 1.1546 1.4562 sec/batch\n",
      "Epoch 18/20  Iteration 3193/3560 Training loss: 1.1548 1.4351 sec/batch\n",
      "Epoch 18/20  Iteration 3194/3560 Training loss: 1.1551 1.4136 sec/batch\n",
      "Epoch 18/20  Iteration 3195/3560 Training loss: 1.1552 1.3931 sec/batch\n",
      "Epoch 18/20  Iteration 3196/3560 Training loss: 1.1552 1.3777 sec/batch\n",
      "Epoch 18/20  Iteration 3197/3560 Training loss: 1.1552 1.4176 sec/batch\n",
      "Epoch 18/20  Iteration 3198/3560 Training loss: 1.1551 1.4630 sec/batch\n",
      "Epoch 18/20  Iteration 3199/3560 Training loss: 1.1553 1.5516 sec/batch\n",
      "Epoch 18/20  Iteration 3200/3560 Training loss: 1.1553 1.4159 sec/batch\n",
      "Epoch 18/20  Iteration 3201/3560 Training loss: 1.1553 1.4074 sec/batch\n",
      "Epoch 18/20  Iteration 3202/3560 Training loss: 1.1551 1.3876 sec/batch\n",
      "Epoch 18/20  Iteration 3203/3560 Training loss: 1.1551 1.4392 sec/batch\n",
      "Epoch 18/20  Iteration 3204/3560 Training loss: 1.1553 1.3895 sec/batch\n",
      "Epoch 19/20  Iteration 3205/3560 Training loss: 1.2644 1.4760 sec/batch\n",
      "Epoch 19/20  Iteration 3206/3560 Training loss: 1.2185 1.4735 sec/batch\n",
      "Epoch 19/20  Iteration 3207/3560 Training loss: 1.1998 1.5742 sec/batch\n",
      "Epoch 19/20  Iteration 3208/3560 Training loss: 1.1969 1.4868 sec/batch\n",
      "Epoch 19/20  Iteration 3209/3560 Training loss: 1.1838 1.3988 sec/batch\n",
      "Epoch 19/20  Iteration 3210/3560 Training loss: 1.1730 1.3684 sec/batch\n",
      "Epoch 19/20  Iteration 3211/3560 Training loss: 1.1719 1.3809 sec/batch\n",
      "Epoch 19/20  Iteration 3212/3560 Training loss: 1.1690 1.3966 sec/batch\n",
      "Epoch 19/20  Iteration 3213/3560 Training loss: 1.1675 1.4513 sec/batch\n",
      "Epoch 19/20  Iteration 3214/3560 Training loss: 1.1651 1.3545 sec/batch\n",
      "Epoch 19/20  Iteration 3215/3560 Training loss: 1.1613 1.4797 sec/batch\n",
      "Epoch 19/20  Iteration 3216/3560 Training loss: 1.1614 1.3552 sec/batch\n",
      "Epoch 19/20  Iteration 3217/3560 Training loss: 1.1616 1.3605 sec/batch\n",
      "Epoch 19/20  Iteration 3218/3560 Training loss: 1.1619 1.3489 sec/batch\n",
      "Epoch 19/20  Iteration 3219/3560 Training loss: 1.1606 1.4257 sec/batch\n",
      "Epoch 19/20  Iteration 3220/3560 Training loss: 1.1591 1.3661 sec/batch\n",
      "Epoch 19/20  Iteration 3221/3560 Training loss: 1.1591 1.5068 sec/batch\n",
      "Epoch 19/20  Iteration 3222/3560 Training loss: 1.1599 1.4289 sec/batch\n",
      "Epoch 19/20  Iteration 3223/3560 Training loss: 1.1600 1.4329 sec/batch\n",
      "Epoch 19/20  Iteration 3224/3560 Training loss: 1.1610 1.4353 sec/batch\n",
      "Epoch 19/20  Iteration 3225/3560 Training loss: 1.1604 1.4295 sec/batch\n",
      "Epoch 19/20  Iteration 3226/3560 Training loss: 1.1607 1.4607 sec/batch\n",
      "Epoch 19/20  Iteration 3227/3560 Training loss: 1.1603 1.4508 sec/batch\n",
      "Epoch 19/20  Iteration 3228/3560 Training loss: 1.1605 1.5105 sec/batch\n",
      "Epoch 19/20  Iteration 3229/3560 Training loss: 1.1604 1.6151 sec/batch\n",
      "Epoch 19/20  Iteration 3230/3560 Training loss: 1.1585 1.4225 sec/batch\n",
      "Epoch 19/20  Iteration 3231/3560 Training loss: 1.1574 1.3856 sec/batch\n",
      "Epoch 19/20  Iteration 3232/3560 Training loss: 1.1577 1.4180 sec/batch\n",
      "Epoch 19/20  Iteration 3233/3560 Training loss: 1.1575 1.4474 sec/batch\n",
      "Epoch 19/20  Iteration 3234/3560 Training loss: 1.1574 1.4256 sec/batch\n",
      "Epoch 19/20  Iteration 3235/3560 Training loss: 1.1567 1.4316 sec/batch\n",
      "Epoch 19/20  Iteration 3236/3560 Training loss: 1.1562 1.4664 sec/batch\n",
      "Epoch 19/20  Iteration 3237/3560 Training loss: 1.1560 1.6422 sec/batch\n",
      "Epoch 19/20  Iteration 3238/3560 Training loss: 1.1560 1.9223 sec/batch\n",
      "Epoch 19/20  Iteration 3239/3560 Training loss: 1.1555 1.8760 sec/batch\n",
      "Epoch 19/20  Iteration 3240/3560 Training loss: 1.1553 1.9332 sec/batch\n",
      "Epoch 19/20  Iteration 3241/3560 Training loss: 1.1548 1.9752 sec/batch\n",
      "Epoch 19/20  Iteration 3242/3560 Training loss: 1.1536 1.8028 sec/batch\n",
      "Epoch 19/20  Iteration 3243/3560 Training loss: 1.1525 1.4669 sec/batch\n",
      "Epoch 19/20  Iteration 3244/3560 Training loss: 1.1522 1.4697 sec/batch\n",
      "Epoch 19/20  Iteration 3245/3560 Training loss: 1.1516 1.3972 sec/batch\n",
      "Epoch 19/20  Iteration 3246/3560 Training loss: 1.1525 1.3933 sec/batch\n",
      "Epoch 19/20  Iteration 3247/3560 Training loss: 1.1522 1.3813 sec/batch\n",
      "Epoch 19/20  Iteration 3248/3560 Training loss: 1.1515 1.4328 sec/batch\n",
      "Epoch 19/20  Iteration 3249/3560 Training loss: 1.1518 1.5802 sec/batch\n",
      "Epoch 19/20  Iteration 3250/3560 Training loss: 1.1512 1.4742 sec/batch\n",
      "Epoch 19/20  Iteration 3251/3560 Training loss: 1.1507 1.4550 sec/batch\n",
      "Epoch 19/20  Iteration 3252/3560 Training loss: 1.1505 1.6059 sec/batch\n",
      "Epoch 19/20  Iteration 3253/3560 Training loss: 1.1503 1.4829 sec/batch\n",
      "Epoch 19/20  Iteration 3254/3560 Training loss: 1.1504 1.4638 sec/batch\n",
      "Epoch 19/20  Iteration 3255/3560 Training loss: 1.1500 1.5077 sec/batch\n",
      "Epoch 19/20  Iteration 3256/3560 Training loss: 1.1507 1.4591 sec/batch\n",
      "Epoch 19/20  Iteration 3257/3560 Training loss: 1.1507 1.4475 sec/batch\n",
      "Epoch 19/20  Iteration 3258/3560 Training loss: 1.1510 1.4676 sec/batch\n",
      "Epoch 19/20  Iteration 3259/3560 Training loss: 1.1507 1.4267 sec/batch\n",
      "Epoch 19/20  Iteration 3260/3560 Training loss: 1.1508 1.4890 sec/batch\n",
      "Epoch 19/20  Iteration 3261/3560 Training loss: 1.1510 1.4731 sec/batch\n",
      "Epoch 19/20  Iteration 3262/3560 Training loss: 1.1507 1.5058 sec/batch\n",
      "Epoch 19/20  Iteration 3263/3560 Training loss: 1.1501 1.4201 sec/batch\n",
      "Epoch 19/20  Iteration 3264/3560 Training loss: 1.1509 1.4307 sec/batch\n",
      "Epoch 19/20  Iteration 3265/3560 Training loss: 1.1509 1.5250 sec/batch\n",
      "Epoch 19/20  Iteration 3266/3560 Training loss: 1.1515 1.4304 sec/batch\n",
      "Epoch 19/20  Iteration 3267/3560 Training loss: 1.1518 1.4624 sec/batch\n",
      "Epoch 19/20  Iteration 3268/3560 Training loss: 1.1519 1.5166 sec/batch\n",
      "Epoch 19/20  Iteration 3269/3560 Training loss: 1.1521 1.5721 sec/batch\n",
      "Epoch 19/20  Iteration 3270/3560 Training loss: 1.1522 1.4817 sec/batch\n",
      "Epoch 19/20  Iteration 3271/3560 Training loss: 1.1525 1.4845 sec/batch\n",
      "Epoch 19/20  Iteration 3272/3560 Training loss: 1.1525 1.4359 sec/batch\n",
      "Epoch 19/20  Iteration 3273/3560 Training loss: 1.1528 1.4336 sec/batch\n",
      "Epoch 19/20  Iteration 3274/3560 Training loss: 1.1525 1.4325 sec/batch\n",
      "Epoch 19/20  Iteration 3275/3560 Training loss: 1.1530 1.4040 sec/batch\n",
      "Epoch 19/20  Iteration 3276/3560 Training loss: 1.1533 1.3824 sec/batch\n",
      "Epoch 19/20  Iteration 3277/3560 Training loss: 1.1538 1.4867 sec/batch\n",
      "Epoch 19/20  Iteration 3278/3560 Training loss: 1.1535 1.4911 sec/batch\n",
      "Epoch 19/20  Iteration 3279/3560 Training loss: 1.1534 1.3991 sec/batch\n",
      "Epoch 19/20  Iteration 3280/3560 Training loss: 1.1534 1.4011 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3281/3560 Training loss: 1.1533 1.4297 sec/batch\n",
      "Epoch 19/20  Iteration 3282/3560 Training loss: 1.1533 1.3700 sec/batch\n",
      "Epoch 19/20  Iteration 3283/3560 Training loss: 1.1526 1.4076 sec/batch\n",
      "Epoch 19/20  Iteration 3284/3560 Training loss: 1.1527 1.2949 sec/batch\n",
      "Epoch 19/20  Iteration 3285/3560 Training loss: 1.1523 1.4136 sec/batch\n",
      "Epoch 19/20  Iteration 3286/3560 Training loss: 1.1522 1.3430 sec/batch\n",
      "Epoch 19/20  Iteration 3287/3560 Training loss: 1.1518 1.3145 sec/batch\n",
      "Epoch 19/20  Iteration 3288/3560 Training loss: 1.1518 1.3736 sec/batch\n",
      "Epoch 19/20  Iteration 3289/3560 Training loss: 1.1515 1.4143 sec/batch\n",
      "Epoch 19/20  Iteration 3290/3560 Training loss: 1.1515 1.4082 sec/batch\n",
      "Epoch 19/20  Iteration 3291/3560 Training loss: 1.1514 1.5143 sec/batch\n",
      "Epoch 19/20  Iteration 3292/3560 Training loss: 1.1512 1.4081 sec/batch\n",
      "Epoch 19/20  Iteration 3293/3560 Training loss: 1.1509 1.3847 sec/batch\n",
      "Epoch 19/20  Iteration 3294/3560 Training loss: 1.1510 1.7145 sec/batch\n",
      "Epoch 19/20  Iteration 3295/3560 Training loss: 1.1508 1.3957 sec/batch\n",
      "Epoch 19/20  Iteration 3296/3560 Training loss: 1.1507 1.5536 sec/batch\n",
      "Epoch 19/20  Iteration 3297/3560 Training loss: 1.1504 1.4093 sec/batch\n",
      "Epoch 19/20  Iteration 3298/3560 Training loss: 1.1500 1.3223 sec/batch\n",
      "Epoch 19/20  Iteration 3299/3560 Training loss: 1.1499 1.4737 sec/batch\n",
      "Epoch 19/20  Iteration 3300/3560 Training loss: 1.1499 1.3255 sec/batch\n",
      "Epoch 19/20  Iteration 3301/3560 Training loss: 1.1499 1.3060 sec/batch\n",
      "Epoch 19/20  Iteration 3302/3560 Training loss: 1.1495 1.2992 sec/batch\n",
      "Epoch 19/20  Iteration 3303/3560 Training loss: 1.1493 1.2769 sec/batch\n",
      "Epoch 19/20  Iteration 3304/3560 Training loss: 1.1490 1.2706 sec/batch\n",
      "Epoch 19/20  Iteration 3305/3560 Training loss: 1.1489 1.3182 sec/batch\n",
      "Epoch 19/20  Iteration 3306/3560 Training loss: 1.1488 1.2660 sec/batch\n",
      "Epoch 19/20  Iteration 3307/3560 Training loss: 1.1488 1.2620 sec/batch\n",
      "Epoch 19/20  Iteration 3308/3560 Training loss: 1.1487 1.2852 sec/batch\n",
      "Epoch 19/20  Iteration 3309/3560 Training loss: 1.1485 1.2729 sec/batch\n",
      "Epoch 19/20  Iteration 3310/3560 Training loss: 1.1482 1.2681 sec/batch\n",
      "Epoch 19/20  Iteration 3311/3560 Training loss: 1.1481 1.4635 sec/batch\n",
      "Epoch 19/20  Iteration 3312/3560 Training loss: 1.1480 1.4000 sec/batch\n",
      "Epoch 19/20  Iteration 3313/3560 Training loss: 1.1477 1.3687 sec/batch\n",
      "Epoch 19/20  Iteration 3314/3560 Training loss: 1.1478 1.3035 sec/batch\n",
      "Epoch 19/20  Iteration 3315/3560 Training loss: 1.1475 1.2584 sec/batch\n",
      "Epoch 19/20  Iteration 3316/3560 Training loss: 1.1474 1.2643 sec/batch\n",
      "Epoch 19/20  Iteration 3317/3560 Training loss: 1.1473 1.2853 sec/batch\n",
      "Epoch 19/20  Iteration 3318/3560 Training loss: 1.1472 1.2930 sec/batch\n",
      "Epoch 19/20  Iteration 3319/3560 Training loss: 1.1469 1.2845 sec/batch\n",
      "Epoch 19/20  Iteration 3320/3560 Training loss: 1.1466 1.3459 sec/batch\n",
      "Epoch 19/20  Iteration 3321/3560 Training loss: 1.1466 1.3127 sec/batch\n",
      "Epoch 19/20  Iteration 3322/3560 Training loss: 1.1466 1.3079 sec/batch\n",
      "Epoch 19/20  Iteration 3323/3560 Training loss: 1.1465 1.2967 sec/batch\n",
      "Epoch 19/20  Iteration 3324/3560 Training loss: 1.1463 1.2813 sec/batch\n",
      "Epoch 19/20  Iteration 3325/3560 Training loss: 1.1463 1.3129 sec/batch\n",
      "Epoch 19/20  Iteration 3326/3560 Training loss: 1.1460 1.2929 sec/batch\n",
      "Epoch 19/20  Iteration 3327/3560 Training loss: 1.1457 1.2647 sec/batch\n",
      "Epoch 19/20  Iteration 3328/3560 Training loss: 1.1456 1.3577 sec/batch\n",
      "Epoch 19/20  Iteration 3329/3560 Training loss: 1.1455 1.2905 sec/batch\n",
      "Epoch 19/20  Iteration 3330/3560 Training loss: 1.1452 1.2899 sec/batch\n",
      "Epoch 19/20  Iteration 3331/3560 Training loss: 1.1452 1.2823 sec/batch\n",
      "Epoch 19/20  Iteration 3332/3560 Training loss: 1.1451 1.2939 sec/batch\n",
      "Epoch 19/20  Iteration 3333/3560 Training loss: 1.1449 1.3623 sec/batch\n",
      "Epoch 19/20  Iteration 3334/3560 Training loss: 1.1445 1.3637 sec/batch\n",
      "Epoch 19/20  Iteration 3335/3560 Training loss: 1.1442 1.3039 sec/batch\n",
      "Epoch 19/20  Iteration 3336/3560 Training loss: 1.1440 1.3337 sec/batch\n",
      "Epoch 19/20  Iteration 3337/3560 Training loss: 1.1441 1.2999 sec/batch\n",
      "Epoch 19/20  Iteration 3338/3560 Training loss: 1.1441 1.2860 sec/batch\n",
      "Epoch 19/20  Iteration 3339/3560 Training loss: 1.1440 1.2982 sec/batch\n",
      "Epoch 19/20  Iteration 3340/3560 Training loss: 1.1441 1.2920 sec/batch\n",
      "Epoch 19/20  Iteration 3341/3560 Training loss: 1.1441 1.2895 sec/batch\n",
      "Epoch 19/20  Iteration 3342/3560 Training loss: 1.1442 1.4254 sec/batch\n",
      "Epoch 19/20  Iteration 3343/3560 Training loss: 1.1443 1.3709 sec/batch\n",
      "Epoch 19/20  Iteration 3344/3560 Training loss: 1.1443 1.3106 sec/batch\n",
      "Epoch 19/20  Iteration 3345/3560 Training loss: 1.1447 1.3014 sec/batch\n",
      "Epoch 19/20  Iteration 3346/3560 Training loss: 1.1448 1.2837 sec/batch\n",
      "Epoch 19/20  Iteration 3347/3560 Training loss: 1.1448 1.3138 sec/batch\n",
      "Epoch 19/20  Iteration 3348/3560 Training loss: 1.1451 1.2912 sec/batch\n",
      "Epoch 19/20  Iteration 3349/3560 Training loss: 1.1451 1.2875 sec/batch\n",
      "Epoch 19/20  Iteration 3350/3560 Training loss: 1.1453 1.3397 sec/batch\n",
      "Epoch 19/20  Iteration 3351/3560 Training loss: 1.1454 1.3679 sec/batch\n",
      "Epoch 19/20  Iteration 3352/3560 Training loss: 1.1456 1.3615 sec/batch\n",
      "Epoch 19/20  Iteration 3353/3560 Training loss: 1.1457 1.3069 sec/batch\n",
      "Epoch 19/20  Iteration 3354/3560 Training loss: 1.1458 1.2830 sec/batch\n",
      "Epoch 19/20  Iteration 3355/3560 Training loss: 1.1455 1.3044 sec/batch\n",
      "Epoch 19/20  Iteration 3356/3560 Training loss: 1.1453 1.2804 sec/batch\n",
      "Epoch 19/20  Iteration 3357/3560 Training loss: 1.1454 1.3072 sec/batch\n",
      "Epoch 19/20  Iteration 3358/3560 Training loss: 1.1454 1.3255 sec/batch\n",
      "Epoch 19/20  Iteration 3359/3560 Training loss: 1.1454 1.3296 sec/batch\n",
      "Epoch 19/20  Iteration 3360/3560 Training loss: 1.1454 1.3205 sec/batch\n",
      "Epoch 19/20  Iteration 3361/3560 Training loss: 1.1454 1.3164 sec/batch\n",
      "Epoch 19/20  Iteration 3362/3560 Training loss: 1.1453 1.3550 sec/batch\n",
      "Epoch 19/20  Iteration 3363/3560 Training loss: 1.1453 1.3211 sec/batch\n",
      "Epoch 19/20  Iteration 3364/3560 Training loss: 1.1454 1.3272 sec/batch\n",
      "Epoch 19/20  Iteration 3365/3560 Training loss: 1.1455 1.2945 sec/batch\n",
      "Epoch 19/20  Iteration 3366/3560 Training loss: 1.1455 1.7526 sec/batch\n",
      "Epoch 19/20  Iteration 3367/3560 Training loss: 1.1454 1.6680 sec/batch\n",
      "Epoch 19/20  Iteration 3368/3560 Training loss: 1.1454 1.7329 sec/batch\n",
      "Epoch 19/20  Iteration 3369/3560 Training loss: 1.1454 1.8302 sec/batch\n",
      "Epoch 19/20  Iteration 3370/3560 Training loss: 1.1453 1.7378 sec/batch\n",
      "Epoch 19/20  Iteration 3371/3560 Training loss: 1.1454 1.4732 sec/batch\n",
      "Epoch 19/20  Iteration 3372/3560 Training loss: 1.1457 1.3429 sec/batch\n",
      "Epoch 19/20  Iteration 3373/3560 Training loss: 1.1457 1.3230 sec/batch\n",
      "Epoch 19/20  Iteration 3374/3560 Training loss: 1.1458 1.3107 sec/batch\n",
      "Epoch 19/20  Iteration 3375/3560 Training loss: 1.1458 1.2929 sec/batch\n",
      "Epoch 19/20  Iteration 3376/3560 Training loss: 1.1456 1.3094 sec/batch\n",
      "Epoch 19/20  Iteration 3377/3560 Training loss: 1.1459 1.3598 sec/batch\n",
      "Epoch 19/20  Iteration 3378/3560 Training loss: 1.1459 1.3081 sec/batch\n",
      "Epoch 19/20  Iteration 3379/3560 Training loss: 1.1459 1.2906 sec/batch\n",
      "Epoch 19/20  Iteration 3380/3560 Training loss: 1.1458 1.3383 sec/batch\n",
      "Epoch 19/20  Iteration 3381/3560 Training loss: 1.1457 1.3326 sec/batch\n",
      "Epoch 19/20  Iteration 3382/3560 Training loss: 1.1459 1.5413 sec/batch\n",
      "Epoch 20/20  Iteration 3383/3560 Training loss: 1.2441 1.6444 sec/batch\n",
      "Epoch 20/20  Iteration 3384/3560 Training loss: 1.2040 1.5470 sec/batch\n",
      "Epoch 20/20  Iteration 3385/3560 Training loss: 1.1887 1.7120 sec/batch\n",
      "Epoch 20/20  Iteration 3386/3560 Training loss: 1.1858 1.6082 sec/batch\n",
      "Epoch 20/20  Iteration 3387/3560 Training loss: 1.1740 1.6360 sec/batch\n",
      "Epoch 20/20  Iteration 3388/3560 Training loss: 1.1621 1.5090 sec/batch\n",
      "Epoch 20/20  Iteration 3389/3560 Training loss: 1.1607 1.4090 sec/batch\n",
      "Epoch 20/20  Iteration 3390/3560 Training loss: 1.1577 1.3263 sec/batch\n",
      "Epoch 20/20  Iteration 3391/3560 Training loss: 1.1564 1.2798 sec/batch\n",
      "Epoch 20/20  Iteration 3392/3560 Training loss: 1.1537 1.4223 sec/batch\n",
      "Epoch 20/20  Iteration 3393/3560 Training loss: 1.1500 1.4147 sec/batch\n",
      "Epoch 20/20  Iteration 3394/3560 Training loss: 1.1502 1.4240 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3395/3560 Training loss: 1.1500 1.4624 sec/batch\n",
      "Epoch 20/20  Iteration 3396/3560 Training loss: 1.1494 1.5764 sec/batch\n",
      "Epoch 20/20  Iteration 3397/3560 Training loss: 1.1476 1.9480 sec/batch\n",
      "Epoch 20/20  Iteration 3398/3560 Training loss: 1.1460 1.4332 sec/batch\n",
      "Epoch 20/20  Iteration 3399/3560 Training loss: 1.1458 1.3627 sec/batch\n",
      "Epoch 20/20  Iteration 3400/3560 Training loss: 1.1467 1.2623 sec/batch\n",
      "Epoch 20/20  Iteration 3401/3560 Training loss: 1.1462 1.2856 sec/batch\n",
      "Epoch 20/20  Iteration 3402/3560 Training loss: 1.1473 1.3158 sec/batch\n",
      "Epoch 20/20  Iteration 3403/3560 Training loss: 1.1470 1.2545 sec/batch\n",
      "Epoch 20/20  Iteration 3404/3560 Training loss: 1.1476 1.4088 sec/batch\n",
      "Epoch 20/20  Iteration 3405/3560 Training loss: 1.1472 1.3585 sec/batch\n",
      "Epoch 20/20  Iteration 3406/3560 Training loss: 1.1478 1.5196 sec/batch\n",
      "Epoch 20/20  Iteration 3407/3560 Training loss: 1.1476 1.2265 sec/batch\n",
      "Epoch 20/20  Iteration 3408/3560 Training loss: 1.1457 1.2978 sec/batch\n",
      "Epoch 20/20  Iteration 3409/3560 Training loss: 1.1446 1.2599 sec/batch\n",
      "Epoch 20/20  Iteration 3410/3560 Training loss: 1.1450 1.3004 sec/batch\n",
      "Epoch 20/20  Iteration 3411/3560 Training loss: 1.1445 1.5800 sec/batch\n",
      "Epoch 20/20  Iteration 3412/3560 Training loss: 1.1447 1.3693 sec/batch\n",
      "Epoch 20/20  Iteration 3413/3560 Training loss: 1.1440 1.2862 sec/batch\n",
      "Epoch 20/20  Iteration 3414/3560 Training loss: 1.1433 1.2649 sec/batch\n",
      "Epoch 20/20  Iteration 3415/3560 Training loss: 1.1433 1.2709 sec/batch\n",
      "Epoch 20/20  Iteration 3416/3560 Training loss: 1.1434 1.2804 sec/batch\n",
      "Epoch 20/20  Iteration 3417/3560 Training loss: 1.1428 1.2429 sec/batch\n",
      "Epoch 20/20  Iteration 3418/3560 Training loss: 1.1424 1.2463 sec/batch\n",
      "Epoch 20/20  Iteration 3419/3560 Training loss: 1.1421 1.2787 sec/batch\n",
      "Epoch 20/20  Iteration 3420/3560 Training loss: 1.1411 1.3868 sec/batch\n",
      "Epoch 20/20  Iteration 3421/3560 Training loss: 1.1404 1.4906 sec/batch\n",
      "Epoch 20/20  Iteration 3422/3560 Training loss: 1.1402 1.4036 sec/batch\n",
      "Epoch 20/20  Iteration 3423/3560 Training loss: 1.1398 1.5516 sec/batch\n",
      "Epoch 20/20  Iteration 3424/3560 Training loss: 1.1405 1.2532 sec/batch\n",
      "Epoch 20/20  Iteration 3425/3560 Training loss: 1.1405 1.2620 sec/batch\n",
      "Epoch 20/20  Iteration 3426/3560 Training loss: 1.1397 1.2171 sec/batch\n",
      "Epoch 20/20  Iteration 3427/3560 Training loss: 1.1397 1.2346 sec/batch\n",
      "Epoch 20/20  Iteration 3428/3560 Training loss: 1.1394 1.2300 sec/batch\n",
      "Epoch 20/20  Iteration 3429/3560 Training loss: 1.1391 1.2949 sec/batch\n",
      "Epoch 20/20  Iteration 3430/3560 Training loss: 1.1387 1.3393 sec/batch\n",
      "Epoch 20/20  Iteration 3431/3560 Training loss: 1.1385 1.5692 sec/batch\n",
      "Epoch 20/20  Iteration 3432/3560 Training loss: 1.1388 1.9673 sec/batch\n",
      "Epoch 20/20  Iteration 3433/3560 Training loss: 1.1383 2.1208 sec/batch\n",
      "Epoch 20/20  Iteration 3434/3560 Training loss: 1.1388 1.7611 sec/batch\n",
      "Epoch 20/20  Iteration 3435/3560 Training loss: 1.1387 1.7545 sec/batch\n",
      "Epoch 20/20  Iteration 3436/3560 Training loss: 1.1386 1.6651 sec/batch\n",
      "Epoch 20/20  Iteration 3437/3560 Training loss: 1.1384 1.3732 sec/batch\n",
      "Epoch 20/20  Iteration 3438/3560 Training loss: 1.1385 1.4191 sec/batch\n",
      "Epoch 20/20  Iteration 3439/3560 Training loss: 1.1383 1.9088 sec/batch\n",
      "Epoch 20/20  Iteration 3440/3560 Training loss: 1.1383 2.1421 sec/batch\n",
      "Epoch 20/20  Iteration 3441/3560 Training loss: 1.1377 1.9747 sec/batch\n",
      "Epoch 20/20  Iteration 3442/3560 Training loss: 1.1384 1.4114 sec/batch\n",
      "Epoch 20/20  Iteration 3443/3560 Training loss: 1.1383 1.6567 sec/batch\n",
      "Epoch 20/20  Iteration 3444/3560 Training loss: 1.1391 1.3874 sec/batch\n",
      "Epoch 20/20  Iteration 3445/3560 Training loss: 1.1393 1.2572 sec/batch\n",
      "Epoch 20/20  Iteration 3446/3560 Training loss: 1.1393 1.2556 sec/batch\n",
      "Epoch 20/20  Iteration 3447/3560 Training loss: 1.1393 1.6507 sec/batch\n",
      "Epoch 20/20  Iteration 3448/3560 Training loss: 1.1395 1.4480 sec/batch\n",
      "Epoch 20/20  Iteration 3449/3560 Training loss: 1.1399 1.4182 sec/batch\n",
      "Epoch 20/20  Iteration 3450/3560 Training loss: 1.1397 1.8563 sec/batch\n",
      "Epoch 20/20  Iteration 3451/3560 Training loss: 1.1398 1.8349 sec/batch\n",
      "Epoch 20/20  Iteration 3452/3560 Training loss: 1.1397 1.2767 sec/batch\n",
      "Epoch 20/20  Iteration 3453/3560 Training loss: 1.1403 1.2461 sec/batch\n",
      "Epoch 20/20  Iteration 3454/3560 Training loss: 1.1406 1.2554 sec/batch\n",
      "Epoch 20/20  Iteration 3455/3560 Training loss: 1.1412 1.4278 sec/batch\n",
      "Epoch 20/20  Iteration 3456/3560 Training loss: 1.1409 1.4502 sec/batch\n",
      "Epoch 20/20  Iteration 3457/3560 Training loss: 1.1409 1.7456 sec/batch\n",
      "Epoch 20/20  Iteration 3458/3560 Training loss: 1.1410 1.7990 sec/batch\n",
      "Epoch 20/20  Iteration 3459/3560 Training loss: 1.1410 1.8419 sec/batch\n",
      "Epoch 20/20  Iteration 3460/3560 Training loss: 1.1409 1.4420 sec/batch\n",
      "Epoch 20/20  Iteration 3461/3560 Training loss: 1.1404 1.7909 sec/batch\n",
      "Epoch 20/20  Iteration 3462/3560 Training loss: 1.1403 1.3892 sec/batch\n",
      "Epoch 20/20  Iteration 3463/3560 Training loss: 1.1400 1.6525 sec/batch\n",
      "Epoch 20/20  Iteration 3464/3560 Training loss: 1.1398 1.3758 sec/batch\n",
      "Epoch 20/20  Iteration 3465/3560 Training loss: 1.1393 1.2802 sec/batch\n",
      "Epoch 20/20  Iteration 3466/3560 Training loss: 1.1393 1.2488 sec/batch\n",
      "Epoch 20/20  Iteration 3467/3560 Training loss: 1.1390 1.3581 sec/batch\n",
      "Epoch 20/20  Iteration 3468/3560 Training loss: 1.1388 1.3061 sec/batch\n",
      "Epoch 20/20  Iteration 3469/3560 Training loss: 1.1387 1.3201 sec/batch\n",
      "Epoch 20/20  Iteration 3470/3560 Training loss: 1.1387 1.6136 sec/batch\n",
      "Epoch 20/20  Iteration 3471/3560 Training loss: 1.1385 1.2472 sec/batch\n",
      "Epoch 20/20  Iteration 3472/3560 Training loss: 1.1385 1.2471 sec/batch\n",
      "Epoch 20/20  Iteration 3473/3560 Training loss: 1.1383 1.2805 sec/batch\n",
      "Epoch 20/20  Iteration 3474/3560 Training loss: 1.1384 1.2479 sec/batch\n",
      "Epoch 20/20  Iteration 3475/3560 Training loss: 1.1381 1.2446 sec/batch\n",
      "Epoch 20/20  Iteration 3476/3560 Training loss: 1.1377 1.3012 sec/batch\n",
      "Epoch 20/20  Iteration 3477/3560 Training loss: 1.1374 1.3181 sec/batch\n",
      "Epoch 20/20  Iteration 3478/3560 Training loss: 1.1375 1.2928 sec/batch\n",
      "Epoch 20/20  Iteration 3479/3560 Training loss: 1.1375 1.2498 sec/batch\n",
      "Epoch 20/20  Iteration 3480/3560 Training loss: 1.1372 1.2524 sec/batch\n",
      "Epoch 20/20  Iteration 3481/3560 Training loss: 1.1369 1.2416 sec/batch\n",
      "Epoch 20/20  Iteration 3482/3560 Training loss: 1.1366 1.2390 sec/batch\n",
      "Epoch 20/20  Iteration 3483/3560 Training loss: 1.1365 1.2512 sec/batch\n",
      "Epoch 20/20  Iteration 3484/3560 Training loss: 1.1365 1.2692 sec/batch\n",
      "Epoch 20/20  Iteration 3485/3560 Training loss: 1.1364 1.2996 sec/batch\n",
      "Epoch 20/20  Iteration 3486/3560 Training loss: 1.1364 1.3113 sec/batch\n",
      "Epoch 20/20  Iteration 3487/3560 Training loss: 1.1362 1.2452 sec/batch\n",
      "Epoch 20/20  Iteration 3488/3560 Training loss: 1.1360 1.4047 sec/batch\n",
      "Epoch 20/20  Iteration 3489/3560 Training loss: 1.1359 1.5630 sec/batch\n",
      "Epoch 20/20  Iteration 3490/3560 Training loss: 1.1359 1.5488 sec/batch\n",
      "Epoch 20/20  Iteration 3491/3560 Training loss: 1.1357 1.5831 sec/batch\n",
      "Epoch 20/20  Iteration 3492/3560 Training loss: 1.1358 1.5426 sec/batch\n",
      "Epoch 20/20  Iteration 3493/3560 Training loss: 1.1356 1.4788 sec/batch\n",
      "Epoch 20/20  Iteration 3494/3560 Training loss: 1.1355 1.6222 sec/batch\n",
      "Epoch 20/20  Iteration 3495/3560 Training loss: 1.1353 1.6728 sec/batch\n",
      "Epoch 20/20  Iteration 3496/3560 Training loss: 1.1352 1.3529 sec/batch\n",
      "Epoch 20/20  Iteration 3497/3560 Training loss: 1.1349 1.3343 sec/batch\n",
      "Epoch 20/20  Iteration 3498/3560 Training loss: 1.1347 1.3430 sec/batch\n",
      "Epoch 20/20  Iteration 3499/3560 Training loss: 1.1347 1.3934 sec/batch\n",
      "Epoch 20/20  Iteration 3500/3560 Training loss: 1.1347 1.3453 sec/batch\n",
      "Epoch 20/20  Iteration 3501/3560 Training loss: 1.1347 1.3391 sec/batch\n",
      "Epoch 20/20  Iteration 3502/3560 Training loss: 1.1346 1.3407 sec/batch\n",
      "Epoch 20/20  Iteration 3503/3560 Training loss: 1.1346 1.3491 sec/batch\n",
      "Epoch 20/20  Iteration 3504/3560 Training loss: 1.1344 1.3230 sec/batch\n",
      "Epoch 20/20  Iteration 3505/3560 Training loss: 1.1341 1.3275 sec/batch\n",
      "Epoch 20/20  Iteration 3506/3560 Training loss: 1.1341 1.3811 sec/batch\n",
      "Epoch 20/20  Iteration 3507/3560 Training loss: 1.1339 1.3722 sec/batch\n",
      "Epoch 20/20  Iteration 3508/3560 Training loss: 1.1336 1.3272 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3509/3560 Training loss: 1.1336 1.3306 sec/batch\n",
      "Epoch 20/20  Iteration 3510/3560 Training loss: 1.1336 1.3297 sec/batch\n",
      "Epoch 20/20  Iteration 3511/3560 Training loss: 1.1335 1.3335 sec/batch\n",
      "Epoch 20/20  Iteration 3512/3560 Training loss: 1.1332 1.3480 sec/batch\n",
      "Epoch 20/20  Iteration 3513/3560 Training loss: 1.1329 1.3170 sec/batch\n",
      "Epoch 20/20  Iteration 3514/3560 Training loss: 1.1328 1.4252 sec/batch\n",
      "Epoch 20/20  Iteration 3515/3560 Training loss: 1.1329 1.3479 sec/batch\n",
      "Epoch 20/20  Iteration 3516/3560 Training loss: 1.1329 1.2386 sec/batch\n",
      "Epoch 20/20  Iteration 3517/3560 Training loss: 1.1329 1.2547 sec/batch\n",
      "Epoch 20/20  Iteration 3518/3560 Training loss: 1.1330 1.2403 sec/batch\n",
      "Epoch 20/20  Iteration 3519/3560 Training loss: 1.1331 1.2531 sec/batch\n",
      "Epoch 20/20  Iteration 3520/3560 Training loss: 1.1333 1.2552 sec/batch\n",
      "Epoch 20/20  Iteration 3521/3560 Training loss: 1.1333 1.2341 sec/batch\n",
      "Epoch 20/20  Iteration 3522/3560 Training loss: 1.1333 1.3049 sec/batch\n",
      "Epoch 20/20  Iteration 3523/3560 Training loss: 1.1337 1.2504 sec/batch\n",
      "Epoch 20/20  Iteration 3524/3560 Training loss: 1.1338 1.2388 sec/batch\n",
      "Epoch 20/20  Iteration 3525/3560 Training loss: 1.1338 1.2423 sec/batch\n",
      "Epoch 20/20  Iteration 3526/3560 Training loss: 1.1341 1.2398 sec/batch\n",
      "Epoch 20/20  Iteration 3527/3560 Training loss: 1.1342 1.2429 sec/batch\n",
      "Epoch 20/20  Iteration 3528/3560 Training loss: 1.1344 1.2418 sec/batch\n",
      "Epoch 20/20  Iteration 3529/3560 Training loss: 1.1345 1.2433 sec/batch\n",
      "Epoch 20/20  Iteration 3530/3560 Training loss: 1.1347 1.2699 sec/batch\n",
      "Epoch 20/20  Iteration 3531/3560 Training loss: 1.1349 1.2502 sec/batch\n",
      "Epoch 20/20  Iteration 3532/3560 Training loss: 1.1348 1.2334 sec/batch\n",
      "Epoch 20/20  Iteration 3533/3560 Training loss: 1.1346 1.2405 sec/batch\n",
      "Epoch 20/20  Iteration 3534/3560 Training loss: 1.1344 1.2441 sec/batch\n",
      "Epoch 20/20  Iteration 3535/3560 Training loss: 1.1345 1.2472 sec/batch\n",
      "Epoch 20/20  Iteration 3536/3560 Training loss: 1.1345 1.2371 sec/batch\n",
      "Epoch 20/20  Iteration 3537/3560 Training loss: 1.1344 1.5609 sec/batch\n",
      "Epoch 20/20  Iteration 3538/3560 Training loss: 1.1344 1.3968 sec/batch\n",
      "Epoch 20/20  Iteration 3539/3560 Training loss: 1.1344 1.2447 sec/batch\n",
      "Epoch 20/20  Iteration 3540/3560 Training loss: 1.1344 1.2304 sec/batch\n",
      "Epoch 20/20  Iteration 3541/3560 Training loss: 1.1343 1.2405 sec/batch\n",
      "Epoch 20/20  Iteration 3542/3560 Training loss: 1.1344 1.2419 sec/batch\n",
      "Epoch 20/20  Iteration 3543/3560 Training loss: 1.1346 1.2277 sec/batch\n",
      "Epoch 20/20  Iteration 3544/3560 Training loss: 1.1346 1.2481 sec/batch\n",
      "Epoch 20/20  Iteration 3545/3560 Training loss: 1.1345 1.2343 sec/batch\n",
      "Epoch 20/20  Iteration 3546/3560 Training loss: 1.1345 1.2907 sec/batch\n",
      "Epoch 20/20  Iteration 3547/3560 Training loss: 1.1345 1.2398 sec/batch\n",
      "Epoch 20/20  Iteration 3548/3560 Training loss: 1.1345 1.2483 sec/batch\n",
      "Epoch 20/20  Iteration 3549/3560 Training loss: 1.1346 1.2454 sec/batch\n",
      "Epoch 20/20  Iteration 3550/3560 Training loss: 1.1349 1.2574 sec/batch\n",
      "Epoch 20/20  Iteration 3551/3560 Training loss: 1.1349 1.2238 sec/batch\n",
      "Epoch 20/20  Iteration 3552/3560 Training loss: 1.1350 1.3617 sec/batch\n",
      "Epoch 20/20  Iteration 3553/3560 Training loss: 1.1349 1.3283 sec/batch\n",
      "Epoch 20/20  Iteration 3554/3560 Training loss: 1.1348 1.5599 sec/batch\n",
      "Epoch 20/20  Iteration 3555/3560 Training loss: 1.1350 1.3009 sec/batch\n",
      "Epoch 20/20  Iteration 3556/3560 Training loss: 1.1350 1.2418 sec/batch\n",
      "Epoch 20/20  Iteration 3557/3560 Training loss: 1.1351 1.7382 sec/batch\n",
      "Epoch 20/20  Iteration 3558/3560 Training loss: 1.1350 1.3016 sec/batch\n",
      "Epoch 20/20  Iteration 3559/3560 Training loss: 1.1349 1.6522 sec/batch\n",
      "Epoch 20/20  Iteration 3560/3560 Training loss: 1.1352 1.6589 sec/batch\n",
      "Epoch 1/20  Iteration 1/3560 Training loss: 4.4190 1.3327 sec/batch\n",
      "Epoch 1/20  Iteration 2/3560 Training loss: 4.3927 1.5288 sec/batch\n",
      "Epoch 1/20  Iteration 3/3560 Training loss: 4.3523 1.2691 sec/batch\n",
      "Epoch 1/20  Iteration 4/3560 Training loss: 4.2246 1.2782 sec/batch\n",
      "Epoch 1/20  Iteration 5/3560 Training loss: 4.1176 1.5078 sec/batch\n",
      "Epoch 1/20  Iteration 6/3560 Training loss: 4.0211 1.3096 sec/batch\n",
      "Epoch 1/20  Iteration 7/3560 Training loss: 3.9382 1.4836 sec/batch\n",
      "Epoch 1/20  Iteration 8/3560 Training loss: 3.8687 1.2820 sec/batch\n",
      "Epoch 1/20  Iteration 9/3560 Training loss: 3.8132 1.4410 sec/batch\n",
      "Epoch 1/20  Iteration 10/3560 Training loss: 3.7662 1.2742 sec/batch\n",
      "Epoch 1/20  Iteration 11/3560 Training loss: 3.7219 1.4998 sec/batch\n",
      "Epoch 1/20  Iteration 12/3560 Training loss: 3.6850 1.2470 sec/batch\n",
      "Epoch 1/20  Iteration 13/3560 Training loss: 3.6529 1.5350 sec/batch\n",
      "Epoch 1/20  Iteration 14/3560 Training loss: 3.6256 1.2763 sec/batch\n",
      "Epoch 1/20  Iteration 15/3560 Training loss: 3.6006 1.4979 sec/batch\n",
      "Epoch 1/20  Iteration 16/3560 Training loss: 3.5780 1.2812 sec/batch\n",
      "Epoch 1/20  Iteration 17/3560 Training loss: 3.5562 1.2707 sec/batch\n",
      "Epoch 1/20  Iteration 18/3560 Training loss: 3.5385 1.4812 sec/batch\n",
      "Epoch 1/20  Iteration 19/3560 Training loss: 3.5215 1.2407 sec/batch\n",
      "Epoch 1/20  Iteration 20/3560 Training loss: 3.5047 1.7698 sec/batch\n",
      "Epoch 1/20  Iteration 21/3560 Training loss: 3.4897 1.5696 sec/batch\n",
      "Epoch 1/20  Iteration 22/3560 Training loss: 3.4761 1.5505 sec/batch\n",
      "Epoch 1/20  Iteration 23/3560 Training loss: 3.4632 1.2477 sec/batch\n",
      "Epoch 1/20  Iteration 24/3560 Training loss: 3.4510 1.4469 sec/batch\n",
      "Epoch 1/20  Iteration 25/3560 Training loss: 3.4392 1.2728 sec/batch\n",
      "Epoch 1/20  Iteration 26/3560 Training loss: 3.4293 1.4357 sec/batch\n",
      "Epoch 1/20  Iteration 27/3560 Training loss: 3.4201 1.2663 sec/batch\n",
      "Epoch 1/20  Iteration 28/3560 Training loss: 3.4102 1.4913 sec/batch\n",
      "Epoch 1/20  Iteration 29/3560 Training loss: 3.4010 1.3200 sec/batch\n",
      "Epoch 1/20  Iteration 30/3560 Training loss: 3.3928 1.5069 sec/batch\n",
      "Epoch 1/20  Iteration 31/3560 Training loss: 3.3856 1.2630 sec/batch\n",
      "Epoch 1/20  Iteration 32/3560 Training loss: 3.3779 1.2690 sec/batch\n",
      "Epoch 1/20  Iteration 33/3560 Training loss: 3.3703 1.4484 sec/batch\n",
      "Epoch 1/20  Iteration 34/3560 Training loss: 3.3637 1.2543 sec/batch\n",
      "Epoch 1/20  Iteration 35/3560 Training loss: 3.3567 1.4761 sec/batch\n",
      "Epoch 1/20  Iteration 36/3560 Training loss: 3.3505 1.2812 sec/batch\n",
      "Epoch 1/20  Iteration 37/3560 Training loss: 3.3440 1.5686 sec/batch\n",
      "Epoch 1/20  Iteration 38/3560 Training loss: 3.3378 1.2768 sec/batch\n",
      "Epoch 1/20  Iteration 39/3560 Training loss: 3.3315 1.4930 sec/batch\n",
      "Epoch 1/20  Iteration 40/3560 Training loss: 3.3257 1.2630 sec/batch\n",
      "Epoch 1/20  Iteration 41/3560 Training loss: 3.3200 1.5692 sec/batch\n",
      "Epoch 1/20  Iteration 42/3560 Training loss: 3.3147 1.6578 sec/batch\n",
      "Epoch 1/20  Iteration 43/3560 Training loss: 3.3094 1.5054 sec/batch\n",
      "Epoch 1/20  Iteration 44/3560 Training loss: 3.3043 1.3219 sec/batch\n",
      "Epoch 1/20  Iteration 45/3560 Training loss: 3.2992 1.5144 sec/batch\n",
      "Epoch 1/20  Iteration 46/3560 Training loss: 3.2946 1.2752 sec/batch\n",
      "Epoch 1/20  Iteration 47/3560 Training loss: 3.2903 1.5213 sec/batch\n",
      "Epoch 1/20  Iteration 48/3560 Training loss: 3.2863 1.2467 sec/batch\n",
      "Epoch 1/20  Iteration 49/3560 Training loss: 3.2822 1.2644 sec/batch\n",
      "Epoch 1/20  Iteration 50/3560 Training loss: 3.2783 1.5237 sec/batch\n",
      "Epoch 1/20  Iteration 51/3560 Training loss: 3.2743 1.2960 sec/batch\n",
      "Epoch 1/20  Iteration 52/3560 Training loss: 3.2703 1.4639 sec/batch\n",
      "Epoch 1/20  Iteration 53/3560 Training loss: 3.2664 1.2461 sec/batch\n",
      "Epoch 1/20  Iteration 54/3560 Training loss: 3.2624 1.4820 sec/batch\n",
      "Epoch 1/20  Iteration 55/3560 Training loss: 3.2587 1.2573 sec/batch\n",
      "Epoch 1/20  Iteration 56/3560 Training loss: 3.2549 1.4810 sec/batch\n",
      "Epoch 1/20  Iteration 57/3560 Training loss: 3.2512 1.2912 sec/batch\n",
      "Epoch 1/20  Iteration 58/3560 Training loss: 3.2476 1.5406 sec/batch\n",
      "Epoch 1/20  Iteration 59/3560 Training loss: 3.2439 1.2740 sec/batch\n",
      "Epoch 1/20  Iteration 60/3560 Training loss: 3.2405 1.3720 sec/batch\n",
      "Epoch 1/20  Iteration 61/3560 Training loss: 3.2372 1.4059 sec/batch\n",
      "Epoch 1/20  Iteration 62/3560 Training loss: 3.2341 1.2658 sec/batch\n",
      "Epoch 1/20  Iteration 63/3560 Training loss: 3.2313 1.7255 sec/batch\n",
      "Epoch 1/20  Iteration 64/3560 Training loss: 3.2277 1.7898 sec/batch\n",
      "Epoch 1/20  Iteration 65/3560 Training loss: 3.2242 1.3721 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 66/3560 Training loss: 3.2212 1.2890 sec/batch\n",
      "Epoch 1/20  Iteration 67/3560 Training loss: 3.2181 1.4377 sec/batch\n",
      "Epoch 1/20  Iteration 68/3560 Training loss: 3.2144 1.2880 sec/batch\n",
      "Epoch 1/20  Iteration 69/3560 Training loss: 3.2110 1.4898 sec/batch\n",
      "Epoch 1/20  Iteration 70/3560 Training loss: 3.2079 1.2827 sec/batch\n",
      "Epoch 1/20  Iteration 71/3560 Training loss: 3.2048 1.4974 sec/batch\n",
      "Epoch 1/20  Iteration 72/3560 Training loss: 3.2018 1.3088 sec/batch\n",
      "Epoch 1/20  Iteration 73/3560 Training loss: 3.1987 1.4876 sec/batch\n",
      "Epoch 1/20  Iteration 74/3560 Training loss: 3.1954 1.2586 sec/batch\n",
      "Epoch 1/20  Iteration 75/3560 Training loss: 3.1923 1.5203 sec/batch\n",
      "Epoch 1/20  Iteration 76/3560 Training loss: 3.1894 1.2554 sec/batch\n",
      "Epoch 1/20  Iteration 77/3560 Training loss: 3.1861 1.4332 sec/batch\n",
      "Epoch 1/20  Iteration 78/3560 Training loss: 3.1829 1.3049 sec/batch\n",
      "Epoch 1/20  Iteration 79/3560 Training loss: 3.1796 1.2673 sec/batch\n",
      "Epoch 1/20  Iteration 80/3560 Training loss: 3.1762 1.5208 sec/batch\n",
      "Epoch 1/20  Iteration 81/3560 Training loss: 3.1729 1.2640 sec/batch\n",
      "Epoch 1/20  Iteration 82/3560 Training loss: 3.1698 1.4752 sec/batch\n",
      "Epoch 1/20  Iteration 83/3560 Training loss: 3.1665 1.2700 sec/batch\n",
      "Epoch 1/20  Iteration 84/3560 Training loss: 3.1632 1.6484 sec/batch\n",
      "Epoch 1/20  Iteration 85/3560 Training loss: 3.1596 1.5250 sec/batch\n",
      "Epoch 1/20  Iteration 86/3560 Training loss: 3.1561 1.4923 sec/batch\n",
      "Epoch 1/20  Iteration 87/3560 Training loss: 3.1526 1.2942 sec/batch\n",
      "Epoch 1/20  Iteration 88/3560 Training loss: 3.1491 1.4959 sec/batch\n",
      "Epoch 1/20  Iteration 89/3560 Training loss: 3.1456 1.2616 sec/batch\n",
      "Epoch 1/20  Iteration 90/3560 Training loss: 3.1422 1.4689 sec/batch\n",
      "Epoch 1/20  Iteration 91/3560 Training loss: 3.1388 1.2379 sec/batch\n",
      "Epoch 1/20  Iteration 92/3560 Training loss: 3.1353 1.4392 sec/batch\n",
      "Epoch 1/20  Iteration 93/3560 Training loss: 3.1317 1.3534 sec/batch\n",
      "Epoch 1/20  Iteration 94/3560 Training loss: 3.1281 1.2805 sec/batch\n",
      "Epoch 1/20  Iteration 95/3560 Training loss: 3.1243 1.4827 sec/batch\n",
      "Epoch 1/20  Iteration 96/3560 Training loss: 3.1206 1.2707 sec/batch\n",
      "Epoch 1/20  Iteration 97/3560 Training loss: 3.1170 1.4693 sec/batch\n",
      "Epoch 1/20  Iteration 98/3560 Training loss: 3.1132 1.2449 sec/batch\n",
      "Epoch 1/20  Iteration 99/3560 Training loss: 3.1094 1.4669 sec/batch\n",
      "Epoch 1/20  Iteration 100/3560 Training loss: 3.1056 1.2710 sec/batch\n",
      "Epoch 1/20  Iteration 101/3560 Training loss: 3.1020 1.4904 sec/batch\n",
      "Epoch 1/20  Iteration 102/3560 Training loss: 3.0981 1.2703 sec/batch\n",
      "Epoch 1/20  Iteration 103/3560 Training loss: 3.0942 1.4543 sec/batch\n",
      "Epoch 1/20  Iteration 104/3560 Training loss: 3.0903 1.3222 sec/batch\n",
      "Epoch 1/20  Iteration 105/3560 Training loss: 3.0863 1.2479 sec/batch\n",
      "Epoch 1/20  Iteration 106/3560 Training loss: 3.0824 1.7893 sec/batch\n",
      "Epoch 1/20  Iteration 107/3560 Training loss: 3.0784 1.6890 sec/batch\n",
      "Epoch 1/20  Iteration 108/3560 Training loss: 3.0745 1.2769 sec/batch\n",
      "Epoch 1/20  Iteration 109/3560 Training loss: 3.0707 1.3207 sec/batch\n",
      "Epoch 1/20  Iteration 110/3560 Training loss: 3.0665 1.4579 sec/batch\n",
      "Epoch 1/20  Iteration 111/3560 Training loss: 3.0626 1.2601 sec/batch\n",
      "Epoch 1/20  Iteration 112/3560 Training loss: 3.0588 1.4909 sec/batch\n",
      "Epoch 1/20  Iteration 113/3560 Training loss: 3.0548 1.2476 sec/batch\n",
      "Epoch 1/20  Iteration 114/3560 Training loss: 3.0508 1.4802 sec/batch\n",
      "Epoch 1/20  Iteration 115/3560 Training loss: 3.0468 1.2707 sec/batch\n",
      "Epoch 1/20  Iteration 116/3560 Training loss: 3.0427 1.5326 sec/batch\n",
      "Epoch 1/20  Iteration 117/3560 Training loss: 3.0388 1.2654 sec/batch\n",
      "Epoch 1/20  Iteration 118/3560 Training loss: 3.0349 1.5082 sec/batch\n",
      "Epoch 1/20  Iteration 119/3560 Training loss: 3.0312 1.2572 sec/batch\n",
      "Epoch 1/20  Iteration 120/3560 Training loss: 3.0274 1.5130 sec/batch\n",
      "Epoch 1/20  Iteration 121/3560 Training loss: 3.0237 1.3906 sec/batch\n",
      "Epoch 1/20  Iteration 122/3560 Training loss: 3.0198 1.2731 sec/batch\n",
      "Epoch 1/20  Iteration 123/3560 Training loss: 3.0160 1.4878 sec/batch\n",
      "Epoch 1/20  Iteration 124/3560 Training loss: 3.0123 1.2705 sec/batch\n",
      "Epoch 1/20  Iteration 125/3560 Training loss: 3.0085 1.5032 sec/batch\n",
      "Epoch 1/20  Iteration 126/3560 Training loss: 3.0049 1.2868 sec/batch\n",
      "Epoch 1/20  Iteration 127/3560 Training loss: 3.0015 1.5802 sec/batch\n",
      "Epoch 1/20  Iteration 128/3560 Training loss: 2.9980 1.6271 sec/batch\n",
      "Epoch 1/20  Iteration 129/3560 Training loss: 2.9943 1.4883 sec/batch\n",
      "Epoch 1/20  Iteration 130/3560 Training loss: 2.9908 1.2725 sec/batch\n",
      "Epoch 1/20  Iteration 131/3560 Training loss: 2.9872 1.4813 sec/batch\n",
      "Epoch 1/20  Iteration 132/3560 Training loss: 2.9836 1.2569 sec/batch\n",
      "Epoch 1/20  Iteration 133/3560 Training loss: 2.9801 1.5069 sec/batch\n",
      "Epoch 1/20  Iteration 134/3560 Training loss: 2.9767 1.2458 sec/batch\n",
      "Epoch 1/20  Iteration 135/3560 Training loss: 2.9731 1.5139 sec/batch\n",
      "Epoch 1/20  Iteration 136/3560 Training loss: 2.9696 1.2983 sec/batch\n",
      "Epoch 1/20  Iteration 137/3560 Training loss: 2.9661 1.3122 sec/batch\n",
      "Epoch 1/20  Iteration 138/3560 Training loss: 2.9626 1.4199 sec/batch\n",
      "Epoch 1/20  Iteration 139/3560 Training loss: 2.9593 1.2547 sec/batch\n",
      "Epoch 1/20  Iteration 140/3560 Training loss: 2.9559 1.4882 sec/batch\n",
      "Epoch 1/20  Iteration 141/3560 Training loss: 2.9526 1.2581 sec/batch\n",
      "Epoch 1/20  Iteration 142/3560 Training loss: 2.9491 1.4545 sec/batch\n",
      "Epoch 1/20  Iteration 143/3560 Training loss: 2.9457 1.2588 sec/batch\n",
      "Epoch 1/20  Iteration 144/3560 Training loss: 2.9423 1.5101 sec/batch\n",
      "Epoch 1/20  Iteration 145/3560 Training loss: 2.9389 1.2927 sec/batch\n",
      "Epoch 1/20  Iteration 146/3560 Training loss: 2.9358 1.4980 sec/batch\n",
      "Epoch 1/20  Iteration 147/3560 Training loss: 2.9325 1.2749 sec/batch\n",
      "Epoch 1/20  Iteration 148/3560 Training loss: 2.9293 1.3415 sec/batch\n",
      "Epoch 1/20  Iteration 149/3560 Training loss: 2.9260 1.6647 sec/batch\n",
      "Epoch 1/20  Iteration 150/3560 Training loss: 2.9227 1.6990 sec/batch\n",
      "Epoch 1/20  Iteration 151/3560 Training loss: 2.9197 1.2720 sec/batch\n",
      "Epoch 1/20  Iteration 152/3560 Training loss: 2.9168 1.4128 sec/batch\n",
      "Epoch 1/20  Iteration 153/3560 Training loss: 2.9137 1.4156 sec/batch\n",
      "Epoch 1/20  Iteration 154/3560 Training loss: 2.9107 1.2952 sec/batch\n",
      "Epoch 1/20  Iteration 155/3560 Training loss: 2.9075 1.5314 sec/batch\n",
      "Epoch 1/20  Iteration 156/3560 Training loss: 2.9043 1.4633 sec/batch\n",
      "Epoch 1/20  Iteration 157/3560 Training loss: 2.9011 1.6340 sec/batch\n",
      "Epoch 1/20  Iteration 158/3560 Training loss: 2.8980 1.7568 sec/batch\n",
      "Epoch 1/20  Iteration 159/3560 Training loss: 2.8948 1.6079 sec/batch\n",
      "Epoch 1/20  Iteration 160/3560 Training loss: 2.8919 1.5002 sec/batch\n",
      "Epoch 1/20  Iteration 161/3560 Training loss: 2.8889 1.6147 sec/batch\n",
      "Epoch 1/20  Iteration 162/3560 Training loss: 2.8858 1.3729 sec/batch\n",
      "Epoch 1/20  Iteration 163/3560 Training loss: 2.8827 1.5553 sec/batch\n",
      "Epoch 1/20  Iteration 164/3560 Training loss: 2.8796 1.7012 sec/batch\n",
      "Epoch 1/20  Iteration 165/3560 Training loss: 2.8767 1.5666 sec/batch\n",
      "Epoch 1/20  Iteration 166/3560 Training loss: 2.8738 1.8315 sec/batch\n",
      "Epoch 1/20  Iteration 167/3560 Training loss: 2.8709 1.3162 sec/batch\n",
      "Epoch 1/20  Iteration 168/3560 Training loss: 2.8681 1.1932 sec/batch\n",
      "Epoch 1/20  Iteration 169/3560 Training loss: 2.8653 1.9492 sec/batch\n",
      "Epoch 1/20  Iteration 170/3560 Training loss: 2.8623 1.2166 sec/batch\n",
      "Epoch 1/20  Iteration 171/3560 Training loss: 2.8595 1.4897 sec/batch\n",
      "Epoch 1/20  Iteration 172/3560 Training loss: 2.8567 1.7786 sec/batch\n",
      "Epoch 1/20  Iteration 173/3560 Training loss: 2.8543 1.3183 sec/batch\n",
      "Epoch 1/20  Iteration 174/3560 Training loss: 2.8521 1.8251 sec/batch\n",
      "Epoch 1/20  Iteration 175/3560 Training loss: 2.8497 1.5999 sec/batch\n",
      "Epoch 1/20  Iteration 176/3560 Training loss: 2.8477 1.4140 sec/batch\n",
      "Epoch 1/20  Iteration 177/3560 Training loss: 2.8455 1.3586 sec/batch\n",
      "Epoch 1/20  Iteration 178/3560 Training loss: 2.8433 1.2302 sec/batch\n",
      "Epoch 2/20  Iteration 179/3560 Training loss: 2.4616 1.4245 sec/batch\n",
      "Epoch 2/20  Iteration 180/3560 Training loss: 2.3966 1.2933 sec/batch\n",
      "Epoch 2/20  Iteration 181/3560 Training loss: 2.3788 1.3146 sec/batch\n",
      "Epoch 2/20  Iteration 182/3560 Training loss: 2.3713 1.2379 sec/batch\n",
      "Epoch 2/20  Iteration 183/3560 Training loss: 2.3676 1.2548 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 184/3560 Training loss: 2.3619 1.2780 sec/batch\n",
      "Epoch 2/20  Iteration 185/3560 Training loss: 2.3599 1.2990 sec/batch\n",
      "Epoch 2/20  Iteration 186/3560 Training loss: 2.3594 1.2567 sec/batch\n",
      "Epoch 2/20  Iteration 187/3560 Training loss: 2.3584 1.2913 sec/batch\n",
      "Epoch 2/20  Iteration 188/3560 Training loss: 2.3571 1.2611 sec/batch\n",
      "Epoch 2/20  Iteration 189/3560 Training loss: 2.3543 1.3196 sec/batch\n",
      "Epoch 2/20  Iteration 190/3560 Training loss: 2.3527 1.6055 sec/batch\n",
      "Epoch 2/20  Iteration 191/3560 Training loss: 2.3514 1.4658 sec/batch\n",
      "Epoch 2/20  Iteration 192/3560 Training loss: 2.3528 1.5188 sec/batch\n",
      "Epoch 2/20  Iteration 193/3560 Training loss: 2.3516 1.4392 sec/batch\n",
      "Epoch 2/20  Iteration 194/3560 Training loss: 2.3500 1.2931 sec/batch\n",
      "Epoch 2/20  Iteration 195/3560 Training loss: 2.3493 1.1599 sec/batch\n",
      "Epoch 2/20  Iteration 196/3560 Training loss: 2.3494 1.1857 sec/batch\n",
      "Epoch 2/20  Iteration 197/3560 Training loss: 2.3487 1.1793 sec/batch\n",
      "Epoch 2/20  Iteration 198/3560 Training loss: 2.3464 1.1797 sec/batch\n",
      "Epoch 2/20  Iteration 199/3560 Training loss: 2.3447 1.1966 sec/batch\n",
      "Epoch 2/20  Iteration 200/3560 Training loss: 2.3453 1.2289 sec/batch\n",
      "Epoch 2/20  Iteration 201/3560 Training loss: 2.3440 1.1618 sec/batch\n",
      "Epoch 2/20  Iteration 202/3560 Training loss: 2.3420 1.2482 sec/batch\n",
      "Epoch 2/20  Iteration 203/3560 Training loss: 2.3407 1.2040 sec/batch\n",
      "Epoch 2/20  Iteration 204/3560 Training loss: 2.3391 1.2374 sec/batch\n",
      "Epoch 2/20  Iteration 205/3560 Training loss: 2.3376 1.1816 sec/batch\n",
      "Epoch 2/20  Iteration 206/3560 Training loss: 2.3364 1.1955 sec/batch\n",
      "Epoch 2/20  Iteration 207/3560 Training loss: 2.3359 1.2244 sec/batch\n",
      "Epoch 2/20  Iteration 208/3560 Training loss: 2.3352 1.4481 sec/batch\n",
      "Epoch 2/20  Iteration 209/3560 Training loss: 2.3347 1.3790 sec/batch\n",
      "Epoch 2/20  Iteration 210/3560 Training loss: 2.3330 1.5456 sec/batch\n",
      "Epoch 2/20  Iteration 211/3560 Training loss: 2.3315 1.7552 sec/batch\n",
      "Epoch 2/20  Iteration 212/3560 Training loss: 2.3308 1.3752 sec/batch\n",
      "Epoch 2/20  Iteration 213/3560 Training loss: 2.3296 2.1737 sec/batch\n",
      "Epoch 2/20  Iteration 214/3560 Training loss: 2.3287 1.5602 sec/batch\n",
      "Epoch 2/20  Iteration 215/3560 Training loss: 2.3274 1.4018 sec/batch\n",
      "Epoch 2/20  Iteration 216/3560 Training loss: 2.3252 1.3930 sec/batch\n",
      "Epoch 2/20  Iteration 217/3560 Training loss: 2.3234 1.3080 sec/batch\n",
      "Epoch 2/20  Iteration 218/3560 Training loss: 2.3221 1.2933 sec/batch\n",
      "Epoch 2/20  Iteration 219/3560 Training loss: 2.3210 1.2995 sec/batch\n",
      "Epoch 2/20  Iteration 220/3560 Training loss: 2.3197 1.2697 sec/batch\n",
      "Epoch 2/20  Iteration 221/3560 Training loss: 2.3185 1.2659 sec/batch\n",
      "Epoch 2/20  Iteration 222/3560 Training loss: 2.3169 1.2887 sec/batch\n",
      "Epoch 2/20  Iteration 223/3560 Training loss: 2.3157 1.2547 sec/batch\n",
      "Epoch 2/20  Iteration 224/3560 Training loss: 2.3138 1.2657 sec/batch\n",
      "Epoch 2/20  Iteration 225/3560 Training loss: 2.3131 1.3359 sec/batch\n",
      "Epoch 2/20  Iteration 226/3560 Training loss: 2.3117 1.2340 sec/batch\n",
      "Epoch 2/20  Iteration 227/3560 Training loss: 2.3105 1.3046 sec/batch\n",
      "Epoch 2/20  Iteration 228/3560 Training loss: 2.3101 1.2540 sec/batch\n",
      "Epoch 2/20  Iteration 229/3560 Training loss: 2.3086 1.2613 sec/batch\n",
      "Epoch 2/20  Iteration 230/3560 Training loss: 2.3079 1.2983 sec/batch\n",
      "Epoch 2/20  Iteration 231/3560 Training loss: 2.3066 1.2429 sec/batch\n",
      "Epoch 2/20  Iteration 232/3560 Training loss: 2.3055 1.3391 sec/batch\n",
      "Epoch 2/20  Iteration 233/3560 Training loss: 2.3043 1.2420 sec/batch\n",
      "Epoch 2/20  Iteration 234/3560 Training loss: 2.3035 1.2463 sec/batch\n",
      "Epoch 2/20  Iteration 235/3560 Training loss: 2.3027 1.3598 sec/batch\n",
      "Epoch 2/20  Iteration 236/3560 Training loss: 2.3016 1.5915 sec/batch\n",
      "Epoch 2/20  Iteration 237/3560 Training loss: 2.3004 1.2749 sec/batch\n",
      "Epoch 2/20  Iteration 238/3560 Training loss: 2.2999 1.2590 sec/batch\n",
      "Epoch 2/20  Iteration 239/3560 Training loss: 2.2988 1.2713 sec/batch\n",
      "Epoch 2/20  Iteration 240/3560 Training loss: 2.2981 1.3418 sec/batch\n",
      "Epoch 2/20  Iteration 241/3560 Training loss: 2.2976 1.2509 sec/batch\n",
      "Epoch 2/20  Iteration 242/3560 Training loss: 2.2969 1.2651 sec/batch\n",
      "Epoch 2/20  Iteration 243/3560 Training loss: 2.2960 1.2436 sec/batch\n",
      "Epoch 2/20  Iteration 244/3560 Training loss: 2.2956 1.2435 sec/batch\n",
      "Epoch 2/20  Iteration 245/3560 Training loss: 2.2947 1.2790 sec/batch\n",
      "Epoch 2/20  Iteration 246/3560 Training loss: 2.2936 1.2852 sec/batch\n",
      "Epoch 2/20  Iteration 247/3560 Training loss: 2.2926 1.2731 sec/batch\n",
      "Epoch 2/20  Iteration 248/3560 Training loss: 2.2919 1.2946 sec/batch\n",
      "Epoch 2/20  Iteration 249/3560 Training loss: 2.2914 1.2620 sec/batch\n",
      "Epoch 2/20  Iteration 250/3560 Training loss: 2.2906 1.3322 sec/batch\n",
      "Epoch 2/20  Iteration 251/3560 Training loss: 2.2898 1.2965 sec/batch\n",
      "Epoch 2/20  Iteration 252/3560 Training loss: 2.2887 1.2753 sec/batch\n",
      "Epoch 2/20  Iteration 253/3560 Training loss: 2.2877 1.2786 sec/batch\n",
      "Epoch 2/20  Iteration 254/3560 Training loss: 2.2874 1.2534 sec/batch\n",
      "Epoch 2/20  Iteration 255/3560 Training loss: 2.2864 1.2847 sec/batch\n",
      "Epoch 2/20  Iteration 256/3560 Training loss: 2.2858 1.2774 sec/batch\n",
      "Epoch 2/20  Iteration 257/3560 Training loss: 2.2847 1.2311 sec/batch\n",
      "Epoch 2/20  Iteration 258/3560 Training loss: 2.2837 1.4479 sec/batch\n",
      "Epoch 2/20  Iteration 259/3560 Training loss: 2.2826 1.4859 sec/batch\n",
      "Epoch 2/20  Iteration 260/3560 Training loss: 2.2820 1.3005 sec/batch\n",
      "Epoch 2/20  Iteration 261/3560 Training loss: 2.2808 1.2400 sec/batch\n",
      "Epoch 2/20  Iteration 262/3560 Training loss: 2.2798 1.2477 sec/batch\n",
      "Epoch 2/20  Iteration 263/3560 Training loss: 2.2784 1.2957 sec/batch\n",
      "Epoch 2/20  Iteration 264/3560 Training loss: 2.2774 1.2642 sec/batch\n",
      "Epoch 2/20  Iteration 265/3560 Training loss: 2.2765 1.2664 sec/batch\n",
      "Epoch 2/20  Iteration 266/3560 Training loss: 2.2756 1.2497 sec/batch\n",
      "Epoch 2/20  Iteration 267/3560 Training loss: 2.2745 1.2386 sec/batch\n",
      "Epoch 2/20  Iteration 268/3560 Training loss: 2.2739 1.3116 sec/batch\n",
      "Epoch 2/20  Iteration 269/3560 Training loss: 2.2730 1.2407 sec/batch\n",
      "Epoch 2/20  Iteration 270/3560 Training loss: 2.2721 1.2936 sec/batch\n",
      "Epoch 2/20  Iteration 271/3560 Training loss: 2.2710 1.2435 sec/batch\n",
      "Epoch 2/20  Iteration 272/3560 Training loss: 2.2699 1.2934 sec/batch\n",
      "Epoch 2/20  Iteration 273/3560 Training loss: 2.2689 1.2787 sec/batch\n",
      "Epoch 2/20  Iteration 274/3560 Training loss: 2.2680 1.2502 sec/batch\n",
      "Epoch 2/20  Iteration 275/3560 Training loss: 2.2671 1.2710 sec/batch\n",
      "Epoch 2/20  Iteration 276/3560 Training loss: 2.2661 1.2826 sec/batch\n",
      "Epoch 2/20  Iteration 277/3560 Training loss: 2.2650 1.2581 sec/batch\n",
      "Epoch 2/20  Iteration 278/3560 Training loss: 2.2640 1.2942 sec/batch\n",
      "Epoch 2/20  Iteration 279/3560 Training loss: 2.2633 1.2953 sec/batch\n",
      "Epoch 2/20  Iteration 280/3560 Training loss: 2.2625 1.2454 sec/batch\n",
      "Epoch 2/20  Iteration 281/3560 Training loss: 2.2615 1.2706 sec/batch\n",
      "Epoch 2/20  Iteration 282/3560 Training loss: 2.2605 1.7072 sec/batch\n",
      "Epoch 2/20  Iteration 283/3560 Training loss: 2.2597 1.2832 sec/batch\n",
      "Epoch 2/20  Iteration 284/3560 Training loss: 2.2588 1.2476 sec/batch\n",
      "Epoch 2/20  Iteration 285/3560 Training loss: 2.2580 1.2942 sec/batch\n",
      "Epoch 2/20  Iteration 286/3560 Training loss: 2.2574 1.2628 sec/batch\n",
      "Epoch 2/20  Iteration 287/3560 Training loss: 2.2568 1.3020 sec/batch\n",
      "Epoch 2/20  Iteration 288/3560 Training loss: 2.2559 1.2937 sec/batch\n",
      "Epoch 2/20  Iteration 289/3560 Training loss: 2.2552 1.2571 sec/batch\n",
      "Epoch 2/20  Iteration 290/3560 Training loss: 2.2546 1.2319 sec/batch\n",
      "Epoch 2/20  Iteration 291/3560 Training loss: 2.2537 1.2965 sec/batch\n",
      "Epoch 2/20  Iteration 292/3560 Training loss: 2.2529 1.2591 sec/batch\n",
      "Epoch 2/20  Iteration 293/3560 Training loss: 2.2520 1.3199 sec/batch\n",
      "Epoch 2/20  Iteration 294/3560 Training loss: 2.2509 1.2593 sec/batch\n",
      "Epoch 2/20  Iteration 295/3560 Training loss: 2.2502 1.3022 sec/batch\n",
      "Epoch 2/20  Iteration 296/3560 Training loss: 2.2495 1.2901 sec/batch\n",
      "Epoch 2/20  Iteration 297/3560 Training loss: 2.2490 1.2660 sec/batch\n",
      "Epoch 2/20  Iteration 298/3560 Training loss: 2.2483 1.2576 sec/batch\n",
      "Epoch 2/20  Iteration 299/3560 Training loss: 2.2477 1.2823 sec/batch\n",
      "Epoch 2/20  Iteration 300/3560 Training loss: 2.2468 1.2393 sec/batch\n",
      "Epoch 2/20  Iteration 301/3560 Training loss: 2.2459 1.2723 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 302/3560 Training loss: 2.2454 1.2550 sec/batch\n",
      "Epoch 2/20  Iteration 303/3560 Training loss: 2.2447 1.3129 sec/batch\n",
      "Epoch 2/20  Iteration 304/3560 Training loss: 2.2439 1.2360 sec/batch\n",
      "Epoch 2/20  Iteration 305/3560 Training loss: 2.2434 1.5478 sec/batch\n",
      "Epoch 2/20  Iteration 306/3560 Training loss: 2.2429 1.4373 sec/batch\n",
      "Epoch 2/20  Iteration 307/3560 Training loss: 2.2422 1.2339 sec/batch\n",
      "Epoch 2/20  Iteration 308/3560 Training loss: 2.2416 1.2910 sec/batch\n",
      "Epoch 2/20  Iteration 309/3560 Training loss: 2.2408 1.2403 sec/batch\n",
      "Epoch 2/20  Iteration 310/3560 Training loss: 2.2399 1.3139 sec/batch\n",
      "Epoch 2/20  Iteration 311/3560 Training loss: 2.2392 1.2861 sec/batch\n",
      "Epoch 2/20  Iteration 312/3560 Training loss: 2.2387 1.2337 sec/batch\n",
      "Epoch 2/20  Iteration 313/3560 Training loss: 2.2381 1.2982 sec/batch\n",
      "Epoch 2/20  Iteration 314/3560 Training loss: 2.2376 1.2549 sec/batch\n",
      "Epoch 2/20  Iteration 315/3560 Training loss: 2.2370 1.2393 sec/batch\n",
      "Epoch 2/20  Iteration 316/3560 Training loss: 2.2364 1.2842 sec/batch\n",
      "Epoch 2/20  Iteration 317/3560 Training loss: 2.2361 1.2300 sec/batch\n",
      "Epoch 2/20  Iteration 318/3560 Training loss: 2.2355 1.2931 sec/batch\n",
      "Epoch 2/20  Iteration 319/3560 Training loss: 2.2350 1.3050 sec/batch\n",
      "Epoch 2/20  Iteration 320/3560 Training loss: 2.2343 1.2442 sec/batch\n",
      "Epoch 2/20  Iteration 321/3560 Training loss: 2.2337 1.2830 sec/batch\n",
      "Epoch 2/20  Iteration 322/3560 Training loss: 2.2331 1.2353 sec/batch\n",
      "Epoch 2/20  Iteration 323/3560 Training loss: 2.2325 1.2363 sec/batch\n",
      "Epoch 2/20  Iteration 324/3560 Training loss: 2.2321 1.2969 sec/batch\n",
      "Epoch 2/20  Iteration 325/3560 Training loss: 2.2315 1.2494 sec/batch\n",
      "Epoch 2/20  Iteration 326/3560 Training loss: 2.2312 1.3092 sec/batch\n",
      "Epoch 2/20  Iteration 327/3560 Training loss: 2.2305 1.2377 sec/batch\n",
      "Epoch 2/20  Iteration 328/3560 Training loss: 2.2298 1.2665 sec/batch\n",
      "Epoch 2/20  Iteration 329/3560 Training loss: 2.2292 1.6051 sec/batch\n",
      "Epoch 2/20  Iteration 330/3560 Training loss: 2.2288 1.3733 sec/batch\n",
      "Epoch 2/20  Iteration 331/3560 Training loss: 2.2283 1.2670 sec/batch\n",
      "Epoch 2/20  Iteration 332/3560 Training loss: 2.2278 1.2378 sec/batch\n",
      "Epoch 2/20  Iteration 333/3560 Training loss: 2.2271 1.2378 sec/batch\n",
      "Epoch 2/20  Iteration 334/3560 Training loss: 2.2265 1.3402 sec/batch\n",
      "Epoch 2/20  Iteration 335/3560 Training loss: 2.2259 1.2426 sec/batch\n",
      "Epoch 2/20  Iteration 336/3560 Training loss: 2.2253 1.2824 sec/batch\n",
      "Epoch 2/20  Iteration 337/3560 Training loss: 2.2245 1.2469 sec/batch\n",
      "Epoch 2/20  Iteration 338/3560 Training loss: 2.2243 1.2434 sec/batch\n",
      "Epoch 2/20  Iteration 339/3560 Training loss: 2.2238 1.2806 sec/batch\n",
      "Epoch 2/20  Iteration 340/3560 Training loss: 2.2232 1.2532 sec/batch\n",
      "Epoch 2/20  Iteration 341/3560 Training loss: 2.2226 1.2560 sec/batch\n",
      "Epoch 2/20  Iteration 342/3560 Training loss: 2.2220 1.3014 sec/batch\n",
      "Epoch 2/20  Iteration 343/3560 Training loss: 2.2215 1.2494 sec/batch\n",
      "Epoch 2/20  Iteration 344/3560 Training loss: 2.2209 1.2752 sec/batch\n",
      "Epoch 2/20  Iteration 345/3560 Training loss: 2.2204 1.2353 sec/batch\n",
      "Epoch 2/20  Iteration 346/3560 Training loss: 2.2200 1.2541 sec/batch\n",
      "Epoch 2/20  Iteration 347/3560 Training loss: 2.2194 1.2814 sec/batch\n",
      "Epoch 2/20  Iteration 348/3560 Training loss: 2.2187 1.2400 sec/batch\n",
      "Epoch 2/20  Iteration 349/3560 Training loss: 2.2181 1.2786 sec/batch\n",
      "Epoch 2/20  Iteration 350/3560 Training loss: 2.2175 1.2788 sec/batch\n",
      "Epoch 2/20  Iteration 351/3560 Training loss: 2.2170 1.2507 sec/batch\n",
      "Epoch 2/20  Iteration 352/3560 Training loss: 2.2166 1.7040 sec/batch\n",
      "Epoch 2/20  Iteration 353/3560 Training loss: 2.2161 1.2463 sec/batch\n",
      "Epoch 2/20  Iteration 354/3560 Training loss: 2.2155 1.2846 sec/batch\n",
      "Epoch 2/20  Iteration 355/3560 Training loss: 2.2149 1.2450 sec/batch\n",
      "Epoch 2/20  Iteration 356/3560 Training loss: 2.2144 1.2279 sec/batch\n",
      "Epoch 3/20  Iteration 357/3560 Training loss: 2.2026 1.3056 sec/batch\n",
      "Epoch 3/20  Iteration 358/3560 Training loss: 2.1441 1.2733 sec/batch\n",
      "Epoch 3/20  Iteration 359/3560 Training loss: 2.1282 1.3122 sec/batch\n",
      "Epoch 3/20  Iteration 360/3560 Training loss: 2.1207 1.2432 sec/batch\n",
      "Epoch 3/20  Iteration 361/3560 Training loss: 2.1175 1.2429 sec/batch\n",
      "Epoch 3/20  Iteration 362/3560 Training loss: 2.1107 1.2683 sec/batch\n",
      "Epoch 3/20  Iteration 363/3560 Training loss: 2.1102 1.2440 sec/batch\n",
      "Epoch 3/20  Iteration 364/3560 Training loss: 2.1107 1.2612 sec/batch\n",
      "Epoch 3/20  Iteration 365/3560 Training loss: 2.1122 1.5255 sec/batch\n",
      "Epoch 3/20  Iteration 366/3560 Training loss: 2.1121 1.4526 sec/batch\n",
      "Epoch 3/20  Iteration 367/3560 Training loss: 2.1095 1.3137 sec/batch\n",
      "Epoch 3/20  Iteration 368/3560 Training loss: 2.1069 1.2427 sec/batch\n",
      "Epoch 3/20  Iteration 369/3560 Training loss: 2.1068 1.2911 sec/batch\n",
      "Epoch 3/20  Iteration 370/3560 Training loss: 2.1081 1.2691 sec/batch\n",
      "Epoch 3/20  Iteration 371/3560 Training loss: 2.1067 1.2512 sec/batch\n",
      "Epoch 3/20  Iteration 372/3560 Training loss: 2.1057 1.3108 sec/batch\n",
      "Epoch 3/20  Iteration 373/3560 Training loss: 2.1048 1.3806 sec/batch\n",
      "Epoch 3/20  Iteration 374/3560 Training loss: 2.1067 1.4909 sec/batch\n",
      "Epoch 3/20  Iteration 375/3560 Training loss: 2.1067 1.8234 sec/batch\n",
      "Epoch 3/20  Iteration 376/3560 Training loss: 2.1064 1.3565 sec/batch\n",
      "Epoch 3/20  Iteration 377/3560 Training loss: 2.1051 1.3599 sec/batch\n",
      "Epoch 3/20  Iteration 378/3560 Training loss: 2.1063 1.3297 sec/batch\n",
      "Epoch 3/20  Iteration 379/3560 Training loss: 2.1054 1.3082 sec/batch\n",
      "Epoch 3/20  Iteration 380/3560 Training loss: 2.1042 1.4416 sec/batch\n",
      "Epoch 3/20  Iteration 381/3560 Training loss: 2.1037 1.3092 sec/batch\n",
      "Epoch 3/20  Iteration 382/3560 Training loss: 2.1025 1.3579 sec/batch\n",
      "Epoch 3/20  Iteration 383/3560 Training loss: 2.1013 1.3449 sec/batch\n",
      "Epoch 3/20  Iteration 384/3560 Training loss: 2.1009 1.3287 sec/batch\n",
      "Epoch 3/20  Iteration 385/3560 Training loss: 2.1013 1.3115 sec/batch\n",
      "Epoch 3/20  Iteration 386/3560 Training loss: 2.1008 1.3221 sec/batch\n",
      "Epoch 3/20  Iteration 387/3560 Training loss: 2.1001 1.3739 sec/batch\n",
      "Epoch 3/20  Iteration 388/3560 Training loss: 2.0990 1.4406 sec/batch\n",
      "Epoch 3/20  Iteration 389/3560 Training loss: 2.0983 1.3735 sec/batch\n",
      "Epoch 3/20  Iteration 390/3560 Training loss: 2.0987 1.3088 sec/batch\n",
      "Epoch 3/20  Iteration 391/3560 Training loss: 2.0979 1.3519 sec/batch\n",
      "Epoch 3/20  Iteration 392/3560 Training loss: 2.0970 1.3011 sec/batch\n",
      "Epoch 3/20  Iteration 393/3560 Training loss: 2.0963 1.3619 sec/batch\n",
      "Epoch 3/20  Iteration 394/3560 Training loss: 2.0945 1.3077 sec/batch\n",
      "Epoch 3/20  Iteration 395/3560 Training loss: 2.0933 1.3903 sec/batch\n",
      "Epoch 3/20  Iteration 396/3560 Training loss: 2.0921 1.5352 sec/batch\n",
      "Epoch 3/20  Iteration 397/3560 Training loss: 2.0913 1.5603 sec/batch\n",
      "Epoch 3/20  Iteration 398/3560 Training loss: 2.0906 1.3410 sec/batch\n",
      "Epoch 3/20  Iteration 399/3560 Training loss: 2.0899 1.3189 sec/batch\n",
      "Epoch 3/20  Iteration 400/3560 Training loss: 2.0887 1.3029 sec/batch\n",
      "Epoch 3/20  Iteration 401/3560 Training loss: 2.0885 1.3320 sec/batch\n",
      "Epoch 3/20  Iteration 402/3560 Training loss: 2.0867 1.3676 sec/batch\n",
      "Epoch 3/20  Iteration 403/3560 Training loss: 2.0864 1.3495 sec/batch\n",
      "Epoch 3/20  Iteration 404/3560 Training loss: 2.0855 1.3993 sec/batch\n",
      "Epoch 3/20  Iteration 405/3560 Training loss: 2.0849 1.3180 sec/batch\n",
      "Epoch 3/20  Iteration 406/3560 Training loss: 2.0852 1.3040 sec/batch\n",
      "Epoch 3/20  Iteration 407/3560 Training loss: 2.0842 1.3173 sec/batch\n",
      "Epoch 3/20  Iteration 408/3560 Training loss: 2.0844 1.3348 sec/batch\n",
      "Epoch 3/20  Iteration 409/3560 Training loss: 2.0837 1.3630 sec/batch\n",
      "Epoch 3/20  Iteration 410/3560 Training loss: 2.0829 1.4051 sec/batch\n",
      "Epoch 3/20  Iteration 411/3560 Training loss: 2.0822 1.3246 sec/batch\n",
      "Epoch 3/20  Iteration 412/3560 Training loss: 2.0820 1.2958 sec/batch\n",
      "Epoch 3/20  Iteration 413/3560 Training loss: 2.0816 1.3174 sec/batch\n",
      "Epoch 3/20  Iteration 414/3560 Training loss: 2.0809 1.3381 sec/batch\n",
      "Epoch 3/20  Iteration 415/3560 Training loss: 2.0801 1.3255 sec/batch\n",
      "Epoch 3/20  Iteration 416/3560 Training loss: 2.0803 1.3717 sec/batch\n",
      "Epoch 3/20  Iteration 417/3560 Training loss: 2.0799 1.3706 sec/batch\n",
      "Epoch 3/20  Iteration 418/3560 Training loss: 2.0800 1.4040 sec/batch\n",
      "Epoch 3/20  Iteration 419/3560 Training loss: 2.0799 1.6453 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 420/3560 Training loss: 2.0796 1.5081 sec/batch\n",
      "Epoch 3/20  Iteration 421/3560 Training loss: 2.0789 1.3048 sec/batch\n",
      "Epoch 3/20  Iteration 422/3560 Training loss: 2.0788 1.3242 sec/batch\n",
      "Epoch 3/20  Iteration 423/3560 Training loss: 2.0785 1.3557 sec/batch\n",
      "Epoch 3/20  Iteration 424/3560 Training loss: 2.0777 1.3741 sec/batch\n",
      "Epoch 3/20  Iteration 425/3560 Training loss: 2.0771 1.3806 sec/batch\n",
      "Epoch 3/20  Iteration 426/3560 Training loss: 2.0767 1.3234 sec/batch\n",
      "Epoch 3/20  Iteration 427/3560 Training loss: 2.0766 1.3115 sec/batch\n",
      "Epoch 3/20  Iteration 428/3560 Training loss: 2.0762 1.3578 sec/batch\n",
      "Epoch 3/20  Iteration 429/3560 Training loss: 2.0761 1.3296 sec/batch\n",
      "Epoch 3/20  Iteration 430/3560 Training loss: 2.0753 1.3329 sec/batch\n",
      "Epoch 3/20  Iteration 431/3560 Training loss: 2.0748 1.3229 sec/batch\n",
      "Epoch 3/20  Iteration 432/3560 Training loss: 2.0749 1.4072 sec/batch\n",
      "Epoch 3/20  Iteration 433/3560 Training loss: 2.0744 1.3483 sec/batch\n",
      "Epoch 3/20  Iteration 434/3560 Training loss: 2.0742 1.2962 sec/batch\n",
      "Epoch 3/20  Iteration 435/3560 Training loss: 2.0733 1.3426 sec/batch\n",
      "Epoch 3/20  Iteration 436/3560 Training loss: 2.0726 1.3571 sec/batch\n",
      "Epoch 3/20  Iteration 437/3560 Training loss: 2.0718 1.2988 sec/batch\n",
      "Epoch 3/20  Iteration 438/3560 Training loss: 2.0715 1.3507 sec/batch\n",
      "Epoch 3/20  Iteration 439/3560 Training loss: 2.0706 1.3422 sec/batch\n",
      "Epoch 3/20  Iteration 440/3560 Training loss: 2.0701 1.5560 sec/batch\n",
      "Epoch 3/20  Iteration 441/3560 Training loss: 2.0693 1.6403 sec/batch\n",
      "Epoch 3/20  Iteration 442/3560 Training loss: 2.0686 1.3720 sec/batch\n",
      "Epoch 3/20  Iteration 443/3560 Training loss: 2.0680 1.3079 sec/batch\n",
      "Epoch 3/20  Iteration 444/3560 Training loss: 2.0674 1.2934 sec/batch\n",
      "Epoch 3/20  Iteration 445/3560 Training loss: 2.0665 1.3416 sec/batch\n",
      "Epoch 3/20  Iteration 446/3560 Training loss: 2.0663 1.3165 sec/batch\n",
      "Epoch 3/20  Iteration 447/3560 Training loss: 2.0657 1.3562 sec/batch\n",
      "Epoch 3/20  Iteration 448/3560 Training loss: 2.0653 1.3215 sec/batch\n",
      "Epoch 3/20  Iteration 449/3560 Training loss: 2.0644 1.3430 sec/batch\n",
      "Epoch 3/20  Iteration 450/3560 Training loss: 2.0637 1.3438 sec/batch\n",
      "Epoch 3/20  Iteration 451/3560 Training loss: 2.0630 1.3414 sec/batch\n",
      "Epoch 3/20  Iteration 452/3560 Training loss: 2.0625 1.3473 sec/batch\n",
      "Epoch 3/20  Iteration 453/3560 Training loss: 2.0619 1.3293 sec/batch\n",
      "Epoch 3/20  Iteration 454/3560 Training loss: 2.0613 1.3529 sec/batch\n",
      "Epoch 3/20  Iteration 455/3560 Training loss: 2.0605 1.3427 sec/batch\n",
      "Epoch 3/20  Iteration 456/3560 Training loss: 2.0596 1.3166 sec/batch\n",
      "Epoch 3/20  Iteration 457/3560 Training loss: 2.0593 1.3495 sec/batch\n",
      "Epoch 3/20  Iteration 458/3560 Training loss: 2.0589 1.3355 sec/batch\n",
      "Epoch 3/20  Iteration 459/3560 Training loss: 2.0582 1.3382 sec/batch\n",
      "Epoch 3/20  Iteration 460/3560 Training loss: 2.0577 1.3072 sec/batch\n",
      "Epoch 3/20  Iteration 461/3560 Training loss: 2.0571 1.3278 sec/batch\n",
      "Epoch 3/20  Iteration 462/3560 Training loss: 2.0565 1.5435 sec/batch\n",
      "Epoch 3/20  Iteration 463/3560 Training loss: 2.0561 1.6314 sec/batch\n",
      "Epoch 3/20  Iteration 464/3560 Training loss: 2.0558 1.3636 sec/batch\n",
      "Epoch 3/20  Iteration 465/3560 Training loss: 2.0554 1.3710 sec/batch\n",
      "Epoch 3/20  Iteration 466/3560 Training loss: 2.0550 1.3204 sec/batch\n",
      "Epoch 3/20  Iteration 467/3560 Training loss: 2.0546 1.3312 sec/batch\n",
      "Epoch 3/20  Iteration 468/3560 Training loss: 2.0541 1.3581 sec/batch\n",
      "Epoch 3/20  Iteration 469/3560 Training loss: 2.0535 1.4035 sec/batch\n",
      "Epoch 3/20  Iteration 470/3560 Training loss: 2.0530 1.3266 sec/batch\n",
      "Epoch 3/20  Iteration 471/3560 Training loss: 2.0524 1.2989 sec/batch\n",
      "Epoch 3/20  Iteration 472/3560 Training loss: 2.0516 1.3387 sec/batch\n",
      "Epoch 3/20  Iteration 473/3560 Training loss: 2.0511 1.3226 sec/batch\n",
      "Epoch 3/20  Iteration 474/3560 Training loss: 2.0506 1.3165 sec/batch\n",
      "Epoch 3/20  Iteration 475/3560 Training loss: 2.0503 1.3336 sec/batch\n",
      "Epoch 3/20  Iteration 476/3560 Training loss: 2.0499 1.4149 sec/batch\n",
      "Epoch 3/20  Iteration 477/3560 Training loss: 2.0495 1.5227 sec/batch\n",
      "Epoch 3/20  Iteration 478/3560 Training loss: 2.0489 1.4640 sec/batch\n",
      "Epoch 3/20  Iteration 479/3560 Training loss: 2.0483 1.3315 sec/batch\n",
      "Epoch 3/20  Iteration 480/3560 Training loss: 2.0480 1.3485 sec/batch\n",
      "Epoch 3/20  Iteration 481/3560 Training loss: 2.0476 1.3695 sec/batch\n",
      "Epoch 3/20  Iteration 482/3560 Training loss: 2.0470 1.3525 sec/batch\n",
      "Epoch 3/20  Iteration 483/3560 Training loss: 2.0467 1.3325 sec/batch\n",
      "Epoch 3/20  Iteration 484/3560 Training loss: 2.0463 1.7013 sec/batch\n",
      "Epoch 3/20  Iteration 485/3560 Training loss: 2.0460 1.4513 sec/batch\n",
      "Epoch 3/20  Iteration 486/3560 Training loss: 2.0456 1.3078 sec/batch\n",
      "Epoch 3/20  Iteration 487/3560 Training loss: 2.0450 1.3457 sec/batch\n",
      "Epoch 3/20  Iteration 488/3560 Training loss: 2.0444 1.3001 sec/batch\n",
      "Epoch 3/20  Iteration 489/3560 Training loss: 2.0440 1.3478 sec/batch\n",
      "Epoch 3/20  Iteration 490/3560 Training loss: 2.0437 1.3316 sec/batch\n",
      "Epoch 3/20  Iteration 491/3560 Training loss: 2.0434 1.3640 sec/batch\n",
      "Epoch 3/20  Iteration 492/3560 Training loss: 2.0431 1.3263 sec/batch\n",
      "Epoch 3/20  Iteration 493/3560 Training loss: 2.0428 1.3360 sec/batch\n",
      "Epoch 3/20  Iteration 494/3560 Training loss: 2.0425 1.3046 sec/batch\n",
      "Epoch 3/20  Iteration 495/3560 Training loss: 2.0424 1.3025 sec/batch\n",
      "Epoch 3/20  Iteration 496/3560 Training loss: 2.0419 1.3213 sec/batch\n",
      "Epoch 3/20  Iteration 497/3560 Training loss: 2.0416 1.3582 sec/batch\n",
      "Epoch 3/20  Iteration 498/3560 Training loss: 2.0412 1.3598 sec/batch\n",
      "Epoch 3/20  Iteration 499/3560 Training loss: 2.0409 1.3527 sec/batch\n",
      "Epoch 3/20  Iteration 500/3560 Training loss: 2.0405 1.3025 sec/batch\n",
      "Epoch 3/20  Iteration 501/3560 Training loss: 2.0400 1.3513 sec/batch\n",
      "Epoch 3/20  Iteration 502/3560 Training loss: 2.0398 1.3121 sec/batch\n",
      "Epoch 3/20  Iteration 503/3560 Training loss: 2.0395 1.3166 sec/batch\n",
      "Epoch 3/20  Iteration 504/3560 Training loss: 2.0394 1.3150 sec/batch\n",
      "Epoch 3/20  Iteration 505/3560 Training loss: 2.0390 1.3436 sec/batch\n",
      "Epoch 3/20  Iteration 506/3560 Training loss: 2.0386 1.3752 sec/batch\n",
      "Epoch 3/20  Iteration 507/3560 Training loss: 2.0381 1.5950 sec/batch\n",
      "Epoch 3/20  Iteration 508/3560 Training loss: 2.0380 1.5872 sec/batch\n",
      "Epoch 3/20  Iteration 509/3560 Training loss: 2.0378 1.3807 sec/batch\n",
      "Epoch 3/20  Iteration 510/3560 Training loss: 2.0375 1.3353 sec/batch\n",
      "Epoch 3/20  Iteration 511/3560 Training loss: 2.0371 1.3265 sec/batch\n",
      "Epoch 3/20  Iteration 512/3560 Training loss: 2.0367 1.3349 sec/batch\n",
      "Epoch 3/20  Iteration 513/3560 Training loss: 2.0363 1.3793 sec/batch\n",
      "Epoch 3/20  Iteration 514/3560 Training loss: 2.0359 1.3631 sec/batch\n",
      "Epoch 3/20  Iteration 515/3560 Training loss: 2.0353 1.3432 sec/batch\n",
      "Epoch 3/20  Iteration 516/3560 Training loss: 2.0352 1.3077 sec/batch\n",
      "Epoch 3/20  Iteration 517/3560 Training loss: 2.0349 1.3408 sec/batch\n",
      "Epoch 3/20  Iteration 518/3560 Training loss: 2.0346 1.3063 sec/batch\n",
      "Epoch 3/20  Iteration 519/3560 Training loss: 2.0343 1.3163 sec/batch\n",
      "Epoch 3/20  Iteration 520/3560 Training loss: 2.0340 1.4366 sec/batch\n",
      "Epoch 3/20  Iteration 521/3560 Training loss: 2.0336 1.4668 sec/batch\n",
      "Epoch 3/20  Iteration 522/3560 Training loss: 2.0332 1.3453 sec/batch\n",
      "Epoch 3/20  Iteration 523/3560 Training loss: 2.0330 1.3929 sec/batch\n",
      "Epoch 3/20  Iteration 524/3560 Training loss: 2.0329 1.3270 sec/batch\n",
      "Epoch 3/20  Iteration 525/3560 Training loss: 2.0325 1.3068 sec/batch\n",
      "Epoch 3/20  Iteration 526/3560 Training loss: 2.0321 1.3297 sec/batch\n",
      "Epoch 3/20  Iteration 527/3560 Training loss: 2.0316 1.5206 sec/batch\n",
      "Epoch 3/20  Iteration 528/3560 Training loss: 2.0312 1.7239 sec/batch\n",
      "Epoch 3/20  Iteration 529/3560 Training loss: 2.0309 1.3651 sec/batch\n",
      "Epoch 3/20  Iteration 530/3560 Training loss: 2.0306 1.2585 sec/batch\n",
      "Epoch 3/20  Iteration 531/3560 Training loss: 2.0303 1.2354 sec/batch\n",
      "Epoch 3/20  Iteration 532/3560 Training loss: 2.0299 1.2725 sec/batch\n",
      "Epoch 3/20  Iteration 533/3560 Training loss: 2.0295 1.3281 sec/batch\n",
      "Epoch 3/20  Iteration 534/3560 Training loss: 2.0292 1.3612 sec/batch\n",
      "Epoch 4/20  Iteration 535/3560 Training loss: 2.0544 1.2624 sec/batch\n",
      "Epoch 4/20  Iteration 536/3560 Training loss: 1.9971 1.4392 sec/batch\n",
      "Epoch 4/20  Iteration 537/3560 Training loss: 1.9837 1.3892 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 538/3560 Training loss: 1.9760 1.2840 sec/batch\n",
      "Epoch 4/20  Iteration 539/3560 Training loss: 1.9708 1.2577 sec/batch\n",
      "Epoch 4/20  Iteration 540/3560 Training loss: 1.9621 1.2731 sec/batch\n",
      "Epoch 4/20  Iteration 541/3560 Training loss: 1.9607 1.2633 sec/batch\n",
      "Epoch 4/20  Iteration 542/3560 Training loss: 1.9590 1.4436 sec/batch\n",
      "Epoch 4/20  Iteration 543/3560 Training loss: 1.9610 1.4347 sec/batch\n",
      "Epoch 4/20  Iteration 544/3560 Training loss: 1.9595 1.4969 sec/batch\n",
      "Epoch 4/20  Iteration 545/3560 Training loss: 1.9559 1.3520 sec/batch\n",
      "Epoch 4/20  Iteration 546/3560 Training loss: 1.9535 1.4067 sec/batch\n",
      "Epoch 4/20  Iteration 547/3560 Training loss: 1.9533 1.2520 sec/batch\n",
      "Epoch 4/20  Iteration 548/3560 Training loss: 1.9553 1.2540 sec/batch\n",
      "Epoch 4/20  Iteration 549/3560 Training loss: 1.9541 1.2852 sec/batch\n",
      "Epoch 4/20  Iteration 550/3560 Training loss: 1.9525 1.4112 sec/batch\n",
      "Epoch 4/20  Iteration 551/3560 Training loss: 1.9516 1.9847 sec/batch\n",
      "Epoch 4/20  Iteration 552/3560 Training loss: 1.9536 1.4099 sec/batch\n",
      "Epoch 4/20  Iteration 553/3560 Training loss: 1.9540 1.3962 sec/batch\n",
      "Epoch 4/20  Iteration 554/3560 Training loss: 1.9541 1.2640 sec/batch\n",
      "Epoch 4/20  Iteration 555/3560 Training loss: 1.9538 1.2724 sec/batch\n",
      "Epoch 4/20  Iteration 556/3560 Training loss: 1.9548 1.2605 sec/batch\n",
      "Epoch 4/20  Iteration 557/3560 Training loss: 1.9542 1.2515 sec/batch\n",
      "Epoch 4/20  Iteration 558/3560 Training loss: 1.9536 1.3410 sec/batch\n",
      "Epoch 4/20  Iteration 559/3560 Training loss: 1.9535 1.2718 sec/batch\n",
      "Epoch 4/20  Iteration 560/3560 Training loss: 1.9522 1.3003 sec/batch\n",
      "Epoch 4/20  Iteration 561/3560 Training loss: 1.9512 1.3731 sec/batch\n",
      "Epoch 4/20  Iteration 562/3560 Training loss: 1.9517 1.3685 sec/batch\n",
      "Epoch 4/20  Iteration 563/3560 Training loss: 1.9523 1.3771 sec/batch\n",
      "Epoch 4/20  Iteration 564/3560 Training loss: 1.9520 1.4075 sec/batch\n",
      "Epoch 4/20  Iteration 565/3560 Training loss: 1.9518 1.3269 sec/batch\n",
      "Epoch 4/20  Iteration 566/3560 Training loss: 1.9509 1.3045 sec/batch\n",
      "Epoch 4/20  Iteration 567/3560 Training loss: 1.9507 1.2494 sec/batch\n",
      "Epoch 4/20  Iteration 568/3560 Training loss: 1.9512 1.3575 sec/batch\n",
      "Epoch 4/20  Iteration 569/3560 Training loss: 1.9508 1.2863 sec/batch\n",
      "Epoch 4/20  Iteration 570/3560 Training loss: 1.9501 1.2433 sec/batch\n",
      "Epoch 4/20  Iteration 571/3560 Training loss: 1.9494 1.2428 sec/batch\n",
      "Epoch 4/20  Iteration 572/3560 Training loss: 1.9481 1.3174 sec/batch\n",
      "Epoch 4/20  Iteration 573/3560 Training loss: 1.9466 1.7788 sec/batch\n",
      "Epoch 4/20  Iteration 574/3560 Training loss: 1.9457 1.4260 sec/batch\n",
      "Epoch 4/20  Iteration 575/3560 Training loss: 1.9451 1.7029 sec/batch\n",
      "Epoch 4/20  Iteration 576/3560 Training loss: 1.9450 1.7686 sec/batch\n",
      "Epoch 4/20  Iteration 577/3560 Training loss: 1.9445 1.4675 sec/batch\n",
      "Epoch 4/20  Iteration 578/3560 Training loss: 1.9433 1.7286 sec/batch\n",
      "Epoch 4/20  Iteration 579/3560 Training loss: 1.9434 1.3760 sec/batch\n",
      "Epoch 4/20  Iteration 580/3560 Training loss: 1.9419 1.4060 sec/batch\n",
      "Epoch 4/20  Iteration 581/3560 Training loss: 1.9415 1.3426 sec/batch\n",
      "Epoch 4/20  Iteration 582/3560 Training loss: 1.9407 1.3629 sec/batch\n",
      "Epoch 4/20  Iteration 583/3560 Training loss: 1.9402 1.5900 sec/batch\n",
      "Epoch 4/20  Iteration 584/3560 Training loss: 1.9408 1.3604 sec/batch\n",
      "Epoch 4/20  Iteration 585/3560 Training loss: 1.9399 1.3356 sec/batch\n",
      "Epoch 4/20  Iteration 586/3560 Training loss: 1.9402 1.3408 sec/batch\n",
      "Epoch 4/20  Iteration 587/3560 Training loss: 1.9398 1.6804 sec/batch\n",
      "Epoch 4/20  Iteration 588/3560 Training loss: 1.9394 1.5332 sec/batch\n",
      "Epoch 4/20  Iteration 589/3560 Training loss: 1.9388 1.3950 sec/batch\n",
      "Epoch 4/20  Iteration 590/3560 Training loss: 1.9386 1.3500 sec/batch\n",
      "Epoch 4/20  Iteration 591/3560 Training loss: 1.9384 1.2505 sec/batch\n",
      "Epoch 4/20  Iteration 592/3560 Training loss: 1.9379 1.2454 sec/batch\n",
      "Epoch 4/20  Iteration 593/3560 Training loss: 1.9369 1.2751 sec/batch\n",
      "Epoch 4/20  Iteration 594/3560 Training loss: 1.9372 1.7132 sec/batch\n",
      "Epoch 4/20  Iteration 595/3560 Training loss: 1.9366 1.2593 sec/batch\n",
      "Epoch 4/20  Iteration 596/3560 Training loss: 1.9369 1.3009 sec/batch\n",
      "Epoch 4/20  Iteration 597/3560 Training loss: 1.9369 1.4108 sec/batch\n",
      "Epoch 4/20  Iteration 598/3560 Training loss: 1.9368 1.4115 sec/batch\n",
      "Epoch 4/20  Iteration 599/3560 Training loss: 1.9362 1.5259 sec/batch\n",
      "Epoch 4/20  Iteration 600/3560 Training loss: 1.9362 1.4143 sec/batch\n",
      "Epoch 4/20  Iteration 601/3560 Training loss: 1.9361 1.4091 sec/batch\n",
      "Epoch 4/20  Iteration 602/3560 Training loss: 1.9354 1.3881 sec/batch\n",
      "Epoch 4/20  Iteration 603/3560 Training loss: 1.9350 1.3562 sec/batch\n",
      "Epoch 4/20  Iteration 604/3560 Training loss: 1.9347 1.3214 sec/batch\n",
      "Epoch 4/20  Iteration 605/3560 Training loss: 1.9348 1.3204 sec/batch\n",
      "Epoch 4/20  Iteration 606/3560 Training loss: 1.9345 1.3391 sec/batch\n",
      "Epoch 4/20  Iteration 607/3560 Training loss: 1.9346 1.3060 sec/batch\n",
      "Epoch 4/20  Iteration 608/3560 Training loss: 1.9339 1.2615 sec/batch\n",
      "Epoch 4/20  Iteration 609/3560 Training loss: 1.9334 1.2726 sec/batch\n",
      "Epoch 4/20  Iteration 610/3560 Training loss: 1.9335 1.2638 sec/batch\n",
      "Epoch 4/20  Iteration 611/3560 Training loss: 1.9331 1.4602 sec/batch\n",
      "Epoch 4/20  Iteration 612/3560 Training loss: 1.9330 1.3451 sec/batch\n",
      "Epoch 4/20  Iteration 613/3560 Training loss: 1.9320 1.3703 sec/batch\n",
      "Epoch 4/20  Iteration 614/3560 Training loss: 1.9315 1.2934 sec/batch\n",
      "Epoch 4/20  Iteration 615/3560 Training loss: 1.9306 1.6353 sec/batch\n",
      "Epoch 4/20  Iteration 616/3560 Training loss: 1.9305 1.5129 sec/batch\n",
      "Epoch 4/20  Iteration 617/3560 Training loss: 1.9296 1.5694 sec/batch\n",
      "Epoch 4/20  Iteration 618/3560 Training loss: 1.9292 1.3878 sec/batch\n",
      "Epoch 4/20  Iteration 619/3560 Training loss: 1.9282 1.5249 sec/batch\n",
      "Epoch 4/20  Iteration 620/3560 Training loss: 1.9275 1.5518 sec/batch\n",
      "Epoch 4/20  Iteration 621/3560 Training loss: 1.9271 1.7450 sec/batch\n",
      "Epoch 4/20  Iteration 622/3560 Training loss: 1.9266 1.5553 sec/batch\n",
      "Epoch 4/20  Iteration 623/3560 Training loss: 1.9257 1.4626 sec/batch\n",
      "Epoch 4/20  Iteration 624/3560 Training loss: 1.9256 1.3571 sec/batch\n",
      "Epoch 4/20  Iteration 625/3560 Training loss: 1.9250 1.3210 sec/batch\n",
      "Epoch 4/20  Iteration 626/3560 Training loss: 1.9248 1.2948 sec/batch\n",
      "Epoch 4/20  Iteration 627/3560 Training loss: 1.9241 1.5150 sec/batch\n",
      "Epoch 4/20  Iteration 628/3560 Training loss: 1.9235 1.4975 sec/batch\n",
      "Epoch 4/20  Iteration 629/3560 Training loss: 1.9230 1.4192 sec/batch\n",
      "Epoch 4/20  Iteration 630/3560 Training loss: 1.9226 1.3995 sec/batch\n",
      "Epoch 4/20  Iteration 631/3560 Training loss: 1.9222 1.3587 sec/batch\n",
      "Epoch 4/20  Iteration 632/3560 Training loss: 1.9217 1.3376 sec/batch\n",
      "Epoch 4/20  Iteration 633/3560 Training loss: 1.9211 1.3451 sec/batch\n",
      "Epoch 4/20  Iteration 634/3560 Training loss: 1.9203 1.5351 sec/batch\n",
      "Epoch 4/20  Iteration 635/3560 Training loss: 1.9199 1.8448 sec/batch\n",
      "Epoch 4/20  Iteration 636/3560 Training loss: 1.9195 1.9426 sec/batch\n",
      "Epoch 4/20  Iteration 637/3560 Training loss: 1.9190 1.5087 sec/batch\n",
      "Epoch 4/20  Iteration 638/3560 Training loss: 1.9185 1.4368 sec/batch\n",
      "Epoch 4/20  Iteration 639/3560 Training loss: 1.9180 1.3863 sec/batch\n",
      "Epoch 4/20  Iteration 640/3560 Training loss: 1.9176 1.3613 sec/batch\n",
      "Epoch 4/20  Iteration 641/3560 Training loss: 1.9172 1.3396 sec/batch\n",
      "Epoch 4/20  Iteration 642/3560 Training loss: 1.9169 1.3360 sec/batch\n",
      "Epoch 4/20  Iteration 643/3560 Training loss: 1.9166 1.3937 sec/batch\n",
      "Epoch 4/20  Iteration 644/3560 Training loss: 1.9162 1.4351 sec/batch\n",
      "Epoch 4/20  Iteration 645/3560 Training loss: 1.9159 1.3593 sec/batch\n",
      "Epoch 4/20  Iteration 646/3560 Training loss: 1.9155 1.3522 sec/batch\n",
      "Epoch 4/20  Iteration 647/3560 Training loss: 1.9150 1.3681 sec/batch\n",
      "Epoch 4/20  Iteration 648/3560 Training loss: 1.9146 1.3488 sec/batch\n",
      "Epoch 4/20  Iteration 649/3560 Training loss: 1.9141 1.3390 sec/batch\n",
      "Epoch 4/20  Iteration 650/3560 Training loss: 1.9134 1.3377 sec/batch\n",
      "Epoch 4/20  Iteration 651/3560 Training loss: 1.9130 1.3925 sec/batch\n",
      "Epoch 4/20  Iteration 652/3560 Training loss: 1.9126 1.3744 sec/batch\n",
      "Epoch 4/20  Iteration 653/3560 Training loss: 1.9122 1.3391 sec/batch\n",
      "Epoch 4/20  Iteration 654/3560 Training loss: 1.9118 1.3569 sec/batch\n",
      "Epoch 4/20  Iteration 655/3560 Training loss: 1.9116 1.3994 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 656/3560 Training loss: 1.9110 1.3087 sec/batch\n",
      "Epoch 4/20  Iteration 657/3560 Training loss: 1.9104 1.3046 sec/batch\n",
      "Epoch 4/20  Iteration 658/3560 Training loss: 1.9102 1.3305 sec/batch\n",
      "Epoch 4/20  Iteration 659/3560 Training loss: 1.9100 1.2619 sec/batch\n",
      "Epoch 4/20  Iteration 660/3560 Training loss: 1.9093 1.2443 sec/batch\n",
      "Epoch 4/20  Iteration 661/3560 Training loss: 1.9092 1.2500 sec/batch\n",
      "Epoch 4/20  Iteration 662/3560 Training loss: 1.9090 1.2397 sec/batch\n",
      "Epoch 4/20  Iteration 663/3560 Training loss: 1.9086 1.2597 sec/batch\n",
      "Epoch 4/20  Iteration 664/3560 Training loss: 1.9083 1.2763 sec/batch\n",
      "Epoch 4/20  Iteration 665/3560 Training loss: 1.9077 1.2764 sec/batch\n",
      "Epoch 4/20  Iteration 666/3560 Training loss: 1.9072 1.6780 sec/batch\n",
      "Epoch 4/20  Iteration 667/3560 Training loss: 1.9068 1.2996 sec/batch\n",
      "Epoch 4/20  Iteration 668/3560 Training loss: 1.9065 1.2572 sec/batch\n",
      "Epoch 4/20  Iteration 669/3560 Training loss: 1.9062 1.2599 sec/batch\n",
      "Epoch 4/20  Iteration 670/3560 Training loss: 1.9060 1.2470 sec/batch\n",
      "Epoch 4/20  Iteration 671/3560 Training loss: 1.9057 1.2628 sec/batch\n",
      "Epoch 4/20  Iteration 672/3560 Training loss: 1.9055 1.2554 sec/batch\n",
      "Epoch 4/20  Iteration 673/3560 Training loss: 1.9055 1.2640 sec/batch\n",
      "Epoch 4/20  Iteration 674/3560 Training loss: 1.9051 1.2935 sec/batch\n",
      "Epoch 4/20  Iteration 675/3560 Training loss: 1.9051 1.2418 sec/batch\n",
      "Epoch 4/20  Iteration 676/3560 Training loss: 1.9048 1.3421 sec/batch\n",
      "Epoch 4/20  Iteration 677/3560 Training loss: 1.9045 1.2525 sec/batch\n",
      "Epoch 4/20  Iteration 678/3560 Training loss: 1.9043 1.2528 sec/batch\n",
      "Epoch 4/20  Iteration 679/3560 Training loss: 1.9039 1.2590 sec/batch\n",
      "Epoch 4/20  Iteration 680/3560 Training loss: 1.9037 1.2604 sec/batch\n",
      "Epoch 4/20  Iteration 681/3560 Training loss: 1.9035 1.2797 sec/batch\n",
      "Epoch 4/20  Iteration 682/3560 Training loss: 1.9034 1.2791 sec/batch\n",
      "Epoch 4/20  Iteration 683/3560 Training loss: 1.9031 1.2262 sec/batch\n",
      "Epoch 4/20  Iteration 684/3560 Training loss: 1.9027 1.2350 sec/batch\n",
      "Epoch 4/20  Iteration 685/3560 Training loss: 1.9023 1.2758 sec/batch\n",
      "Epoch 4/20  Iteration 686/3560 Training loss: 1.9021 1.3310 sec/batch\n",
      "Epoch 4/20  Iteration 687/3560 Training loss: 1.9020 1.2522 sec/batch\n",
      "Epoch 4/20  Iteration 688/3560 Training loss: 1.9018 1.2602 sec/batch\n",
      "Epoch 4/20  Iteration 689/3560 Training loss: 1.9015 1.2606 sec/batch\n",
      "Epoch 4/20  Iteration 690/3560 Training loss: 1.9012 1.3244 sec/batch\n",
      "Epoch 4/20  Iteration 691/3560 Training loss: 1.9009 1.2674 sec/batch\n",
      "Epoch 4/20  Iteration 692/3560 Training loss: 1.9006 1.2440 sec/batch\n",
      "Epoch 4/20  Iteration 693/3560 Training loss: 1.9001 1.2601 sec/batch\n",
      "Epoch 4/20  Iteration 694/3560 Training loss: 1.9000 1.2529 sec/batch\n",
      "Epoch 4/20  Iteration 695/3560 Training loss: 1.8999 1.3806 sec/batch\n",
      "Epoch 4/20  Iteration 696/3560 Training loss: 1.8996 1.2652 sec/batch\n",
      "Epoch 4/20  Iteration 697/3560 Training loss: 1.8994 1.2465 sec/batch\n",
      "Epoch 4/20  Iteration 698/3560 Training loss: 1.8992 1.3326 sec/batch\n",
      "Epoch 4/20  Iteration 699/3560 Training loss: 1.8989 1.2568 sec/batch\n",
      "Epoch 4/20  Iteration 700/3560 Training loss: 1.8985 1.2355 sec/batch\n",
      "Epoch 4/20  Iteration 701/3560 Training loss: 1.8984 1.2387 sec/batch\n",
      "Epoch 4/20  Iteration 702/3560 Training loss: 1.8985 1.2547 sec/batch\n",
      "Epoch 4/20  Iteration 703/3560 Training loss: 1.8982 1.2380 sec/batch\n",
      "Epoch 4/20  Iteration 704/3560 Training loss: 1.8978 1.3406 sec/batch\n",
      "Epoch 4/20  Iteration 705/3560 Training loss: 1.8974 1.2724 sec/batch\n",
      "Epoch 4/20  Iteration 706/3560 Training loss: 1.8971 1.3013 sec/batch\n",
      "Epoch 4/20  Iteration 707/3560 Training loss: 1.8970 1.3377 sec/batch\n",
      "Epoch 4/20  Iteration 708/3560 Training loss: 1.8968 1.5402 sec/batch\n",
      "Epoch 4/20  Iteration 709/3560 Training loss: 1.8966 1.3528 sec/batch\n",
      "Epoch 4/20  Iteration 710/3560 Training loss: 1.8963 1.3047 sec/batch\n",
      "Epoch 4/20  Iteration 711/3560 Training loss: 1.8960 1.2566 sec/batch\n",
      "Epoch 4/20  Iteration 712/3560 Training loss: 1.8958 1.2765 sec/batch\n",
      "Epoch 5/20  Iteration 713/3560 Training loss: 1.9355 1.3311 sec/batch\n",
      "Epoch 5/20  Iteration 714/3560 Training loss: 1.8872 1.2985 sec/batch\n",
      "Epoch 5/20  Iteration 715/3560 Training loss: 1.8726 1.2476 sec/batch\n",
      "Epoch 5/20  Iteration 716/3560 Training loss: 1.8645 1.2620 sec/batch\n",
      "Epoch 5/20  Iteration 717/3560 Training loss: 1.8594 1.2405 sec/batch\n",
      "Epoch 5/20  Iteration 718/3560 Training loss: 1.8474 1.2381 sec/batch\n",
      "Epoch 5/20  Iteration 719/3560 Training loss: 1.8462 1.2355 sec/batch\n",
      "Epoch 5/20  Iteration 720/3560 Training loss: 1.8442 1.2441 sec/batch\n",
      "Epoch 5/20  Iteration 721/3560 Training loss: 1.8462 1.2576 sec/batch\n",
      "Epoch 5/20  Iteration 722/3560 Training loss: 1.8443 1.2616 sec/batch\n",
      "Epoch 5/20  Iteration 723/3560 Training loss: 1.8410 1.3460 sec/batch\n",
      "Epoch 5/20  Iteration 724/3560 Training loss: 1.8379 1.2423 sec/batch\n",
      "Epoch 5/20  Iteration 725/3560 Training loss: 1.8384 1.2337 sec/batch\n",
      "Epoch 5/20  Iteration 726/3560 Training loss: 1.8402 1.2302 sec/batch\n",
      "Epoch 5/20  Iteration 727/3560 Training loss: 1.8396 1.2433 sec/batch\n",
      "Epoch 5/20  Iteration 728/3560 Training loss: 1.8377 1.2482 sec/batch\n",
      "Epoch 5/20  Iteration 729/3560 Training loss: 1.8370 1.2545 sec/batch\n",
      "Epoch 5/20  Iteration 730/3560 Training loss: 1.8391 1.2659 sec/batch\n",
      "Epoch 5/20  Iteration 731/3560 Training loss: 1.8390 1.2403 sec/batch\n",
      "Epoch 5/20  Iteration 732/3560 Training loss: 1.8395 1.3089 sec/batch\n",
      "Epoch 5/20  Iteration 733/3560 Training loss: 1.8385 1.3367 sec/batch\n",
      "Epoch 5/20  Iteration 734/3560 Training loss: 1.8395 1.2390 sec/batch\n",
      "Epoch 5/20  Iteration 735/3560 Training loss: 1.8385 1.2470 sec/batch\n",
      "Epoch 5/20  Iteration 736/3560 Training loss: 1.8378 1.2372 sec/batch\n",
      "Epoch 5/20  Iteration 737/3560 Training loss: 1.8376 1.2742 sec/batch\n",
      "Epoch 5/20  Iteration 738/3560 Training loss: 1.8359 1.2748 sec/batch\n",
      "Epoch 5/20  Iteration 739/3560 Training loss: 1.8342 1.2391 sec/batch\n",
      "Epoch 5/20  Iteration 740/3560 Training loss: 1.8342 1.2561 sec/batch\n",
      "Epoch 5/20  Iteration 741/3560 Training loss: 1.8346 1.2408 sec/batch\n",
      "Epoch 5/20  Iteration 742/3560 Training loss: 1.8344 1.3530 sec/batch\n",
      "Epoch 5/20  Iteration 743/3560 Training loss: 1.8339 1.2515 sec/batch\n",
      "Epoch 5/20  Iteration 744/3560 Training loss: 1.8326 1.2392 sec/batch\n",
      "Epoch 5/20  Iteration 745/3560 Training loss: 1.8326 1.3066 sec/batch\n",
      "Epoch 5/20  Iteration 746/3560 Training loss: 1.8329 1.2400 sec/batch\n",
      "Epoch 5/20  Iteration 747/3560 Training loss: 1.8323 1.2353 sec/batch\n",
      "Epoch 5/20  Iteration 748/3560 Training loss: 1.8317 1.2459 sec/batch\n",
      "Epoch 5/20  Iteration 749/3560 Training loss: 1.8309 1.2317 sec/batch\n",
      "Epoch 5/20  Iteration 750/3560 Training loss: 1.8295 1.3089 sec/batch\n",
      "Epoch 5/20  Iteration 751/3560 Training loss: 1.8279 1.3128 sec/batch\n",
      "Epoch 5/20  Iteration 752/3560 Training loss: 1.8271 1.3057 sec/batch\n",
      "Epoch 5/20  Iteration 753/3560 Training loss: 1.8264 1.3063 sec/batch\n",
      "Epoch 5/20  Iteration 754/3560 Training loss: 1.8265 1.2498 sec/batch\n",
      "Epoch 5/20  Iteration 755/3560 Training loss: 1.8259 1.2344 sec/batch\n",
      "Epoch 5/20  Iteration 756/3560 Training loss: 1.8250 1.2325 sec/batch\n",
      "Epoch 5/20  Iteration 757/3560 Training loss: 1.8249 1.2609 sec/batch\n",
      "Epoch 5/20  Iteration 758/3560 Training loss: 1.8236 1.2353 sec/batch\n",
      "Epoch 5/20  Iteration 759/3560 Training loss: 1.8231 1.3358 sec/batch\n",
      "Epoch 5/20  Iteration 760/3560 Training loss: 1.8223 1.2978 sec/batch\n",
      "Epoch 5/20  Iteration 761/3560 Training loss: 1.8220 1.3042 sec/batch\n",
      "Epoch 5/20  Iteration 762/3560 Training loss: 1.8226 1.2352 sec/batch\n",
      "Epoch 5/20  Iteration 763/3560 Training loss: 1.8218 1.2280 sec/batch\n",
      "Epoch 5/20  Iteration 764/3560 Training loss: 1.8223 1.2506 sec/batch\n",
      "Epoch 5/20  Iteration 765/3560 Training loss: 1.8221 1.2511 sec/batch\n",
      "Epoch 5/20  Iteration 766/3560 Training loss: 1.8219 1.2333 sec/batch\n",
      "Epoch 5/20  Iteration 767/3560 Training loss: 1.8215 1.2422 sec/batch\n",
      "Epoch 5/20  Iteration 768/3560 Training loss: 1.8213 1.2461 sec/batch\n",
      "Epoch 5/20  Iteration 769/3560 Training loss: 1.8215 1.3718 sec/batch\n",
      "Epoch 5/20  Iteration 770/3560 Training loss: 1.8209 1.2501 sec/batch\n",
      "Epoch 5/20  Iteration 771/3560 Training loss: 1.8202 1.2590 sec/batch\n",
      "Epoch 5/20  Iteration 772/3560 Training loss: 1.8203 1.2206 sec/batch\n",
      "Epoch 5/20  Iteration 773/3560 Training loss: 1.8200 1.2638 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 774/3560 Training loss: 1.8205 1.2431 sec/batch\n",
      "Epoch 5/20  Iteration 775/3560 Training loss: 1.8207 1.3598 sec/batch\n",
      "Epoch 5/20  Iteration 776/3560 Training loss: 1.8208 1.2437 sec/batch\n",
      "Epoch 5/20  Iteration 777/3560 Training loss: 1.8203 1.3000 sec/batch\n",
      "Epoch 5/20  Iteration 778/3560 Training loss: 1.8204 1.2574 sec/batch\n",
      "Epoch 5/20  Iteration 779/3560 Training loss: 1.8204 1.2535 sec/batch\n",
      "Epoch 5/20  Iteration 780/3560 Training loss: 1.8198 1.5328 sec/batch\n",
      "Epoch 5/20  Iteration 781/3560 Training loss: 1.8196 1.3203 sec/batch\n",
      "Epoch 5/20  Iteration 782/3560 Training loss: 1.8192 1.2298 sec/batch\n",
      "Epoch 5/20  Iteration 783/3560 Training loss: 1.8195 1.2805 sec/batch\n",
      "Epoch 5/20  Iteration 784/3560 Training loss: 1.8193 1.3307 sec/batch\n",
      "Epoch 5/20  Iteration 785/3560 Training loss: 1.8194 1.3414 sec/batch\n",
      "Epoch 5/20  Iteration 786/3560 Training loss: 1.8189 1.2581 sec/batch\n",
      "Epoch 5/20  Iteration 787/3560 Training loss: 1.8185 1.2566 sec/batch\n",
      "Epoch 5/20  Iteration 788/3560 Training loss: 1.8186 1.2510 sec/batch\n",
      "Epoch 5/20  Iteration 789/3560 Training loss: 1.8184 1.2538 sec/batch\n",
      "Epoch 5/20  Iteration 790/3560 Training loss: 1.8183 1.2270 sec/batch\n",
      "Epoch 5/20  Iteration 791/3560 Training loss: 1.8175 1.2485 sec/batch\n",
      "Epoch 5/20  Iteration 792/3560 Training loss: 1.8170 1.2367 sec/batch\n",
      "Epoch 5/20  Iteration 793/3560 Training loss: 1.8164 1.2827 sec/batch\n",
      "Epoch 5/20  Iteration 794/3560 Training loss: 1.8163 1.3210 sec/batch\n",
      "Epoch 5/20  Iteration 795/3560 Training loss: 1.8155 1.2331 sec/batch\n",
      "Epoch 5/20  Iteration 796/3560 Training loss: 1.8153 1.2303 sec/batch\n",
      "Epoch 5/20  Iteration 797/3560 Training loss: 1.8146 1.2466 sec/batch\n",
      "Epoch 5/20  Iteration 798/3560 Training loss: 1.8140 1.2303 sec/batch\n",
      "Epoch 5/20  Iteration 799/3560 Training loss: 1.8136 1.3021 sec/batch\n",
      "Epoch 5/20  Iteration 800/3560 Training loss: 1.8131 1.2348 sec/batch\n",
      "Epoch 5/20  Iteration 801/3560 Training loss: 1.8124 1.2690 sec/batch\n",
      "Epoch 5/20  Iteration 802/3560 Training loss: 1.8124 1.2334 sec/batch\n",
      "Epoch 5/20  Iteration 803/3560 Training loss: 1.8118 1.3314 sec/batch\n",
      "Epoch 5/20  Iteration 804/3560 Training loss: 1.8114 1.2371 sec/batch\n",
      "Epoch 5/20  Iteration 805/3560 Training loss: 1.8107 1.2322 sec/batch\n",
      "Epoch 5/20  Iteration 806/3560 Training loss: 1.8101 1.2648 sec/batch\n",
      "Epoch 5/20  Iteration 807/3560 Training loss: 1.8096 1.2621 sec/batch\n",
      "Epoch 5/20  Iteration 808/3560 Training loss: 1.8094 1.2315 sec/batch\n",
      "Epoch 5/20  Iteration 809/3560 Training loss: 1.8090 1.2741 sec/batch\n",
      "Epoch 5/20  Iteration 810/3560 Training loss: 1.8084 1.2348 sec/batch\n",
      "Epoch 5/20  Iteration 811/3560 Training loss: 1.8078 1.2352 sec/batch\n",
      "Epoch 5/20  Iteration 812/3560 Training loss: 1.8070 1.2368 sec/batch\n",
      "Epoch 5/20  Iteration 813/3560 Training loss: 1.8066 1.3169 sec/batch\n",
      "Epoch 5/20  Iteration 814/3560 Training loss: 1.8064 1.2709 sec/batch\n",
      "Epoch 5/20  Iteration 815/3560 Training loss: 1.8059 1.2544 sec/batch\n",
      "Epoch 5/20  Iteration 816/3560 Training loss: 1.8055 1.2553 sec/batch\n",
      "Epoch 5/20  Iteration 817/3560 Training loss: 1.8051 1.2842 sec/batch\n",
      "Epoch 5/20  Iteration 818/3560 Training loss: 1.8047 1.2552 sec/batch\n",
      "Epoch 5/20  Iteration 819/3560 Training loss: 1.8044 1.3300 sec/batch\n",
      "Epoch 5/20  Iteration 820/3560 Training loss: 1.8041 1.2789 sec/batch\n",
      "Epoch 5/20  Iteration 821/3560 Training loss: 1.8039 1.4876 sec/batch\n",
      "Epoch 5/20  Iteration 822/3560 Training loss: 1.8037 1.5064 sec/batch\n",
      "Epoch 5/20  Iteration 823/3560 Training loss: 1.8034 1.2778 sec/batch\n",
      "Epoch 5/20  Iteration 824/3560 Training loss: 1.8031 1.3113 sec/batch\n",
      "Epoch 5/20  Iteration 825/3560 Training loss: 1.8027 1.2764 sec/batch\n",
      "Epoch 5/20  Iteration 826/3560 Training loss: 1.8023 1.2428 sec/batch\n",
      "Epoch 5/20  Iteration 827/3560 Training loss: 1.8018 1.2615 sec/batch\n",
      "Epoch 5/20  Iteration 828/3560 Training loss: 1.8013 1.2558 sec/batch\n",
      "Epoch 5/20  Iteration 829/3560 Training loss: 1.8010 1.2378 sec/batch\n",
      "Epoch 5/20  Iteration 830/3560 Training loss: 1.8006 1.2812 sec/batch\n",
      "Epoch 5/20  Iteration 831/3560 Training loss: 1.8003 1.2800 sec/batch\n",
      "Epoch 5/20  Iteration 832/3560 Training loss: 1.8000 1.3402 sec/batch\n",
      "Epoch 5/20  Iteration 833/3560 Training loss: 1.7997 1.2876 sec/batch\n",
      "Epoch 5/20  Iteration 834/3560 Training loss: 1.7992 1.2329 sec/batch\n",
      "Epoch 5/20  Iteration 835/3560 Training loss: 1.7986 1.2344 sec/batch\n",
      "Epoch 5/20  Iteration 836/3560 Training loss: 1.7985 1.2653 sec/batch\n",
      "Epoch 5/20  Iteration 837/3560 Training loss: 1.7982 1.2503 sec/batch\n",
      "Epoch 5/20  Iteration 838/3560 Training loss: 1.7977 1.2701 sec/batch\n",
      "Epoch 5/20  Iteration 839/3560 Training loss: 1.7976 1.2624 sec/batch\n",
      "Epoch 5/20  Iteration 840/3560 Training loss: 1.7974 1.2690 sec/batch\n",
      "Epoch 5/20  Iteration 841/3560 Training loss: 1.7971 1.3565 sec/batch\n",
      "Epoch 5/20  Iteration 842/3560 Training loss: 1.7968 1.2490 sec/batch\n",
      "Epoch 5/20  Iteration 843/3560 Training loss: 1.7963 1.2705 sec/batch\n",
      "Epoch 5/20  Iteration 844/3560 Training loss: 1.7958 1.2470 sec/batch\n",
      "Epoch 5/20  Iteration 845/3560 Training loss: 1.7955 1.2435 sec/batch\n",
      "Epoch 5/20  Iteration 846/3560 Training loss: 1.7953 1.2474 sec/batch\n",
      "Epoch 5/20  Iteration 847/3560 Training loss: 1.7951 1.2389 sec/batch\n",
      "Epoch 5/20  Iteration 848/3560 Training loss: 1.7949 1.2820 sec/batch\n",
      "Epoch 5/20  Iteration 849/3560 Training loss: 1.7948 1.2410 sec/batch\n",
      "Epoch 5/20  Iteration 850/3560 Training loss: 1.7947 1.3018 sec/batch\n",
      "Epoch 5/20  Iteration 851/3560 Training loss: 1.7946 1.2967 sec/batch\n",
      "Epoch 5/20  Iteration 852/3560 Training loss: 1.7943 1.2522 sec/batch\n",
      "Epoch 5/20  Iteration 853/3560 Training loss: 1.7944 1.2685 sec/batch\n",
      "Epoch 5/20  Iteration 854/3560 Training loss: 1.7941 1.2501 sec/batch\n",
      "Epoch 5/20  Iteration 855/3560 Training loss: 1.7939 1.2537 sec/batch\n",
      "Epoch 5/20  Iteration 856/3560 Training loss: 1.7938 1.2596 sec/batch\n",
      "Epoch 5/20  Iteration 857/3560 Training loss: 1.7934 1.2559 sec/batch\n",
      "Epoch 5/20  Iteration 858/3560 Training loss: 1.7933 1.2563 sec/batch\n",
      "Epoch 5/20  Iteration 859/3560 Training loss: 1.7931 1.2494 sec/batch\n",
      "Epoch 5/20  Iteration 860/3560 Training loss: 1.7931 1.3516 sec/batch\n",
      "Epoch 5/20  Iteration 861/3560 Training loss: 1.7929 1.2713 sec/batch\n",
      "Epoch 5/20  Iteration 862/3560 Training loss: 1.7926 1.2389 sec/batch\n",
      "Epoch 5/20  Iteration 863/3560 Training loss: 1.7921 1.2583 sec/batch\n",
      "Epoch 5/20  Iteration 864/3560 Training loss: 1.7920 1.2573 sec/batch\n",
      "Epoch 5/20  Iteration 865/3560 Training loss: 1.7919 1.2798 sec/batch\n",
      "Epoch 5/20  Iteration 866/3560 Training loss: 1.7918 1.2475 sec/batch\n",
      "Epoch 5/20  Iteration 867/3560 Training loss: 1.7916 1.2321 sec/batch\n",
      "Epoch 5/20  Iteration 868/3560 Training loss: 1.7913 1.2460 sec/batch\n",
      "Epoch 5/20  Iteration 869/3560 Training loss: 1.7912 1.3006 sec/batch\n",
      "Epoch 5/20  Iteration 870/3560 Training loss: 1.7910 1.3167 sec/batch\n",
      "Epoch 5/20  Iteration 871/3560 Training loss: 1.7905 1.2522 sec/batch\n",
      "Epoch 5/20  Iteration 872/3560 Training loss: 1.7905 1.2841 sec/batch\n",
      "Epoch 5/20  Iteration 873/3560 Training loss: 1.7905 1.2345 sec/batch\n",
      "Epoch 5/20  Iteration 874/3560 Training loss: 1.7902 1.2520 sec/batch\n",
      "Epoch 5/20  Iteration 875/3560 Training loss: 1.7902 1.2397 sec/batch\n",
      "Epoch 5/20  Iteration 876/3560 Training loss: 1.7900 1.2431 sec/batch\n",
      "Epoch 5/20  Iteration 877/3560 Training loss: 1.7898 1.2503 sec/batch\n",
      "Epoch 5/20  Iteration 878/3560 Training loss: 1.7895 1.2362 sec/batch\n",
      "Epoch 5/20  Iteration 879/3560 Training loss: 1.7895 1.3185 sec/batch\n",
      "Epoch 5/20  Iteration 880/3560 Training loss: 1.7896 1.3039 sec/batch\n",
      "Epoch 5/20  Iteration 881/3560 Training loss: 1.7894 1.2539 sec/batch\n",
      "Epoch 5/20  Iteration 882/3560 Training loss: 1.7891 1.2348 sec/batch\n",
      "Epoch 5/20  Iteration 883/3560 Training loss: 1.7888 1.2354 sec/batch\n",
      "Epoch 5/20  Iteration 884/3560 Training loss: 1.7884 1.2423 sec/batch\n",
      "Epoch 5/20  Iteration 885/3560 Training loss: 1.7883 1.2343 sec/batch\n",
      "Epoch 5/20  Iteration 886/3560 Training loss: 1.7881 1.2349 sec/batch\n",
      "Epoch 5/20  Iteration 887/3560 Training loss: 1.7880 1.2362 sec/batch\n",
      "Epoch 5/20  Iteration 888/3560 Training loss: 1.7877 1.2989 sec/batch\n",
      "Epoch 5/20  Iteration 889/3560 Training loss: 1.7873 1.3132 sec/batch\n",
      "Epoch 5/20  Iteration 890/3560 Training loss: 1.7872 1.2605 sec/batch\n",
      "Epoch 6/20  Iteration 891/3560 Training loss: 1.8321 1.2382 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 892/3560 Training loss: 1.7875 1.2632 sec/batch\n",
      "Epoch 6/20  Iteration 893/3560 Training loss: 1.7742 1.2492 sec/batch\n",
      "Epoch 6/20  Iteration 894/3560 Training loss: 1.7664 1.2429 sec/batch\n",
      "Epoch 6/20  Iteration 895/3560 Training loss: 1.7606 1.2572 sec/batch\n",
      "Epoch 6/20  Iteration 896/3560 Training loss: 1.7505 1.2741 sec/batch\n",
      "Epoch 6/20  Iteration 897/3560 Training loss: 1.7502 1.2766 sec/batch\n",
      "Epoch 6/20  Iteration 898/3560 Training loss: 1.7466 1.3340 sec/batch\n",
      "Epoch 6/20  Iteration 899/3560 Training loss: 1.7477 1.3053 sec/batch\n",
      "Epoch 6/20  Iteration 900/3560 Training loss: 1.7461 1.2445 sec/batch\n",
      "Epoch 6/20  Iteration 901/3560 Training loss: 1.7432 1.2586 sec/batch\n",
      "Epoch 6/20  Iteration 902/3560 Training loss: 1.7401 1.2518 sec/batch\n",
      "Epoch 6/20  Iteration 903/3560 Training loss: 1.7402 1.2698 sec/batch\n",
      "Epoch 6/20  Iteration 904/3560 Training loss: 1.7424 1.2920 sec/batch\n",
      "Epoch 6/20  Iteration 905/3560 Training loss: 1.7413 1.2445 sec/batch\n",
      "Epoch 6/20  Iteration 906/3560 Training loss: 1.7393 1.2288 sec/batch\n",
      "Epoch 6/20  Iteration 907/3560 Training loss: 1.7393 1.3017 sec/batch\n",
      "Epoch 6/20  Iteration 908/3560 Training loss: 1.7412 1.3347 sec/batch\n",
      "Epoch 6/20  Iteration 909/3560 Training loss: 1.7416 1.2555 sec/batch\n",
      "Epoch 6/20  Iteration 910/3560 Training loss: 1.7423 1.2656 sec/batch\n",
      "Epoch 6/20  Iteration 911/3560 Training loss: 1.7417 1.2495 sec/batch\n",
      "Epoch 6/20  Iteration 912/3560 Training loss: 1.7421 1.2987 sec/batch\n",
      "Epoch 6/20  Iteration 913/3560 Training loss: 1.7414 1.2694 sec/batch\n",
      "Epoch 6/20  Iteration 914/3560 Training loss: 1.7410 1.2498 sec/batch\n",
      "Epoch 6/20  Iteration 915/3560 Training loss: 1.7407 1.2514 sec/batch\n",
      "Epoch 6/20  Iteration 916/3560 Training loss: 1.7390 1.2553 sec/batch\n",
      "Epoch 6/20  Iteration 917/3560 Training loss: 1.7375 1.3685 sec/batch\n",
      "Epoch 6/20  Iteration 918/3560 Training loss: 1.7380 1.2689 sec/batch\n",
      "Epoch 6/20  Iteration 919/3560 Training loss: 1.7388 1.2509 sec/batch\n",
      "Epoch 6/20  Iteration 920/3560 Training loss: 1.7393 1.2981 sec/batch\n",
      "Epoch 6/20  Iteration 921/3560 Training loss: 1.7389 1.2618 sec/batch\n",
      "Epoch 6/20  Iteration 922/3560 Training loss: 1.7380 1.2371 sec/batch\n",
      "Epoch 6/20  Iteration 923/3560 Training loss: 1.7381 1.2581 sec/batch\n",
      "Epoch 6/20  Iteration 924/3560 Training loss: 1.7387 1.2260 sec/batch\n",
      "Epoch 6/20  Iteration 925/3560 Training loss: 1.7385 1.2817 sec/batch\n",
      "Epoch 6/20  Iteration 926/3560 Training loss: 1.7379 1.3021 sec/batch\n",
      "Epoch 6/20  Iteration 927/3560 Training loss: 1.7372 1.2844 sec/batch\n",
      "Epoch 6/20  Iteration 928/3560 Training loss: 1.7358 1.3325 sec/batch\n",
      "Epoch 6/20  Iteration 929/3560 Training loss: 1.7344 1.2744 sec/batch\n",
      "Epoch 6/20  Iteration 930/3560 Training loss: 1.7335 1.2472 sec/batch\n",
      "Epoch 6/20  Iteration 931/3560 Training loss: 1.7327 1.2828 sec/batch\n",
      "Epoch 6/20  Iteration 932/3560 Training loss: 1.7329 1.2473 sec/batch\n",
      "Epoch 6/20  Iteration 933/3560 Training loss: 1.7321 1.2460 sec/batch\n",
      "Epoch 6/20  Iteration 934/3560 Training loss: 1.7312 1.2551 sec/batch\n",
      "Epoch 6/20  Iteration 935/3560 Training loss: 1.7312 1.3094 sec/batch\n",
      "Epoch 6/20  Iteration 936/3560 Training loss: 1.7300 1.2936 sec/batch\n",
      "Epoch 6/20  Iteration 937/3560 Training loss: 1.7295 1.2314 sec/batch\n",
      "Epoch 6/20  Iteration 938/3560 Training loss: 1.7289 1.2562 sec/batch\n",
      "Epoch 6/20  Iteration 939/3560 Training loss: 1.7286 1.2471 sec/batch\n",
      "Epoch 6/20  Iteration 940/3560 Training loss: 1.7292 1.2824 sec/batch\n",
      "Epoch 6/20  Iteration 941/3560 Training loss: 1.7285 1.3649 sec/batch\n",
      "Epoch 6/20  Iteration 942/3560 Training loss: 1.7292 1.2675 sec/batch\n",
      "Epoch 6/20  Iteration 943/3560 Training loss: 1.7290 1.2614 sec/batch\n",
      "Epoch 6/20  Iteration 944/3560 Training loss: 1.7288 1.2971 sec/batch\n",
      "Epoch 6/20  Iteration 945/3560 Training loss: 1.7283 1.3364 sec/batch\n",
      "Epoch 6/20  Iteration 946/3560 Training loss: 1.7284 1.2985 sec/batch\n",
      "Epoch 6/20  Iteration 947/3560 Training loss: 1.7287 1.2692 sec/batch\n",
      "Epoch 6/20  Iteration 948/3560 Training loss: 1.7282 1.2585 sec/batch\n",
      "Epoch 6/20  Iteration 949/3560 Training loss: 1.7274 1.2488 sec/batch\n",
      "Epoch 6/20  Iteration 950/3560 Training loss: 1.7279 1.3660 sec/batch\n",
      "Epoch 6/20  Iteration 951/3560 Training loss: 1.7277 1.2654 sec/batch\n",
      "Epoch 6/20  Iteration 952/3560 Training loss: 1.7283 1.2629 sec/batch\n",
      "Epoch 6/20  Iteration 953/3560 Training loss: 1.7285 1.2301 sec/batch\n",
      "Epoch 6/20  Iteration 954/3560 Training loss: 1.7287 1.2311 sec/batch\n",
      "Epoch 6/20  Iteration 955/3560 Training loss: 1.7283 1.2348 sec/batch\n",
      "Epoch 6/20  Iteration 956/3560 Training loss: 1.7284 1.2375 sec/batch\n",
      "Epoch 6/20  Iteration 957/3560 Training loss: 1.7283 1.2320 sec/batch\n",
      "Epoch 6/20  Iteration 958/3560 Training loss: 1.7279 1.2504 sec/batch\n",
      "Epoch 6/20  Iteration 959/3560 Training loss: 1.7278 1.4370 sec/batch\n",
      "Epoch 6/20  Iteration 960/3560 Training loss: 1.7274 1.5579 sec/batch\n",
      "Epoch 6/20  Iteration 961/3560 Training loss: 1.7279 1.2448 sec/batch\n",
      "Epoch 6/20  Iteration 962/3560 Training loss: 1.7280 1.2577 sec/batch\n",
      "Epoch 6/20  Iteration 963/3560 Training loss: 1.7282 1.2460 sec/batch\n",
      "Epoch 6/20  Iteration 964/3560 Training loss: 1.7277 1.2606 sec/batch\n",
      "Epoch 6/20  Iteration 965/3560 Training loss: 1.7275 1.2764 sec/batch\n",
      "Epoch 6/20  Iteration 966/3560 Training loss: 1.7276 1.2576 sec/batch\n",
      "Epoch 6/20  Iteration 967/3560 Training loss: 1.7274 1.2584 sec/batch\n",
      "Epoch 6/20  Iteration 968/3560 Training loss: 1.7273 1.3299 sec/batch\n",
      "Epoch 6/20  Iteration 969/3560 Training loss: 1.7265 1.3216 sec/batch\n",
      "Epoch 6/20  Iteration 970/3560 Training loss: 1.7263 1.2445 sec/batch\n",
      "Epoch 6/20  Iteration 971/3560 Training loss: 1.7256 1.2547 sec/batch\n",
      "Epoch 6/20  Iteration 972/3560 Training loss: 1.7256 1.2566 sec/batch\n",
      "Epoch 6/20  Iteration 973/3560 Training loss: 1.7249 1.2436 sec/batch\n",
      "Epoch 6/20  Iteration 974/3560 Training loss: 1.7248 1.2466 sec/batch\n",
      "Epoch 6/20  Iteration 975/3560 Training loss: 1.7241 1.2411 sec/batch\n",
      "Epoch 6/20  Iteration 976/3560 Training loss: 1.7236 1.3182 sec/batch\n",
      "Epoch 6/20  Iteration 977/3560 Training loss: 1.7232 1.2520 sec/batch\n",
      "Epoch 6/20  Iteration 978/3560 Training loss: 1.7228 1.3389 sec/batch\n",
      "Epoch 6/20  Iteration 979/3560 Training loss: 1.7220 1.2859 sec/batch\n",
      "Epoch 6/20  Iteration 980/3560 Training loss: 1.7219 1.2503 sec/batch\n",
      "Epoch 6/20  Iteration 981/3560 Training loss: 1.7215 1.2550 sec/batch\n",
      "Epoch 6/20  Iteration 982/3560 Training loss: 1.7212 1.2552 sec/batch\n",
      "Epoch 6/20  Iteration 983/3560 Training loss: 1.7206 1.2538 sec/batch\n",
      "Epoch 6/20  Iteration 984/3560 Training loss: 1.7201 1.2754 sec/batch\n",
      "Epoch 6/20  Iteration 985/3560 Training loss: 1.7196 1.2555 sec/batch\n",
      "Epoch 6/20  Iteration 986/3560 Training loss: 1.7194 1.2427 sec/batch\n",
      "Epoch 6/20  Iteration 987/3560 Training loss: 1.7191 1.3275 sec/batch\n",
      "Epoch 6/20  Iteration 988/3560 Training loss: 1.7185 1.3079 sec/batch\n",
      "Epoch 6/20  Iteration 989/3560 Training loss: 1.7180 1.2619 sec/batch\n",
      "Epoch 6/20  Iteration 990/3560 Training loss: 1.7173 1.2661 sec/batch\n",
      "Epoch 6/20  Iteration 991/3560 Training loss: 1.7172 1.2513 sec/batch\n",
      "Epoch 6/20  Iteration 992/3560 Training loss: 1.7169 1.2961 sec/batch\n",
      "Epoch 6/20  Iteration 993/3560 Training loss: 1.7165 1.2426 sec/batch\n",
      "Epoch 6/20  Iteration 994/3560 Training loss: 1.7162 1.2475 sec/batch\n",
      "Epoch 6/20  Iteration 995/3560 Training loss: 1.7159 1.2557 sec/batch\n",
      "Epoch 6/20  Iteration 996/3560 Training loss: 1.7156 1.2367 sec/batch\n",
      "Epoch 6/20  Iteration 997/3560 Training loss: 1.7154 1.3383 sec/batch\n",
      "Epoch 6/20  Iteration 998/3560 Training loss: 1.7152 1.2485 sec/batch\n",
      "Epoch 6/20  Iteration 999/3560 Training loss: 1.7150 1.2363 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3560 Training loss: 1.7149 1.2773 sec/batch\n",
      "Epoch 6/20  Iteration 1001/3560 Training loss: 1.7147 1.2283 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3560 Training loss: 1.7145 1.2363 sec/batch\n",
      "Epoch 6/20  Iteration 1003/3560 Training loss: 1.7142 1.2458 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3560 Training loss: 1.7139 1.2381 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3560 Training loss: 1.7134 1.3157 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3560 Training loss: 1.7128 1.5345 sec/batch\n",
      "Epoch 6/20  Iteration 1007/3560 Training loss: 1.7126 1.5325 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3560 Training loss: 1.7123 1.6768 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 1009/3560 Training loss: 1.7121 1.6035 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3560 Training loss: 1.7119 1.3352 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3560 Training loss: 1.7117 1.2359 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3560 Training loss: 1.7112 1.2868 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3560 Training loss: 1.7107 1.3263 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3560 Training loss: 1.7104 1.3754 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3560 Training loss: 1.7103 1.4255 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3560 Training loss: 1.7097 1.3401 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3560 Training loss: 1.7097 1.2530 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3560 Training loss: 1.7095 1.2769 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3560 Training loss: 1.7092 1.3321 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3560 Training loss: 1.7090 1.2459 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3560 Training loss: 1.7084 1.2640 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3560 Training loss: 1.7080 1.2694 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3560 Training loss: 1.7079 1.2961 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3560 Training loss: 1.7077 1.3876 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3560 Training loss: 1.7076 1.2584 sec/batch\n",
      "Epoch 6/20  Iteration 1026/3560 Training loss: 1.7075 1.2977 sec/batch\n",
      "Epoch 6/20  Iteration 1027/3560 Training loss: 1.7074 1.3683 sec/batch\n",
      "Epoch 6/20  Iteration 1028/3560 Training loss: 1.7073 1.4670 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3560 Training loss: 1.7072 1.3540 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3560 Training loss: 1.7069 1.3371 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3560 Training loss: 1.7070 1.2746 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3560 Training loss: 1.7069 1.2832 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3560 Training loss: 1.7066 1.3581 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3560 Training loss: 1.7065 1.2294 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3560 Training loss: 1.7063 1.2284 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3560 Training loss: 1.7063 1.2606 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3560 Training loss: 1.7061 1.2563 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3560 Training loss: 1.7062 1.4120 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3560 Training loss: 1.7061 1.2534 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3560 Training loss: 1.7059 1.2823 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3560 Training loss: 1.7054 1.2487 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3560 Training loss: 1.7054 1.3329 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3560 Training loss: 1.7054 1.2852 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3560 Training loss: 1.7053 1.2557 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3560 Training loss: 1.7052 1.2321 sec/batch\n",
      "Epoch 6/20  Iteration 1046/3560 Training loss: 1.7050 1.2696 sec/batch\n",
      "Epoch 6/20  Iteration 1047/3560 Training loss: 1.7049 1.2457 sec/batch\n",
      "Epoch 6/20  Iteration 1048/3560 Training loss: 1.7047 1.2409 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3560 Training loss: 1.7043 1.2305 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3560 Training loss: 1.7043 1.2404 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3560 Training loss: 1.7044 1.2265 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3560 Training loss: 1.7042 1.3491 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3560 Training loss: 1.7041 1.2254 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3560 Training loss: 1.7040 1.2864 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3560 Training loss: 1.7039 1.2351 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3560 Training loss: 1.7037 1.2437 sec/batch\n",
      "Epoch 6/20  Iteration 1057/3560 Training loss: 1.7037 1.2271 sec/batch\n",
      "Epoch 6/20  Iteration 1058/3560 Training loss: 1.7039 1.2492 sec/batch\n",
      "Epoch 6/20  Iteration 1059/3560 Training loss: 1.7037 1.2192 sec/batch\n",
      "Epoch 6/20  Iteration 1060/3560 Training loss: 1.7036 1.2513 sec/batch\n",
      "Epoch 6/20  Iteration 1061/3560 Training loss: 1.7033 1.2477 sec/batch\n",
      "Epoch 6/20  Iteration 1062/3560 Training loss: 1.7030 1.3573 sec/batch\n",
      "Epoch 6/20  Iteration 1063/3560 Training loss: 1.7030 1.2570 sec/batch\n",
      "Epoch 6/20  Iteration 1064/3560 Training loss: 1.7029 1.2502 sec/batch\n",
      "Epoch 6/20  Iteration 1065/3560 Training loss: 1.7028 1.2509 sec/batch\n",
      "Epoch 6/20  Iteration 1066/3560 Training loss: 1.7026 1.2474 sec/batch\n",
      "Epoch 6/20  Iteration 1067/3560 Training loss: 1.7023 1.2337 sec/batch\n",
      "Epoch 6/20  Iteration 1068/3560 Training loss: 1.7022 1.2848 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3560 Training loss: 1.7660 1.2347 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3560 Training loss: 1.7183 1.3003 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3560 Training loss: 1.7040 1.3678 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3560 Training loss: 1.6955 1.2712 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3560 Training loss: 1.6883 1.2472 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3560 Training loss: 1.6778 1.2387 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3560 Training loss: 1.6763 1.2675 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3560 Training loss: 1.6736 1.2465 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3560 Training loss: 1.6742 1.2335 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3560 Training loss: 1.6727 1.2788 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3560 Training loss: 1.6690 1.2307 sec/batch\n",
      "Epoch 7/20  Iteration 1080/3560 Training loss: 1.6672 1.2802 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3560 Training loss: 1.6670 1.3395 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3560 Training loss: 1.6700 1.2520 sec/batch\n",
      "Epoch 7/20  Iteration 1083/3560 Training loss: 1.6688 1.2446 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3560 Training loss: 1.6671 1.2831 sec/batch\n",
      "Epoch 7/20  Iteration 1085/3560 Training loss: 1.6669 1.2625 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3560 Training loss: 1.6685 1.2993 sec/batch\n",
      "Epoch 7/20  Iteration 1087/3560 Training loss: 1.6690 1.2633 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3560 Training loss: 1.6699 1.2682 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3560 Training loss: 1.6690 1.2505 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3560 Training loss: 1.6697 1.3449 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3560 Training loss: 1.6689 1.2433 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3560 Training loss: 1.6685 1.2594 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3560 Training loss: 1.6685 1.2517 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3560 Training loss: 1.6668 1.2703 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3560 Training loss: 1.6652 1.2425 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3560 Training loss: 1.6653 1.2489 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3560 Training loss: 1.6661 1.2326 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3560 Training loss: 1.6665 1.2495 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3560 Training loss: 1.6662 1.2945 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3560 Training loss: 1.6650 1.2921 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3560 Training loss: 1.6652 1.2337 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3560 Training loss: 1.6656 1.2761 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3560 Training loss: 1.6652 1.2472 sec/batch\n",
      "Epoch 7/20  Iteration 1104/3560 Training loss: 1.6646 1.2403 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3560 Training loss: 1.6637 1.2391 sec/batch\n",
      "Epoch 7/20  Iteration 1106/3560 Training loss: 1.6624 1.2542 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3560 Training loss: 1.6609 1.2419 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3560 Training loss: 1.6601 1.2498 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3560 Training loss: 1.6594 1.3615 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3560 Training loss: 1.6598 1.3137 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3560 Training loss: 1.6591 1.2554 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3560 Training loss: 1.6583 1.2653 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3560 Training loss: 1.6584 1.2358 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3560 Training loss: 1.6572 1.2417 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3560 Training loss: 1.6567 1.2354 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3560 Training loss: 1.6562 1.2409 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3560 Training loss: 1.6558 1.2595 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3560 Training loss: 1.6562 1.3309 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3560 Training loss: 1.6556 1.3552 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3560 Training loss: 1.6562 1.2491 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3560 Training loss: 1.6562 1.3496 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3560 Training loss: 1.6562 1.4402 sec/batch\n",
      "Epoch 7/20  Iteration 1123/3560 Training loss: 1.6559 1.5065 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3560 Training loss: 1.6560 1.3061 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1125/3560 Training loss: 1.6563 1.3096 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3560 Training loss: 1.6558 1.2947 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3560 Training loss: 1.6552 1.3372 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3560 Training loss: 1.6555 1.4848 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3560 Training loss: 1.6554 1.7368 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3560 Training loss: 1.6561 1.8934 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3560 Training loss: 1.6563 2.0184 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3560 Training loss: 1.6565 1.6882 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3560 Training loss: 1.6561 2.1176 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3560 Training loss: 1.6561 1.9282 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3560 Training loss: 1.6561 1.6199 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3560 Training loss: 1.6556 1.4625 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3560 Training loss: 1.6556 1.6281 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3560 Training loss: 1.6552 1.4270 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3560 Training loss: 1.6557 1.2834 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3560 Training loss: 1.6558 1.2445 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3560 Training loss: 1.6560 1.2806 sec/batch\n",
      "Epoch 7/20  Iteration 1142/3560 Training loss: 1.6555 1.2493 sec/batch\n",
      "Epoch 7/20  Iteration 1143/3560 Training loss: 1.6552 1.2928 sec/batch\n",
      "Epoch 7/20  Iteration 1144/3560 Training loss: 1.6552 1.4664 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3560 Training loss: 1.6550 1.5442 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3560 Training loss: 1.6549 1.2862 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3560 Training loss: 1.6542 1.2526 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3560 Training loss: 1.6540 1.2744 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3560 Training loss: 1.6533 1.2468 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3560 Training loss: 1.6533 1.2632 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3560 Training loss: 1.6526 1.2601 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3560 Training loss: 1.6526 1.3163 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3560 Training loss: 1.6521 1.4103 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3560 Training loss: 1.6516 1.2842 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3560 Training loss: 1.6513 1.2596 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3560 Training loss: 1.6509 1.2469 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3560 Training loss: 1.6503 1.2369 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3560 Training loss: 1.6503 1.2576 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3560 Training loss: 1.6499 1.2702 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3560 Training loss: 1.6496 1.2497 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3560 Training loss: 1.6490 1.3335 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3560 Training loss: 1.6486 1.3483 sec/batch\n",
      "Epoch 7/20  Iteration 1163/3560 Training loss: 1.6482 1.2504 sec/batch\n",
      "Epoch 7/20  Iteration 1164/3560 Training loss: 1.6481 1.2812 sec/batch\n",
      "Epoch 7/20  Iteration 1165/3560 Training loss: 1.6479 1.2331 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3560 Training loss: 1.6473 1.2645 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3560 Training loss: 1.6468 1.2624 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3560 Training loss: 1.6462 1.2429 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3560 Training loss: 1.6460 1.2944 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3560 Training loss: 1.6458 1.2665 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3560 Training loss: 1.6454 1.3124 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3560 Training loss: 1.6450 1.3021 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3560 Training loss: 1.6447 1.2367 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3560 Training loss: 1.6445 1.2681 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3560 Training loss: 1.6442 1.2828 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3560 Training loss: 1.6440 1.2695 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3560 Training loss: 1.6437 1.2876 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3560 Training loss: 1.6436 1.2586 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3560 Training loss: 1.6434 1.2655 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3560 Training loss: 1.6432 1.2406 sec/batch\n",
      "Epoch 7/20  Iteration 1181/3560 Training loss: 1.6429 1.3479 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3560 Training loss: 1.6426 1.2572 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3560 Training loss: 1.6422 1.3086 sec/batch\n",
      "Epoch 7/20  Iteration 1184/3560 Training loss: 1.6416 1.2647 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3560 Training loss: 1.6415 1.2890 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3560 Training loss: 1.6414 1.2679 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3560 Training loss: 1.6413 1.2577 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3560 Training loss: 1.6411 1.2589 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3560 Training loss: 1.6409 1.2565 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3560 Training loss: 1.6405 1.3353 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3560 Training loss: 1.6400 1.2883 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3560 Training loss: 1.6400 1.2459 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3560 Training loss: 1.6399 1.2599 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3560 Training loss: 1.6394 1.2536 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3560 Training loss: 1.6394 1.2373 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3560 Training loss: 1.6394 1.2559 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3560 Training loss: 1.6392 1.2415 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3560 Training loss: 1.6388 1.2364 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3560 Training loss: 1.6384 1.2788 sec/batch\n",
      "Epoch 7/20  Iteration 1200/3560 Training loss: 1.6380 1.3779 sec/batch\n",
      "Epoch 7/20  Iteration 1201/3560 Training loss: 1.6380 1.2853 sec/batch\n",
      "Epoch 7/20  Iteration 1202/3560 Training loss: 1.6379 1.2912 sec/batch\n",
      "Epoch 7/20  Iteration 1203/3560 Training loss: 1.6378 1.2489 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3560 Training loss: 1.6377 1.2468 sec/batch\n",
      "Epoch 7/20  Iteration 1205/3560 Training loss: 1.6377 1.2555 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3560 Training loss: 1.6376 1.2494 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3560 Training loss: 1.6376 1.2691 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3560 Training loss: 1.6375 1.2667 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3560 Training loss: 1.6377 1.4128 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3560 Training loss: 1.6375 1.2773 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3560 Training loss: 1.6374 1.3100 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3560 Training loss: 1.6374 1.2610 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3560 Training loss: 1.6372 1.3501 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3560 Training loss: 1.6372 1.2716 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3560 Training loss: 1.6371 1.2526 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3560 Training loss: 1.6372 1.2585 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3560 Training loss: 1.6372 1.2809 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3560 Training loss: 1.6369 1.2961 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3560 Training loss: 1.6365 1.3257 sec/batch\n",
      "Epoch 7/20  Iteration 1220/3560 Training loss: 1.6364 1.2354 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3560 Training loss: 1.6363 1.2465 sec/batch\n",
      "Epoch 7/20  Iteration 1222/3560 Training loss: 1.6363 1.3561 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3560 Training loss: 1.6362 1.2817 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3560 Training loss: 1.6360 1.3197 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3560 Training loss: 1.6359 1.2992 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3560 Training loss: 1.6358 1.2772 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3560 Training loss: 1.6354 1.2761 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3560 Training loss: 1.6354 1.3145 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3560 Training loss: 1.6355 1.2536 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3560 Training loss: 1.6354 1.2701 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3560 Training loss: 1.6353 1.3125 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3560 Training loss: 1.6352 1.2878 sec/batch\n",
      "Epoch 7/20  Iteration 1233/3560 Training loss: 1.6351 1.2580 sec/batch\n",
      "Epoch 7/20  Iteration 1234/3560 Training loss: 1.6349 1.2360 sec/batch\n",
      "Epoch 7/20  Iteration 1235/3560 Training loss: 1.6350 1.2407 sec/batch\n",
      "Epoch 7/20  Iteration 1236/3560 Training loss: 1.6354 1.2527 sec/batch\n",
      "Epoch 7/20  Iteration 1237/3560 Training loss: 1.6352 1.3502 sec/batch\n",
      "Epoch 7/20  Iteration 1238/3560 Training loss: 1.6351 1.3053 sec/batch\n",
      "Epoch 7/20  Iteration 1239/3560 Training loss: 1.6349 1.2612 sec/batch\n",
      "Epoch 7/20  Iteration 1240/3560 Training loss: 1.6347 1.2939 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1241/3560 Training loss: 1.6347 1.2870 sec/batch\n",
      "Epoch 7/20  Iteration 1242/3560 Training loss: 1.6346 1.2527 sec/batch\n",
      "Epoch 7/20  Iteration 1243/3560 Training loss: 1.6346 1.2379 sec/batch\n",
      "Epoch 7/20  Iteration 1244/3560 Training loss: 1.6344 1.2412 sec/batch\n",
      "Epoch 7/20  Iteration 1245/3560 Training loss: 1.6342 1.2527 sec/batch\n",
      "Epoch 7/20  Iteration 1246/3560 Training loss: 1.6341 1.3153 sec/batch\n",
      "Epoch 8/20  Iteration 1247/3560 Training loss: 1.7046 1.2883 sec/batch\n",
      "Epoch 8/20  Iteration 1248/3560 Training loss: 1.6589 1.3096 sec/batch\n",
      "Epoch 8/20  Iteration 1249/3560 Training loss: 1.6454 1.2450 sec/batch\n",
      "Epoch 8/20  Iteration 1250/3560 Training loss: 1.6383 1.2683 sec/batch\n",
      "Epoch 8/20  Iteration 1251/3560 Training loss: 1.6296 1.2394 sec/batch\n",
      "Epoch 8/20  Iteration 1252/3560 Training loss: 1.6174 1.2591 sec/batch\n",
      "Epoch 8/20  Iteration 1253/3560 Training loss: 1.6174 1.2411 sec/batch\n",
      "Epoch 8/20  Iteration 1254/3560 Training loss: 1.6145 1.2767 sec/batch\n",
      "Epoch 8/20  Iteration 1255/3560 Training loss: 1.6145 1.2971 sec/batch\n",
      "Epoch 8/20  Iteration 1256/3560 Training loss: 1.6129 1.4457 sec/batch\n",
      "Epoch 8/20  Iteration 1257/3560 Training loss: 1.6098 1.2659 sec/batch\n",
      "Epoch 8/20  Iteration 1258/3560 Training loss: 1.6077 1.2654 sec/batch\n",
      "Epoch 8/20  Iteration 1259/3560 Training loss: 1.6076 1.2437 sec/batch\n",
      "Epoch 8/20  Iteration 1260/3560 Training loss: 1.6097 1.2630 sec/batch\n",
      "Epoch 8/20  Iteration 1261/3560 Training loss: 1.6087 1.2775 sec/batch\n",
      "Epoch 8/20  Iteration 1262/3560 Training loss: 1.6065 1.2455 sec/batch\n",
      "Epoch 8/20  Iteration 1263/3560 Training loss: 1.6068 1.2467 sec/batch\n",
      "Epoch 8/20  Iteration 1264/3560 Training loss: 1.6081 1.3085 sec/batch\n",
      "Epoch 8/20  Iteration 1265/3560 Training loss: 1.6079 1.3218 sec/batch\n",
      "Epoch 8/20  Iteration 1266/3560 Training loss: 1.6086 1.2734 sec/batch\n",
      "Epoch 8/20  Iteration 1267/3560 Training loss: 1.6073 1.2445 sec/batch\n",
      "Epoch 8/20  Iteration 1268/3560 Training loss: 1.6075 1.2627 sec/batch\n",
      "Epoch 8/20  Iteration 1269/3560 Training loss: 1.6065 1.2595 sec/batch\n",
      "Epoch 8/20  Iteration 1270/3560 Training loss: 1.6061 1.2341 sec/batch\n",
      "Epoch 8/20  Iteration 1271/3560 Training loss: 1.6062 1.2520 sec/batch\n",
      "Epoch 8/20  Iteration 1272/3560 Training loss: 1.6045 1.2854 sec/batch\n",
      "Epoch 8/20  Iteration 1273/3560 Training loss: 1.6028 1.2549 sec/batch\n",
      "Epoch 8/20  Iteration 1274/3560 Training loss: 1.6027 1.2638 sec/batch\n",
      "Epoch 8/20  Iteration 1275/3560 Training loss: 1.6031 1.3584 sec/batch\n",
      "Epoch 8/20  Iteration 1276/3560 Training loss: 1.6037 1.2609 sec/batch\n",
      "Epoch 8/20  Iteration 1277/3560 Training loss: 1.6032 1.2902 sec/batch\n",
      "Epoch 8/20  Iteration 1278/3560 Training loss: 1.6023 1.3126 sec/batch\n",
      "Epoch 8/20  Iteration 1279/3560 Training loss: 1.6022 1.3818 sec/batch\n",
      "Epoch 8/20  Iteration 1280/3560 Training loss: 1.6027 1.3521 sec/batch\n",
      "Epoch 8/20  Iteration 1281/3560 Training loss: 1.6023 1.2962 sec/batch\n",
      "Epoch 8/20  Iteration 1282/3560 Training loss: 1.6016 1.4396 sec/batch\n",
      "Epoch 8/20  Iteration 1283/3560 Training loss: 1.6009 1.6147 sec/batch\n",
      "Epoch 8/20  Iteration 1284/3560 Training loss: 1.5997 2.3406 sec/batch\n",
      "Epoch 8/20  Iteration 1285/3560 Training loss: 1.5986 2.0806 sec/batch\n",
      "Epoch 8/20  Iteration 1286/3560 Training loss: 1.5981 1.9720 sec/batch\n",
      "Epoch 8/20  Iteration 1287/3560 Training loss: 1.5973 1.9774 sec/batch\n",
      "Epoch 8/20  Iteration 1288/3560 Training loss: 1.5979 1.7477 sec/batch\n",
      "Epoch 8/20  Iteration 1289/3560 Training loss: 1.5973 1.6950 sec/batch\n",
      "Epoch 8/20  Iteration 1290/3560 Training loss: 1.5965 1.8151 sec/batch\n",
      "Epoch 8/20  Iteration 1291/3560 Training loss: 1.5968 1.7775 sec/batch\n",
      "Epoch 8/20  Iteration 1292/3560 Training loss: 1.5958 1.5724 sec/batch\n",
      "Epoch 8/20  Iteration 1293/3560 Training loss: 1.5954 1.4231 sec/batch\n",
      "Epoch 8/20  Iteration 1294/3560 Training loss: 1.5949 1.4262 sec/batch\n",
      "Epoch 8/20  Iteration 1295/3560 Training loss: 1.5946 1.3964 sec/batch\n",
      "Epoch 8/20  Iteration 1296/3560 Training loss: 1.5951 1.4091 sec/batch\n",
      "Epoch 8/20  Iteration 1297/3560 Training loss: 1.5946 1.3548 sec/batch\n",
      "Epoch 8/20  Iteration 1298/3560 Training loss: 1.5953 1.4606 sec/batch\n",
      "Epoch 8/20  Iteration 1299/3560 Training loss: 1.5952 1.3671 sec/batch\n",
      "Epoch 8/20  Iteration 1300/3560 Training loss: 1.5953 1.5838 sec/batch\n",
      "Epoch 8/20  Iteration 1301/3560 Training loss: 1.5951 1.4278 sec/batch\n",
      "Epoch 8/20  Iteration 1302/3560 Training loss: 1.5951 1.3674 sec/batch\n",
      "Epoch 8/20  Iteration 1303/3560 Training loss: 1.5956 1.3623 sec/batch\n",
      "Epoch 8/20  Iteration 1304/3560 Training loss: 1.5952 1.3639 sec/batch\n",
      "Epoch 8/20  Iteration 1305/3560 Training loss: 1.5946 1.3963 sec/batch\n",
      "Epoch 8/20  Iteration 1306/3560 Training loss: 1.5950 1.4031 sec/batch\n",
      "Epoch 8/20  Iteration 1307/3560 Training loss: 1.5949 1.3645 sec/batch\n",
      "Epoch 8/20  Iteration 1308/3560 Training loss: 1.5958 1.3207 sec/batch\n",
      "Epoch 8/20  Iteration 1309/3560 Training loss: 1.5961 1.3277 sec/batch\n",
      "Epoch 8/20  Iteration 1310/3560 Training loss: 1.5963 1.4222 sec/batch\n",
      "Epoch 8/20  Iteration 1311/3560 Training loss: 1.5961 1.4187 sec/batch\n",
      "Epoch 8/20  Iteration 1312/3560 Training loss: 1.5961 1.3348 sec/batch\n",
      "Epoch 8/20  Iteration 1313/3560 Training loss: 1.5963 1.4043 sec/batch\n",
      "Epoch 8/20  Iteration 1314/3560 Training loss: 1.5958 1.3290 sec/batch\n",
      "Epoch 8/20  Iteration 1315/3560 Training loss: 1.5955 1.4186 sec/batch\n",
      "Epoch 8/20  Iteration 1316/3560 Training loss: 1.5952 1.3208 sec/batch\n",
      "Epoch 8/20  Iteration 1317/3560 Training loss: 1.5957 1.3846 sec/batch\n",
      "Epoch 8/20  Iteration 1318/3560 Training loss: 1.5957 1.3145 sec/batch\n",
      "Epoch 8/20  Iteration 1319/3560 Training loss: 1.5961 1.3454 sec/batch\n",
      "Epoch 8/20  Iteration 1320/3560 Training loss: 1.5958 1.3925 sec/batch\n",
      "Epoch 8/20  Iteration 1321/3560 Training loss: 1.5955 1.3428 sec/batch\n",
      "Epoch 8/20  Iteration 1322/3560 Training loss: 1.5956 1.3033 sec/batch\n",
      "Epoch 8/20  Iteration 1323/3560 Training loss: 1.5956 1.3207 sec/batch\n",
      "Epoch 8/20  Iteration 1324/3560 Training loss: 1.5956 1.3635 sec/batch\n",
      "Epoch 8/20  Iteration 1325/3560 Training loss: 1.5950 1.3782 sec/batch\n",
      "Epoch 8/20  Iteration 1326/3560 Training loss: 1.5949 1.3750 sec/batch\n",
      "Epoch 8/20  Iteration 1327/3560 Training loss: 1.5944 1.3832 sec/batch\n",
      "Epoch 8/20  Iteration 1328/3560 Training loss: 1.5943 1.3818 sec/batch\n",
      "Epoch 8/20  Iteration 1329/3560 Training loss: 1.5936 1.3310 sec/batch\n",
      "Epoch 8/20  Iteration 1330/3560 Training loss: 1.5935 1.3372 sec/batch\n",
      "Epoch 8/20  Iteration 1331/3560 Training loss: 1.5930 1.3117 sec/batch\n",
      "Epoch 8/20  Iteration 1332/3560 Training loss: 1.5927 1.3239 sec/batch\n",
      "Epoch 8/20  Iteration 1333/3560 Training loss: 1.5923 1.4857 sec/batch\n",
      "Epoch 8/20  Iteration 1334/3560 Training loss: 1.5919 1.3325 sec/batch\n",
      "Epoch 8/20  Iteration 1335/3560 Training loss: 1.5914 1.4042 sec/batch\n",
      "Epoch 8/20  Iteration 1336/3560 Training loss: 1.5913 1.3257 sec/batch\n",
      "Epoch 8/20  Iteration 1337/3560 Training loss: 1.5911 1.3324 sec/batch\n",
      "Epoch 8/20  Iteration 1338/3560 Training loss: 1.5909 1.3595 sec/batch\n",
      "Epoch 8/20  Iteration 1339/3560 Training loss: 1.5903 1.3405 sec/batch\n",
      "Epoch 8/20  Iteration 1340/3560 Training loss: 1.5899 1.3485 sec/batch\n",
      "Epoch 8/20  Iteration 1341/3560 Training loss: 1.5895 1.3765 sec/batch\n",
      "Epoch 8/20  Iteration 1342/3560 Training loss: 1.5895 1.4837 sec/batch\n",
      "Epoch 8/20  Iteration 1343/3560 Training loss: 1.5893 1.4078 sec/batch\n",
      "Epoch 8/20  Iteration 1344/3560 Training loss: 1.5888 1.3568 sec/batch\n",
      "Epoch 8/20  Iteration 1345/3560 Training loss: 1.5885 1.3456 sec/batch\n",
      "Epoch 8/20  Iteration 1346/3560 Training loss: 1.5879 1.3746 sec/batch\n",
      "Epoch 8/20  Iteration 1347/3560 Training loss: 1.5877 1.3678 sec/batch\n",
      "Epoch 8/20  Iteration 1348/3560 Training loss: 1.5874 1.3689 sec/batch\n",
      "Epoch 8/20  Iteration 1349/3560 Training loss: 1.5871 1.3754 sec/batch\n",
      "Epoch 8/20  Iteration 1350/3560 Training loss: 1.5868 1.4631 sec/batch\n",
      "Epoch 8/20  Iteration 1351/3560 Training loss: 1.5866 1.3892 sec/batch\n",
      "Epoch 8/20  Iteration 1352/3560 Training loss: 1.5863 1.3411 sec/batch\n",
      "Epoch 8/20  Iteration 1353/3560 Training loss: 1.5862 1.3661 sec/batch\n",
      "Epoch 8/20  Iteration 1354/3560 Training loss: 1.5860 1.3348 sec/batch\n",
      "Epoch 8/20  Iteration 1355/3560 Training loss: 1.5858 1.6509 sec/batch\n",
      "Epoch 8/20  Iteration 1356/3560 Training loss: 1.5857 1.8837 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1357/3560 Training loss: 1.5855 1.9160 sec/batch\n",
      "Epoch 8/20  Iteration 1358/3560 Training loss: 1.5853 1.8705 sec/batch\n",
      "Epoch 8/20  Iteration 1359/3560 Training loss: 1.5851 1.6266 sec/batch\n",
      "Epoch 8/20  Iteration 1360/3560 Training loss: 1.5848 1.5630 sec/batch\n",
      "Epoch 8/20  Iteration 1361/3560 Training loss: 1.5845 1.6164 sec/batch\n",
      "Epoch 8/20  Iteration 1362/3560 Training loss: 1.5840 1.5788 sec/batch\n",
      "Epoch 8/20  Iteration 1363/3560 Training loss: 1.5839 1.6171 sec/batch\n",
      "Epoch 8/20  Iteration 1364/3560 Training loss: 1.5837 1.5434 sec/batch\n",
      "Epoch 8/20  Iteration 1365/3560 Training loss: 1.5835 1.6216 sec/batch\n",
      "Epoch 8/20  Iteration 1366/3560 Training loss: 1.5833 1.6306 sec/batch\n",
      "Epoch 8/20  Iteration 1367/3560 Training loss: 1.5831 1.5642 sec/batch\n",
      "Epoch 8/20  Iteration 1368/3560 Training loss: 1.5828 1.5433 sec/batch\n",
      "Epoch 8/20  Iteration 1369/3560 Training loss: 1.5823 1.5815 sec/batch\n",
      "Epoch 8/20  Iteration 1370/3560 Training loss: 1.5823 1.5094 sec/batch\n",
      "Epoch 8/20  Iteration 1371/3560 Training loss: 1.5822 1.5237 sec/batch\n",
      "Epoch 8/20  Iteration 1372/3560 Training loss: 1.5817 1.5886 sec/batch\n",
      "Epoch 8/20  Iteration 1373/3560 Training loss: 1.5817 1.7010 sec/batch\n",
      "Epoch 8/20  Iteration 1374/3560 Training loss: 1.5817 1.5678 sec/batch\n",
      "Epoch 8/20  Iteration 1375/3560 Training loss: 1.5815 1.6101 sec/batch\n",
      "Epoch 8/20  Iteration 1376/3560 Training loss: 1.5812 1.6556 sec/batch\n",
      "Epoch 8/20  Iteration 1377/3560 Training loss: 1.5807 1.8296 sec/batch\n",
      "Epoch 8/20  Iteration 1378/3560 Training loss: 1.5803 1.4912 sec/batch\n",
      "Epoch 8/20  Iteration 1379/3560 Training loss: 1.5803 1.6148 sec/batch\n",
      "Epoch 8/20  Iteration 1380/3560 Training loss: 1.5803 1.5666 sec/batch\n",
      "Epoch 8/20  Iteration 1381/3560 Training loss: 1.5802 1.5877 sec/batch\n",
      "Epoch 8/20  Iteration 1382/3560 Training loss: 1.5801 1.5496 sec/batch\n",
      "Epoch 8/20  Iteration 1383/3560 Training loss: 1.5801 1.4198 sec/batch\n",
      "Epoch 8/20  Iteration 1384/3560 Training loss: 1.5801 1.3996 sec/batch\n",
      "Epoch 8/20  Iteration 1385/3560 Training loss: 1.5801 1.4200 sec/batch\n",
      "Epoch 8/20  Iteration 1386/3560 Training loss: 1.5799 1.4181 sec/batch\n",
      "Epoch 8/20  Iteration 1387/3560 Training loss: 1.5802 1.3888 sec/batch\n",
      "Epoch 8/20  Iteration 1388/3560 Training loss: 1.5801 1.3907 sec/batch\n",
      "Epoch 8/20  Iteration 1389/3560 Training loss: 1.5799 1.5490 sec/batch\n",
      "Epoch 8/20  Iteration 1390/3560 Training loss: 1.5799 1.3790 sec/batch\n",
      "Epoch 8/20  Iteration 1391/3560 Training loss: 1.5797 1.3683 sec/batch\n",
      "Epoch 8/20  Iteration 1392/3560 Training loss: 1.5797 1.8103 sec/batch\n",
      "Epoch 8/20  Iteration 1393/3560 Training loss: 1.5797 1.5991 sec/batch\n",
      "Epoch 8/20  Iteration 1394/3560 Training loss: 1.5799 1.9519 sec/batch\n",
      "Epoch 8/20  Iteration 1395/3560 Training loss: 1.5799 1.7261 sec/batch\n",
      "Epoch 8/20  Iteration 1396/3560 Training loss: 1.5797 1.7297 sec/batch\n",
      "Epoch 8/20  Iteration 1397/3560 Training loss: 1.5792 1.7759 sec/batch\n",
      "Epoch 8/20  Iteration 1398/3560 Training loss: 1.5791 1.6603 sec/batch\n",
      "Epoch 8/20  Iteration 1399/3560 Training loss: 1.5790 1.7160 sec/batch\n",
      "Epoch 8/20  Iteration 1400/3560 Training loss: 1.5790 1.7329 sec/batch\n",
      "Epoch 8/20  Iteration 1401/3560 Training loss: 1.5788 1.7742 sec/batch\n",
      "Epoch 8/20  Iteration 1402/3560 Training loss: 1.5787 1.7515 sec/batch\n",
      "Epoch 8/20  Iteration 1403/3560 Training loss: 1.5787 2.0873 sec/batch\n",
      "Epoch 8/20  Iteration 1404/3560 Training loss: 1.5786 2.0677 sec/batch\n",
      "Epoch 8/20  Iteration 1405/3560 Training loss: 1.5782 1.7678 sec/batch\n",
      "Epoch 8/20  Iteration 1406/3560 Training loss: 1.5782 1.7301 sec/batch\n",
      "Epoch 8/20  Iteration 1407/3560 Training loss: 1.5783 1.8043 sec/batch\n",
      "Epoch 8/20  Iteration 1408/3560 Training loss: 1.5783 1.7109 sec/batch\n",
      "Epoch 8/20  Iteration 1409/3560 Training loss: 1.5783 1.8230 sec/batch\n",
      "Epoch 8/20  Iteration 1410/3560 Training loss: 1.5782 1.8005 sec/batch\n",
      "Epoch 8/20  Iteration 1411/3560 Training loss: 1.5781 1.7224 sec/batch\n",
      "Epoch 8/20  Iteration 1412/3560 Training loss: 1.5780 1.8755 sec/batch\n",
      "Epoch 8/20  Iteration 1413/3560 Training loss: 1.5781 1.7173 sec/batch\n",
      "Epoch 8/20  Iteration 1414/3560 Training loss: 1.5784 1.7427 sec/batch\n",
      "Epoch 8/20  Iteration 1415/3560 Training loss: 1.5783 1.7168 sec/batch\n",
      "Epoch 8/20  Iteration 1416/3560 Training loss: 1.5782 1.8885 sec/batch\n",
      "Epoch 8/20  Iteration 1417/3560 Training loss: 1.5781 1.7928 sec/batch\n",
      "Epoch 8/20  Iteration 1418/3560 Training loss: 1.5778 1.7963 sec/batch\n",
      "Epoch 8/20  Iteration 1419/3560 Training loss: 1.5778 1.6975 sec/batch\n",
      "Epoch 8/20  Iteration 1420/3560 Training loss: 1.5778 1.7145 sec/batch\n",
      "Epoch 8/20  Iteration 1421/3560 Training loss: 1.5777 1.7154 sec/batch\n",
      "Epoch 8/20  Iteration 1422/3560 Training loss: 1.5775 1.7535 sec/batch\n",
      "Epoch 8/20  Iteration 1423/3560 Training loss: 1.5773 1.8482 sec/batch\n",
      "Epoch 8/20  Iteration 1424/3560 Training loss: 1.5774 1.7263 sec/batch\n",
      "Epoch 9/20  Iteration 1425/3560 Training loss: 1.6512 1.6787 sec/batch\n",
      "Epoch 9/20  Iteration 1426/3560 Training loss: 1.6074 1.6909 sec/batch\n",
      "Epoch 9/20  Iteration 1427/3560 Training loss: 1.5920 1.7436 sec/batch\n",
      "Epoch 9/20  Iteration 1428/3560 Training loss: 1.5843 1.8272 sec/batch\n",
      "Epoch 9/20  Iteration 1429/3560 Training loss: 1.5755 1.8381 sec/batch\n",
      "Epoch 9/20  Iteration 1430/3560 Training loss: 1.5638 1.6540 sec/batch\n",
      "Epoch 9/20  Iteration 1431/3560 Training loss: 1.5634 1.8728 sec/batch\n",
      "Epoch 9/20  Iteration 1432/3560 Training loss: 1.5610 1.6276 sec/batch\n",
      "Epoch 9/20  Iteration 1433/3560 Training loss: 1.5601 1.4257 sec/batch\n",
      "Epoch 9/20  Iteration 1434/3560 Training loss: 1.5595 1.7789 sec/batch\n",
      "Epoch 9/20  Iteration 1435/3560 Training loss: 1.5555 1.8481 sec/batch\n",
      "Epoch 9/20  Iteration 1436/3560 Training loss: 1.5549 1.7741 sec/batch\n",
      "Epoch 9/20  Iteration 1437/3560 Training loss: 1.5551 1.8335 sec/batch\n",
      "Epoch 9/20  Iteration 1438/3560 Training loss: 1.5567 1.5558 sec/batch\n",
      "Epoch 9/20  Iteration 1439/3560 Training loss: 1.5566 1.5397 sec/batch\n",
      "Epoch 9/20  Iteration 1440/3560 Training loss: 1.5549 1.5651 sec/batch\n",
      "Epoch 9/20  Iteration 1441/3560 Training loss: 1.5545 1.6358 sec/batch\n",
      "Epoch 9/20  Iteration 1442/3560 Training loss: 1.5558 1.5948 sec/batch\n",
      "Epoch 9/20  Iteration 1443/3560 Training loss: 1.5560 1.5190 sec/batch\n",
      "Epoch 9/20  Iteration 1444/3560 Training loss: 1.5569 1.6436 sec/batch\n",
      "Epoch 9/20  Iteration 1445/3560 Training loss: 1.5563 1.6631 sec/batch\n",
      "Epoch 9/20  Iteration 1446/3560 Training loss: 1.5565 1.5529 sec/batch\n",
      "Epoch 9/20  Iteration 1447/3560 Training loss: 1.5553 1.5389 sec/batch\n",
      "Epoch 9/20  Iteration 1448/3560 Training loss: 1.5548 1.5946 sec/batch\n",
      "Epoch 9/20  Iteration 1449/3560 Training loss: 1.5550 1.5462 sec/batch\n",
      "Epoch 9/20  Iteration 1450/3560 Training loss: 1.5533 1.5871 sec/batch\n",
      "Epoch 9/20  Iteration 1451/3560 Training loss: 1.5517 1.5769 sec/batch\n",
      "Epoch 9/20  Iteration 1452/3560 Training loss: 1.5520 1.6880 sec/batch\n",
      "Epoch 9/20  Iteration 1453/3560 Training loss: 1.5520 1.5463 sec/batch\n",
      "Epoch 9/20  Iteration 1454/3560 Training loss: 1.5520 1.6281 sec/batch\n",
      "Epoch 9/20  Iteration 1455/3560 Training loss: 1.5516 1.5931 sec/batch\n",
      "Epoch 9/20  Iteration 1456/3560 Training loss: 1.5508 1.4898 sec/batch\n",
      "Epoch 9/20  Iteration 1457/3560 Training loss: 1.5508 1.5894 sec/batch\n",
      "Epoch 9/20  Iteration 1458/3560 Training loss: 1.5512 1.5923 sec/batch\n",
      "Epoch 9/20  Iteration 1459/3560 Training loss: 1.5509 1.7296 sec/batch\n",
      "Epoch 9/20  Iteration 1460/3560 Training loss: 1.5505 1.6138 sec/batch\n",
      "Epoch 9/20  Iteration 1461/3560 Training loss: 1.5496 1.6147 sec/batch\n",
      "Epoch 9/20  Iteration 1462/3560 Training loss: 1.5482 1.5843 sec/batch\n",
      "Epoch 9/20  Iteration 1463/3560 Training loss: 1.5469 1.6896 sec/batch\n",
      "Epoch 9/20  Iteration 1464/3560 Training loss: 1.5464 1.6496 sec/batch\n",
      "Epoch 9/20  Iteration 1465/3560 Training loss: 1.5457 1.6286 sec/batch\n",
      "Epoch 9/20  Iteration 1466/3560 Training loss: 1.5464 1.6579 sec/batch\n",
      "Epoch 9/20  Iteration 1467/3560 Training loss: 1.5457 1.7929 sec/batch\n",
      "Epoch 9/20  Iteration 1468/3560 Training loss: 1.5448 1.6581 sec/batch\n",
      "Epoch 9/20  Iteration 1469/3560 Training loss: 1.5450 1.6466 sec/batch\n",
      "Epoch 9/20  Iteration 1470/3560 Training loss: 1.5439 1.6166 sec/batch\n",
      "Epoch 9/20  Iteration 1471/3560 Training loss: 1.5435 1.5593 sec/batch\n",
      "Epoch 9/20  Iteration 1472/3560 Training loss: 1.5431 1.5947 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1473/3560 Training loss: 1.5428 1.6279 sec/batch\n",
      "Epoch 9/20  Iteration 1474/3560 Training loss: 1.5432 1.6491 sec/batch\n",
      "Epoch 9/20  Iteration 1475/3560 Training loss: 1.5426 1.5877 sec/batch\n",
      "Epoch 9/20  Iteration 1476/3560 Training loss: 1.5434 1.5512 sec/batch\n",
      "Epoch 9/20  Iteration 1477/3560 Training loss: 1.5433 1.5678 sec/batch\n",
      "Epoch 9/20  Iteration 1478/3560 Training loss: 1.5435 1.6361 sec/batch\n",
      "Epoch 9/20  Iteration 1479/3560 Training loss: 1.5431 1.8135 sec/batch\n",
      "Epoch 9/20  Iteration 1480/3560 Training loss: 1.5433 1.5514 sec/batch\n",
      "Epoch 9/20  Iteration 1481/3560 Training loss: 1.5436 1.5349 sec/batch\n",
      "Epoch 9/20  Iteration 1482/3560 Training loss: 1.5432 1.5692 sec/batch\n",
      "Epoch 9/20  Iteration 1483/3560 Training loss: 1.5426 1.7870 sec/batch\n",
      "Epoch 9/20  Iteration 1484/3560 Training loss: 1.5430 1.8602 sec/batch\n",
      "Epoch 9/20  Iteration 1485/3560 Training loss: 1.5431 1.5323 sec/batch\n",
      "Epoch 9/20  Iteration 1486/3560 Training loss: 1.5440 1.6911 sec/batch\n",
      "Epoch 9/20  Iteration 1487/3560 Training loss: 1.5442 1.5420 sec/batch\n",
      "Epoch 9/20  Iteration 1488/3560 Training loss: 1.5443 1.4605 sec/batch\n",
      "Epoch 9/20  Iteration 1489/3560 Training loss: 1.5441 1.4264 sec/batch\n",
      "Epoch 9/20  Iteration 1490/3560 Training loss: 1.5444 1.4498 sec/batch\n",
      "Epoch 9/20  Iteration 1491/3560 Training loss: 1.5445 1.4024 sec/batch\n",
      "Epoch 9/20  Iteration 1492/3560 Training loss: 1.5441 1.5360 sec/batch\n",
      "Epoch 9/20  Iteration 1493/3560 Training loss: 1.5442 1.4662 sec/batch\n",
      "Epoch 9/20  Iteration 1494/3560 Training loss: 1.5439 1.3927 sec/batch\n",
      "Epoch 9/20  Iteration 1495/3560 Training loss: 1.5444 1.3788 sec/batch\n",
      "Epoch 9/20  Iteration 1496/3560 Training loss: 1.5446 1.3499 sec/batch\n",
      "Epoch 9/20  Iteration 1497/3560 Training loss: 1.5450 1.3325 sec/batch\n",
      "Epoch 9/20  Iteration 1498/3560 Training loss: 1.5447 1.4109 sec/batch\n",
      "Epoch 9/20  Iteration 1499/3560 Training loss: 1.5445 1.3808 sec/batch\n",
      "Epoch 9/20  Iteration 1500/3560 Training loss: 1.5447 1.3865 sec/batch\n",
      "Epoch 9/20  Iteration 1501/3560 Training loss: 1.5445 1.4789 sec/batch\n",
      "Epoch 9/20  Iteration 1502/3560 Training loss: 1.5444 1.3690 sec/batch\n",
      "Epoch 9/20  Iteration 1503/3560 Training loss: 1.5438 1.3541 sec/batch\n",
      "Epoch 9/20  Iteration 1504/3560 Training loss: 1.5436 1.3623 sec/batch\n",
      "Epoch 9/20  Iteration 1505/3560 Training loss: 1.5431 1.4038 sec/batch\n",
      "Epoch 9/20  Iteration 1506/3560 Training loss: 1.5429 1.3754 sec/batch\n",
      "Epoch 9/20  Iteration 1507/3560 Training loss: 1.5423 1.4187 sec/batch\n",
      "Epoch 9/20  Iteration 1508/3560 Training loss: 1.5423 1.3700 sec/batch\n",
      "Epoch 9/20  Iteration 1509/3560 Training loss: 1.5419 1.4109 sec/batch\n",
      "Epoch 9/20  Iteration 1510/3560 Training loss: 1.5416 1.3861 sec/batch\n",
      "Epoch 9/20  Iteration 1511/3560 Training loss: 1.5412 1.3671 sec/batch\n",
      "Epoch 9/20  Iteration 1512/3560 Training loss: 1.5408 1.4060 sec/batch\n",
      "Epoch 9/20  Iteration 1513/3560 Training loss: 1.5402 1.3647 sec/batch\n",
      "Epoch 9/20  Iteration 1514/3560 Training loss: 1.5403 1.4140 sec/batch\n",
      "Epoch 9/20  Iteration 1515/3560 Training loss: 1.5400 1.8668 sec/batch\n",
      "Epoch 9/20  Iteration 1516/3560 Training loss: 1.5397 1.5288 sec/batch\n",
      "Epoch 9/20  Iteration 1517/3560 Training loss: 1.5391 1.4113 sec/batch\n",
      "Epoch 9/20  Iteration 1518/3560 Training loss: 1.5387 1.4228 sec/batch\n",
      "Epoch 9/20  Iteration 1519/3560 Training loss: 1.5384 1.4103 sec/batch\n",
      "Epoch 9/20  Iteration 1520/3560 Training loss: 1.5384 1.4122 sec/batch\n",
      "Epoch 9/20  Iteration 1521/3560 Training loss: 1.5382 1.4377 sec/batch\n",
      "Epoch 9/20  Iteration 1522/3560 Training loss: 1.5377 1.3727 sec/batch\n",
      "Epoch 9/20  Iteration 1523/3560 Training loss: 1.5373 1.4950 sec/batch\n",
      "Epoch 9/20  Iteration 1524/3560 Training loss: 1.5368 1.3904 sec/batch\n",
      "Epoch 9/20  Iteration 1525/3560 Training loss: 1.5365 1.3838 sec/batch\n",
      "Epoch 9/20  Iteration 1526/3560 Training loss: 1.5363 1.4262 sec/batch\n",
      "Epoch 9/20  Iteration 1527/3560 Training loss: 1.5360 1.3535 sec/batch\n",
      "Epoch 9/20  Iteration 1528/3560 Training loss: 1.5358 1.4316 sec/batch\n",
      "Epoch 9/20  Iteration 1529/3560 Training loss: 1.5356 1.3844 sec/batch\n",
      "Epoch 9/20  Iteration 1530/3560 Training loss: 1.5354 1.3789 sec/batch\n",
      "Epoch 9/20  Iteration 1531/3560 Training loss: 1.5353 1.4282 sec/batch\n",
      "Epoch 9/20  Iteration 1532/3560 Training loss: 1.5351 1.4464 sec/batch\n",
      "Epoch 9/20  Iteration 1533/3560 Training loss: 1.5350 1.7149 sec/batch\n",
      "Epoch 9/20  Iteration 1534/3560 Training loss: 1.5350 1.7753 sec/batch\n",
      "Epoch 9/20  Iteration 1535/3560 Training loss: 1.5349 1.9870 sec/batch\n",
      "Epoch 9/20  Iteration 1536/3560 Training loss: 1.5348 1.6646 sec/batch\n",
      "Epoch 9/20  Iteration 1537/3560 Training loss: 1.5347 1.6028 sec/batch\n",
      "Epoch 9/20  Iteration 1538/3560 Training loss: 1.5345 1.5393 sec/batch\n",
      "Epoch 9/20  Iteration 1539/3560 Training loss: 1.5341 1.6943 sec/batch\n",
      "Epoch 9/20  Iteration 1540/3560 Training loss: 1.5337 1.5686 sec/batch\n",
      "Epoch 9/20  Iteration 1541/3560 Training loss: 1.5336 1.5924 sec/batch\n",
      "Epoch 9/20  Iteration 1542/3560 Training loss: 1.5335 1.5664 sec/batch\n",
      "Epoch 9/20  Iteration 1543/3560 Training loss: 1.5333 1.5403 sec/batch\n",
      "Epoch 9/20  Iteration 1544/3560 Training loss: 1.5332 1.6222 sec/batch\n",
      "Epoch 9/20  Iteration 1545/3560 Training loss: 1.5331 1.5657 sec/batch\n",
      "Epoch 9/20  Iteration 1546/3560 Training loss: 1.5327 1.6765 sec/batch\n",
      "Epoch 9/20  Iteration 1547/3560 Training loss: 1.5323 1.5742 sec/batch\n",
      "Epoch 9/20  Iteration 1548/3560 Training loss: 1.5323 1.7264 sec/batch\n",
      "Epoch 9/20  Iteration 1549/3560 Training loss: 1.5322 1.5819 sec/batch\n",
      "Epoch 9/20  Iteration 1550/3560 Training loss: 1.5317 1.5369 sec/batch\n",
      "Epoch 9/20  Iteration 1551/3560 Training loss: 1.5317 1.5488 sec/batch\n",
      "Epoch 9/20  Iteration 1552/3560 Training loss: 1.5317 1.5297 sec/batch\n",
      "Epoch 9/20  Iteration 1553/3560 Training loss: 1.5315 1.5987 sec/batch\n",
      "Epoch 9/20  Iteration 1554/3560 Training loss: 1.5312 1.6589 sec/batch\n",
      "Epoch 9/20  Iteration 1555/3560 Training loss: 1.5308 1.5242 sec/batch\n",
      "Epoch 9/20  Iteration 1556/3560 Training loss: 1.5304 1.5151 sec/batch\n",
      "Epoch 9/20  Iteration 1557/3560 Training loss: 1.5305 1.5141 sec/batch\n",
      "Epoch 9/20  Iteration 1558/3560 Training loss: 1.5305 1.5449 sec/batch\n",
      "Epoch 9/20  Iteration 1559/3560 Training loss: 1.5304 1.5234 sec/batch\n",
      "Epoch 9/20  Iteration 1560/3560 Training loss: 1.5304 1.5493 sec/batch\n",
      "Epoch 9/20  Iteration 1561/3560 Training loss: 1.5303 1.6062 sec/batch\n",
      "Epoch 9/20  Iteration 1562/3560 Training loss: 1.5303 1.5346 sec/batch\n",
      "Epoch 9/20  Iteration 1563/3560 Training loss: 1.5303 1.5355 sec/batch\n",
      "Epoch 9/20  Iteration 1564/3560 Training loss: 1.5302 1.5480 sec/batch\n",
      "Epoch 9/20  Iteration 1565/3560 Training loss: 1.5304 1.5563 sec/batch\n",
      "Epoch 9/20  Iteration 1566/3560 Training loss: 1.5303 1.6174 sec/batch\n",
      "Epoch 9/20  Iteration 1567/3560 Training loss: 1.5302 1.5489 sec/batch\n",
      "Epoch 9/20  Iteration 1568/3560 Training loss: 1.5302 1.5845 sec/batch\n",
      "Epoch 9/20  Iteration 1569/3560 Training loss: 1.5301 1.6738 sec/batch\n",
      "Epoch 9/20  Iteration 1570/3560 Training loss: 1.5301 1.5509 sec/batch\n",
      "Epoch 9/20  Iteration 1571/3560 Training loss: 1.5300 1.5409 sec/batch\n",
      "Epoch 9/20  Iteration 1572/3560 Training loss: 1.5301 1.6114 sec/batch\n",
      "Epoch 9/20  Iteration 1573/3560 Training loss: 1.5301 1.6311 sec/batch\n",
      "Epoch 9/20  Iteration 1574/3560 Training loss: 1.5298 1.5450 sec/batch\n",
      "Epoch 9/20  Iteration 1575/3560 Training loss: 1.5294 1.5466 sec/batch\n",
      "Epoch 9/20  Iteration 1576/3560 Training loss: 1.5293 1.5796 sec/batch\n",
      "Epoch 9/20  Iteration 1577/3560 Training loss: 1.5292 1.7200 sec/batch\n",
      "Epoch 9/20  Iteration 1578/3560 Training loss: 1.5292 1.5290 sec/batch\n",
      "Epoch 9/20  Iteration 1579/3560 Training loss: 1.5291 1.6194 sec/batch\n",
      "Epoch 9/20  Iteration 1580/3560 Training loss: 1.5290 1.5983 sec/batch\n",
      "Epoch 9/20  Iteration 1581/3560 Training loss: 1.5289 1.6137 sec/batch\n",
      "Epoch 9/20  Iteration 1582/3560 Training loss: 1.5288 1.6088 sec/batch\n",
      "Epoch 9/20  Iteration 1583/3560 Training loss: 1.5284 1.5994 sec/batch\n",
      "Epoch 9/20  Iteration 1584/3560 Training loss: 1.5284 1.7937 sec/batch\n",
      "Epoch 9/20  Iteration 1585/3560 Training loss: 1.5285 1.6898 sec/batch\n",
      "Epoch 9/20  Iteration 1586/3560 Training loss: 1.5284 1.5474 sec/batch\n",
      "Epoch 9/20  Iteration 1587/3560 Training loss: 1.5284 1.6056 sec/batch\n",
      "Epoch 9/20  Iteration 1588/3560 Training loss: 1.5283 1.5787 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1589/3560 Training loss: 1.5282 1.5615 sec/batch\n",
      "Epoch 9/20  Iteration 1590/3560 Training loss: 1.5281 1.5925 sec/batch\n",
      "Epoch 9/20  Iteration 1591/3560 Training loss: 1.5281 1.6616 sec/batch\n",
      "Epoch 9/20  Iteration 1592/3560 Training loss: 1.5285 1.6682 sec/batch\n",
      "Epoch 9/20  Iteration 1593/3560 Training loss: 1.5283 1.5841 sec/batch\n",
      "Epoch 9/20  Iteration 1594/3560 Training loss: 1.5282 1.5504 sec/batch\n",
      "Epoch 9/20  Iteration 1595/3560 Training loss: 1.5280 1.5476 sec/batch\n",
      "Epoch 9/20  Iteration 1596/3560 Training loss: 1.5277 1.6022 sec/batch\n",
      "Epoch 9/20  Iteration 1597/3560 Training loss: 1.5277 1.5740 sec/batch\n",
      "Epoch 9/20  Iteration 1598/3560 Training loss: 1.5276 1.6176 sec/batch\n",
      "Epoch 9/20  Iteration 1599/3560 Training loss: 1.5276 1.7389 sec/batch\n",
      "Epoch 9/20  Iteration 1600/3560 Training loss: 1.5273 1.5668 sec/batch\n",
      "Epoch 9/20  Iteration 1601/3560 Training loss: 1.5271 1.5843 sec/batch\n",
      "Epoch 9/20  Iteration 1602/3560 Training loss: 1.5271 1.5612 sec/batch\n",
      "Epoch 10/20  Iteration 1603/3560 Training loss: 1.6096 1.6030 sec/batch\n",
      "Epoch 10/20  Iteration 1604/3560 Training loss: 1.5624 1.5915 sec/batch\n",
      "Epoch 10/20  Iteration 1605/3560 Training loss: 1.5412 1.5588 sec/batch\n",
      "Epoch 10/20  Iteration 1606/3560 Training loss: 1.5341 1.5597 sec/batch\n",
      "Epoch 10/20  Iteration 1607/3560 Training loss: 1.5258 1.5563 sec/batch\n",
      "Epoch 10/20  Iteration 1608/3560 Training loss: 1.5136 1.4260 sec/batch\n",
      "Epoch 10/20  Iteration 1609/3560 Training loss: 1.5132 1.4463 sec/batch\n",
      "Epoch 10/20  Iteration 1610/3560 Training loss: 1.5098 1.4416 sec/batch\n",
      "Epoch 10/20  Iteration 1611/3560 Training loss: 1.5094 1.4867 sec/batch\n",
      "Epoch 10/20  Iteration 1612/3560 Training loss: 1.5076 1.4370 sec/batch\n",
      "Epoch 10/20  Iteration 1613/3560 Training loss: 1.5035 1.3946 sec/batch\n",
      "Epoch 10/20  Iteration 1614/3560 Training loss: 1.5020 1.4294 sec/batch\n",
      "Epoch 10/20  Iteration 1615/3560 Training loss: 1.5018 1.4936 sec/batch\n",
      "Epoch 10/20  Iteration 1616/3560 Training loss: 1.5034 1.3871 sec/batch\n",
      "Epoch 10/20  Iteration 1617/3560 Training loss: 1.5028 1.3875 sec/batch\n",
      "Epoch 10/20  Iteration 1618/3560 Training loss: 1.5009 1.4431 sec/batch\n",
      "Epoch 10/20  Iteration 1619/3560 Training loss: 1.5009 1.3850 sec/batch\n",
      "Epoch 10/20  Iteration 1620/3560 Training loss: 1.5016 1.3555 sec/batch\n",
      "Epoch 10/20  Iteration 1621/3560 Training loss: 1.5013 1.3804 sec/batch\n",
      "Epoch 10/20  Iteration 1622/3560 Training loss: 1.5021 1.3726 sec/batch\n",
      "Epoch 10/20  Iteration 1623/3560 Training loss: 1.5010 1.4565 sec/batch\n",
      "Epoch 10/20  Iteration 1624/3560 Training loss: 1.5013 1.4011 sec/batch\n",
      "Epoch 10/20  Iteration 1625/3560 Training loss: 1.5006 1.4486 sec/batch\n",
      "Epoch 10/20  Iteration 1626/3560 Training loss: 1.4998 1.3970 sec/batch\n",
      "Epoch 10/20  Iteration 1627/3560 Training loss: 1.4998 1.3944 sec/batch\n",
      "Epoch 10/20  Iteration 1628/3560 Training loss: 1.4983 1.3946 sec/batch\n",
      "Epoch 10/20  Iteration 1629/3560 Training loss: 1.4968 1.3644 sec/batch\n",
      "Epoch 10/20  Iteration 1630/3560 Training loss: 1.4976 1.5274 sec/batch\n",
      "Epoch 10/20  Iteration 1631/3560 Training loss: 1.4978 1.4731 sec/batch\n",
      "Epoch 10/20  Iteration 1632/3560 Training loss: 1.4982 2.0661 sec/batch\n",
      "Epoch 10/20  Iteration 1633/3560 Training loss: 1.4979 1.4448 sec/batch\n",
      "Epoch 10/20  Iteration 1634/3560 Training loss: 1.4970 1.3939 sec/batch\n",
      "Epoch 10/20  Iteration 1635/3560 Training loss: 1.4973 1.4346 sec/batch\n",
      "Epoch 10/20  Iteration 1636/3560 Training loss: 1.4974 1.3993 sec/batch\n",
      "Epoch 10/20  Iteration 1637/3560 Training loss: 1.4971 1.4283 sec/batch\n",
      "Epoch 10/20  Iteration 1638/3560 Training loss: 1.4971 1.4692 sec/batch\n",
      "Epoch 10/20  Iteration 1639/3560 Training loss: 1.4963 1.5108 sec/batch\n",
      "Epoch 10/20  Iteration 1640/3560 Training loss: 1.4949 1.4510 sec/batch\n",
      "Epoch 10/20  Iteration 1641/3560 Training loss: 1.4936 1.4035 sec/batch\n",
      "Epoch 10/20  Iteration 1642/3560 Training loss: 1.4930 1.4101 sec/batch\n",
      "Epoch 10/20  Iteration 1643/3560 Training loss: 1.4925 1.4762 sec/batch\n",
      "Epoch 10/20  Iteration 1644/3560 Training loss: 1.4931 1.4725 sec/batch\n",
      "Epoch 10/20  Iteration 1645/3560 Training loss: 1.4925 1.4101 sec/batch\n",
      "Epoch 10/20  Iteration 1646/3560 Training loss: 1.4919 1.5149 sec/batch\n",
      "Epoch 10/20  Iteration 1647/3560 Training loss: 1.4920 1.4849 sec/batch\n",
      "Epoch 10/20  Iteration 1648/3560 Training loss: 1.4911 1.4760 sec/batch\n",
      "Epoch 10/20  Iteration 1649/3560 Training loss: 1.4907 1.4316 sec/batch\n",
      "Epoch 10/20  Iteration 1650/3560 Training loss: 1.4903 1.3671 sec/batch\n",
      "Epoch 10/20  Iteration 1651/3560 Training loss: 1.4900 1.4059 sec/batch\n",
      "Epoch 10/20  Iteration 1652/3560 Training loss: 1.4904 1.4883 sec/batch\n",
      "Epoch 10/20  Iteration 1653/3560 Training loss: 1.4898 1.6642 sec/batch\n",
      "Epoch 10/20  Iteration 1654/3560 Training loss: 1.4907 1.4756 sec/batch\n",
      "Epoch 10/20  Iteration 1655/3560 Training loss: 1.4905 1.4742 sec/batch\n",
      "Epoch 10/20  Iteration 1656/3560 Training loss: 1.4908 1.4538 sec/batch\n",
      "Epoch 10/20  Iteration 1657/3560 Training loss: 1.4905 1.4449 sec/batch\n",
      "Epoch 10/20  Iteration 1658/3560 Training loss: 1.4907 1.4524 sec/batch\n",
      "Epoch 10/20  Iteration 1659/3560 Training loss: 1.4911 1.4876 sec/batch\n",
      "Epoch 10/20  Iteration 1660/3560 Training loss: 1.4907 1.4271 sec/batch\n",
      "Epoch 10/20  Iteration 1661/3560 Training loss: 1.4901 1.5385 sec/batch\n",
      "Epoch 10/20  Iteration 1662/3560 Training loss: 1.4905 1.4041 sec/batch\n",
      "Epoch 10/20  Iteration 1663/3560 Training loss: 1.4903 1.4149 sec/batch\n",
      "Epoch 10/20  Iteration 1664/3560 Training loss: 1.4912 1.4355 sec/batch\n",
      "Epoch 10/20  Iteration 1665/3560 Training loss: 1.4913 1.4245 sec/batch\n",
      "Epoch 10/20  Iteration 1666/3560 Training loss: 1.4914 1.4371 sec/batch\n",
      "Epoch 10/20  Iteration 1667/3560 Training loss: 1.4911 1.5500 sec/batch\n",
      "Epoch 10/20  Iteration 1668/3560 Training loss: 1.4913 1.5002 sec/batch\n",
      "Epoch 10/20  Iteration 1669/3560 Training loss: 1.4913 1.6553 sec/batch\n",
      "Epoch 10/20  Iteration 1670/3560 Training loss: 1.4910 1.4680 sec/batch\n",
      "Epoch 10/20  Iteration 1671/3560 Training loss: 1.4909 1.4540 sec/batch\n",
      "Epoch 10/20  Iteration 1672/3560 Training loss: 1.4907 1.5011 sec/batch\n",
      "Epoch 10/20  Iteration 1673/3560 Training loss: 1.4912 1.5610 sec/batch\n",
      "Epoch 10/20  Iteration 1674/3560 Training loss: 1.4914 1.4561 sec/batch\n",
      "Epoch 10/20  Iteration 1675/3560 Training loss: 1.4919 1.4515 sec/batch\n",
      "Epoch 10/20  Iteration 1676/3560 Training loss: 1.4916 1.5074 sec/batch\n",
      "Epoch 10/20  Iteration 1677/3560 Training loss: 1.4915 1.5656 sec/batch\n",
      "Epoch 10/20  Iteration 1678/3560 Training loss: 1.4918 1.5042 sec/batch\n",
      "Epoch 10/20  Iteration 1679/3560 Training loss: 1.4916 1.3980 sec/batch\n",
      "Epoch 10/20  Iteration 1680/3560 Training loss: 1.4915 1.5118 sec/batch\n",
      "Epoch 10/20  Iteration 1681/3560 Training loss: 1.4907 1.4736 sec/batch\n",
      "Epoch 10/20  Iteration 1682/3560 Training loss: 1.4907 1.4538 sec/batch\n",
      "Epoch 10/20  Iteration 1683/3560 Training loss: 1.4900 1.4581 sec/batch\n",
      "Epoch 10/20  Iteration 1684/3560 Training loss: 1.4900 1.4732 sec/batch\n",
      "Epoch 10/20  Iteration 1685/3560 Training loss: 1.4893 1.5704 sec/batch\n",
      "Epoch 10/20  Iteration 1686/3560 Training loss: 1.4893 1.4509 sec/batch\n",
      "Epoch 10/20  Iteration 1687/3560 Training loss: 1.4889 1.4275 sec/batch\n",
      "Epoch 10/20  Iteration 1688/3560 Training loss: 1.4886 1.3953 sec/batch\n",
      "Epoch 10/20  Iteration 1689/3560 Training loss: 1.4882 1.3920 sec/batch\n",
      "Epoch 10/20  Iteration 1690/3560 Training loss: 1.4879 1.3592 sec/batch\n",
      "Epoch 10/20  Iteration 1691/3560 Training loss: 1.4873 1.4217 sec/batch\n",
      "Epoch 10/20  Iteration 1692/3560 Training loss: 1.4875 1.5322 sec/batch\n",
      "Epoch 10/20  Iteration 1693/3560 Training loss: 1.4872 1.5700 sec/batch\n",
      "Epoch 10/20  Iteration 1694/3560 Training loss: 1.4870 1.5652 sec/batch\n",
      "Epoch 10/20  Iteration 1695/3560 Training loss: 1.4864 1.4609 sec/batch\n",
      "Epoch 10/20  Iteration 1696/3560 Training loss: 1.4861 1.4578 sec/batch\n",
      "Epoch 10/20  Iteration 1697/3560 Training loss: 1.4857 1.4150 sec/batch\n",
      "Epoch 10/20  Iteration 1698/3560 Training loss: 1.4859 1.4249 sec/batch\n",
      "Epoch 10/20  Iteration 1699/3560 Training loss: 1.4856 1.4132 sec/batch\n",
      "Epoch 10/20  Iteration 1700/3560 Training loss: 1.4852 1.4622 sec/batch\n",
      "Epoch 10/20  Iteration 1701/3560 Training loss: 1.4848 1.6353 sec/batch\n",
      "Epoch 10/20  Iteration 1702/3560 Training loss: 1.4842 1.4269 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1703/3560 Training loss: 1.4841 1.4577 sec/batch\n",
      "Epoch 10/20  Iteration 1704/3560 Training loss: 1.4839 1.4230 sec/batch\n",
      "Epoch 10/20  Iteration 1705/3560 Training loss: 1.4836 1.3991 sec/batch\n",
      "Epoch 10/20  Iteration 1706/3560 Training loss: 1.4833 1.4991 sec/batch\n",
      "Epoch 10/20  Iteration 1707/3560 Training loss: 1.4831 1.4296 sec/batch\n",
      "Epoch 10/20  Iteration 1708/3560 Training loss: 1.4829 1.4915 sec/batch\n",
      "Epoch 10/20  Iteration 1709/3560 Training loss: 1.4827 1.4718 sec/batch\n",
      "Epoch 10/20  Iteration 1710/3560 Training loss: 1.4827 1.4150 sec/batch\n",
      "Epoch 10/20  Iteration 1711/3560 Training loss: 1.4824 1.3863 sec/batch\n",
      "Epoch 10/20  Iteration 1712/3560 Training loss: 1.4824 1.4325 sec/batch\n",
      "Epoch 10/20  Iteration 1713/3560 Training loss: 1.4822 1.4084 sec/batch\n",
      "Epoch 10/20  Iteration 1714/3560 Training loss: 1.4821 1.4051 sec/batch\n",
      "Epoch 10/20  Iteration 1715/3560 Training loss: 1.4819 1.4831 sec/batch\n",
      "Epoch 10/20  Iteration 1716/3560 Training loss: 1.4816 1.4758 sec/batch\n",
      "Epoch 10/20  Iteration 1717/3560 Training loss: 1.4813 1.4012 sec/batch\n",
      "Epoch 10/20  Iteration 1718/3560 Training loss: 1.4809 1.4149 sec/batch\n",
      "Epoch 10/20  Iteration 1719/3560 Training loss: 1.4808 1.4079 sec/batch\n",
      "Epoch 10/20  Iteration 1720/3560 Training loss: 1.4807 1.4122 sec/batch\n",
      "Epoch 10/20  Iteration 1721/3560 Training loss: 1.4805 1.4329 sec/batch\n",
      "Epoch 10/20  Iteration 1722/3560 Training loss: 1.4804 1.4727 sec/batch\n",
      "Epoch 10/20  Iteration 1723/3560 Training loss: 1.4802 1.3706 sec/batch\n",
      "Epoch 10/20  Iteration 1724/3560 Training loss: 1.4799 1.6195 sec/batch\n",
      "Epoch 10/20  Iteration 1725/3560 Training loss: 1.4794 1.4704 sec/batch\n",
      "Epoch 10/20  Iteration 1726/3560 Training loss: 1.4794 1.4113 sec/batch\n",
      "Epoch 10/20  Iteration 1727/3560 Training loss: 1.4793 1.4062 sec/batch\n",
      "Epoch 10/20  Iteration 1728/3560 Training loss: 1.4788 1.3978 sec/batch\n",
      "Epoch 10/20  Iteration 1729/3560 Training loss: 1.4788 1.4854 sec/batch\n",
      "Epoch 10/20  Iteration 1730/3560 Training loss: 1.4788 1.3792 sec/batch\n",
      "Epoch 10/20  Iteration 1731/3560 Training loss: 1.4786 1.4026 sec/batch\n",
      "Epoch 10/20  Iteration 1732/3560 Training loss: 1.4781 1.4970 sec/batch\n",
      "Epoch 10/20  Iteration 1733/3560 Training loss: 1.4777 1.4303 sec/batch\n",
      "Epoch 10/20  Iteration 1734/3560 Training loss: 1.4773 1.4685 sec/batch\n",
      "Epoch 10/20  Iteration 1735/3560 Training loss: 1.4773 1.4166 sec/batch\n",
      "Epoch 10/20  Iteration 1736/3560 Training loss: 1.4772 1.4554 sec/batch\n",
      "Epoch 10/20  Iteration 1737/3560 Training loss: 1.4772 1.3883 sec/batch\n",
      "Epoch 10/20  Iteration 1738/3560 Training loss: 1.4772 1.4236 sec/batch\n",
      "Epoch 10/20  Iteration 1739/3560 Training loss: 1.4772 1.4373 sec/batch\n",
      "Epoch 10/20  Iteration 1740/3560 Training loss: 1.4773 1.4081 sec/batch\n",
      "Epoch 10/20  Iteration 1741/3560 Training loss: 1.4773 1.3963 sec/batch\n",
      "Epoch 10/20  Iteration 1742/3560 Training loss: 1.4771 1.3995 sec/batch\n",
      "Epoch 10/20  Iteration 1743/3560 Training loss: 1.4774 1.5008 sec/batch\n",
      "Epoch 10/20  Iteration 1744/3560 Training loss: 1.4774 1.4003 sec/batch\n",
      "Epoch 10/20  Iteration 1745/3560 Training loss: 1.4773 1.4132 sec/batch\n",
      "Epoch 10/20  Iteration 1746/3560 Training loss: 1.4774 1.4173 sec/batch\n",
      "Epoch 10/20  Iteration 1747/3560 Training loss: 1.4772 1.4137 sec/batch\n",
      "Epoch 10/20  Iteration 1748/3560 Training loss: 1.4773 1.4065 sec/batch\n",
      "Epoch 10/20  Iteration 1749/3560 Training loss: 1.4772 1.4436 sec/batch\n",
      "Epoch 10/20  Iteration 1750/3560 Training loss: 1.4774 1.5253 sec/batch\n",
      "Epoch 10/20  Iteration 1751/3560 Training loss: 1.4775 1.4299 sec/batch\n",
      "Epoch 10/20  Iteration 1752/3560 Training loss: 1.4773 1.4617 sec/batch\n",
      "Epoch 10/20  Iteration 1753/3560 Training loss: 1.4769 1.4867 sec/batch\n",
      "Epoch 10/20  Iteration 1754/3560 Training loss: 1.4768 1.8565 sec/batch\n",
      "Epoch 10/20  Iteration 1755/3560 Training loss: 1.4768 2.2455 sec/batch\n",
      "Epoch 10/20  Iteration 1756/3560 Training loss: 1.4769 1.4271 sec/batch\n",
      "Epoch 10/20  Iteration 1757/3560 Training loss: 1.4768 1.3572 sec/batch\n",
      "Epoch 10/20  Iteration 1758/3560 Training loss: 1.4767 1.4133 sec/batch\n",
      "Epoch 10/20  Iteration 1759/3560 Training loss: 1.4767 1.3551 sec/batch\n",
      "Epoch 10/20  Iteration 1760/3560 Training loss: 1.4765 1.3282 sec/batch\n",
      "Epoch 10/20  Iteration 1761/3560 Training loss: 1.4762 1.3799 sec/batch\n",
      "Epoch 10/20  Iteration 1762/3560 Training loss: 1.4763 1.3759 sec/batch\n",
      "Epoch 10/20  Iteration 1763/3560 Training loss: 1.4764 1.4215 sec/batch\n",
      "Epoch 10/20  Iteration 1764/3560 Training loss: 1.4763 1.3724 sec/batch\n",
      "Epoch 10/20  Iteration 1765/3560 Training loss: 1.4763 1.3315 sec/batch\n",
      "Epoch 10/20  Iteration 1766/3560 Training loss: 1.4762 1.3304 sec/batch\n",
      "Epoch 10/20  Iteration 1767/3560 Training loss: 1.4761 1.3069 sec/batch\n",
      "Epoch 10/20  Iteration 1768/3560 Training loss: 1.4760 1.3392 sec/batch\n",
      "Epoch 10/20  Iteration 1769/3560 Training loss: 1.4761 1.3430 sec/batch\n",
      "Epoch 10/20  Iteration 1770/3560 Training loss: 1.4765 1.3389 sec/batch\n",
      "Epoch 10/20  Iteration 1771/3560 Training loss: 1.4766 1.3804 sec/batch\n",
      "Epoch 10/20  Iteration 1772/3560 Training loss: 1.4765 1.3557 sec/batch\n",
      "Epoch 10/20  Iteration 1773/3560 Training loss: 1.4764 1.3305 sec/batch\n",
      "Epoch 10/20  Iteration 1774/3560 Training loss: 1.4762 1.3356 sec/batch\n",
      "Epoch 10/20  Iteration 1775/3560 Training loss: 1.4763 1.3494 sec/batch\n",
      "Epoch 10/20  Iteration 1776/3560 Training loss: 1.4762 1.3377 sec/batch\n",
      "Epoch 10/20  Iteration 1777/3560 Training loss: 1.4762 1.4140 sec/batch\n",
      "Epoch 10/20  Iteration 1778/3560 Training loss: 1.4760 1.3656 sec/batch\n",
      "Epoch 10/20  Iteration 1779/3560 Training loss: 1.4758 1.3644 sec/batch\n",
      "Epoch 10/20  Iteration 1780/3560 Training loss: 1.4759 1.3307 sec/batch\n",
      "Epoch 11/20  Iteration 1781/3560 Training loss: 1.5738 1.3589 sec/batch\n",
      "Epoch 11/20  Iteration 1782/3560 Training loss: 1.5163 1.3335 sec/batch\n",
      "Epoch 11/20  Iteration 1783/3560 Training loss: 1.4988 1.3635 sec/batch\n",
      "Epoch 11/20  Iteration 1784/3560 Training loss: 1.4898 1.3743 sec/batch\n",
      "Epoch 11/20  Iteration 1785/3560 Training loss: 1.4805 1.3243 sec/batch\n",
      "Epoch 11/20  Iteration 1786/3560 Training loss: 1.4701 1.3841 sec/batch\n",
      "Epoch 11/20  Iteration 1787/3560 Training loss: 1.4702 1.3835 sec/batch\n",
      "Epoch 11/20  Iteration 1788/3560 Training loss: 1.4665 1.3145 sec/batch\n",
      "Epoch 11/20  Iteration 1789/3560 Training loss: 1.4658 1.3247 sec/batch\n",
      "Epoch 11/20  Iteration 1790/3560 Training loss: 1.4644 1.3596 sec/batch\n",
      "Epoch 11/20  Iteration 1791/3560 Training loss: 1.4613 1.3527 sec/batch\n",
      "Epoch 11/20  Iteration 1792/3560 Training loss: 1.4602 1.3719 sec/batch\n",
      "Epoch 11/20  Iteration 1793/3560 Training loss: 1.4600 1.3987 sec/batch\n",
      "Epoch 11/20  Iteration 1794/3560 Training loss: 1.4619 1.3740 sec/batch\n",
      "Epoch 11/20  Iteration 1795/3560 Training loss: 1.4603 1.3397 sec/batch\n",
      "Epoch 11/20  Iteration 1796/3560 Training loss: 1.4584 1.3398 sec/batch\n",
      "Epoch 11/20  Iteration 1797/3560 Training loss: 1.4590 1.3502 sec/batch\n",
      "Epoch 11/20  Iteration 1798/3560 Training loss: 1.4596 1.3114 sec/batch\n",
      "Epoch 11/20  Iteration 1799/3560 Training loss: 1.4599 1.3640 sec/batch\n",
      "Epoch 11/20  Iteration 1800/3560 Training loss: 1.4611 1.3555 sec/batch\n",
      "Epoch 11/20  Iteration 1801/3560 Training loss: 1.4601 1.3937 sec/batch\n",
      "Epoch 11/20  Iteration 1802/3560 Training loss: 1.4604 1.3781 sec/batch\n",
      "Epoch 11/20  Iteration 1803/3560 Training loss: 1.4596 1.3249 sec/batch\n",
      "Epoch 11/20  Iteration 1804/3560 Training loss: 1.4594 1.3831 sec/batch\n",
      "Epoch 11/20  Iteration 1805/3560 Training loss: 1.4595 1.3644 sec/batch\n",
      "Epoch 11/20  Iteration 1806/3560 Training loss: 1.4578 1.3526 sec/batch\n",
      "Epoch 11/20  Iteration 1807/3560 Training loss: 1.4564 1.3793 sec/batch\n",
      "Epoch 11/20  Iteration 1808/3560 Training loss: 1.4568 1.3934 sec/batch\n",
      "Epoch 11/20  Iteration 1809/3560 Training loss: 1.4568 1.3371 sec/batch\n",
      "Epoch 11/20  Iteration 1810/3560 Training loss: 1.4571 1.4887 sec/batch\n",
      "Epoch 11/20  Iteration 1811/3560 Training loss: 1.4570 1.4199 sec/batch\n",
      "Epoch 11/20  Iteration 1812/3560 Training loss: 1.4561 1.3752 sec/batch\n",
      "Epoch 11/20  Iteration 1813/3560 Training loss: 1.4562 1.3317 sec/batch\n",
      "Epoch 11/20  Iteration 1814/3560 Training loss: 1.4565 1.3439 sec/batch\n",
      "Epoch 11/20  Iteration 1815/3560 Training loss: 1.4562 1.3910 sec/batch\n",
      "Epoch 11/20  Iteration 1816/3560 Training loss: 1.4559 1.3724 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1817/3560 Training loss: 1.4552 1.3162 sec/batch\n",
      "Epoch 11/20  Iteration 1818/3560 Training loss: 1.4541 1.3312 sec/batch\n",
      "Epoch 11/20  Iteration 1819/3560 Training loss: 1.4527 1.3798 sec/batch\n",
      "Epoch 11/20  Iteration 1820/3560 Training loss: 1.4521 1.3494 sec/batch\n",
      "Epoch 11/20  Iteration 1821/3560 Training loss: 1.4513 1.4127 sec/batch\n",
      "Epoch 11/20  Iteration 1822/3560 Training loss: 1.4520 1.3321 sec/batch\n",
      "Epoch 11/20  Iteration 1823/3560 Training loss: 1.4517 1.4172 sec/batch\n",
      "Epoch 11/20  Iteration 1824/3560 Training loss: 1.4509 1.3652 sec/batch\n",
      "Epoch 11/20  Iteration 1825/3560 Training loss: 1.4510 1.3231 sec/batch\n",
      "Epoch 11/20  Iteration 1826/3560 Training loss: 1.4501 1.3241 sec/batch\n",
      "Epoch 11/20  Iteration 1827/3560 Training loss: 1.4497 1.3544 sec/batch\n",
      "Epoch 11/20  Iteration 1828/3560 Training loss: 1.4492 1.3383 sec/batch\n",
      "Epoch 11/20  Iteration 1829/3560 Training loss: 1.4489 1.3801 sec/batch\n",
      "Epoch 11/20  Iteration 1830/3560 Training loss: 1.4493 1.4661 sec/batch\n",
      "Epoch 11/20  Iteration 1831/3560 Training loss: 1.4488 1.3714 sec/batch\n",
      "Epoch 11/20  Iteration 1832/3560 Training loss: 1.4495 1.3548 sec/batch\n",
      "Epoch 11/20  Iteration 1833/3560 Training loss: 1.4493 1.3497 sec/batch\n",
      "Epoch 11/20  Iteration 1834/3560 Training loss: 1.4493 1.3493 sec/batch\n",
      "Epoch 11/20  Iteration 1835/3560 Training loss: 1.4490 1.3356 sec/batch\n",
      "Epoch 11/20  Iteration 1836/3560 Training loss: 1.4491 1.3777 sec/batch\n",
      "Epoch 11/20  Iteration 1837/3560 Training loss: 1.4493 1.3395 sec/batch\n",
      "Epoch 11/20  Iteration 1838/3560 Training loss: 1.4492 1.4014 sec/batch\n",
      "Epoch 11/20  Iteration 1839/3560 Training loss: 1.4486 1.3522 sec/batch\n",
      "Epoch 11/20  Iteration 1840/3560 Training loss: 1.4491 1.3787 sec/batch\n",
      "Epoch 11/20  Iteration 1841/3560 Training loss: 1.4492 1.3575 sec/batch\n",
      "Epoch 11/20  Iteration 1842/3560 Training loss: 1.4501 1.3473 sec/batch\n",
      "Epoch 11/20  Iteration 1843/3560 Training loss: 1.4502 1.3869 sec/batch\n",
      "Epoch 11/20  Iteration 1844/3560 Training loss: 1.4505 1.3299 sec/batch\n",
      "Epoch 11/20  Iteration 1845/3560 Training loss: 1.4503 1.3676 sec/batch\n",
      "Epoch 11/20  Iteration 1846/3560 Training loss: 1.4504 1.3598 sec/batch\n",
      "Epoch 11/20  Iteration 1847/3560 Training loss: 1.4505 1.3780 sec/batch\n",
      "Epoch 11/20  Iteration 1848/3560 Training loss: 1.4502 1.3580 sec/batch\n",
      "Epoch 11/20  Iteration 1849/3560 Training loss: 1.4502 1.3664 sec/batch\n",
      "Epoch 11/20  Iteration 1850/3560 Training loss: 1.4500 1.3527 sec/batch\n",
      "Epoch 11/20  Iteration 1851/3560 Training loss: 1.4506 1.3802 sec/batch\n",
      "Epoch 11/20  Iteration 1852/3560 Training loss: 1.4508 1.4277 sec/batch\n",
      "Epoch 11/20  Iteration 1853/3560 Training loss: 1.4512 1.3804 sec/batch\n",
      "Epoch 11/20  Iteration 1854/3560 Training loss: 1.4510 1.3750 sec/batch\n",
      "Epoch 11/20  Iteration 1855/3560 Training loss: 1.4509 1.3511 sec/batch\n",
      "Epoch 11/20  Iteration 1856/3560 Training loss: 1.4511 1.3668 sec/batch\n",
      "Epoch 11/20  Iteration 1857/3560 Training loss: 1.4510 1.3824 sec/batch\n",
      "Epoch 11/20  Iteration 1858/3560 Training loss: 1.4510 1.3918 sec/batch\n",
      "Epoch 11/20  Iteration 1859/3560 Training loss: 1.4502 1.4863 sec/batch\n",
      "Epoch 11/20  Iteration 1860/3560 Training loss: 1.4502 1.7000 sec/batch\n",
      "Epoch 11/20  Iteration 1861/3560 Training loss: 1.4496 1.3993 sec/batch\n",
      "Epoch 11/20  Iteration 1862/3560 Training loss: 1.4495 1.7855 sec/batch\n",
      "Epoch 11/20  Iteration 1863/3560 Training loss: 1.4488 1.6014 sec/batch\n",
      "Epoch 11/20  Iteration 1864/3560 Training loss: 1.4488 1.5445 sec/batch\n",
      "Epoch 11/20  Iteration 1865/3560 Training loss: 1.4484 1.7407 sec/batch\n",
      "Epoch 11/20  Iteration 1866/3560 Training loss: 1.4481 1.5466 sec/batch\n",
      "Epoch 11/20  Iteration 1867/3560 Training loss: 1.4477 1.4362 sec/batch\n",
      "Epoch 11/20  Iteration 1868/3560 Training loss: 1.4473 1.4772 sec/batch\n",
      "Epoch 11/20  Iteration 1869/3560 Training loss: 1.4468 1.4592 sec/batch\n",
      "Epoch 11/20  Iteration 1870/3560 Training loss: 1.4469 1.4297 sec/batch\n",
      "Epoch 11/20  Iteration 1871/3560 Training loss: 1.4466 1.4258 sec/batch\n",
      "Epoch 11/20  Iteration 1872/3560 Training loss: 1.4463 1.4541 sec/batch\n",
      "Epoch 11/20  Iteration 1873/3560 Training loss: 1.4459 1.4626 sec/batch\n",
      "Epoch 11/20  Iteration 1874/3560 Training loss: 1.4455 1.4276 sec/batch\n",
      "Epoch 11/20  Iteration 1875/3560 Training loss: 1.4452 1.4233 sec/batch\n",
      "Epoch 11/20  Iteration 1876/3560 Training loss: 1.4453 1.4348 sec/batch\n",
      "Epoch 11/20  Iteration 1877/3560 Training loss: 1.4452 1.4705 sec/batch\n",
      "Epoch 11/20  Iteration 1878/3560 Training loss: 1.4448 1.4463 sec/batch\n",
      "Epoch 11/20  Iteration 1879/3560 Training loss: 1.4444 1.4424 sec/batch\n",
      "Epoch 11/20  Iteration 1880/3560 Training loss: 1.4439 1.4996 sec/batch\n",
      "Epoch 11/20  Iteration 1881/3560 Training loss: 1.4438 1.4443 sec/batch\n",
      "Epoch 11/20  Iteration 1882/3560 Training loss: 1.4436 1.4351 sec/batch\n",
      "Epoch 11/20  Iteration 1883/3560 Training loss: 1.4434 1.4365 sec/batch\n",
      "Epoch 11/20  Iteration 1884/3560 Training loss: 1.4433 1.4506 sec/batch\n",
      "Epoch 11/20  Iteration 1885/3560 Training loss: 1.4431 1.4701 sec/batch\n",
      "Epoch 11/20  Iteration 1886/3560 Training loss: 1.4429 1.4390 sec/batch\n",
      "Epoch 11/20  Iteration 1887/3560 Training loss: 1.4428 1.4818 sec/batch\n",
      "Epoch 11/20  Iteration 1888/3560 Training loss: 1.4428 1.7086 sec/batch\n",
      "Epoch 11/20  Iteration 1889/3560 Training loss: 1.4426 1.6172 sec/batch\n",
      "Epoch 11/20  Iteration 1890/3560 Training loss: 1.4425 1.3818 sec/batch\n",
      "Epoch 11/20  Iteration 1891/3560 Training loss: 1.4423 1.3475 sec/batch\n",
      "Epoch 11/20  Iteration 1892/3560 Training loss: 1.4421 1.3784 sec/batch\n",
      "Epoch 11/20  Iteration 1893/3560 Training loss: 1.4419 1.3627 sec/batch\n",
      "Epoch 11/20  Iteration 1894/3560 Training loss: 1.4416 1.4089 sec/batch\n",
      "Epoch 11/20  Iteration 1895/3560 Training loss: 1.4414 1.4622 sec/batch\n",
      "Epoch 11/20  Iteration 1896/3560 Training loss: 1.4410 1.4424 sec/batch\n",
      "Epoch 11/20  Iteration 1897/3560 Training loss: 1.4409 1.4272 sec/batch\n",
      "Epoch 11/20  Iteration 1898/3560 Training loss: 1.4408 1.4092 sec/batch\n",
      "Epoch 11/20  Iteration 1899/3560 Training loss: 1.4406 1.4633 sec/batch\n",
      "Epoch 11/20  Iteration 1900/3560 Training loss: 1.4405 1.4087 sec/batch\n",
      "Epoch 11/20  Iteration 1901/3560 Training loss: 1.4404 1.4574 sec/batch\n",
      "Epoch 11/20  Iteration 1902/3560 Training loss: 1.4401 1.4141 sec/batch\n",
      "Epoch 11/20  Iteration 1903/3560 Training loss: 1.4397 1.4332 sec/batch\n",
      "Epoch 11/20  Iteration 1904/3560 Training loss: 1.4396 1.4212 sec/batch\n",
      "Epoch 11/20  Iteration 1905/3560 Training loss: 1.4395 1.3852 sec/batch\n",
      "Epoch 11/20  Iteration 1906/3560 Training loss: 1.4390 1.3902 sec/batch\n",
      "Epoch 11/20  Iteration 1907/3560 Training loss: 1.4390 1.3802 sec/batch\n",
      "Epoch 11/20  Iteration 1908/3560 Training loss: 1.4390 1.4690 sec/batch\n",
      "Epoch 11/20  Iteration 1909/3560 Training loss: 1.4388 1.3563 sec/batch\n",
      "Epoch 11/20  Iteration 1910/3560 Training loss: 1.4384 1.3672 sec/batch\n",
      "Epoch 11/20  Iteration 1911/3560 Training loss: 1.4380 1.3880 sec/batch\n",
      "Epoch 11/20  Iteration 1912/3560 Training loss: 1.4377 1.3888 sec/batch\n",
      "Epoch 11/20  Iteration 1913/3560 Training loss: 1.4378 1.3927 sec/batch\n",
      "Epoch 11/20  Iteration 1914/3560 Training loss: 1.4378 1.4349 sec/batch\n",
      "Epoch 11/20  Iteration 1915/3560 Training loss: 1.4377 1.4741 sec/batch\n",
      "Epoch 11/20  Iteration 1916/3560 Training loss: 1.4378 1.4203 sec/batch\n",
      "Epoch 11/20  Iteration 1917/3560 Training loss: 1.4378 1.4241 sec/batch\n",
      "Epoch 11/20  Iteration 1918/3560 Training loss: 1.4378 1.3779 sec/batch\n",
      "Epoch 11/20  Iteration 1919/3560 Training loss: 1.4379 1.3649 sec/batch\n",
      "Epoch 11/20  Iteration 1920/3560 Training loss: 1.4378 1.3708 sec/batch\n",
      "Epoch 11/20  Iteration 1921/3560 Training loss: 1.4380 1.3823 sec/batch\n",
      "Epoch 11/20  Iteration 1922/3560 Training loss: 1.4380 1.4169 sec/batch\n",
      "Epoch 11/20  Iteration 1923/3560 Training loss: 1.4378 1.4159 sec/batch\n",
      "Epoch 11/20  Iteration 1924/3560 Training loss: 1.4380 1.3610 sec/batch\n",
      "Epoch 11/20  Iteration 1925/3560 Training loss: 1.4379 1.3572 sec/batch\n",
      "Epoch 11/20  Iteration 1926/3560 Training loss: 1.4380 1.4582 sec/batch\n",
      "Epoch 11/20  Iteration 1927/3560 Training loss: 1.4379 1.4160 sec/batch\n",
      "Epoch 11/20  Iteration 1928/3560 Training loss: 1.4382 1.4842 sec/batch\n",
      "Epoch 11/20  Iteration 1929/3560 Training loss: 1.4383 1.4060 sec/batch\n",
      "Epoch 11/20  Iteration 1930/3560 Training loss: 1.4381 1.5653 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1931/3560 Training loss: 1.4377 1.3687 sec/batch\n",
      "Epoch 11/20  Iteration 1932/3560 Training loss: 1.4375 1.3872 sec/batch\n",
      "Epoch 11/20  Iteration 1933/3560 Training loss: 1.4376 1.4694 sec/batch\n",
      "Epoch 11/20  Iteration 1934/3560 Training loss: 1.4375 1.4845 sec/batch\n",
      "Epoch 11/20  Iteration 1935/3560 Training loss: 1.4375 1.4496 sec/batch\n",
      "Epoch 11/20  Iteration 1936/3560 Training loss: 1.4374 1.4982 sec/batch\n",
      "Epoch 11/20  Iteration 1937/3560 Training loss: 1.4373 1.4514 sec/batch\n",
      "Epoch 11/20  Iteration 1938/3560 Training loss: 1.4372 1.4312 sec/batch\n",
      "Epoch 11/20  Iteration 1939/3560 Training loss: 1.4369 1.4286 sec/batch\n",
      "Epoch 11/20  Iteration 1940/3560 Training loss: 1.4370 1.3956 sec/batch\n",
      "Epoch 11/20  Iteration 1941/3560 Training loss: 1.4371 1.3813 sec/batch\n",
      "Epoch 11/20  Iteration 1942/3560 Training loss: 1.4371 1.3868 sec/batch\n",
      "Epoch 11/20  Iteration 1943/3560 Training loss: 1.4371 1.4634 sec/batch\n",
      "Epoch 11/20  Iteration 1944/3560 Training loss: 1.4371 1.5006 sec/batch\n",
      "Epoch 11/20  Iteration 1945/3560 Training loss: 1.4371 1.4240 sec/batch\n",
      "Epoch 11/20  Iteration 1946/3560 Training loss: 1.4370 1.4500 sec/batch\n",
      "Epoch 11/20  Iteration 1947/3560 Training loss: 1.4371 1.4034 sec/batch\n",
      "Epoch 11/20  Iteration 1948/3560 Training loss: 1.4375 1.4619 sec/batch\n",
      "Epoch 11/20  Iteration 1949/3560 Training loss: 1.4374 1.4194 sec/batch\n",
      "Epoch 11/20  Iteration 1950/3560 Training loss: 1.4374 1.5514 sec/batch\n",
      "Epoch 11/20  Iteration 1951/3560 Training loss: 1.4373 1.3977 sec/batch\n",
      "Epoch 11/20  Iteration 1952/3560 Training loss: 1.4371 1.3879 sec/batch\n",
      "Epoch 11/20  Iteration 1953/3560 Training loss: 1.4371 1.3720 sec/batch\n",
      "Epoch 11/20  Iteration 1954/3560 Training loss: 1.4371 1.3776 sec/batch\n",
      "Epoch 11/20  Iteration 1955/3560 Training loss: 1.4371 1.4480 sec/batch\n",
      "Epoch 11/20  Iteration 1956/3560 Training loss: 1.4369 1.3964 sec/batch\n",
      "Epoch 11/20  Iteration 1957/3560 Training loss: 1.4368 1.3955 sec/batch\n",
      "Epoch 11/20  Iteration 1958/3560 Training loss: 1.4369 1.4192 sec/batch\n",
      "Epoch 12/20  Iteration 1959/3560 Training loss: 1.5452 1.3832 sec/batch\n",
      "Epoch 12/20  Iteration 1960/3560 Training loss: 1.4900 1.3971 sec/batch\n",
      "Epoch 12/20  Iteration 1961/3560 Training loss: 1.4678 1.3701 sec/batch\n",
      "Epoch 12/20  Iteration 1962/3560 Training loss: 1.4567 1.3649 sec/batch\n",
      "Epoch 12/20  Iteration 1963/3560 Training loss: 1.4456 1.4160 sec/batch\n",
      "Epoch 12/20  Iteration 1964/3560 Training loss: 1.4346 1.3427 sec/batch\n",
      "Epoch 12/20  Iteration 1965/3560 Training loss: 1.4340 1.4362 sec/batch\n",
      "Epoch 12/20  Iteration 1966/3560 Training loss: 1.4313 1.3644 sec/batch\n",
      "Epoch 12/20  Iteration 1967/3560 Training loss: 1.4296 1.3564 sec/batch\n",
      "Epoch 12/20  Iteration 1968/3560 Training loss: 1.4294 1.3985 sec/batch\n",
      "Epoch 12/20  Iteration 1969/3560 Training loss: 1.4257 1.4238 sec/batch\n",
      "Epoch 12/20  Iteration 1970/3560 Training loss: 1.4249 1.3967 sec/batch\n",
      "Epoch 12/20  Iteration 1971/3560 Training loss: 1.4254 1.3943 sec/batch\n",
      "Epoch 12/20  Iteration 1972/3560 Training loss: 1.4267 1.4202 sec/batch\n",
      "Epoch 12/20  Iteration 1973/3560 Training loss: 1.4255 1.4076 sec/batch\n",
      "Epoch 12/20  Iteration 1974/3560 Training loss: 1.4232 1.3737 sec/batch\n",
      "Epoch 12/20  Iteration 1975/3560 Training loss: 1.4235 1.3926 sec/batch\n",
      "Epoch 12/20  Iteration 1976/3560 Training loss: 1.4246 1.3902 sec/batch\n",
      "Epoch 12/20  Iteration 1977/3560 Training loss: 1.4246 1.4195 sec/batch\n",
      "Epoch 12/20  Iteration 1978/3560 Training loss: 1.4256 1.4160 sec/batch\n",
      "Epoch 12/20  Iteration 1979/3560 Training loss: 1.4247 1.4514 sec/batch\n",
      "Epoch 12/20  Iteration 1980/3560 Training loss: 1.4251 1.3822 sec/batch\n",
      "Epoch 12/20  Iteration 1981/3560 Training loss: 1.4244 1.4857 sec/batch\n",
      "Epoch 12/20  Iteration 1982/3560 Training loss: 1.4238 1.8624 sec/batch\n",
      "Epoch 12/20  Iteration 1983/3560 Training loss: 1.4238 2.0805 sec/batch\n",
      "Epoch 12/20  Iteration 1984/3560 Training loss: 1.4220 1.8454 sec/batch\n",
      "Epoch 12/20  Iteration 1985/3560 Training loss: 1.4203 1.7939 sec/batch\n",
      "Epoch 12/20  Iteration 1986/3560 Training loss: 1.4208 1.7854 sec/batch\n",
      "Epoch 12/20  Iteration 1987/3560 Training loss: 1.4210 1.5265 sec/batch\n",
      "Epoch 12/20  Iteration 1988/3560 Training loss: 1.4212 1.5845 sec/batch\n",
      "Epoch 12/20  Iteration 1989/3560 Training loss: 1.4208 1.7420 sec/batch\n",
      "Epoch 12/20  Iteration 1990/3560 Training loss: 1.4197 1.5811 sec/batch\n",
      "Epoch 12/20  Iteration 1991/3560 Training loss: 1.4199 1.7042 sec/batch\n",
      "Epoch 12/20  Iteration 1992/3560 Training loss: 1.4201 1.5134 sec/batch\n",
      "Epoch 12/20  Iteration 1993/3560 Training loss: 1.4199 1.5844 sec/batch\n",
      "Epoch 12/20  Iteration 1994/3560 Training loss: 1.4197 1.6228 sec/batch\n",
      "Epoch 12/20  Iteration 1995/3560 Training loss: 1.4191 1.6392 sec/batch\n",
      "Epoch 12/20  Iteration 1996/3560 Training loss: 1.4179 1.6849 sec/batch\n",
      "Epoch 12/20  Iteration 1997/3560 Training loss: 1.4165 1.7404 sec/batch\n",
      "Epoch 12/20  Iteration 1998/3560 Training loss: 1.4161 1.4059 sec/batch\n",
      "Epoch 12/20  Iteration 1999/3560 Training loss: 1.4155 1.6124 sec/batch\n",
      "Epoch 12/20  Iteration 2000/3560 Training loss: 1.4161 1.4373 sec/batch\n",
      "Epoch 12/20  Iteration 2001/3560 Training loss: 1.4157 1.8995 sec/batch\n",
      "Epoch 12/20  Iteration 2002/3560 Training loss: 1.4149 1.4021 sec/batch\n",
      "Epoch 12/20  Iteration 2003/3560 Training loss: 1.4149 1.5710 sec/batch\n",
      "Epoch 12/20  Iteration 2004/3560 Training loss: 1.4140 1.9889 sec/batch\n",
      "Epoch 12/20  Iteration 2005/3560 Training loss: 1.4136 1.3990 sec/batch\n",
      "Epoch 12/20  Iteration 2006/3560 Training loss: 1.4131 1.7104 sec/batch\n",
      "Epoch 12/20  Iteration 2007/3560 Training loss: 1.4129 1.3919 sec/batch\n",
      "Epoch 12/20  Iteration 2008/3560 Training loss: 1.4133 1.6256 sec/batch\n",
      "Epoch 12/20  Iteration 2009/3560 Training loss: 1.4128 1.6861 sec/batch\n",
      "Epoch 12/20  Iteration 2010/3560 Training loss: 1.4137 1.6593 sec/batch\n",
      "Epoch 12/20  Iteration 2011/3560 Training loss: 1.4134 1.7013 sec/batch\n",
      "Epoch 12/20  Iteration 2012/3560 Training loss: 1.4137 1.7403 sec/batch\n",
      "Epoch 12/20  Iteration 2013/3560 Training loss: 1.4135 1.5147 sec/batch\n",
      "Epoch 12/20  Iteration 2014/3560 Training loss: 1.4136 1.6759 sec/batch\n",
      "Epoch 12/20  Iteration 2015/3560 Training loss: 1.4141 1.3613 sec/batch\n",
      "Epoch 12/20  Iteration 2016/3560 Training loss: 1.4139 1.9431 sec/batch\n",
      "Epoch 12/20  Iteration 2017/3560 Training loss: 1.4133 1.6473 sec/batch\n",
      "Epoch 12/20  Iteration 2018/3560 Training loss: 1.4139 1.3746 sec/batch\n",
      "Epoch 12/20  Iteration 2019/3560 Training loss: 1.4139 1.8783 sec/batch\n",
      "Epoch 12/20  Iteration 2020/3560 Training loss: 1.4149 1.3757 sec/batch\n",
      "Epoch 12/20  Iteration 2021/3560 Training loss: 1.4151 1.7064 sec/batch\n",
      "Epoch 12/20  Iteration 2022/3560 Training loss: 1.4154 1.7637 sec/batch\n",
      "Epoch 12/20  Iteration 2023/3560 Training loss: 1.4153 2.0766 sec/batch\n",
      "Epoch 12/20  Iteration 2024/3560 Training loss: 1.4156 1.5640 sec/batch\n",
      "Epoch 12/20  Iteration 2025/3560 Training loss: 1.4157 1.5530 sec/batch\n",
      "Epoch 12/20  Iteration 2026/3560 Training loss: 1.4155 1.5280 sec/batch\n",
      "Epoch 12/20  Iteration 2027/3560 Training loss: 1.4156 1.5254 sec/batch\n",
      "Epoch 12/20  Iteration 2028/3560 Training loss: 1.4154 1.5942 sec/batch\n",
      "Epoch 12/20  Iteration 2029/3560 Training loss: 1.4159 1.6591 sec/batch\n",
      "Epoch 12/20  Iteration 2030/3560 Training loss: 1.4162 1.6079 sec/batch\n",
      "Epoch 12/20  Iteration 2031/3560 Training loss: 1.4166 1.5441 sec/batch\n",
      "Epoch 12/20  Iteration 2032/3560 Training loss: 1.4163 1.6079 sec/batch\n",
      "Epoch 12/20  Iteration 2033/3560 Training loss: 1.4163 1.5077 sec/batch\n",
      "Epoch 12/20  Iteration 2034/3560 Training loss: 1.4164 1.5455 sec/batch\n",
      "Epoch 12/20  Iteration 2035/3560 Training loss: 1.4162 1.5725 sec/batch\n",
      "Epoch 12/20  Iteration 2036/3560 Training loss: 1.4163 1.4922 sec/batch\n",
      "Epoch 12/20  Iteration 2037/3560 Training loss: 1.4157 1.5336 sec/batch\n",
      "Epoch 12/20  Iteration 2038/3560 Training loss: 1.4157 1.5387 sec/batch\n",
      "Epoch 12/20  Iteration 2039/3560 Training loss: 1.4152 1.5038 sec/batch\n",
      "Epoch 12/20  Iteration 2040/3560 Training loss: 1.4151 1.5223 sec/batch\n",
      "Epoch 12/20  Iteration 2041/3560 Training loss: 1.4146 1.5398 sec/batch\n",
      "Epoch 12/20  Iteration 2042/3560 Training loss: 1.4146 1.5273 sec/batch\n",
      "Epoch 12/20  Iteration 2043/3560 Training loss: 1.4143 1.5465 sec/batch\n",
      "Epoch 12/20  Iteration 2044/3560 Training loss: 1.4141 1.5021 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 2045/3560 Training loss: 1.4138 1.5193 sec/batch\n",
      "Epoch 12/20  Iteration 2046/3560 Training loss: 1.4135 1.5193 sec/batch\n",
      "Epoch 12/20  Iteration 2047/3560 Training loss: 1.4130 1.5208 sec/batch\n",
      "Epoch 12/20  Iteration 2048/3560 Training loss: 1.4131 1.5647 sec/batch\n",
      "Epoch 12/20  Iteration 2049/3560 Training loss: 1.4130 1.4724 sec/batch\n",
      "Epoch 12/20  Iteration 2050/3560 Training loss: 1.4129 1.4868 sec/batch\n",
      "Epoch 12/20  Iteration 2051/3560 Training loss: 1.4124 1.5221 sec/batch\n",
      "Epoch 12/20  Iteration 2052/3560 Training loss: 1.4120 1.4805 sec/batch\n",
      "Epoch 12/20  Iteration 2053/3560 Training loss: 1.4117 1.4850 sec/batch\n",
      "Epoch 12/20  Iteration 2054/3560 Training loss: 1.4117 1.4895 sec/batch\n",
      "Epoch 12/20  Iteration 2055/3560 Training loss: 1.4116 1.5684 sec/batch\n",
      "Epoch 12/20  Iteration 2056/3560 Training loss: 1.4113 1.5008 sec/batch\n",
      "Epoch 12/20  Iteration 2057/3560 Training loss: 1.4110 1.5222 sec/batch\n",
      "Epoch 12/20  Iteration 2058/3560 Training loss: 1.4106 1.6424 sec/batch\n",
      "Epoch 12/20  Iteration 2059/3560 Training loss: 1.4105 1.4751 sec/batch\n",
      "Epoch 12/20  Iteration 2060/3560 Training loss: 1.4103 1.5179 sec/batch\n",
      "Epoch 12/20  Iteration 2061/3560 Training loss: 1.4101 1.5127 sec/batch\n",
      "Epoch 12/20  Iteration 2062/3560 Training loss: 1.4099 1.5188 sec/batch\n",
      "Epoch 12/20  Iteration 2063/3560 Training loss: 1.4097 1.5055 sec/batch\n",
      "Epoch 12/20  Iteration 2064/3560 Training loss: 1.4095 1.4713 sec/batch\n",
      "Epoch 12/20  Iteration 2065/3560 Training loss: 1.4094 1.4802 sec/batch\n",
      "Epoch 12/20  Iteration 2066/3560 Training loss: 1.4094 1.4596 sec/batch\n",
      "Epoch 12/20  Iteration 2067/3560 Training loss: 1.4092 1.5272 sec/batch\n",
      "Epoch 12/20  Iteration 2068/3560 Training loss: 1.4092 1.4454 sec/batch\n",
      "Epoch 12/20  Iteration 2069/3560 Training loss: 1.4089 1.6159 sec/batch\n",
      "Epoch 12/20  Iteration 2070/3560 Training loss: 1.4088 1.5701 sec/batch\n",
      "Epoch 12/20  Iteration 2071/3560 Training loss: 1.4087 1.5266 sec/batch\n",
      "Epoch 12/20  Iteration 2072/3560 Training loss: 1.4085 1.5040 sec/batch\n",
      "Epoch 12/20  Iteration 2073/3560 Training loss: 1.4082 1.4961 sec/batch\n",
      "Epoch 12/20  Iteration 2074/3560 Training loss: 1.4078 1.4677 sec/batch\n",
      "Epoch 12/20  Iteration 2075/3560 Training loss: 1.4077 1.5708 sec/batch\n",
      "Epoch 12/20  Iteration 2076/3560 Training loss: 1.4077 1.4649 sec/batch\n",
      "Epoch 12/20  Iteration 2077/3560 Training loss: 1.4075 1.4651 sec/batch\n",
      "Epoch 12/20  Iteration 2078/3560 Training loss: 1.4075 1.4869 sec/batch\n",
      "Epoch 12/20  Iteration 2079/3560 Training loss: 1.4074 1.4627 sec/batch\n",
      "Epoch 12/20  Iteration 2080/3560 Training loss: 1.4070 1.4775 sec/batch\n",
      "Epoch 12/20  Iteration 2081/3560 Training loss: 1.4065 1.4295 sec/batch\n",
      "Epoch 12/20  Iteration 2082/3560 Training loss: 1.4065 1.4964 sec/batch\n",
      "Epoch 12/20  Iteration 2083/3560 Training loss: 1.4065 1.5109 sec/batch\n",
      "Epoch 12/20  Iteration 2084/3560 Training loss: 1.4060 1.4465 sec/batch\n",
      "Epoch 12/20  Iteration 2085/3560 Training loss: 1.4060 1.5175 sec/batch\n",
      "Epoch 12/20  Iteration 2086/3560 Training loss: 1.4060 1.4786 sec/batch\n",
      "Epoch 12/20  Iteration 2087/3560 Training loss: 1.4058 1.4925 sec/batch\n",
      "Epoch 12/20  Iteration 2088/3560 Training loss: 1.4054 1.4562 sec/batch\n",
      "Epoch 12/20  Iteration 2089/3560 Training loss: 1.4050 1.5411 sec/batch\n",
      "Epoch 12/20  Iteration 2090/3560 Training loss: 1.4046 1.4320 sec/batch\n",
      "Epoch 12/20  Iteration 2091/3560 Training loss: 1.4047 1.4517 sec/batch\n",
      "Epoch 12/20  Iteration 2092/3560 Training loss: 1.4047 1.4491 sec/batch\n",
      "Epoch 12/20  Iteration 2093/3560 Training loss: 1.4047 1.4392 sec/batch\n",
      "Epoch 12/20  Iteration 2094/3560 Training loss: 1.4047 1.5103 sec/batch\n",
      "Epoch 12/20  Iteration 2095/3560 Training loss: 1.4049 1.4663 sec/batch\n",
      "Epoch 12/20  Iteration 2096/3560 Training loss: 1.4049 1.6625 sec/batch\n",
      "Epoch 12/20  Iteration 2097/3560 Training loss: 1.4049 1.5296 sec/batch\n",
      "Epoch 12/20  Iteration 2098/3560 Training loss: 1.4049 1.5524 sec/batch\n",
      "Epoch 12/20  Iteration 2099/3560 Training loss: 1.4053 1.4978 sec/batch\n",
      "Epoch 12/20  Iteration 2100/3560 Training loss: 1.4052 1.4922 sec/batch\n",
      "Epoch 12/20  Iteration 2101/3560 Training loss: 1.4050 1.4966 sec/batch\n",
      "Epoch 12/20  Iteration 2102/3560 Training loss: 1.4052 1.5098 sec/batch\n",
      "Epoch 12/20  Iteration 2103/3560 Training loss: 1.4051 1.4813 sec/batch\n",
      "Epoch 12/20  Iteration 2104/3560 Training loss: 1.4053 1.5314 sec/batch\n",
      "Epoch 12/20  Iteration 2105/3560 Training loss: 1.4053 1.5066 sec/batch\n",
      "Epoch 12/20  Iteration 2106/3560 Training loss: 1.4055 1.5200 sec/batch\n",
      "Epoch 12/20  Iteration 2107/3560 Training loss: 1.4056 1.4904 sec/batch\n",
      "Epoch 12/20  Iteration 2108/3560 Training loss: 1.4054 1.5431 sec/batch\n",
      "Epoch 12/20  Iteration 2109/3560 Training loss: 1.4051 1.5823 sec/batch\n",
      "Epoch 12/20  Iteration 2110/3560 Training loss: 1.4049 1.5521 sec/batch\n",
      "Epoch 12/20  Iteration 2111/3560 Training loss: 1.4050 1.4851 sec/batch\n",
      "Epoch 12/20  Iteration 2112/3560 Training loss: 1.4049 1.5478 sec/batch\n",
      "Epoch 12/20  Iteration 2113/3560 Training loss: 1.4048 1.5138 sec/batch\n",
      "Epoch 12/20  Iteration 2114/3560 Training loss: 1.4048 1.4137 sec/batch\n",
      "Epoch 12/20  Iteration 2115/3560 Training loss: 1.4048 1.4250 sec/batch\n",
      "Epoch 12/20  Iteration 2116/3560 Training loss: 1.4047 1.4383 sec/batch\n",
      "Epoch 12/20  Iteration 2117/3560 Training loss: 1.4044 1.4184 sec/batch\n",
      "Epoch 12/20  Iteration 2118/3560 Training loss: 1.4045 1.3741 sec/batch\n",
      "Epoch 12/20  Iteration 2119/3560 Training loss: 1.4046 1.4198 sec/batch\n",
      "Epoch 12/20  Iteration 2120/3560 Training loss: 1.4046 1.4687 sec/batch\n",
      "Epoch 12/20  Iteration 2121/3560 Training loss: 1.4046 1.4694 sec/batch\n",
      "Epoch 12/20  Iteration 2122/3560 Training loss: 1.4046 1.4180 sec/batch\n",
      "Epoch 12/20  Iteration 2123/3560 Training loss: 1.4046 1.4385 sec/batch\n",
      "Epoch 12/20  Iteration 2124/3560 Training loss: 1.4045 1.4072 sec/batch\n",
      "Epoch 12/20  Iteration 2125/3560 Training loss: 1.4047 1.3850 sec/batch\n",
      "Epoch 12/20  Iteration 2126/3560 Training loss: 1.4051 1.4129 sec/batch\n",
      "Epoch 12/20  Iteration 2127/3560 Training loss: 1.4051 1.4377 sec/batch\n",
      "Epoch 12/20  Iteration 2128/3560 Training loss: 1.4050 1.4672 sec/batch\n",
      "Epoch 12/20  Iteration 2129/3560 Training loss: 1.4049 1.5303 sec/batch\n",
      "Epoch 12/20  Iteration 2130/3560 Training loss: 1.4047 1.7763 sec/batch\n",
      "Epoch 12/20  Iteration 2131/3560 Training loss: 1.4048 1.7917 sec/batch\n",
      "Epoch 12/20  Iteration 2132/3560 Training loss: 1.4048 1.4807 sec/batch\n",
      "Epoch 12/20  Iteration 2133/3560 Training loss: 1.4048 1.5249 sec/batch\n",
      "Epoch 12/20  Iteration 2134/3560 Training loss: 1.4047 1.5365 sec/batch\n",
      "Epoch 12/20  Iteration 2135/3560 Training loss: 1.4045 1.4776 sec/batch\n",
      "Epoch 12/20  Iteration 2136/3560 Training loss: 1.4047 1.6383 sec/batch\n",
      "Epoch 13/20  Iteration 2137/3560 Training loss: 1.5261 1.5570 sec/batch\n",
      "Epoch 13/20  Iteration 2138/3560 Training loss: 1.4674 1.5770 sec/batch\n",
      "Epoch 13/20  Iteration 2139/3560 Training loss: 1.4468 1.3849 sec/batch\n",
      "Epoch 13/20  Iteration 2140/3560 Training loss: 1.4357 1.4051 sec/batch\n",
      "Epoch 13/20  Iteration 2141/3560 Training loss: 1.4243 1.4244 sec/batch\n",
      "Epoch 13/20  Iteration 2142/3560 Training loss: 1.4109 1.4874 sec/batch\n",
      "Epoch 13/20  Iteration 2143/3560 Training loss: 1.4104 1.5401 sec/batch\n",
      "Epoch 13/20  Iteration 2144/3560 Training loss: 1.4070 1.5287 sec/batch\n",
      "Epoch 13/20  Iteration 2145/3560 Training loss: 1.4056 1.6357 sec/batch\n",
      "Epoch 13/20  Iteration 2146/3560 Training loss: 1.4045 1.5179 sec/batch\n",
      "Epoch 13/20  Iteration 2147/3560 Training loss: 1.3999 1.4799 sec/batch\n",
      "Epoch 13/20  Iteration 2148/3560 Training loss: 1.3996 1.4661 sec/batch\n",
      "Epoch 13/20  Iteration 2149/3560 Training loss: 1.3991 1.4210 sec/batch\n",
      "Epoch 13/20  Iteration 2150/3560 Training loss: 1.4007 1.4946 sec/batch\n",
      "Epoch 13/20  Iteration 2151/3560 Training loss: 1.3995 1.3838 sec/batch\n",
      "Epoch 13/20  Iteration 2152/3560 Training loss: 1.3975 1.4452 sec/batch\n",
      "Epoch 13/20  Iteration 2153/3560 Training loss: 1.3974 1.3896 sec/batch\n",
      "Epoch 13/20  Iteration 2154/3560 Training loss: 1.3982 1.3928 sec/batch\n",
      "Epoch 13/20  Iteration 2155/3560 Training loss: 1.3980 1.4473 sec/batch\n",
      "Epoch 13/20  Iteration 2156/3560 Training loss: 1.3987 1.4312 sec/batch\n",
      "Epoch 13/20  Iteration 2157/3560 Training loss: 1.3978 1.4606 sec/batch\n",
      "Epoch 13/20  Iteration 2158/3560 Training loss: 1.3980 1.4261 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2159/3560 Training loss: 1.3968 1.4310 sec/batch\n",
      "Epoch 13/20  Iteration 2160/3560 Training loss: 1.3965 1.3603 sec/batch\n",
      "Epoch 13/20  Iteration 2161/3560 Training loss: 1.3962 1.3716 sec/batch\n",
      "Epoch 13/20  Iteration 2162/3560 Training loss: 1.3944 1.4598 sec/batch\n",
      "Epoch 13/20  Iteration 2163/3560 Training loss: 1.3930 1.4570 sec/batch\n",
      "Epoch 13/20  Iteration 2164/3560 Training loss: 1.3934 1.4990 sec/batch\n",
      "Epoch 13/20  Iteration 2165/3560 Training loss: 1.3937 1.3777 sec/batch\n",
      "Epoch 13/20  Iteration 2166/3560 Training loss: 1.3939 1.5108 sec/batch\n",
      "Epoch 13/20  Iteration 2167/3560 Training loss: 1.3934 1.4190 sec/batch\n",
      "Epoch 13/20  Iteration 2168/3560 Training loss: 1.3922 1.4457 sec/batch\n",
      "Epoch 13/20  Iteration 2169/3560 Training loss: 1.3924 1.4081 sec/batch\n",
      "Epoch 13/20  Iteration 2170/3560 Training loss: 1.3930 1.9243 sec/batch\n",
      "Epoch 13/20  Iteration 2171/3560 Training loss: 1.3929 1.4470 sec/batch\n",
      "Epoch 13/20  Iteration 2172/3560 Training loss: 1.3932 1.9007 sec/batch\n",
      "Epoch 13/20  Iteration 2173/3560 Training loss: 1.3926 1.5877 sec/batch\n",
      "Epoch 13/20  Iteration 2174/3560 Training loss: 1.3917 1.9011 sec/batch\n",
      "Epoch 13/20  Iteration 2175/3560 Training loss: 1.3902 1.6449 sec/batch\n",
      "Epoch 13/20  Iteration 2176/3560 Training loss: 1.3898 1.4706 sec/batch\n",
      "Epoch 13/20  Iteration 2177/3560 Training loss: 1.3892 1.4229 sec/batch\n",
      "Epoch 13/20  Iteration 2178/3560 Training loss: 1.3897 1.6754 sec/batch\n",
      "Epoch 13/20  Iteration 2179/3560 Training loss: 1.3894 1.9775 sec/batch\n",
      "Epoch 13/20  Iteration 2180/3560 Training loss: 1.3887 1.5914 sec/batch\n",
      "Epoch 13/20  Iteration 2181/3560 Training loss: 1.3888 2.8270 sec/batch\n",
      "Epoch 13/20  Iteration 2182/3560 Training loss: 1.3878 2.6797 sec/batch\n",
      "Epoch 13/20  Iteration 2183/3560 Training loss: 1.3874 2.2082 sec/batch\n",
      "Epoch 13/20  Iteration 2184/3560 Training loss: 1.3871 1.4924 sec/batch\n",
      "Epoch 13/20  Iteration 2185/3560 Training loss: 1.3869 1.4234 sec/batch\n",
      "Epoch 13/20  Iteration 2186/3560 Training loss: 1.3871 1.3825 sec/batch\n",
      "Epoch 13/20  Iteration 2187/3560 Training loss: 1.3867 1.3567 sec/batch\n",
      "Epoch 13/20  Iteration 2188/3560 Training loss: 1.3876 1.4752 sec/batch\n",
      "Epoch 13/20  Iteration 2189/3560 Training loss: 1.3874 1.3640 sec/batch\n",
      "Epoch 13/20  Iteration 2190/3560 Training loss: 1.3875 1.5169 sec/batch\n",
      "Epoch 13/20  Iteration 2191/3560 Training loss: 1.3873 1.4889 sec/batch\n",
      "Epoch 13/20  Iteration 2192/3560 Training loss: 1.3872 1.4494 sec/batch\n",
      "Epoch 13/20  Iteration 2193/3560 Training loss: 1.3876 1.3787 sec/batch\n",
      "Epoch 13/20  Iteration 2194/3560 Training loss: 1.3874 1.3223 sec/batch\n",
      "Epoch 13/20  Iteration 2195/3560 Training loss: 1.3869 1.5375 sec/batch\n",
      "Epoch 13/20  Iteration 2196/3560 Training loss: 1.3873 1.3515 sec/batch\n",
      "Epoch 13/20  Iteration 2197/3560 Training loss: 1.3872 1.6109 sec/batch\n",
      "Epoch 13/20  Iteration 2198/3560 Training loss: 1.3881 1.4100 sec/batch\n",
      "Epoch 13/20  Iteration 2199/3560 Training loss: 1.3881 1.4068 sec/batch\n",
      "Epoch 13/20  Iteration 2200/3560 Training loss: 1.3883 1.6816 sec/batch\n",
      "Epoch 13/20  Iteration 2201/3560 Training loss: 1.3882 1.5954 sec/batch\n",
      "Epoch 13/20  Iteration 2202/3560 Training loss: 1.3883 1.6610 sec/batch\n",
      "Epoch 13/20  Iteration 2203/3560 Training loss: 1.3885 1.5821 sec/batch\n",
      "Epoch 13/20  Iteration 2204/3560 Training loss: 1.3882 1.6272 sec/batch\n",
      "Epoch 13/20  Iteration 2205/3560 Training loss: 1.3882 1.6453 sec/batch\n",
      "Epoch 13/20  Iteration 2206/3560 Training loss: 1.3880 1.7427 sec/batch\n",
      "Epoch 13/20  Iteration 2207/3560 Training loss: 1.3888 1.8279 sec/batch\n",
      "Epoch 13/20  Iteration 2208/3560 Training loss: 1.3890 1.7320 sec/batch\n",
      "Epoch 13/20  Iteration 2209/3560 Training loss: 1.3894 1.7112 sec/batch\n",
      "Epoch 13/20  Iteration 2210/3560 Training loss: 1.3891 1.9752 sec/batch\n",
      "Epoch 13/20  Iteration 2211/3560 Training loss: 1.3891 2.1379 sec/batch\n",
      "Epoch 13/20  Iteration 2212/3560 Training loss: 1.3893 2.0139 sec/batch\n",
      "Epoch 13/20  Iteration 2213/3560 Training loss: 1.3890 2.1220 sec/batch\n",
      "Epoch 13/20  Iteration 2214/3560 Training loss: 1.3890 2.0229 sec/batch\n",
      "Epoch 13/20  Iteration 2215/3560 Training loss: 1.3883 1.4287 sec/batch\n",
      "Epoch 13/20  Iteration 2216/3560 Training loss: 1.3882 1.6323 sec/batch\n",
      "Epoch 13/20  Iteration 2217/3560 Training loss: 1.3877 1.3607 sec/batch\n",
      "Epoch 13/20  Iteration 2218/3560 Training loss: 1.3877 1.4924 sec/batch\n",
      "Epoch 13/20  Iteration 2219/3560 Training loss: 1.3872 1.5626 sec/batch\n",
      "Epoch 13/20  Iteration 2220/3560 Training loss: 1.3871 1.6271 sec/batch\n",
      "Epoch 13/20  Iteration 2221/3560 Training loss: 1.3868 1.4939 sec/batch\n",
      "Epoch 13/20  Iteration 2222/3560 Training loss: 1.3865 1.5533 sec/batch\n",
      "Epoch 13/20  Iteration 2223/3560 Training loss: 1.3862 1.3577 sec/batch\n",
      "Epoch 13/20  Iteration 2224/3560 Training loss: 1.3859 1.4155 sec/batch\n",
      "Epoch 13/20  Iteration 2225/3560 Training loss: 1.3855 1.6066 sec/batch\n",
      "Epoch 13/20  Iteration 2226/3560 Training loss: 1.3856 1.6502 sec/batch\n",
      "Epoch 13/20  Iteration 2227/3560 Training loss: 1.3854 1.5559 sec/batch\n",
      "Epoch 13/20  Iteration 2228/3560 Training loss: 1.3853 1.4071 sec/batch\n",
      "Epoch 13/20  Iteration 2229/3560 Training loss: 1.3848 1.5009 sec/batch\n",
      "Epoch 13/20  Iteration 2230/3560 Training loss: 1.3845 1.4294 sec/batch\n",
      "Epoch 13/20  Iteration 2231/3560 Training loss: 1.3843 1.4449 sec/batch\n",
      "Epoch 13/20  Iteration 2232/3560 Training loss: 1.3843 1.4605 sec/batch\n",
      "Epoch 13/20  Iteration 2233/3560 Training loss: 1.3842 1.6736 sec/batch\n",
      "Epoch 13/20  Iteration 2234/3560 Training loss: 1.3838 1.5114 sec/batch\n",
      "Epoch 13/20  Iteration 2235/3560 Training loss: 1.3834 1.5228 sec/batch\n",
      "Epoch 13/20  Iteration 2236/3560 Training loss: 1.3829 1.5106 sec/batch\n",
      "Epoch 13/20  Iteration 2237/3560 Training loss: 1.3829 1.5696 sec/batch\n",
      "Epoch 13/20  Iteration 2238/3560 Training loss: 1.3827 1.5763 sec/batch\n",
      "Epoch 13/20  Iteration 2239/3560 Training loss: 1.3826 1.6268 sec/batch\n",
      "Epoch 13/20  Iteration 2240/3560 Training loss: 1.3824 1.6152 sec/batch\n",
      "Epoch 13/20  Iteration 2241/3560 Training loss: 1.3822 1.4601 sec/batch\n",
      "Epoch 13/20  Iteration 2242/3560 Training loss: 1.3821 1.6675 sec/batch\n",
      "Epoch 13/20  Iteration 2243/3560 Training loss: 1.3821 1.3930 sec/batch\n",
      "Epoch 13/20  Iteration 2244/3560 Training loss: 1.3821 1.4500 sec/batch\n",
      "Epoch 13/20  Iteration 2245/3560 Training loss: 1.3819 2.2145 sec/batch\n",
      "Epoch 13/20  Iteration 2246/3560 Training loss: 1.3819 1.7978 sec/batch\n",
      "Epoch 13/20  Iteration 2247/3560 Training loss: 1.3818 1.4023 sec/batch\n",
      "Epoch 13/20  Iteration 2248/3560 Training loss: 1.3817 1.4824 sec/batch\n",
      "Epoch 13/20  Iteration 2249/3560 Training loss: 1.3815 1.4154 sec/batch\n",
      "Epoch 13/20  Iteration 2250/3560 Training loss: 1.3814 1.3741 sec/batch\n",
      "Epoch 13/20  Iteration 2251/3560 Training loss: 1.3812 1.5821 sec/batch\n",
      "Epoch 13/20  Iteration 2252/3560 Training loss: 1.3808 1.7130 sec/batch\n",
      "Epoch 13/20  Iteration 2253/3560 Training loss: 1.3808 1.4244 sec/batch\n",
      "Epoch 13/20  Iteration 2254/3560 Training loss: 1.3808 1.5791 sec/batch\n",
      "Epoch 13/20  Iteration 2255/3560 Training loss: 1.3806 1.6493 sec/batch\n",
      "Epoch 13/20  Iteration 2256/3560 Training loss: 1.3806 1.8961 sec/batch\n",
      "Epoch 13/20  Iteration 2257/3560 Training loss: 1.3804 1.7973 sec/batch\n",
      "Epoch 13/20  Iteration 2258/3560 Training loss: 1.3801 1.7942 sec/batch\n",
      "Epoch 13/20  Iteration 2259/3560 Training loss: 1.3797 1.6391 sec/batch\n",
      "Epoch 13/20  Iteration 2260/3560 Training loss: 1.3797 1.9474 sec/batch\n",
      "Epoch 13/20  Iteration 2261/3560 Training loss: 1.3796 1.6854 sec/batch\n",
      "Epoch 13/20  Iteration 2262/3560 Training loss: 1.3792 1.4521 sec/batch\n",
      "Epoch 13/20  Iteration 2263/3560 Training loss: 1.3793 1.5047 sec/batch\n",
      "Epoch 13/20  Iteration 2264/3560 Training loss: 1.3792 1.3472 sec/batch\n",
      "Epoch 13/20  Iteration 2265/3560 Training loss: 1.3790 1.4715 sec/batch\n",
      "Epoch 13/20  Iteration 2266/3560 Training loss: 1.3787 1.3423 sec/batch\n",
      "Epoch 13/20  Iteration 2267/3560 Training loss: 1.3781 1.4305 sec/batch\n",
      "Epoch 13/20  Iteration 2268/3560 Training loss: 1.3779 1.5762 sec/batch\n",
      "Epoch 13/20  Iteration 2269/3560 Training loss: 1.3780 1.4669 sec/batch\n",
      "Epoch 13/20  Iteration 2270/3560 Training loss: 1.3780 1.3896 sec/batch\n",
      "Epoch 13/20  Iteration 2271/3560 Training loss: 1.3779 1.5240 sec/batch\n",
      "Epoch 13/20  Iteration 2272/3560 Training loss: 1.3779 1.5289 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2273/3560 Training loss: 1.3780 1.8271 sec/batch\n",
      "Epoch 13/20  Iteration 2274/3560 Training loss: 1.3781 1.3648 sec/batch\n",
      "Epoch 13/20  Iteration 2275/3560 Training loss: 1.3780 1.3513 sec/batch\n",
      "Epoch 13/20  Iteration 2276/3560 Training loss: 1.3781 1.2766 sec/batch\n",
      "Epoch 13/20  Iteration 2277/3560 Training loss: 1.3784 1.5204 sec/batch\n",
      "Epoch 13/20  Iteration 2278/3560 Training loss: 1.3784 1.4154 sec/batch\n",
      "Epoch 13/20  Iteration 2279/3560 Training loss: 1.3783 1.7600 sec/batch\n",
      "Epoch 13/20  Iteration 2280/3560 Training loss: 1.3785 1.4736 sec/batch\n",
      "Epoch 13/20  Iteration 2281/3560 Training loss: 1.3784 1.5677 sec/batch\n",
      "Epoch 13/20  Iteration 2282/3560 Training loss: 1.3786 1.4852 sec/batch\n",
      "Epoch 13/20  Iteration 2283/3560 Training loss: 1.3785 1.4546 sec/batch\n",
      "Epoch 13/20  Iteration 2284/3560 Training loss: 1.3788 1.5027 sec/batch\n",
      "Epoch 13/20  Iteration 2285/3560 Training loss: 1.3790 1.5125 sec/batch\n",
      "Epoch 13/20  Iteration 2286/3560 Training loss: 1.3787 1.4361 sec/batch\n",
      "Epoch 13/20  Iteration 2287/3560 Training loss: 1.3784 1.4519 sec/batch\n",
      "Epoch 13/20  Iteration 2288/3560 Training loss: 1.3783 1.4345 sec/batch\n",
      "Epoch 13/20  Iteration 2289/3560 Training loss: 1.3784 1.4343 sec/batch\n",
      "Epoch 13/20  Iteration 2290/3560 Training loss: 1.3783 1.5425 sec/batch\n",
      "Epoch 13/20  Iteration 2291/3560 Training loss: 1.3783 2.4180 sec/batch\n",
      "Epoch 13/20  Iteration 2292/3560 Training loss: 1.3783 2.2073 sec/batch\n",
      "Epoch 13/20  Iteration 2293/3560 Training loss: 1.3783 1.8604 sec/batch\n",
      "Epoch 13/20  Iteration 2294/3560 Training loss: 1.3782 1.4829 sec/batch\n",
      "Epoch 13/20  Iteration 2295/3560 Training loss: 1.3779 1.4099 sec/batch\n",
      "Epoch 13/20  Iteration 2296/3560 Training loss: 1.3780 1.4162 sec/batch\n",
      "Epoch 13/20  Iteration 2297/3560 Training loss: 1.3782 1.4581 sec/batch\n",
      "Epoch 13/20  Iteration 2298/3560 Training loss: 1.3782 1.4356 sec/batch\n",
      "Epoch 13/20  Iteration 2299/3560 Training loss: 1.3781 1.3906 sec/batch\n",
      "Epoch 13/20  Iteration 2300/3560 Training loss: 1.3781 1.3993 sec/batch\n",
      "Epoch 13/20  Iteration 2301/3560 Training loss: 1.3781 1.4597 sec/batch\n",
      "Epoch 13/20  Iteration 2302/3560 Training loss: 1.3780 1.3833 sec/batch\n",
      "Epoch 13/20  Iteration 2303/3560 Training loss: 1.3781 1.4347 sec/batch\n",
      "Epoch 13/20  Iteration 2304/3560 Training loss: 1.3785 1.4718 sec/batch\n",
      "Epoch 13/20  Iteration 2305/3560 Training loss: 1.3785 1.4872 sec/batch\n",
      "Epoch 13/20  Iteration 2306/3560 Training loss: 1.3785 1.3662 sec/batch\n",
      "Epoch 13/20  Iteration 2307/3560 Training loss: 1.3784 1.4206 sec/batch\n",
      "Epoch 13/20  Iteration 2308/3560 Training loss: 1.3782 1.3968 sec/batch\n",
      "Epoch 13/20  Iteration 2309/3560 Training loss: 1.3783 1.5486 sec/batch\n",
      "Epoch 13/20  Iteration 2310/3560 Training loss: 1.3782 1.4683 sec/batch\n",
      "Epoch 13/20  Iteration 2311/3560 Training loss: 1.3782 1.4446 sec/batch\n",
      "Epoch 13/20  Iteration 2312/3560 Training loss: 1.3780 1.4714 sec/batch\n",
      "Epoch 13/20  Iteration 2313/3560 Training loss: 1.3779 1.4131 sec/batch\n",
      "Epoch 13/20  Iteration 2314/3560 Training loss: 1.3781 1.4265 sec/batch\n",
      "Epoch 14/20  Iteration 2315/3560 Training loss: 1.4989 1.4326 sec/batch\n",
      "Epoch 14/20  Iteration 2316/3560 Training loss: 1.4369 1.4226 sec/batch\n",
      "Epoch 14/20  Iteration 2317/3560 Training loss: 1.4154 1.9264 sec/batch\n",
      "Epoch 14/20  Iteration 2318/3560 Training loss: 1.4085 1.8467 sec/batch\n",
      "Epoch 14/20  Iteration 2319/3560 Training loss: 1.3959 1.5301 sec/batch\n",
      "Epoch 14/20  Iteration 2320/3560 Training loss: 1.3833 1.4756 sec/batch\n",
      "Epoch 14/20  Iteration 2321/3560 Training loss: 1.3822 1.8083 sec/batch\n",
      "Epoch 14/20  Iteration 2322/3560 Training loss: 1.3782 1.4679 sec/batch\n",
      "Epoch 14/20  Iteration 2323/3560 Training loss: 1.3766 1.4661 sec/batch\n",
      "Epoch 14/20  Iteration 2324/3560 Training loss: 1.3746 1.4826 sec/batch\n",
      "Epoch 14/20  Iteration 2325/3560 Training loss: 1.3706 1.9891 sec/batch\n",
      "Epoch 14/20  Iteration 2326/3560 Training loss: 1.3708 1.7986 sec/batch\n",
      "Epoch 14/20  Iteration 2327/3560 Training loss: 1.3710 1.3622 sec/batch\n",
      "Epoch 14/20  Iteration 2328/3560 Training loss: 1.3722 1.4971 sec/batch\n",
      "Epoch 14/20  Iteration 2329/3560 Training loss: 1.3713 1.3769 sec/batch\n",
      "Epoch 14/20  Iteration 2330/3560 Training loss: 1.3693 1.3726 sec/batch\n",
      "Epoch 14/20  Iteration 2331/3560 Training loss: 1.3694 1.8411 sec/batch\n",
      "Epoch 14/20  Iteration 2332/3560 Training loss: 1.3705 1.6693 sec/batch\n",
      "Epoch 14/20  Iteration 2333/3560 Training loss: 1.3700 1.6073 sec/batch\n",
      "Epoch 14/20  Iteration 2334/3560 Training loss: 1.3713 1.4234 sec/batch\n",
      "Epoch 14/20  Iteration 2335/3560 Training loss: 1.3709 1.3743 sec/batch\n",
      "Epoch 14/20  Iteration 2336/3560 Training loss: 1.3711 1.5959 sec/batch\n",
      "Epoch 14/20  Iteration 2337/3560 Training loss: 1.3704 1.4479 sec/batch\n",
      "Epoch 14/20  Iteration 2338/3560 Training loss: 1.3701 1.6914 sec/batch\n",
      "Epoch 14/20  Iteration 2339/3560 Training loss: 1.3700 1.6971 sec/batch\n",
      "Epoch 14/20  Iteration 2340/3560 Training loss: 1.3680 1.5477 sec/batch\n",
      "Epoch 14/20  Iteration 2341/3560 Training loss: 1.3665 1.6941 sec/batch\n",
      "Epoch 14/20  Iteration 2342/3560 Training loss: 1.3673 1.4928 sec/batch\n",
      "Epoch 14/20  Iteration 2343/3560 Training loss: 1.3672 1.4998 sec/batch\n",
      "Epoch 14/20  Iteration 2344/3560 Training loss: 1.3675 1.7097 sec/batch\n",
      "Epoch 14/20  Iteration 2345/3560 Training loss: 1.3669 1.9001 sec/batch\n",
      "Epoch 14/20  Iteration 2346/3560 Training loss: 1.3660 1.5899 sec/batch\n",
      "Epoch 14/20  Iteration 2347/3560 Training loss: 1.3663 1.4392 sec/batch\n",
      "Epoch 14/20  Iteration 2348/3560 Training loss: 1.3664 1.3936 sec/batch\n",
      "Epoch 14/20  Iteration 2349/3560 Training loss: 1.3663 1.4762 sec/batch\n",
      "Epoch 14/20  Iteration 2350/3560 Training loss: 1.3662 1.4361 sec/batch\n",
      "Epoch 14/20  Iteration 2351/3560 Training loss: 1.3654 1.4466 sec/batch\n",
      "Epoch 14/20  Iteration 2352/3560 Training loss: 1.3642 1.5048 sec/batch\n",
      "Epoch 14/20  Iteration 2353/3560 Training loss: 1.3627 1.4529 sec/batch\n",
      "Epoch 14/20  Iteration 2354/3560 Training loss: 1.3623 1.4788 sec/batch\n",
      "Epoch 14/20  Iteration 2355/3560 Training loss: 1.3618 1.4429 sec/batch\n",
      "Epoch 14/20  Iteration 2356/3560 Training loss: 1.3626 1.4265 sec/batch\n",
      "Epoch 14/20  Iteration 2357/3560 Training loss: 1.3622 1.4830 sec/batch\n",
      "Epoch 14/20  Iteration 2358/3560 Training loss: 1.3616 1.4430 sec/batch\n",
      "Epoch 14/20  Iteration 2359/3560 Training loss: 1.3619 1.4892 sec/batch\n",
      "Epoch 14/20  Iteration 2360/3560 Training loss: 1.3611 1.3929 sec/batch\n",
      "Epoch 14/20  Iteration 2361/3560 Training loss: 1.3606 1.7487 sec/batch\n",
      "Epoch 14/20  Iteration 2362/3560 Training loss: 1.3602 1.6680 sec/batch\n",
      "Epoch 14/20  Iteration 2363/3560 Training loss: 1.3600 1.4092 sec/batch\n",
      "Epoch 14/20  Iteration 2364/3560 Training loss: 1.3603 1.4255 sec/batch\n",
      "Epoch 14/20  Iteration 2365/3560 Training loss: 1.3598 1.3667 sec/batch\n",
      "Epoch 14/20  Iteration 2366/3560 Training loss: 1.3606 1.4293 sec/batch\n",
      "Epoch 14/20  Iteration 2367/3560 Training loss: 1.3606 1.4655 sec/batch\n",
      "Epoch 14/20  Iteration 2368/3560 Training loss: 1.3609 1.6322 sec/batch\n",
      "Epoch 14/20  Iteration 2369/3560 Training loss: 1.3606 1.5020 sec/batch\n",
      "Epoch 14/20  Iteration 2370/3560 Training loss: 1.3607 1.4420 sec/batch\n",
      "Epoch 14/20  Iteration 2371/3560 Training loss: 1.3611 1.4656 sec/batch\n",
      "Epoch 14/20  Iteration 2372/3560 Training loss: 1.3609 1.4438 sec/batch\n",
      "Epoch 14/20  Iteration 2373/3560 Training loss: 1.3604 1.4726 sec/batch\n",
      "Epoch 14/20  Iteration 2374/3560 Training loss: 1.3610 1.4651 sec/batch\n",
      "Epoch 14/20  Iteration 2375/3560 Training loss: 1.3608 1.4301 sec/batch\n",
      "Epoch 14/20  Iteration 2376/3560 Training loss: 1.3615 1.4571 sec/batch\n",
      "Epoch 14/20  Iteration 2377/3560 Training loss: 1.3616 1.4518 sec/batch\n",
      "Epoch 14/20  Iteration 2378/3560 Training loss: 1.3617 1.5597 sec/batch\n",
      "Epoch 14/20  Iteration 2379/3560 Training loss: 1.3615 1.4741 sec/batch\n",
      "Epoch 14/20  Iteration 2380/3560 Training loss: 1.3617 1.4608 sec/batch\n",
      "Epoch 14/20  Iteration 2381/3560 Training loss: 1.3619 1.4567 sec/batch\n",
      "Epoch 14/20  Iteration 2382/3560 Training loss: 1.3615 1.4432 sec/batch\n",
      "Epoch 14/20  Iteration 2383/3560 Training loss: 1.3615 1.4681 sec/batch\n",
      "Epoch 14/20  Iteration 2384/3560 Training loss: 1.3613 1.4419 sec/batch\n",
      "Epoch 14/20  Iteration 2385/3560 Training loss: 1.3619 1.4185 sec/batch\n",
      "Epoch 14/20  Iteration 2386/3560 Training loss: 1.3620 1.3858 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2387/3560 Training loss: 1.3625 1.3981 sec/batch\n",
      "Epoch 14/20  Iteration 2388/3560 Training loss: 1.3622 1.4196 sec/batch\n",
      "Epoch 14/20  Iteration 2389/3560 Training loss: 1.3621 1.4859 sec/batch\n",
      "Epoch 14/20  Iteration 2390/3560 Training loss: 1.3623 1.4500 sec/batch\n",
      "Epoch 14/20  Iteration 2391/3560 Training loss: 1.3622 1.4452 sec/batch\n",
      "Epoch 14/20  Iteration 2392/3560 Training loss: 1.3621 1.4903 sec/batch\n",
      "Epoch 14/20  Iteration 2393/3560 Training loss: 1.3615 1.4416 sec/batch\n",
      "Epoch 14/20  Iteration 2394/3560 Training loss: 1.3614 1.4636 sec/batch\n",
      "Epoch 14/20  Iteration 2395/3560 Training loss: 1.3609 1.4560 sec/batch\n",
      "Epoch 14/20  Iteration 2396/3560 Training loss: 1.3609 1.5055 sec/batch\n",
      "Epoch 14/20  Iteration 2397/3560 Training loss: 1.3604 1.4608 sec/batch\n",
      "Epoch 14/20  Iteration 2398/3560 Training loss: 1.3604 1.4478 sec/batch\n",
      "Epoch 14/20  Iteration 2399/3560 Training loss: 1.3601 1.4789 sec/batch\n",
      "Epoch 14/20  Iteration 2400/3560 Training loss: 1.3599 1.4241 sec/batch\n",
      "Epoch 14/20  Iteration 2401/3560 Training loss: 1.3595 1.4653 sec/batch\n",
      "Epoch 14/20  Iteration 2402/3560 Training loss: 1.3592 1.4616 sec/batch\n",
      "Epoch 14/20  Iteration 2403/3560 Training loss: 1.3588 1.4414 sec/batch\n",
      "Epoch 14/20  Iteration 2404/3560 Training loss: 1.3588 1.4990 sec/batch\n",
      "Epoch 14/20  Iteration 2405/3560 Training loss: 1.3586 1.4577 sec/batch\n",
      "Epoch 14/20  Iteration 2406/3560 Training loss: 1.3585 1.5048 sec/batch\n",
      "Epoch 14/20  Iteration 2407/3560 Training loss: 1.3581 1.4436 sec/batch\n",
      "Epoch 14/20  Iteration 2408/3560 Training loss: 1.3578 1.3948 sec/batch\n",
      "Epoch 14/20  Iteration 2409/3560 Training loss: 1.3576 1.3865 sec/batch\n",
      "Epoch 14/20  Iteration 2410/3560 Training loss: 1.3577 1.3963 sec/batch\n",
      "Epoch 14/20  Iteration 2411/3560 Training loss: 1.3576 1.4041 sec/batch\n",
      "Epoch 14/20  Iteration 2412/3560 Training loss: 1.3572 1.3657 sec/batch\n",
      "Epoch 14/20  Iteration 2413/3560 Training loss: 1.3569 1.4176 sec/batch\n",
      "Epoch 14/20  Iteration 2414/3560 Training loss: 1.3565 1.3962 sec/batch\n",
      "Epoch 14/20  Iteration 2415/3560 Training loss: 1.3564 1.3933 sec/batch\n",
      "Epoch 14/20  Iteration 2416/3560 Training loss: 1.3562 1.3576 sec/batch\n",
      "Epoch 14/20  Iteration 2417/3560 Training loss: 1.3561 1.4339 sec/batch\n",
      "Epoch 14/20  Iteration 2418/3560 Training loss: 1.3559 1.4520 sec/batch\n",
      "Epoch 14/20  Iteration 2419/3560 Training loss: 1.3558 1.4242 sec/batch\n",
      "Epoch 14/20  Iteration 2420/3560 Training loss: 1.3557 1.4800 sec/batch\n",
      "Epoch 14/20  Iteration 2421/3560 Training loss: 1.3557 1.4214 sec/batch\n",
      "Epoch 14/20  Iteration 2422/3560 Training loss: 1.3557 1.4443 sec/batch\n",
      "Epoch 14/20  Iteration 2423/3560 Training loss: 1.3556 1.4698 sec/batch\n",
      "Epoch 14/20  Iteration 2424/3560 Training loss: 1.3556 1.4040 sec/batch\n",
      "Epoch 14/20  Iteration 2425/3560 Training loss: 1.3553 1.3985 sec/batch\n",
      "Epoch 14/20  Iteration 2426/3560 Training loss: 1.3553 1.4247 sec/batch\n",
      "Epoch 14/20  Iteration 2427/3560 Training loss: 1.3552 1.4843 sec/batch\n",
      "Epoch 14/20  Iteration 2428/3560 Training loss: 1.3550 1.3782 sec/batch\n",
      "Epoch 14/20  Iteration 2429/3560 Training loss: 1.3548 1.4338 sec/batch\n",
      "Epoch 14/20  Iteration 2430/3560 Training loss: 1.3544 1.4673 sec/batch\n",
      "Epoch 14/20  Iteration 2431/3560 Training loss: 1.3543 1.4181 sec/batch\n",
      "Epoch 14/20  Iteration 2432/3560 Training loss: 1.3543 1.3987 sec/batch\n",
      "Epoch 14/20  Iteration 2433/3560 Training loss: 1.3542 1.4005 sec/batch\n",
      "Epoch 14/20  Iteration 2434/3560 Training loss: 1.3543 1.3937 sec/batch\n",
      "Epoch 14/20  Iteration 2435/3560 Training loss: 1.3541 1.3551 sec/batch\n",
      "Epoch 14/20  Iteration 2436/3560 Training loss: 1.3538 1.3691 sec/batch\n",
      "Epoch 14/20  Iteration 2437/3560 Training loss: 1.3534 1.3499 sec/batch\n",
      "Epoch 14/20  Iteration 2438/3560 Training loss: 1.3534 1.3717 sec/batch\n",
      "Epoch 14/20  Iteration 2439/3560 Training loss: 1.3532 1.3655 sec/batch\n",
      "Epoch 14/20  Iteration 2440/3560 Training loss: 1.3527 1.3910 sec/batch\n",
      "Epoch 14/20  Iteration 2441/3560 Training loss: 1.3528 1.3718 sec/batch\n",
      "Epoch 14/20  Iteration 2442/3560 Training loss: 1.3529 1.4037 sec/batch\n",
      "Epoch 14/20  Iteration 2443/3560 Training loss: 1.3526 1.3652 sec/batch\n",
      "Epoch 14/20  Iteration 2444/3560 Training loss: 1.3523 1.3707 sec/batch\n",
      "Epoch 14/20  Iteration 2445/3560 Training loss: 1.3519 1.3873 sec/batch\n",
      "Epoch 14/20  Iteration 2446/3560 Training loss: 1.3517 1.4024 sec/batch\n",
      "Epoch 14/20  Iteration 2447/3560 Training loss: 1.3519 1.3904 sec/batch\n",
      "Epoch 14/20  Iteration 2448/3560 Training loss: 1.3520 1.3606 sec/batch\n",
      "Epoch 14/20  Iteration 2449/3560 Training loss: 1.3519 1.4433 sec/batch\n",
      "Epoch 14/20  Iteration 2450/3560 Training loss: 1.3520 1.3834 sec/batch\n",
      "Epoch 14/20  Iteration 2451/3560 Training loss: 1.3521 1.4659 sec/batch\n",
      "Epoch 14/20  Iteration 2452/3560 Training loss: 1.3522 1.4900 sec/batch\n",
      "Epoch 14/20  Iteration 2453/3560 Training loss: 1.3522 1.4379 sec/batch\n",
      "Epoch 14/20  Iteration 2454/3560 Training loss: 1.3522 1.4192 sec/batch\n",
      "Epoch 14/20  Iteration 2455/3560 Training loss: 1.3526 1.4378 sec/batch\n",
      "Epoch 14/20  Iteration 2456/3560 Training loss: 1.3526 1.5014 sec/batch\n",
      "Epoch 14/20  Iteration 2457/3560 Training loss: 1.3525 1.4337 sec/batch\n",
      "Epoch 14/20  Iteration 2458/3560 Training loss: 1.3527 1.4623 sec/batch\n",
      "Epoch 14/20  Iteration 2459/3560 Training loss: 1.3526 1.4615 sec/batch\n",
      "Epoch 14/20  Iteration 2460/3560 Training loss: 1.3527 1.4346 sec/batch\n",
      "Epoch 14/20  Iteration 2461/3560 Training loss: 1.3527 1.4660 sec/batch\n",
      "Epoch 14/20  Iteration 2462/3560 Training loss: 1.3529 1.4839 sec/batch\n",
      "Epoch 14/20  Iteration 2463/3560 Training loss: 1.3531 1.5203 sec/batch\n",
      "Epoch 14/20  Iteration 2464/3560 Training loss: 1.3529 1.4621 sec/batch\n",
      "Epoch 14/20  Iteration 2465/3560 Training loss: 1.3526 1.4463 sec/batch\n",
      "Epoch 14/20  Iteration 2466/3560 Training loss: 1.3524 1.4763 sec/batch\n",
      "Epoch 14/20  Iteration 2467/3560 Training loss: 1.3525 1.4515 sec/batch\n",
      "Epoch 14/20  Iteration 2468/3560 Training loss: 1.3525 1.4792 sec/batch\n",
      "Epoch 14/20  Iteration 2469/3560 Training loss: 1.3524 1.4273 sec/batch\n",
      "Epoch 14/20  Iteration 2470/3560 Training loss: 1.3524 1.4776 sec/batch\n",
      "Epoch 14/20  Iteration 2471/3560 Training loss: 1.3524 1.4292 sec/batch\n",
      "Epoch 14/20  Iteration 2472/3560 Training loss: 1.3523 1.4785 sec/batch\n",
      "Epoch 14/20  Iteration 2473/3560 Training loss: 1.3520 1.4688 sec/batch\n",
      "Epoch 14/20  Iteration 2474/3560 Training loss: 1.3520 1.4870 sec/batch\n",
      "Epoch 14/20  Iteration 2475/3560 Training loss: 1.3522 1.4738 sec/batch\n",
      "Epoch 14/20  Iteration 2476/3560 Training loss: 1.3521 1.4872 sec/batch\n",
      "Epoch 14/20  Iteration 2477/3560 Training loss: 1.3521 1.5383 sec/batch\n",
      "Epoch 14/20  Iteration 2478/3560 Training loss: 1.3520 1.4841 sec/batch\n",
      "Epoch 14/20  Iteration 2479/3560 Training loss: 1.3520 1.5049 sec/batch\n",
      "Epoch 14/20  Iteration 2480/3560 Training loss: 1.3520 1.4460 sec/batch\n",
      "Epoch 14/20  Iteration 2481/3560 Training loss: 1.3521 1.3864 sec/batch\n",
      "Epoch 14/20  Iteration 2482/3560 Training loss: 1.3525 1.4284 sec/batch\n",
      "Epoch 14/20  Iteration 2483/3560 Training loss: 1.3525 1.3830 sec/batch\n",
      "Epoch 14/20  Iteration 2484/3560 Training loss: 1.3525 1.4214 sec/batch\n",
      "Epoch 14/20  Iteration 2485/3560 Training loss: 1.3524 1.3425 sec/batch\n",
      "Epoch 14/20  Iteration 2486/3560 Training loss: 1.3522 1.8475 sec/batch\n",
      "Epoch 14/20  Iteration 2487/3560 Training loss: 1.3524 1.5604 sec/batch\n",
      "Epoch 14/20  Iteration 2488/3560 Training loss: 1.3524 1.4040 sec/batch\n",
      "Epoch 14/20  Iteration 2489/3560 Training loss: 1.3524 1.3988 sec/batch\n",
      "Epoch 14/20  Iteration 2490/3560 Training loss: 1.3522 1.4213 sec/batch\n",
      "Epoch 14/20  Iteration 2491/3560 Training loss: 1.3520 1.4520 sec/batch\n",
      "Epoch 14/20  Iteration 2492/3560 Training loss: 1.3522 1.4116 sec/batch\n",
      "Epoch 15/20  Iteration 2493/3560 Training loss: 1.4681 1.3849 sec/batch\n",
      "Epoch 15/20  Iteration 2494/3560 Training loss: 1.4134 1.3865 sec/batch\n",
      "Epoch 15/20  Iteration 2495/3560 Training loss: 1.3879 1.3945 sec/batch\n",
      "Epoch 15/20  Iteration 2496/3560 Training loss: 1.3783 1.4444 sec/batch\n",
      "Epoch 15/20  Iteration 2497/3560 Training loss: 1.3676 1.3796 sec/batch\n",
      "Epoch 15/20  Iteration 2498/3560 Training loss: 1.3552 1.4539 sec/batch\n",
      "Epoch 15/20  Iteration 2499/3560 Training loss: 1.3557 1.3833 sec/batch\n",
      "Epoch 15/20  Iteration 2500/3560 Training loss: 1.3528 1.4183 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2501/3560 Training loss: 1.3512 1.4014 sec/batch\n",
      "Epoch 15/20  Iteration 2502/3560 Training loss: 1.3506 1.3708 sec/batch\n",
      "Epoch 15/20  Iteration 2503/3560 Training loss: 1.3460 1.3888 sec/batch\n",
      "Epoch 15/20  Iteration 2504/3560 Training loss: 1.3462 1.3746 sec/batch\n",
      "Epoch 15/20  Iteration 2505/3560 Training loss: 1.3460 1.4254 sec/batch\n",
      "Epoch 15/20  Iteration 2506/3560 Training loss: 1.3471 1.3472 sec/batch\n",
      "Epoch 15/20  Iteration 2507/3560 Training loss: 1.3459 1.4466 sec/batch\n",
      "Epoch 15/20  Iteration 2508/3560 Training loss: 1.3445 1.3926 sec/batch\n",
      "Epoch 15/20  Iteration 2509/3560 Training loss: 1.3447 1.3733 sec/batch\n",
      "Epoch 15/20  Iteration 2510/3560 Training loss: 1.3456 1.3706 sec/batch\n",
      "Epoch 15/20  Iteration 2511/3560 Training loss: 1.3454 1.4120 sec/batch\n",
      "Epoch 15/20  Iteration 2512/3560 Training loss: 1.3465 1.4404 sec/batch\n",
      "Epoch 15/20  Iteration 2513/3560 Training loss: 1.3462 1.3949 sec/batch\n",
      "Epoch 15/20  Iteration 2514/3560 Training loss: 1.3466 1.4288 sec/batch\n",
      "Epoch 15/20  Iteration 2515/3560 Training loss: 1.3459 1.4208 sec/batch\n",
      "Epoch 15/20  Iteration 2516/3560 Training loss: 1.3456 1.4603 sec/batch\n",
      "Epoch 15/20  Iteration 2517/3560 Training loss: 1.3454 1.4259 sec/batch\n",
      "Epoch 15/20  Iteration 2518/3560 Training loss: 1.3436 1.4657 sec/batch\n",
      "Epoch 15/20  Iteration 2519/3560 Training loss: 1.3423 1.4671 sec/batch\n",
      "Epoch 15/20  Iteration 2520/3560 Training loss: 1.3430 1.5101 sec/batch\n",
      "Epoch 15/20  Iteration 2521/3560 Training loss: 1.3432 1.4712 sec/batch\n",
      "Epoch 15/20  Iteration 2522/3560 Training loss: 1.3436 1.4359 sec/batch\n",
      "Epoch 15/20  Iteration 2523/3560 Training loss: 1.3431 1.4262 sec/batch\n",
      "Epoch 15/20  Iteration 2524/3560 Training loss: 1.3423 1.4610 sec/batch\n",
      "Epoch 15/20  Iteration 2525/3560 Training loss: 1.3426 1.4392 sec/batch\n",
      "Epoch 15/20  Iteration 2526/3560 Training loss: 1.3429 1.4672 sec/batch\n",
      "Epoch 15/20  Iteration 2527/3560 Training loss: 1.3425 1.4836 sec/batch\n",
      "Epoch 15/20  Iteration 2528/3560 Training loss: 1.3425 1.4708 sec/batch\n",
      "Epoch 15/20  Iteration 2529/3560 Training loss: 1.3417 1.4405 sec/batch\n",
      "Epoch 15/20  Iteration 2530/3560 Training loss: 1.3407 1.3844 sec/batch\n",
      "Epoch 15/20  Iteration 2531/3560 Training loss: 1.3394 1.3587 sec/batch\n",
      "Epoch 15/20  Iteration 2532/3560 Training loss: 1.3389 1.4186 sec/batch\n",
      "Epoch 15/20  Iteration 2533/3560 Training loss: 1.3384 1.4929 sec/batch\n",
      "Epoch 15/20  Iteration 2534/3560 Training loss: 1.3391 1.4060 sec/batch\n",
      "Epoch 15/20  Iteration 2535/3560 Training loss: 1.3388 1.5243 sec/batch\n",
      "Epoch 15/20  Iteration 2536/3560 Training loss: 1.3379 1.4892 sec/batch\n",
      "Epoch 15/20  Iteration 2537/3560 Training loss: 1.3380 1.4839 sec/batch\n",
      "Epoch 15/20  Iteration 2538/3560 Training loss: 1.3372 1.4898 sec/batch\n",
      "Epoch 15/20  Iteration 2539/3560 Training loss: 1.3366 1.4805 sec/batch\n",
      "Epoch 15/20  Iteration 2540/3560 Training loss: 1.3363 1.5295 sec/batch\n",
      "Epoch 15/20  Iteration 2541/3560 Training loss: 1.3362 1.4444 sec/batch\n",
      "Epoch 15/20  Iteration 2542/3560 Training loss: 1.3365 1.4194 sec/batch\n",
      "Epoch 15/20  Iteration 2543/3560 Training loss: 1.3359 1.3971 sec/batch\n",
      "Epoch 15/20  Iteration 2544/3560 Training loss: 1.3368 1.3992 sec/batch\n",
      "Epoch 15/20  Iteration 2545/3560 Training loss: 1.3367 1.4261 sec/batch\n",
      "Epoch 15/20  Iteration 2546/3560 Training loss: 1.3366 1.3858 sec/batch\n",
      "Epoch 15/20  Iteration 2547/3560 Training loss: 1.3364 1.4290 sec/batch\n",
      "Epoch 15/20  Iteration 2548/3560 Training loss: 1.3365 1.3841 sec/batch\n",
      "Epoch 15/20  Iteration 2549/3560 Training loss: 1.3368 1.3939 sec/batch\n",
      "Epoch 15/20  Iteration 2550/3560 Training loss: 1.3367 1.4262 sec/batch\n",
      "Epoch 15/20  Iteration 2551/3560 Training loss: 1.3362 1.4339 sec/batch\n",
      "Epoch 15/20  Iteration 2552/3560 Training loss: 1.3367 1.4410 sec/batch\n",
      "Epoch 15/20  Iteration 2553/3560 Training loss: 1.3365 1.4577 sec/batch\n",
      "Epoch 15/20  Iteration 2554/3560 Training loss: 1.3373 1.5343 sec/batch\n",
      "Epoch 15/20  Iteration 2555/3560 Training loss: 1.3374 1.5162 sec/batch\n",
      "Epoch 15/20  Iteration 2556/3560 Training loss: 1.3375 1.5204 sec/batch\n",
      "Epoch 15/20  Iteration 2557/3560 Training loss: 1.3373 1.4680 sec/batch\n",
      "Epoch 15/20  Iteration 2558/3560 Training loss: 1.3376 1.4486 sec/batch\n",
      "Epoch 15/20  Iteration 2559/3560 Training loss: 1.3378 1.4078 sec/batch\n",
      "Epoch 15/20  Iteration 2560/3560 Training loss: 1.3375 1.4119 sec/batch\n",
      "Epoch 15/20  Iteration 2561/3560 Training loss: 1.3375 1.4833 sec/batch\n",
      "Epoch 15/20  Iteration 2562/3560 Training loss: 1.3374 1.4073 sec/batch\n",
      "Epoch 15/20  Iteration 2563/3560 Training loss: 1.3380 1.4415 sec/batch\n",
      "Epoch 15/20  Iteration 2564/3560 Training loss: 1.3383 1.4307 sec/batch\n",
      "Epoch 15/20  Iteration 2565/3560 Training loss: 1.3387 1.3941 sec/batch\n",
      "Epoch 15/20  Iteration 2566/3560 Training loss: 1.3384 1.4270 sec/batch\n",
      "Epoch 15/20  Iteration 2567/3560 Training loss: 1.3384 1.4048 sec/batch\n",
      "Epoch 15/20  Iteration 2568/3560 Training loss: 1.3385 1.5207 sec/batch\n",
      "Epoch 15/20  Iteration 2569/3560 Training loss: 1.3384 1.4494 sec/batch\n",
      "Epoch 15/20  Iteration 2570/3560 Training loss: 1.3383 1.4494 sec/batch\n",
      "Epoch 15/20  Iteration 2571/3560 Training loss: 1.3376 1.5115 sec/batch\n",
      "Epoch 15/20  Iteration 2572/3560 Training loss: 1.3375 1.4448 sec/batch\n",
      "Epoch 15/20  Iteration 2573/3560 Training loss: 1.3370 1.4605 sec/batch\n",
      "Epoch 15/20  Iteration 2574/3560 Training loss: 1.3370 1.4439 sec/batch\n",
      "Epoch 15/20  Iteration 2575/3560 Training loss: 1.3364 1.5062 sec/batch\n",
      "Epoch 15/20  Iteration 2576/3560 Training loss: 1.3365 1.4521 sec/batch\n",
      "Epoch 15/20  Iteration 2577/3560 Training loss: 1.3362 1.4733 sec/batch\n",
      "Epoch 15/20  Iteration 2578/3560 Training loss: 1.3361 1.4170 sec/batch\n",
      "Epoch 15/20  Iteration 2579/3560 Training loss: 1.3358 1.3797 sec/batch\n",
      "Epoch 15/20  Iteration 2580/3560 Training loss: 1.3356 1.3875 sec/batch\n",
      "Epoch 15/20  Iteration 2581/3560 Training loss: 1.3352 1.3846 sec/batch\n",
      "Epoch 15/20  Iteration 2582/3560 Training loss: 1.3351 1.5012 sec/batch\n",
      "Epoch 15/20  Iteration 2583/3560 Training loss: 1.3350 1.4664 sec/batch\n",
      "Epoch 15/20  Iteration 2584/3560 Training loss: 1.3349 1.4771 sec/batch\n",
      "Epoch 15/20  Iteration 2585/3560 Training loss: 1.3344 1.3906 sec/batch\n",
      "Epoch 15/20  Iteration 2586/3560 Training loss: 1.3341 1.3837 sec/batch\n",
      "Epoch 15/20  Iteration 2587/3560 Training loss: 1.3339 1.3798 sec/batch\n",
      "Epoch 15/20  Iteration 2588/3560 Training loss: 1.3340 1.4551 sec/batch\n",
      "Epoch 15/20  Iteration 2589/3560 Training loss: 1.3340 1.4515 sec/batch\n",
      "Epoch 15/20  Iteration 2590/3560 Training loss: 1.3337 1.6503 sec/batch\n",
      "Epoch 15/20  Iteration 2591/3560 Training loss: 1.3335 1.5205 sec/batch\n",
      "Epoch 15/20  Iteration 2592/3560 Training loss: 1.3332 1.5668 sec/batch\n",
      "Epoch 15/20  Iteration 2593/3560 Training loss: 1.3332 1.4738 sec/batch\n",
      "Epoch 15/20  Iteration 2594/3560 Training loss: 1.3331 1.5686 sec/batch\n",
      "Epoch 15/20  Iteration 2595/3560 Training loss: 1.3330 1.5128 sec/batch\n",
      "Epoch 15/20  Iteration 2596/3560 Training loss: 1.3329 1.5982 sec/batch\n",
      "Epoch 15/20  Iteration 2597/3560 Training loss: 1.3329 1.5348 sec/batch\n",
      "Epoch 15/20  Iteration 2598/3560 Training loss: 1.3327 1.4482 sec/batch\n",
      "Epoch 15/20  Iteration 2599/3560 Training loss: 1.3327 1.4497 sec/batch\n",
      "Epoch 15/20  Iteration 2600/3560 Training loss: 1.3328 1.3856 sec/batch\n",
      "Epoch 15/20  Iteration 2601/3560 Training loss: 1.3326 1.3760 sec/batch\n",
      "Epoch 15/20  Iteration 2602/3560 Training loss: 1.3326 1.3891 sec/batch\n",
      "Epoch 15/20  Iteration 2603/3560 Training loss: 1.3324 1.6138 sec/batch\n",
      "Epoch 15/20  Iteration 2604/3560 Training loss: 1.3323 1.6589 sec/batch\n",
      "Epoch 15/20  Iteration 2605/3560 Training loss: 1.3321 1.3746 sec/batch\n",
      "Epoch 15/20  Iteration 2606/3560 Training loss: 1.3319 1.3911 sec/batch\n",
      "Epoch 15/20  Iteration 2607/3560 Training loss: 1.3317 1.4089 sec/batch\n",
      "Epoch 15/20  Iteration 2608/3560 Training loss: 1.3314 1.4440 sec/batch\n",
      "Epoch 15/20  Iteration 2609/3560 Training loss: 1.3314 1.4270 sec/batch\n",
      "Epoch 15/20  Iteration 2610/3560 Training loss: 1.3314 1.4752 sec/batch\n",
      "Epoch 15/20  Iteration 2611/3560 Training loss: 1.3312 1.4172 sec/batch\n",
      "Epoch 15/20  Iteration 2612/3560 Training loss: 1.3312 1.5191 sec/batch\n",
      "Epoch 15/20  Iteration 2613/3560 Training loss: 1.3311 1.4111 sec/batch\n",
      "Epoch 15/20  Iteration 2614/3560 Training loss: 1.3307 1.4083 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2615/3560 Training loss: 1.3303 1.4628 sec/batch\n",
      "Epoch 15/20  Iteration 2616/3560 Training loss: 1.3303 1.5827 sec/batch\n",
      "Epoch 15/20  Iteration 2617/3560 Training loss: 1.3302 1.6353 sec/batch\n",
      "Epoch 15/20  Iteration 2618/3560 Training loss: 1.3297 1.5933 sec/batch\n",
      "Epoch 15/20  Iteration 2619/3560 Training loss: 1.3297 1.4127 sec/batch\n",
      "Epoch 15/20  Iteration 2620/3560 Training loss: 1.3298 1.4239 sec/batch\n",
      "Epoch 15/20  Iteration 2621/3560 Training loss: 1.3296 1.4239 sec/batch\n",
      "Epoch 15/20  Iteration 2622/3560 Training loss: 1.3292 1.4101 sec/batch\n",
      "Epoch 15/20  Iteration 2623/3560 Training loss: 1.3288 1.3788 sec/batch\n",
      "Epoch 15/20  Iteration 2624/3560 Training loss: 1.3287 1.4531 sec/batch\n",
      "Epoch 15/20  Iteration 2625/3560 Training loss: 1.3288 1.3728 sec/batch\n",
      "Epoch 15/20  Iteration 2626/3560 Training loss: 1.3288 1.3916 sec/batch\n",
      "Epoch 15/20  Iteration 2627/3560 Training loss: 1.3288 1.3980 sec/batch\n",
      "Epoch 15/20  Iteration 2628/3560 Training loss: 1.3289 1.3740 sec/batch\n",
      "Epoch 15/20  Iteration 2629/3560 Training loss: 1.3290 1.4036 sec/batch\n",
      "Epoch 15/20  Iteration 2630/3560 Training loss: 1.3291 1.4376 sec/batch\n",
      "Epoch 15/20  Iteration 2631/3560 Training loss: 1.3291 1.5226 sec/batch\n",
      "Epoch 15/20  Iteration 2632/3560 Training loss: 1.3291 1.4643 sec/batch\n",
      "Epoch 15/20  Iteration 2633/3560 Training loss: 1.3295 1.5097 sec/batch\n",
      "Epoch 15/20  Iteration 2634/3560 Training loss: 1.3295 1.4363 sec/batch\n",
      "Epoch 15/20  Iteration 2635/3560 Training loss: 1.3295 1.4576 sec/batch\n",
      "Epoch 15/20  Iteration 2636/3560 Training loss: 1.3298 1.5309 sec/batch\n",
      "Epoch 15/20  Iteration 2637/3560 Training loss: 1.3296 1.5460 sec/batch\n",
      "Epoch 15/20  Iteration 2638/3560 Training loss: 1.3298 1.5741 sec/batch\n",
      "Epoch 15/20  Iteration 2639/3560 Training loss: 1.3297 1.5214 sec/batch\n",
      "Epoch 15/20  Iteration 2640/3560 Training loss: 1.3300 1.4954 sec/batch\n",
      "Epoch 15/20  Iteration 2641/3560 Training loss: 1.3302 1.5256 sec/batch\n",
      "Epoch 15/20  Iteration 2642/3560 Training loss: 1.3300 1.5701 sec/batch\n",
      "Epoch 15/20  Iteration 2643/3560 Training loss: 1.3297 1.7305 sec/batch\n",
      "Epoch 15/20  Iteration 2644/3560 Training loss: 1.3295 1.5774 sec/batch\n",
      "Epoch 15/20  Iteration 2645/3560 Training loss: 1.3296 1.5252 sec/batch\n",
      "Epoch 15/20  Iteration 2646/3560 Training loss: 1.3296 1.5093 sec/batch\n",
      "Epoch 15/20  Iteration 2647/3560 Training loss: 1.3296 1.4527 sec/batch\n",
      "Epoch 15/20  Iteration 2648/3560 Training loss: 1.3296 1.4549 sec/batch\n",
      "Epoch 15/20  Iteration 2649/3560 Training loss: 1.3295 1.4419 sec/batch\n",
      "Epoch 15/20  Iteration 2650/3560 Training loss: 1.3294 1.4020 sec/batch\n",
      "Epoch 15/20  Iteration 2651/3560 Training loss: 1.3292 1.4236 sec/batch\n",
      "Epoch 15/20  Iteration 2652/3560 Training loss: 1.3293 1.3771 sec/batch\n",
      "Epoch 15/20  Iteration 2653/3560 Training loss: 1.3294 1.4481 sec/batch\n",
      "Epoch 15/20  Iteration 2654/3560 Training loss: 1.3294 1.3683 sec/batch\n",
      "Epoch 15/20  Iteration 2655/3560 Training loss: 1.3294 1.3646 sec/batch\n",
      "Epoch 15/20  Iteration 2656/3560 Training loss: 1.3294 1.3695 sec/batch\n",
      "Epoch 15/20  Iteration 2657/3560 Training loss: 1.3294 1.4029 sec/batch\n",
      "Epoch 15/20  Iteration 2658/3560 Training loss: 1.3293 1.4561 sec/batch\n",
      "Epoch 15/20  Iteration 2659/3560 Training loss: 1.3295 1.4080 sec/batch\n",
      "Epoch 15/20  Iteration 2660/3560 Training loss: 1.3299 1.4745 sec/batch\n",
      "Epoch 15/20  Iteration 2661/3560 Training loss: 1.3299 1.5000 sec/batch\n",
      "Epoch 15/20  Iteration 2662/3560 Training loss: 1.3299 1.5038 sec/batch\n",
      "Epoch 15/20  Iteration 2663/3560 Training loss: 1.3299 1.5417 sec/batch\n",
      "Epoch 15/20  Iteration 2664/3560 Training loss: 1.3297 1.4669 sec/batch\n",
      "Epoch 15/20  Iteration 2665/3560 Training loss: 1.3299 1.4998 sec/batch\n",
      "Epoch 15/20  Iteration 2666/3560 Training loss: 1.3299 1.4874 sec/batch\n",
      "Epoch 15/20  Iteration 2667/3560 Training loss: 1.3300 1.4048 sec/batch\n",
      "Epoch 15/20  Iteration 2668/3560 Training loss: 1.3298 1.4286 sec/batch\n",
      "Epoch 15/20  Iteration 2669/3560 Training loss: 1.3297 1.4501 sec/batch\n",
      "Epoch 15/20  Iteration 2670/3560 Training loss: 1.3298 1.4703 sec/batch\n",
      "Epoch 16/20  Iteration 2671/3560 Training loss: 1.4450 1.4821 sec/batch\n",
      "Epoch 16/20  Iteration 2672/3560 Training loss: 1.3896 1.5273 sec/batch\n",
      "Epoch 16/20  Iteration 2673/3560 Training loss: 1.3682 1.5189 sec/batch\n",
      "Epoch 16/20  Iteration 2674/3560 Training loss: 1.3597 1.5456 sec/batch\n",
      "Epoch 16/20  Iteration 2675/3560 Training loss: 1.3467 1.5059 sec/batch\n",
      "Epoch 16/20  Iteration 2676/3560 Training loss: 1.3351 1.4941 sec/batch\n",
      "Epoch 16/20  Iteration 2677/3560 Training loss: 1.3350 1.4645 sec/batch\n",
      "Epoch 16/20  Iteration 2678/3560 Training loss: 1.3323 1.4614 sec/batch\n",
      "Epoch 16/20  Iteration 2679/3560 Training loss: 1.3306 1.5679 sec/batch\n",
      "Epoch 16/20  Iteration 2680/3560 Training loss: 1.3296 1.4543 sec/batch\n",
      "Epoch 16/20  Iteration 2681/3560 Training loss: 1.3258 1.4887 sec/batch\n",
      "Epoch 16/20  Iteration 2682/3560 Training loss: 1.3252 1.5007 sec/batch\n",
      "Epoch 16/20  Iteration 2683/3560 Training loss: 1.3251 1.5155 sec/batch\n",
      "Epoch 16/20  Iteration 2684/3560 Training loss: 1.3259 1.4669 sec/batch\n",
      "Epoch 16/20  Iteration 2685/3560 Training loss: 1.3249 1.4808 sec/batch\n",
      "Epoch 16/20  Iteration 2686/3560 Training loss: 1.3233 1.4761 sec/batch\n",
      "Epoch 16/20  Iteration 2687/3560 Training loss: 1.3235 1.4203 sec/batch\n",
      "Epoch 16/20  Iteration 2688/3560 Training loss: 1.3248 1.4294 sec/batch\n",
      "Epoch 16/20  Iteration 2689/3560 Training loss: 1.3245 1.4485 sec/batch\n",
      "Epoch 16/20  Iteration 2690/3560 Training loss: 1.3256 1.4747 sec/batch\n",
      "Epoch 16/20  Iteration 2691/3560 Training loss: 1.3251 1.4743 sec/batch\n",
      "Epoch 16/20  Iteration 2692/3560 Training loss: 1.3252 1.4891 sec/batch\n",
      "Epoch 16/20  Iteration 2693/3560 Training loss: 1.3246 1.5036 sec/batch\n",
      "Epoch 16/20  Iteration 2694/3560 Training loss: 1.3244 1.5074 sec/batch\n",
      "Epoch 16/20  Iteration 2695/3560 Training loss: 1.3241 1.4310 sec/batch\n",
      "Epoch 16/20  Iteration 2696/3560 Training loss: 1.3220 1.3879 sec/batch\n",
      "Epoch 16/20  Iteration 2697/3560 Training loss: 1.3205 1.3765 sec/batch\n",
      "Epoch 16/20  Iteration 2698/3560 Training loss: 1.3214 1.5292 sec/batch\n",
      "Epoch 16/20  Iteration 2699/3560 Training loss: 1.3214 1.5281 sec/batch\n",
      "Epoch 16/20  Iteration 2700/3560 Training loss: 1.3216 1.5686 sec/batch\n",
      "Epoch 16/20  Iteration 2701/3560 Training loss: 1.3211 1.4932 sec/batch\n",
      "Epoch 16/20  Iteration 2702/3560 Training loss: 1.3201 1.4823 sec/batch\n",
      "Epoch 16/20  Iteration 2703/3560 Training loss: 1.3201 1.5208 sec/batch\n",
      "Epoch 16/20  Iteration 2704/3560 Training loss: 1.3204 1.4987 sec/batch\n",
      "Epoch 16/20  Iteration 2705/3560 Training loss: 1.3201 1.4733 sec/batch\n",
      "Epoch 16/20  Iteration 2706/3560 Training loss: 1.3201 1.4963 sec/batch\n",
      "Epoch 16/20  Iteration 2707/3560 Training loss: 1.3195 1.4098 sec/batch\n",
      "Epoch 16/20  Iteration 2708/3560 Training loss: 1.3187 1.4406 sec/batch\n",
      "Epoch 16/20  Iteration 2709/3560 Training loss: 1.3174 1.4083 sec/batch\n",
      "Epoch 16/20  Iteration 2710/3560 Training loss: 1.3170 1.3521 sec/batch\n",
      "Epoch 16/20  Iteration 2711/3560 Training loss: 1.3163 1.4159 sec/batch\n",
      "Epoch 16/20  Iteration 2712/3560 Training loss: 1.3173 1.4073 sec/batch\n",
      "Epoch 16/20  Iteration 2713/3560 Training loss: 1.3172 1.4176 sec/batch\n",
      "Epoch 16/20  Iteration 2714/3560 Training loss: 1.3165 1.3717 sec/batch\n",
      "Epoch 16/20  Iteration 2715/3560 Training loss: 1.3166 1.3926 sec/batch\n",
      "Epoch 16/20  Iteration 2716/3560 Training loss: 1.3161 1.4011 sec/batch\n",
      "Epoch 16/20  Iteration 2717/3560 Training loss: 1.3156 1.3716 sec/batch\n",
      "Epoch 16/20  Iteration 2718/3560 Training loss: 1.3152 1.3694 sec/batch\n",
      "Epoch 16/20  Iteration 2719/3560 Training loss: 1.3151 1.4306 sec/batch\n",
      "Epoch 16/20  Iteration 2720/3560 Training loss: 1.3156 1.3885 sec/batch\n",
      "Epoch 16/20  Iteration 2721/3560 Training loss: 1.3152 1.4605 sec/batch\n",
      "Epoch 16/20  Iteration 2722/3560 Training loss: 1.3160 1.3651 sec/batch\n",
      "Epoch 16/20  Iteration 2723/3560 Training loss: 1.3161 1.4211 sec/batch\n",
      "Epoch 16/20  Iteration 2724/3560 Training loss: 1.3165 1.4196 sec/batch\n",
      "Epoch 16/20  Iteration 2725/3560 Training loss: 1.3164 1.4094 sec/batch\n",
      "Epoch 16/20  Iteration 2726/3560 Training loss: 1.3167 1.5998 sec/batch\n",
      "Epoch 16/20  Iteration 2727/3560 Training loss: 1.3172 1.6578 sec/batch\n",
      "Epoch 16/20  Iteration 2728/3560 Training loss: 1.3170 1.4769 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2729/3560 Training loss: 1.3166 1.4312 sec/batch\n",
      "Epoch 16/20  Iteration 2730/3560 Training loss: 1.3173 1.4174 sec/batch\n",
      "Epoch 16/20  Iteration 2731/3560 Training loss: 1.3172 1.4038 sec/batch\n",
      "Epoch 16/20  Iteration 2732/3560 Training loss: 1.3180 1.4609 sec/batch\n",
      "Epoch 16/20  Iteration 2733/3560 Training loss: 1.3181 1.4959 sec/batch\n",
      "Epoch 16/20  Iteration 2734/3560 Training loss: 1.3183 1.6215 sec/batch\n",
      "Epoch 16/20  Iteration 2735/3560 Training loss: 1.3183 1.6748 sec/batch\n",
      "Epoch 16/20  Iteration 2736/3560 Training loss: 1.3186 1.5513 sec/batch\n",
      "Epoch 16/20  Iteration 2737/3560 Training loss: 1.3189 1.6342 sec/batch\n",
      "Epoch 16/20  Iteration 2738/3560 Training loss: 1.3187 1.6852 sec/batch\n",
      "Epoch 16/20  Iteration 2739/3560 Training loss: 1.3187 1.4634 sec/batch\n",
      "Epoch 16/20  Iteration 2740/3560 Training loss: 1.3186 1.4491 sec/batch\n",
      "Epoch 16/20  Iteration 2741/3560 Training loss: 1.3191 1.9820 sec/batch\n",
      "Epoch 16/20  Iteration 2742/3560 Training loss: 1.3196 1.7070 sec/batch\n",
      "Epoch 16/20  Iteration 2743/3560 Training loss: 1.3202 1.7705 sec/batch\n",
      "Epoch 16/20  Iteration 2744/3560 Training loss: 1.3200 1.9610 sec/batch\n",
      "Epoch 16/20  Iteration 2745/3560 Training loss: 1.3199 1.7244 sec/batch\n",
      "Epoch 16/20  Iteration 2746/3560 Training loss: 1.3201 2.1057 sec/batch\n",
      "Epoch 16/20  Iteration 2747/3560 Training loss: 1.3201 2.0146 sec/batch\n",
      "Epoch 16/20  Iteration 2748/3560 Training loss: 1.3201 1.6410 sec/batch\n",
      "Epoch 16/20  Iteration 2749/3560 Training loss: 1.3195 1.7356 sec/batch\n",
      "Epoch 16/20  Iteration 2750/3560 Training loss: 1.3195 1.8238 sec/batch\n",
      "Epoch 16/20  Iteration 2751/3560 Training loss: 1.3192 1.5510 sec/batch\n",
      "Epoch 16/20  Iteration 2752/3560 Training loss: 1.3192 1.8587 sec/batch\n",
      "Epoch 16/20  Iteration 2753/3560 Training loss: 1.3188 1.7138 sec/batch\n",
      "Epoch 16/20  Iteration 2754/3560 Training loss: 1.3189 1.6598 sec/batch\n",
      "Epoch 16/20  Iteration 2755/3560 Training loss: 1.3187 1.9043 sec/batch\n",
      "Epoch 16/20  Iteration 2756/3560 Training loss: 1.3187 1.3794 sec/batch\n",
      "Epoch 16/20  Iteration 2757/3560 Training loss: 1.3184 1.4427 sec/batch\n",
      "Epoch 16/20  Iteration 2758/3560 Training loss: 1.3182 1.5564 sec/batch\n",
      "Epoch 16/20  Iteration 2759/3560 Training loss: 1.3179 1.4998 sec/batch\n",
      "Epoch 16/20  Iteration 2760/3560 Training loss: 1.3179 1.4075 sec/batch\n",
      "Epoch 16/20  Iteration 2761/3560 Training loss: 1.3178 1.4641 sec/batch\n",
      "Epoch 16/20  Iteration 2762/3560 Training loss: 1.3178 1.4449 sec/batch\n",
      "Epoch 16/20  Iteration 2763/3560 Training loss: 1.3172 1.4077 sec/batch\n",
      "Epoch 16/20  Iteration 2764/3560 Training loss: 1.3169 1.4860 sec/batch\n",
      "Epoch 16/20  Iteration 2765/3560 Training loss: 1.3166 1.6652 sec/batch\n",
      "Epoch 16/20  Iteration 2766/3560 Training loss: 1.3168 1.4483 sec/batch\n",
      "Epoch 16/20  Iteration 2767/3560 Training loss: 1.3168 1.4458 sec/batch\n",
      "Epoch 16/20  Iteration 2768/3560 Training loss: 1.3165 1.4434 sec/batch\n",
      "Epoch 16/20  Iteration 2769/3560 Training loss: 1.3161 1.3943 sec/batch\n",
      "Epoch 16/20  Iteration 2770/3560 Training loss: 1.3157 1.4494 sec/batch\n",
      "Epoch 16/20  Iteration 2771/3560 Training loss: 1.3157 1.4409 sec/batch\n",
      "Epoch 16/20  Iteration 2772/3560 Training loss: 1.3156 1.5004 sec/batch\n",
      "Epoch 16/20  Iteration 2773/3560 Training loss: 1.3155 1.4094 sec/batch\n",
      "Epoch 16/20  Iteration 2774/3560 Training loss: 1.3154 1.3949 sec/batch\n",
      "Epoch 16/20  Iteration 2775/3560 Training loss: 1.3153 1.3893 sec/batch\n",
      "Epoch 16/20  Iteration 2776/3560 Training loss: 1.3152 1.5031 sec/batch\n",
      "Epoch 16/20  Iteration 2777/3560 Training loss: 1.3153 1.5759 sec/batch\n",
      "Epoch 16/20  Iteration 2778/3560 Training loss: 1.3153 1.4674 sec/batch\n",
      "Epoch 16/20  Iteration 2779/3560 Training loss: 1.3152 1.5367 sec/batch\n",
      "Epoch 16/20  Iteration 2780/3560 Training loss: 1.3151 1.4002 sec/batch\n",
      "Epoch 16/20  Iteration 2781/3560 Training loss: 1.3149 1.3699 sec/batch\n",
      "Epoch 16/20  Iteration 2782/3560 Training loss: 1.3147 1.4676 sec/batch\n",
      "Epoch 16/20  Iteration 2783/3560 Training loss: 1.3147 1.4442 sec/batch\n",
      "Epoch 16/20  Iteration 2784/3560 Training loss: 1.3145 1.3688 sec/batch\n",
      "Epoch 16/20  Iteration 2785/3560 Training loss: 1.3142 1.4755 sec/batch\n",
      "Epoch 16/20  Iteration 2786/3560 Training loss: 1.3139 1.5510 sec/batch\n",
      "Epoch 16/20  Iteration 2787/3560 Training loss: 1.3138 1.4872 sec/batch\n",
      "Epoch 16/20  Iteration 2788/3560 Training loss: 1.3139 1.4423 sec/batch\n",
      "Epoch 16/20  Iteration 2789/3560 Training loss: 1.3137 1.4194 sec/batch\n",
      "Epoch 16/20  Iteration 2790/3560 Training loss: 1.3136 1.4244 sec/batch\n",
      "Epoch 16/20  Iteration 2791/3560 Training loss: 1.3135 1.4178 sec/batch\n",
      "Epoch 16/20  Iteration 2792/3560 Training loss: 1.3131 1.4203 sec/batch\n",
      "Epoch 16/20  Iteration 2793/3560 Training loss: 1.3127 1.4770 sec/batch\n",
      "Epoch 16/20  Iteration 2794/3560 Training loss: 1.3127 1.3643 sec/batch\n",
      "Epoch 16/20  Iteration 2795/3560 Training loss: 1.3125 1.4289 sec/batch\n",
      "Epoch 16/20  Iteration 2796/3560 Training loss: 1.3120 1.3836 sec/batch\n",
      "Epoch 16/20  Iteration 2797/3560 Training loss: 1.3120 1.4664 sec/batch\n",
      "Epoch 16/20  Iteration 2798/3560 Training loss: 1.3119 1.4554 sec/batch\n",
      "Epoch 16/20  Iteration 2799/3560 Training loss: 1.3117 1.4647 sec/batch\n",
      "Epoch 16/20  Iteration 2800/3560 Training loss: 1.3114 1.5710 sec/batch\n",
      "Epoch 16/20  Iteration 2801/3560 Training loss: 1.3109 1.4703 sec/batch\n",
      "Epoch 16/20  Iteration 2802/3560 Training loss: 1.3107 1.5098 sec/batch\n",
      "Epoch 16/20  Iteration 2803/3560 Training loss: 1.3109 1.4580 sec/batch\n",
      "Epoch 16/20  Iteration 2804/3560 Training loss: 1.3109 1.4689 sec/batch\n",
      "Epoch 16/20  Iteration 2805/3560 Training loss: 1.3108 1.4716 sec/batch\n",
      "Epoch 16/20  Iteration 2806/3560 Training loss: 1.3109 1.4832 sec/batch\n",
      "Epoch 16/20  Iteration 2807/3560 Training loss: 1.3110 1.5281 sec/batch\n",
      "Epoch 16/20  Iteration 2808/3560 Training loss: 1.3111 1.4286 sec/batch\n",
      "Epoch 16/20  Iteration 2809/3560 Training loss: 1.3112 1.4287 sec/batch\n",
      "Epoch 16/20  Iteration 2810/3560 Training loss: 1.3112 1.3906 sec/batch\n",
      "Epoch 16/20  Iteration 2811/3560 Training loss: 1.3115 1.4090 sec/batch\n",
      "Epoch 16/20  Iteration 2812/3560 Training loss: 1.3116 1.3864 sec/batch\n",
      "Epoch 16/20  Iteration 2813/3560 Training loss: 1.3115 1.3938 sec/batch\n",
      "Epoch 16/20  Iteration 2814/3560 Training loss: 1.3118 1.5098 sec/batch\n",
      "Epoch 16/20  Iteration 2815/3560 Training loss: 1.3116 1.4532 sec/batch\n",
      "Epoch 16/20  Iteration 2816/3560 Training loss: 1.3118 1.4274 sec/batch\n",
      "Epoch 16/20  Iteration 2817/3560 Training loss: 1.3117 1.4233 sec/batch\n",
      "Epoch 16/20  Iteration 2818/3560 Training loss: 1.3120 1.4663 sec/batch\n",
      "Epoch 16/20  Iteration 2819/3560 Training loss: 1.3121 1.4596 sec/batch\n",
      "Epoch 16/20  Iteration 2820/3560 Training loss: 1.3119 1.4915 sec/batch\n",
      "Epoch 16/20  Iteration 2821/3560 Training loss: 1.3116 1.6612 sec/batch\n",
      "Epoch 16/20  Iteration 2822/3560 Training loss: 1.3115 1.4460 sec/batch\n",
      "Epoch 16/20  Iteration 2823/3560 Training loss: 1.3116 1.3453 sec/batch\n",
      "Epoch 16/20  Iteration 2824/3560 Training loss: 1.3115 1.3846 sec/batch\n",
      "Epoch 16/20  Iteration 2825/3560 Training loss: 1.3114 1.3804 sec/batch\n",
      "Epoch 16/20  Iteration 2826/3560 Training loss: 1.3114 1.3933 sec/batch\n",
      "Epoch 16/20  Iteration 2827/3560 Training loss: 1.3114 1.3338 sec/batch\n",
      "Epoch 16/20  Iteration 2828/3560 Training loss: 1.3114 1.4235 sec/batch\n",
      "Epoch 16/20  Iteration 2829/3560 Training loss: 1.3111 1.3482 sec/batch\n",
      "Epoch 16/20  Iteration 2830/3560 Training loss: 1.3112 1.3133 sec/batch\n",
      "Epoch 16/20  Iteration 2831/3560 Training loss: 1.3113 1.3475 sec/batch\n",
      "Epoch 16/20  Iteration 2832/3560 Training loss: 1.3112 1.3588 sec/batch\n",
      "Epoch 16/20  Iteration 2833/3560 Training loss: 1.3112 1.3122 sec/batch\n",
      "Epoch 16/20  Iteration 2834/3560 Training loss: 1.3112 1.3514 sec/batch\n",
      "Epoch 16/20  Iteration 2835/3560 Training loss: 1.3112 1.4353 sec/batch\n",
      "Epoch 16/20  Iteration 2836/3560 Training loss: 1.3111 1.4480 sec/batch\n",
      "Epoch 16/20  Iteration 2837/3560 Training loss: 1.3113 1.4259 sec/batch\n",
      "Epoch 16/20  Iteration 2838/3560 Training loss: 1.3116 1.3794 sec/batch\n",
      "Epoch 16/20  Iteration 2839/3560 Training loss: 1.3116 1.3562 sec/batch\n",
      "Epoch 16/20  Iteration 2840/3560 Training loss: 1.3116 1.3820 sec/batch\n",
      "Epoch 16/20  Iteration 2841/3560 Training loss: 1.3115 1.3739 sec/batch\n",
      "Epoch 16/20  Iteration 2842/3560 Training loss: 1.3113 1.3400 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2843/3560 Training loss: 1.3115 1.4183 sec/batch\n",
      "Epoch 16/20  Iteration 2844/3560 Training loss: 1.3115 1.3168 sec/batch\n",
      "Epoch 16/20  Iteration 2845/3560 Training loss: 1.3115 1.3473 sec/batch\n",
      "Epoch 16/20  Iteration 2846/3560 Training loss: 1.3114 1.3190 sec/batch\n",
      "Epoch 16/20  Iteration 2847/3560 Training loss: 1.3112 1.3655 sec/batch\n",
      "Epoch 16/20  Iteration 2848/3560 Training loss: 1.3114 1.3585 sec/batch\n",
      "Epoch 17/20  Iteration 2849/3560 Training loss: 1.4302 1.3534 sec/batch\n",
      "Epoch 17/20  Iteration 2850/3560 Training loss: 1.3738 1.4048 sec/batch\n",
      "Epoch 17/20  Iteration 2851/3560 Training loss: 1.3509 1.3904 sec/batch\n",
      "Epoch 17/20  Iteration 2852/3560 Training loss: 1.3419 1.3628 sec/batch\n",
      "Epoch 17/20  Iteration 2853/3560 Training loss: 1.3295 1.3378 sec/batch\n",
      "Epoch 17/20  Iteration 2854/3560 Training loss: 1.3169 1.5808 sec/batch\n",
      "Epoch 17/20  Iteration 2855/3560 Training loss: 1.3173 1.5340 sec/batch\n",
      "Epoch 17/20  Iteration 2856/3560 Training loss: 1.3139 1.3837 sec/batch\n",
      "Epoch 17/20  Iteration 2857/3560 Training loss: 1.3126 1.3735 sec/batch\n",
      "Epoch 17/20  Iteration 2858/3560 Training loss: 1.3107 1.5071 sec/batch\n",
      "Epoch 17/20  Iteration 2859/3560 Training loss: 1.3065 1.5151 sec/batch\n",
      "Epoch 17/20  Iteration 2860/3560 Training loss: 1.3064 1.3901 sec/batch\n",
      "Epoch 17/20  Iteration 2861/3560 Training loss: 1.3062 1.4999 sec/batch\n",
      "Epoch 17/20  Iteration 2862/3560 Training loss: 1.3072 1.5019 sec/batch\n",
      "Epoch 17/20  Iteration 2863/3560 Training loss: 1.3058 1.3868 sec/batch\n",
      "Epoch 17/20  Iteration 2864/3560 Training loss: 1.3042 1.4564 sec/batch\n",
      "Epoch 17/20  Iteration 2865/3560 Training loss: 1.3043 1.4115 sec/batch\n",
      "Epoch 17/20  Iteration 2866/3560 Training loss: 1.3050 1.3646 sec/batch\n",
      "Epoch 17/20  Iteration 2867/3560 Training loss: 1.3050 1.3659 sec/batch\n",
      "Epoch 17/20  Iteration 2868/3560 Training loss: 1.3063 1.3473 sec/batch\n",
      "Epoch 17/20  Iteration 2869/3560 Training loss: 1.3056 1.3434 sec/batch\n",
      "Epoch 17/20  Iteration 2870/3560 Training loss: 1.3059 1.3912 sec/batch\n",
      "Epoch 17/20  Iteration 2871/3560 Training loss: 1.3057 1.3584 sec/batch\n",
      "Epoch 17/20  Iteration 2872/3560 Training loss: 1.3057 1.3850 sec/batch\n",
      "Epoch 17/20  Iteration 2873/3560 Training loss: 1.3061 1.3537 sec/batch\n",
      "Epoch 17/20  Iteration 2874/3560 Training loss: 1.3042 1.3334 sec/batch\n",
      "Epoch 17/20  Iteration 2875/3560 Training loss: 1.3027 1.3350 sec/batch\n",
      "Epoch 17/20  Iteration 2876/3560 Training loss: 1.3034 1.3923 sec/batch\n",
      "Epoch 17/20  Iteration 2877/3560 Training loss: 1.3037 1.3905 sec/batch\n",
      "Epoch 17/20  Iteration 2878/3560 Training loss: 1.3040 1.4273 sec/batch\n",
      "Epoch 17/20  Iteration 2879/3560 Training loss: 1.3033 1.5146 sec/batch\n",
      "Epoch 17/20  Iteration 2880/3560 Training loss: 1.3021 1.4020 sec/batch\n",
      "Epoch 17/20  Iteration 2881/3560 Training loss: 1.3024 1.3971 sec/batch\n",
      "Epoch 17/20  Iteration 2882/3560 Training loss: 1.3027 1.4097 sec/batch\n",
      "Epoch 17/20  Iteration 2883/3560 Training loss: 1.3027 1.4088 sec/batch\n",
      "Epoch 17/20  Iteration 2884/3560 Training loss: 1.3028 1.3819 sec/batch\n",
      "Epoch 17/20  Iteration 2885/3560 Training loss: 1.3020 1.3626 sec/batch\n",
      "Epoch 17/20  Iteration 2886/3560 Training loss: 1.3008 1.5429 sec/batch\n",
      "Epoch 17/20  Iteration 2887/3560 Training loss: 1.2997 1.5345 sec/batch\n",
      "Epoch 17/20  Iteration 2888/3560 Training loss: 1.2993 1.3776 sec/batch\n",
      "Epoch 17/20  Iteration 2889/3560 Training loss: 1.2988 1.3954 sec/batch\n",
      "Epoch 17/20  Iteration 2890/3560 Training loss: 1.2995 1.4627 sec/batch\n",
      "Epoch 17/20  Iteration 2891/3560 Training loss: 1.2995 1.4754 sec/batch\n",
      "Epoch 17/20  Iteration 2892/3560 Training loss: 1.2988 1.5426 sec/batch\n",
      "Epoch 17/20  Iteration 2893/3560 Training loss: 1.2990 1.5963 sec/batch\n",
      "Epoch 17/20  Iteration 2894/3560 Training loss: 1.2983 1.4643 sec/batch\n",
      "Epoch 17/20  Iteration 2895/3560 Training loss: 1.2980 1.4475 sec/batch\n",
      "Epoch 17/20  Iteration 2896/3560 Training loss: 1.2977 1.4374 sec/batch\n",
      "Epoch 17/20  Iteration 2897/3560 Training loss: 1.2978 1.4962 sec/batch\n",
      "Epoch 17/20  Iteration 2898/3560 Training loss: 1.2985 1.4164 sec/batch\n",
      "Epoch 17/20  Iteration 2899/3560 Training loss: 1.2980 1.4115 sec/batch\n",
      "Epoch 17/20  Iteration 2900/3560 Training loss: 1.2987 1.4171 sec/batch\n",
      "Epoch 17/20  Iteration 2901/3560 Training loss: 1.2989 1.3722 sec/batch\n",
      "Epoch 17/20  Iteration 2902/3560 Training loss: 1.2990 1.3305 sec/batch\n",
      "Epoch 17/20  Iteration 2903/3560 Training loss: 1.2989 1.4318 sec/batch\n",
      "Epoch 17/20  Iteration 2904/3560 Training loss: 1.2991 1.3891 sec/batch\n",
      "Epoch 17/20  Iteration 2905/3560 Training loss: 1.2993 1.3968 sec/batch\n",
      "Epoch 17/20  Iteration 2906/3560 Training loss: 1.2991 1.3770 sec/batch\n",
      "Epoch 17/20  Iteration 2907/3560 Training loss: 1.2987 1.4198 sec/batch\n",
      "Epoch 17/20  Iteration 2908/3560 Training loss: 1.2992 1.4383 sec/batch\n",
      "Epoch 17/20  Iteration 2909/3560 Training loss: 1.2993 1.4408 sec/batch\n",
      "Epoch 17/20  Iteration 2910/3560 Training loss: 1.3001 1.4053 sec/batch\n",
      "Epoch 17/20  Iteration 2911/3560 Training loss: 1.3002 1.4289 sec/batch\n",
      "Epoch 17/20  Iteration 2912/3560 Training loss: 1.3004 1.4100 sec/batch\n",
      "Epoch 17/20  Iteration 2913/3560 Training loss: 1.3003 1.4242 sec/batch\n",
      "Epoch 17/20  Iteration 2914/3560 Training loss: 1.3006 1.4241 sec/batch\n",
      "Epoch 17/20  Iteration 2915/3560 Training loss: 1.3009 1.5133 sec/batch\n",
      "Epoch 17/20  Iteration 2916/3560 Training loss: 1.3005 1.3992 sec/batch\n",
      "Epoch 17/20  Iteration 2917/3560 Training loss: 1.3007 1.3882 sec/batch\n",
      "Epoch 17/20  Iteration 2918/3560 Training loss: 1.3006 1.4820 sec/batch\n",
      "Epoch 17/20  Iteration 2919/3560 Training loss: 1.3011 1.4725 sec/batch\n",
      "Epoch 17/20  Iteration 2920/3560 Training loss: 1.3014 1.4263 sec/batch\n",
      "Epoch 17/20  Iteration 2921/3560 Training loss: 1.3020 1.4625 sec/batch\n",
      "Epoch 17/20  Iteration 2922/3560 Training loss: 1.3017 1.4497 sec/batch\n",
      "Epoch 17/20  Iteration 2923/3560 Training loss: 1.3017 1.4392 sec/batch\n",
      "Epoch 17/20  Iteration 2924/3560 Training loss: 1.3019 1.4390 sec/batch\n",
      "Epoch 17/20  Iteration 2925/3560 Training loss: 1.3017 1.4312 sec/batch\n",
      "Epoch 17/20  Iteration 2926/3560 Training loss: 1.3017 1.4965 sec/batch\n",
      "Epoch 17/20  Iteration 2927/3560 Training loss: 1.3010 1.4161 sec/batch\n",
      "Epoch 17/20  Iteration 2928/3560 Training loss: 1.3011 1.4138 sec/batch\n",
      "Epoch 17/20  Iteration 2929/3560 Training loss: 1.3006 1.4876 sec/batch\n",
      "Epoch 17/20  Iteration 2930/3560 Training loss: 1.3006 1.4228 sec/batch\n",
      "Epoch 17/20  Iteration 2931/3560 Training loss: 1.3001 1.3766 sec/batch\n",
      "Epoch 17/20  Iteration 2932/3560 Training loss: 1.3002 1.3616 sec/batch\n",
      "Epoch 17/20  Iteration 2933/3560 Training loss: 1.2999 1.3400 sec/batch\n",
      "Epoch 17/20  Iteration 2934/3560 Training loss: 1.2999 1.3465 sec/batch\n",
      "Epoch 17/20  Iteration 2935/3560 Training loss: 1.2997 1.3473 sec/batch\n",
      "Epoch 17/20  Iteration 2936/3560 Training loss: 1.2994 1.3995 sec/batch\n",
      "Epoch 17/20  Iteration 2937/3560 Training loss: 1.2991 1.3337 sec/batch\n",
      "Epoch 17/20  Iteration 2938/3560 Training loss: 1.2992 1.3228 sec/batch\n",
      "Epoch 17/20  Iteration 2939/3560 Training loss: 1.2991 1.3579 sec/batch\n",
      "Epoch 17/20  Iteration 2940/3560 Training loss: 1.2991 1.3479 sec/batch\n",
      "Epoch 17/20  Iteration 2941/3560 Training loss: 1.2987 1.3682 sec/batch\n",
      "Epoch 17/20  Iteration 2942/3560 Training loss: 1.2983 1.3623 sec/batch\n",
      "Epoch 17/20  Iteration 2943/3560 Training loss: 1.2982 1.3867 sec/batch\n",
      "Epoch 17/20  Iteration 2944/3560 Training loss: 1.2982 1.3419 sec/batch\n",
      "Epoch 17/20  Iteration 2945/3560 Training loss: 1.2982 1.3418 sec/batch\n",
      "Epoch 17/20  Iteration 2946/3560 Training loss: 1.2979 1.5193 sec/batch\n",
      "Epoch 17/20  Iteration 2947/3560 Training loss: 1.2976 1.4711 sec/batch\n",
      "Epoch 17/20  Iteration 2948/3560 Training loss: 1.2973 1.4683 sec/batch\n",
      "Epoch 17/20  Iteration 2949/3560 Training loss: 1.2973 1.5209 sec/batch\n",
      "Epoch 17/20  Iteration 2950/3560 Training loss: 1.2971 1.4973 sec/batch\n",
      "Epoch 17/20  Iteration 2951/3560 Training loss: 1.2969 1.4398 sec/batch\n",
      "Epoch 17/20  Iteration 2952/3560 Training loss: 1.2968 1.4411 sec/batch\n",
      "Epoch 17/20  Iteration 2953/3560 Training loss: 1.2968 1.4706 sec/batch\n",
      "Epoch 17/20  Iteration 2954/3560 Training loss: 1.2967 1.4541 sec/batch\n",
      "Epoch 17/20  Iteration 2955/3560 Training loss: 1.2966 1.5038 sec/batch\n",
      "Epoch 17/20  Iteration 2956/3560 Training loss: 1.2966 1.4793 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 2957/3560 Training loss: 1.2965 1.4150 sec/batch\n",
      "Epoch 17/20  Iteration 2958/3560 Training loss: 1.2966 1.4107 sec/batch\n",
      "Epoch 17/20  Iteration 2959/3560 Training loss: 1.2964 1.4204 sec/batch\n",
      "Epoch 17/20  Iteration 2960/3560 Training loss: 1.2964 1.3913 sec/batch\n",
      "Epoch 17/20  Iteration 2961/3560 Training loss: 1.2963 1.4173 sec/batch\n",
      "Epoch 17/20  Iteration 2962/3560 Training loss: 1.2961 1.4199 sec/batch\n",
      "Epoch 17/20  Iteration 2963/3560 Training loss: 1.2958 1.4286 sec/batch\n",
      "Epoch 17/20  Iteration 2964/3560 Training loss: 1.2955 1.4918 sec/batch\n",
      "Epoch 17/20  Iteration 2965/3560 Training loss: 1.2955 1.4952 sec/batch\n",
      "Epoch 17/20  Iteration 2966/3560 Training loss: 1.2955 1.4162 sec/batch\n",
      "Epoch 17/20  Iteration 2967/3560 Training loss: 1.2953 1.4599 sec/batch\n",
      "Epoch 17/20  Iteration 2968/3560 Training loss: 1.2952 1.4172 sec/batch\n",
      "Epoch 17/20  Iteration 2969/3560 Training loss: 1.2951 1.4352 sec/batch\n",
      "Epoch 17/20  Iteration 2970/3560 Training loss: 1.2947 1.4241 sec/batch\n",
      "Epoch 17/20  Iteration 2971/3560 Training loss: 1.2943 1.4533 sec/batch\n",
      "Epoch 17/20  Iteration 2972/3560 Training loss: 1.2943 1.4787 sec/batch\n",
      "Epoch 17/20  Iteration 2973/3560 Training loss: 1.2941 1.3704 sec/batch\n",
      "Epoch 17/20  Iteration 2974/3560 Training loss: 1.2938 1.4031 sec/batch\n",
      "Epoch 17/20  Iteration 2975/3560 Training loss: 1.2939 1.3647 sec/batch\n",
      "Epoch 17/20  Iteration 2976/3560 Training loss: 1.2938 1.4180 sec/batch\n",
      "Epoch 17/20  Iteration 2977/3560 Training loss: 1.2937 1.4270 sec/batch\n",
      "Epoch 17/20  Iteration 2978/3560 Training loss: 1.2934 1.3759 sec/batch\n",
      "Epoch 17/20  Iteration 2979/3560 Training loss: 1.2930 1.4312 sec/batch\n",
      "Epoch 17/20  Iteration 2980/3560 Training loss: 1.2928 1.5323 sec/batch\n",
      "Epoch 17/20  Iteration 2981/3560 Training loss: 1.2929 1.8559 sec/batch\n",
      "Epoch 17/20  Iteration 2982/3560 Training loss: 1.2929 1.5535 sec/batch\n",
      "Epoch 17/20  Iteration 2983/3560 Training loss: 1.2929 1.4573 sec/batch\n",
      "Epoch 17/20  Iteration 2984/3560 Training loss: 1.2930 1.4840 sec/batch\n",
      "Epoch 17/20  Iteration 2985/3560 Training loss: 1.2932 1.5564 sec/batch\n",
      "Epoch 17/20  Iteration 2986/3560 Training loss: 1.2932 1.4395 sec/batch\n",
      "Epoch 17/20  Iteration 2987/3560 Training loss: 1.2932 1.3911 sec/batch\n",
      "Epoch 17/20  Iteration 2988/3560 Training loss: 1.2931 1.3959 sec/batch\n",
      "Epoch 17/20  Iteration 2989/3560 Training loss: 1.2935 1.4366 sec/batch\n",
      "Epoch 17/20  Iteration 2990/3560 Training loss: 1.2935 1.4192 sec/batch\n",
      "Epoch 17/20  Iteration 2991/3560 Training loss: 1.2934 1.4326 sec/batch\n",
      "Epoch 17/20  Iteration 2992/3560 Training loss: 1.2937 1.4860 sec/batch\n",
      "Epoch 17/20  Iteration 2993/3560 Training loss: 1.2935 1.3626 sec/batch\n",
      "Epoch 17/20  Iteration 2994/3560 Training loss: 1.2937 1.3423 sec/batch\n",
      "Epoch 17/20  Iteration 2995/3560 Training loss: 1.2937 1.3331 sec/batch\n",
      "Epoch 17/20  Iteration 2996/3560 Training loss: 1.2939 1.3703 sec/batch\n",
      "Epoch 17/20  Iteration 2997/3560 Training loss: 1.2940 1.3522 sec/batch\n",
      "Epoch 17/20  Iteration 2998/3560 Training loss: 1.2939 1.3683 sec/batch\n",
      "Epoch 17/20  Iteration 2999/3560 Training loss: 1.2936 1.3418 sec/batch\n",
      "Epoch 17/20  Iteration 3000/3560 Training loss: 1.2935 1.4067 sec/batch\n",
      "Epoch 17/20  Iteration 3001/3560 Training loss: 1.2936 1.4503 sec/batch\n",
      "Epoch 17/20  Iteration 3002/3560 Training loss: 1.2935 1.3986 sec/batch\n",
      "Epoch 17/20  Iteration 3003/3560 Training loss: 1.2935 1.3768 sec/batch\n",
      "Epoch 17/20  Iteration 3004/3560 Training loss: 1.2935 1.3823 sec/batch\n",
      "Epoch 17/20  Iteration 3005/3560 Training loss: 1.2934 1.3784 sec/batch\n",
      "Epoch 17/20  Iteration 3006/3560 Training loss: 1.2933 1.3751 sec/batch\n",
      "Epoch 17/20  Iteration 3007/3560 Training loss: 1.2930 1.4602 sec/batch\n",
      "Epoch 17/20  Iteration 3008/3560 Training loss: 1.2931 1.3838 sec/batch\n",
      "Epoch 17/20  Iteration 3009/3560 Training loss: 1.2932 1.3753 sec/batch\n",
      "Epoch 17/20  Iteration 3010/3560 Training loss: 1.2932 1.3471 sec/batch\n",
      "Epoch 17/20  Iteration 3011/3560 Training loss: 1.2932 1.4259 sec/batch\n",
      "Epoch 17/20  Iteration 3012/3560 Training loss: 1.2932 1.4160 sec/batch\n",
      "Epoch 17/20  Iteration 3013/3560 Training loss: 1.2932 1.4410 sec/batch\n",
      "Epoch 17/20  Iteration 3014/3560 Training loss: 1.2932 1.4099 sec/batch\n",
      "Epoch 17/20  Iteration 3015/3560 Training loss: 1.2933 1.4883 sec/batch\n",
      "Epoch 17/20  Iteration 3016/3560 Training loss: 1.2937 1.4189 sec/batch\n",
      "Epoch 17/20  Iteration 3017/3560 Training loss: 1.2937 1.4181 sec/batch\n",
      "Epoch 17/20  Iteration 3018/3560 Training loss: 1.2937 1.4277 sec/batch\n",
      "Epoch 17/20  Iteration 3019/3560 Training loss: 1.2936 1.4086 sec/batch\n",
      "Epoch 17/20  Iteration 3020/3560 Training loss: 1.2934 1.4316 sec/batch\n",
      "Epoch 17/20  Iteration 3021/3560 Training loss: 1.2936 1.4714 sec/batch\n",
      "Epoch 17/20  Iteration 3022/3560 Training loss: 1.2935 1.3857 sec/batch\n",
      "Epoch 17/20  Iteration 3023/3560 Training loss: 1.2935 1.3485 sec/batch\n",
      "Epoch 17/20  Iteration 3024/3560 Training loss: 1.2934 1.3534 sec/batch\n",
      "Epoch 17/20  Iteration 3025/3560 Training loss: 1.2932 1.3441 sec/batch\n",
      "Epoch 17/20  Iteration 3026/3560 Training loss: 1.2934 1.3424 sec/batch\n",
      "Epoch 18/20  Iteration 3027/3560 Training loss: 1.4103 1.3532 sec/batch\n",
      "Epoch 18/20  Iteration 3028/3560 Training loss: 1.3558 1.3334 sec/batch\n",
      "Epoch 18/20  Iteration 3029/3560 Training loss: 1.3342 1.3859 sec/batch\n",
      "Epoch 18/20  Iteration 3030/3560 Training loss: 1.3262 1.4763 sec/batch\n",
      "Epoch 18/20  Iteration 3031/3560 Training loss: 1.3141 1.3970 sec/batch\n",
      "Epoch 18/20  Iteration 3032/3560 Training loss: 1.3007 1.4058 sec/batch\n",
      "Epoch 18/20  Iteration 3033/3560 Training loss: 1.2998 1.3582 sec/batch\n",
      "Epoch 18/20  Iteration 3034/3560 Training loss: 1.2966 1.4130 sec/batch\n",
      "Epoch 18/20  Iteration 3035/3560 Training loss: 1.2954 1.3496 sec/batch\n",
      "Epoch 18/20  Iteration 3036/3560 Training loss: 1.2934 1.4268 sec/batch\n",
      "Epoch 18/20  Iteration 3037/3560 Training loss: 1.2892 1.3442 sec/batch\n",
      "Epoch 18/20  Iteration 3038/3560 Training loss: 1.2899 1.3688 sec/batch\n",
      "Epoch 18/20  Iteration 3039/3560 Training loss: 1.2896 1.3632 sec/batch\n",
      "Epoch 18/20  Iteration 3040/3560 Training loss: 1.2900 1.3600 sec/batch\n",
      "Epoch 18/20  Iteration 3041/3560 Training loss: 1.2889 1.3721 sec/batch\n",
      "Epoch 18/20  Iteration 3042/3560 Training loss: 1.2880 1.3895 sec/batch\n",
      "Epoch 18/20  Iteration 3043/3560 Training loss: 1.2884 1.4423 sec/batch\n",
      "Epoch 18/20  Iteration 3044/3560 Training loss: 1.2894 1.4818 sec/batch\n",
      "Epoch 18/20  Iteration 3045/3560 Training loss: 1.2894 1.4353 sec/batch\n",
      "Epoch 18/20  Iteration 3046/3560 Training loss: 1.2901 1.4417 sec/batch\n",
      "Epoch 18/20  Iteration 3047/3560 Training loss: 1.2896 1.4365 sec/batch\n",
      "Epoch 18/20  Iteration 3048/3560 Training loss: 1.2903 1.4666 sec/batch\n",
      "Epoch 18/20  Iteration 3049/3560 Training loss: 1.2894 1.4161 sec/batch\n",
      "Epoch 18/20  Iteration 3050/3560 Training loss: 1.2896 1.4435 sec/batch\n",
      "Epoch 18/20  Iteration 3051/3560 Training loss: 1.2895 1.4621 sec/batch\n",
      "Epoch 18/20  Iteration 3052/3560 Training loss: 1.2873 1.3548 sec/batch\n",
      "Epoch 18/20  Iteration 3053/3560 Training loss: 1.2863 1.3604 sec/batch\n",
      "Epoch 18/20  Iteration 3054/3560 Training loss: 1.2871 1.3784 sec/batch\n",
      "Epoch 18/20  Iteration 3055/3560 Training loss: 1.2874 1.3606 sec/batch\n",
      "Epoch 18/20  Iteration 3056/3560 Training loss: 1.2878 1.4016 sec/batch\n",
      "Epoch 18/20  Iteration 3057/3560 Training loss: 1.2873 1.3734 sec/batch\n",
      "Epoch 18/20  Iteration 3058/3560 Training loss: 1.2864 1.4671 sec/batch\n",
      "Epoch 18/20  Iteration 3059/3560 Training loss: 1.2867 1.3525 sec/batch\n",
      "Epoch 18/20  Iteration 3060/3560 Training loss: 1.2869 1.3555 sec/batch\n",
      "Epoch 18/20  Iteration 3061/3560 Training loss: 1.2867 1.3783 sec/batch\n",
      "Epoch 18/20  Iteration 3062/3560 Training loss: 1.2867 1.3885 sec/batch\n",
      "Epoch 18/20  Iteration 3063/3560 Training loss: 1.2860 1.4395 sec/batch\n",
      "Epoch 18/20  Iteration 3064/3560 Training loss: 1.2849 1.4225 sec/batch\n",
      "Epoch 18/20  Iteration 3065/3560 Training loss: 1.2836 1.4495 sec/batch\n",
      "Epoch 18/20  Iteration 3066/3560 Training loss: 1.2832 1.3536 sec/batch\n",
      "Epoch 18/20  Iteration 3067/3560 Training loss: 1.2825 1.3718 sec/batch\n",
      "Epoch 18/20  Iteration 3068/3560 Training loss: 1.2833 1.4344 sec/batch\n",
      "Epoch 18/20  Iteration 3069/3560 Training loss: 1.2831 1.4335 sec/batch\n",
      "Epoch 18/20  Iteration 3070/3560 Training loss: 1.2824 1.4078 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3071/3560 Training loss: 1.2825 1.4871 sec/batch\n",
      "Epoch 18/20  Iteration 3072/3560 Training loss: 1.2817 1.5051 sec/batch\n",
      "Epoch 18/20  Iteration 3073/3560 Training loss: 1.2814 1.4661 sec/batch\n",
      "Epoch 18/20  Iteration 3074/3560 Training loss: 1.2810 1.3973 sec/batch\n",
      "Epoch 18/20  Iteration 3075/3560 Training loss: 1.2811 1.4087 sec/batch\n",
      "Epoch 18/20  Iteration 3076/3560 Training loss: 1.2814 1.4287 sec/batch\n",
      "Epoch 18/20  Iteration 3077/3560 Training loss: 1.2810 1.3756 sec/batch\n",
      "Epoch 18/20  Iteration 3078/3560 Training loss: 1.2818 1.4055 sec/batch\n",
      "Epoch 18/20  Iteration 3079/3560 Training loss: 1.2818 1.4126 sec/batch\n",
      "Epoch 18/20  Iteration 3080/3560 Training loss: 1.2821 1.3891 sec/batch\n",
      "Epoch 18/20  Iteration 3081/3560 Training loss: 1.2819 1.3617 sec/batch\n",
      "Epoch 18/20  Iteration 3082/3560 Training loss: 1.2821 1.3816 sec/batch\n",
      "Epoch 18/20  Iteration 3083/3560 Training loss: 1.2824 1.4639 sec/batch\n",
      "Epoch 18/20  Iteration 3084/3560 Training loss: 1.2823 1.4023 sec/batch\n",
      "Epoch 18/20  Iteration 3085/3560 Training loss: 1.2821 1.3871 sec/batch\n",
      "Epoch 18/20  Iteration 3086/3560 Training loss: 1.2828 1.4338 sec/batch\n",
      "Epoch 18/20  Iteration 3087/3560 Training loss: 1.2827 1.4363 sec/batch\n",
      "Epoch 18/20  Iteration 3088/3560 Training loss: 1.2834 1.3997 sec/batch\n",
      "Epoch 18/20  Iteration 3089/3560 Training loss: 1.2835 1.4450 sec/batch\n",
      "Epoch 18/20  Iteration 3090/3560 Training loss: 1.2838 1.4047 sec/batch\n",
      "Epoch 18/20  Iteration 3091/3560 Training loss: 1.2836 1.4214 sec/batch\n",
      "Epoch 18/20  Iteration 3092/3560 Training loss: 1.2840 1.4315 sec/batch\n",
      "Epoch 18/20  Iteration 3093/3560 Training loss: 1.2842 1.4660 sec/batch\n",
      "Epoch 18/20  Iteration 3094/3560 Training loss: 1.2839 1.4974 sec/batch\n",
      "Epoch 18/20  Iteration 3095/3560 Training loss: 1.2840 1.4363 sec/batch\n",
      "Epoch 18/20  Iteration 3096/3560 Training loss: 1.2840 1.4072 sec/batch\n",
      "Epoch 18/20  Iteration 3097/3560 Training loss: 1.2846 1.3661 sec/batch\n",
      "Epoch 18/20  Iteration 3098/3560 Training loss: 1.2849 1.3637 sec/batch\n",
      "Epoch 18/20  Iteration 3099/3560 Training loss: 1.2855 1.3479 sec/batch\n",
      "Epoch 18/20  Iteration 3100/3560 Training loss: 1.2852 1.3710 sec/batch\n",
      "Epoch 18/20  Iteration 3101/3560 Training loss: 1.2852 1.8507 sec/batch\n",
      "Epoch 18/20  Iteration 3102/3560 Training loss: 1.2854 1.4065 sec/batch\n",
      "Epoch 18/20  Iteration 3103/3560 Training loss: 1.2853 1.4029 sec/batch\n",
      "Epoch 18/20  Iteration 3104/3560 Training loss: 1.2853 1.3860 sec/batch\n",
      "Epoch 18/20  Iteration 3105/3560 Training loss: 1.2846 1.4150 sec/batch\n",
      "Epoch 18/20  Iteration 3106/3560 Training loss: 1.2846 1.4111 sec/batch\n",
      "Epoch 18/20  Iteration 3107/3560 Training loss: 1.2842 1.4010 sec/batch\n",
      "Epoch 18/20  Iteration 3108/3560 Training loss: 1.2841 1.4673 sec/batch\n",
      "Epoch 18/20  Iteration 3109/3560 Training loss: 1.2836 1.3714 sec/batch\n",
      "Epoch 18/20  Iteration 3110/3560 Training loss: 1.2837 1.5816 sec/batch\n",
      "Epoch 18/20  Iteration 3111/3560 Training loss: 1.2834 1.4541 sec/batch\n",
      "Epoch 18/20  Iteration 3112/3560 Training loss: 1.2833 1.4158 sec/batch\n",
      "Epoch 18/20  Iteration 3113/3560 Training loss: 1.2830 1.4685 sec/batch\n",
      "Epoch 18/20  Iteration 3114/3560 Training loss: 1.2828 1.4930 sec/batch\n",
      "Epoch 18/20  Iteration 3115/3560 Training loss: 1.2824 1.5146 sec/batch\n",
      "Epoch 18/20  Iteration 3116/3560 Training loss: 1.2825 1.4983 sec/batch\n",
      "Epoch 18/20  Iteration 3117/3560 Training loss: 1.2823 1.4670 sec/batch\n",
      "Epoch 18/20  Iteration 3118/3560 Training loss: 1.2822 1.4461 sec/batch\n",
      "Epoch 18/20  Iteration 3119/3560 Training loss: 1.2818 1.4866 sec/batch\n",
      "Epoch 18/20  Iteration 3120/3560 Training loss: 1.2815 1.4377 sec/batch\n",
      "Epoch 18/20  Iteration 3121/3560 Training loss: 1.2812 1.4576 sec/batch\n",
      "Epoch 18/20  Iteration 3122/3560 Training loss: 1.2812 1.4439 sec/batch\n",
      "Epoch 18/20  Iteration 3123/3560 Training loss: 1.2812 1.4315 sec/batch\n",
      "Epoch 18/20  Iteration 3124/3560 Training loss: 1.2808 1.4134 sec/batch\n",
      "Epoch 18/20  Iteration 3125/3560 Training loss: 1.2805 1.4317 sec/batch\n",
      "Epoch 18/20  Iteration 3126/3560 Training loss: 1.2802 1.4418 sec/batch\n",
      "Epoch 18/20  Iteration 3127/3560 Training loss: 1.2801 1.4415 sec/batch\n",
      "Epoch 18/20  Iteration 3128/3560 Training loss: 1.2800 1.4409 sec/batch\n",
      "Epoch 18/20  Iteration 3129/3560 Training loss: 1.2799 1.5056 sec/batch\n",
      "Epoch 18/20  Iteration 3130/3560 Training loss: 1.2798 1.4479 sec/batch\n",
      "Epoch 18/20  Iteration 3131/3560 Training loss: 1.2796 1.4315 sec/batch\n",
      "Epoch 18/20  Iteration 3132/3560 Training loss: 1.2795 1.4117 sec/batch\n",
      "Epoch 18/20  Iteration 3133/3560 Training loss: 1.2794 1.4316 sec/batch\n",
      "Epoch 18/20  Iteration 3134/3560 Training loss: 1.2795 1.3291 sec/batch\n",
      "Epoch 18/20  Iteration 3135/3560 Training loss: 1.2793 1.3592 sec/batch\n",
      "Epoch 18/20  Iteration 3136/3560 Training loss: 1.2794 1.4084 sec/batch\n",
      "Epoch 18/20  Iteration 3137/3560 Training loss: 1.2791 1.3575 sec/batch\n",
      "Epoch 18/20  Iteration 3138/3560 Training loss: 1.2791 1.3820 sec/batch\n",
      "Epoch 18/20  Iteration 3139/3560 Training loss: 1.2790 1.3813 sec/batch\n",
      "Epoch 18/20  Iteration 3140/3560 Training loss: 1.2789 1.4361 sec/batch\n",
      "Epoch 18/20  Iteration 3141/3560 Training loss: 1.2786 1.4446 sec/batch\n",
      "Epoch 18/20  Iteration 3142/3560 Training loss: 1.2783 1.4593 sec/batch\n",
      "Epoch 18/20  Iteration 3143/3560 Training loss: 1.2783 1.4626 sec/batch\n",
      "Epoch 18/20  Iteration 3144/3560 Training loss: 1.2783 1.4465 sec/batch\n",
      "Epoch 18/20  Iteration 3145/3560 Training loss: 1.2782 1.4500 sec/batch\n",
      "Epoch 18/20  Iteration 3146/3560 Training loss: 1.2782 1.3680 sec/batch\n",
      "Epoch 18/20  Iteration 3147/3560 Training loss: 1.2781 1.3677 sec/batch\n",
      "Epoch 18/20  Iteration 3148/3560 Training loss: 1.2779 1.3562 sec/batch\n",
      "Epoch 18/20  Iteration 3149/3560 Training loss: 1.2775 1.4092 sec/batch\n",
      "Epoch 18/20  Iteration 3150/3560 Training loss: 1.2775 1.3830 sec/batch\n",
      "Epoch 18/20  Iteration 3151/3560 Training loss: 1.2774 1.3669 sec/batch\n",
      "Epoch 18/20  Iteration 3152/3560 Training loss: 1.2771 1.3841 sec/batch\n",
      "Epoch 18/20  Iteration 3153/3560 Training loss: 1.2772 1.3677 sec/batch\n",
      "Epoch 18/20  Iteration 3154/3560 Training loss: 1.2772 1.3394 sec/batch\n",
      "Epoch 18/20  Iteration 3155/3560 Training loss: 1.2771 1.3469 sec/batch\n",
      "Epoch 18/20  Iteration 3156/3560 Training loss: 1.2768 1.3828 sec/batch\n",
      "Epoch 18/20  Iteration 3157/3560 Training loss: 1.2764 1.3593 sec/batch\n",
      "Epoch 18/20  Iteration 3158/3560 Training loss: 1.2762 1.4197 sec/batch\n",
      "Epoch 18/20  Iteration 3159/3560 Training loss: 1.2763 1.3364 sec/batch\n",
      "Epoch 18/20  Iteration 3160/3560 Training loss: 1.2763 1.3399 sec/batch\n",
      "Epoch 18/20  Iteration 3161/3560 Training loss: 1.2763 1.3268 sec/batch\n",
      "Epoch 18/20  Iteration 3162/3560 Training loss: 1.2764 1.3118 sec/batch\n",
      "Epoch 18/20  Iteration 3163/3560 Training loss: 1.2766 1.3429 sec/batch\n",
      "Epoch 18/20  Iteration 3164/3560 Training loss: 1.2767 1.3411 sec/batch\n",
      "Epoch 18/20  Iteration 3165/3560 Training loss: 1.2767 1.3508 sec/batch\n",
      "Epoch 18/20  Iteration 3166/3560 Training loss: 1.2767 1.4949 sec/batch\n",
      "Epoch 18/20  Iteration 3167/3560 Training loss: 1.2771 1.3907 sec/batch\n",
      "Epoch 18/20  Iteration 3168/3560 Training loss: 1.2772 1.4131 sec/batch\n",
      "Epoch 18/20  Iteration 3169/3560 Training loss: 1.2771 1.3763 sec/batch\n",
      "Epoch 18/20  Iteration 3170/3560 Training loss: 1.2773 1.3511 sec/batch\n",
      "Epoch 18/20  Iteration 3171/3560 Training loss: 1.2772 1.3637 sec/batch\n",
      "Epoch 18/20  Iteration 3172/3560 Training loss: 1.2774 1.3536 sec/batch\n",
      "Epoch 18/20  Iteration 3173/3560 Training loss: 1.2774 1.4040 sec/batch\n",
      "Epoch 18/20  Iteration 3174/3560 Training loss: 1.2776 1.3928 sec/batch\n",
      "Epoch 18/20  Iteration 3175/3560 Training loss: 1.2778 1.3385 sec/batch\n",
      "Epoch 18/20  Iteration 3176/3560 Training loss: 1.2776 1.3608 sec/batch\n",
      "Epoch 18/20  Iteration 3177/3560 Training loss: 1.2773 1.3631 sec/batch\n",
      "Epoch 18/20  Iteration 3178/3560 Training loss: 1.2771 1.3606 sec/batch\n",
      "Epoch 18/20  Iteration 3179/3560 Training loss: 1.2772 1.3693 sec/batch\n",
      "Epoch 18/20  Iteration 3180/3560 Training loss: 1.2772 1.3817 sec/batch\n",
      "Epoch 18/20  Iteration 3181/3560 Training loss: 1.2772 1.3656 sec/batch\n",
      "Epoch 18/20  Iteration 3182/3560 Training loss: 1.2772 1.3456 sec/batch\n",
      "Epoch 18/20  Iteration 3183/3560 Training loss: 1.2772 1.4100 sec/batch\n",
      "Epoch 18/20  Iteration 3184/3560 Training loss: 1.2771 1.4187 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3185/3560 Training loss: 1.2769 1.4585 sec/batch\n",
      "Epoch 18/20  Iteration 3186/3560 Training loss: 1.2769 1.4158 sec/batch\n",
      "Epoch 18/20  Iteration 3187/3560 Training loss: 1.2770 1.4570 sec/batch\n",
      "Epoch 18/20  Iteration 3188/3560 Training loss: 1.2770 1.4960 sec/batch\n",
      "Epoch 18/20  Iteration 3189/3560 Training loss: 1.2771 1.4357 sec/batch\n",
      "Epoch 18/20  Iteration 3190/3560 Training loss: 1.2771 1.4253 sec/batch\n",
      "Epoch 18/20  Iteration 3191/3560 Training loss: 1.2771 1.4960 sec/batch\n",
      "Epoch 18/20  Iteration 3192/3560 Training loss: 1.2770 1.4451 sec/batch\n",
      "Epoch 18/20  Iteration 3193/3560 Training loss: 1.2771 1.4680 sec/batch\n",
      "Epoch 18/20  Iteration 3194/3560 Training loss: 1.2775 1.4448 sec/batch\n",
      "Epoch 18/20  Iteration 3195/3560 Training loss: 1.2776 1.5223 sec/batch\n",
      "Epoch 18/20  Iteration 3196/3560 Training loss: 1.2776 1.4267 sec/batch\n",
      "Epoch 18/20  Iteration 3197/3560 Training loss: 1.2775 1.4458 sec/batch\n",
      "Epoch 18/20  Iteration 3198/3560 Training loss: 1.2774 1.4465 sec/batch\n",
      "Epoch 18/20  Iteration 3199/3560 Training loss: 1.2776 1.5260 sec/batch\n",
      "Epoch 18/20  Iteration 3200/3560 Training loss: 1.2775 1.4808 sec/batch\n",
      "Epoch 18/20  Iteration 3201/3560 Training loss: 1.2776 1.5512 sec/batch\n",
      "Epoch 18/20  Iteration 3202/3560 Training loss: 1.2774 1.4836 sec/batch\n",
      "Epoch 18/20  Iteration 3203/3560 Training loss: 1.2773 1.3997 sec/batch\n",
      "Epoch 18/20  Iteration 3204/3560 Training loss: 1.2774 1.3499 sec/batch\n",
      "Epoch 19/20  Iteration 3205/3560 Training loss: 1.3923 1.3883 sec/batch\n",
      "Epoch 19/20  Iteration 3206/3560 Training loss: 1.3373 1.3412 sec/batch\n",
      "Epoch 19/20  Iteration 3207/3560 Training loss: 1.3173 1.3369 sec/batch\n",
      "Epoch 19/20  Iteration 3208/3560 Training loss: 1.3084 1.3565 sec/batch\n",
      "Epoch 19/20  Iteration 3209/3560 Training loss: 1.2965 1.4156 sec/batch\n",
      "Epoch 19/20  Iteration 3210/3560 Training loss: 1.2843 1.4473 sec/batch\n",
      "Epoch 19/20  Iteration 3211/3560 Training loss: 1.2844 1.4106 sec/batch\n",
      "Epoch 19/20  Iteration 3212/3560 Training loss: 1.2818 1.4568 sec/batch\n",
      "Epoch 19/20  Iteration 3213/3560 Training loss: 1.2800 1.4145 sec/batch\n",
      "Epoch 19/20  Iteration 3214/3560 Training loss: 1.2792 1.5122 sec/batch\n",
      "Epoch 19/20  Iteration 3215/3560 Training loss: 1.2747 1.4668 sec/batch\n",
      "Epoch 19/20  Iteration 3216/3560 Training loss: 1.2750 1.4837 sec/batch\n",
      "Epoch 19/20  Iteration 3217/3560 Training loss: 1.2753 1.4150 sec/batch\n",
      "Epoch 19/20  Iteration 3218/3560 Training loss: 1.2757 1.3793 sec/batch\n",
      "Epoch 19/20  Iteration 3219/3560 Training loss: 1.2748 1.4429 sec/batch\n",
      "Epoch 19/20  Iteration 3220/3560 Training loss: 1.2730 1.3930 sec/batch\n",
      "Epoch 19/20  Iteration 3221/3560 Training loss: 1.2730 1.4296 sec/batch\n",
      "Epoch 19/20  Iteration 3222/3560 Training loss: 1.2741 1.3734 sec/batch\n",
      "Epoch 19/20  Iteration 3223/3560 Training loss: 1.2738 1.3949 sec/batch\n",
      "Epoch 19/20  Iteration 3224/3560 Training loss: 1.2750 1.3464 sec/batch\n",
      "Epoch 19/20  Iteration 3225/3560 Training loss: 1.2743 1.3860 sec/batch\n",
      "Epoch 19/20  Iteration 3226/3560 Training loss: 1.2747 1.3790 sec/batch\n",
      "Epoch 19/20  Iteration 3227/3560 Training loss: 1.2742 1.4230 sec/batch\n",
      "Epoch 19/20  Iteration 3228/3560 Training loss: 1.2742 1.3605 sec/batch\n",
      "Epoch 19/20  Iteration 3229/3560 Training loss: 1.2739 1.4698 sec/batch\n",
      "Epoch 19/20  Iteration 3230/3560 Training loss: 1.2722 1.4834 sec/batch\n",
      "Epoch 19/20  Iteration 3231/3560 Training loss: 1.2711 1.4221 sec/batch\n",
      "Epoch 19/20  Iteration 3232/3560 Training loss: 1.2716 1.4130 sec/batch\n",
      "Epoch 19/20  Iteration 3233/3560 Training loss: 1.2718 1.4537 sec/batch\n",
      "Epoch 19/20  Iteration 3234/3560 Training loss: 1.2722 1.3968 sec/batch\n",
      "Epoch 19/20  Iteration 3235/3560 Training loss: 1.2716 1.7834 sec/batch\n",
      "Epoch 19/20  Iteration 3236/3560 Training loss: 1.2708 1.5190 sec/batch\n",
      "Epoch 19/20  Iteration 3237/3560 Training loss: 1.2710 1.4880 sec/batch\n",
      "Epoch 19/20  Iteration 3238/3560 Training loss: 1.2713 1.4340 sec/batch\n",
      "Epoch 19/20  Iteration 3239/3560 Training loss: 1.2711 1.4095 sec/batch\n",
      "Epoch 19/20  Iteration 3240/3560 Training loss: 1.2713 1.4083 sec/batch\n",
      "Epoch 19/20  Iteration 3241/3560 Training loss: 1.2708 1.3640 sec/batch\n",
      "Epoch 19/20  Iteration 3242/3560 Training loss: 1.2697 1.3400 sec/batch\n",
      "Epoch 19/20  Iteration 3243/3560 Training loss: 1.2686 1.4499 sec/batch\n",
      "Epoch 19/20  Iteration 3244/3560 Training loss: 1.2681 1.4157 sec/batch\n",
      "Epoch 19/20  Iteration 3245/3560 Training loss: 1.2675 1.4501 sec/batch\n",
      "Epoch 19/20  Iteration 3246/3560 Training loss: 1.2683 1.3975 sec/batch\n",
      "Epoch 19/20  Iteration 3247/3560 Training loss: 1.2682 1.4418 sec/batch\n",
      "Epoch 19/20  Iteration 3248/3560 Training loss: 1.2675 1.4027 sec/batch\n",
      "Epoch 19/20  Iteration 3249/3560 Training loss: 1.2677 1.3940 sec/batch\n",
      "Epoch 19/20  Iteration 3250/3560 Training loss: 1.2669 1.3705 sec/batch\n",
      "Epoch 19/20  Iteration 3251/3560 Training loss: 1.2667 1.3454 sec/batch\n",
      "Epoch 19/20  Iteration 3252/3560 Training loss: 1.2663 1.4054 sec/batch\n",
      "Epoch 19/20  Iteration 3253/3560 Training loss: 1.2663 1.3516 sec/batch\n",
      "Epoch 19/20  Iteration 3254/3560 Training loss: 1.2667 1.3244 sec/batch\n",
      "Epoch 19/20  Iteration 3255/3560 Training loss: 1.2662 1.3953 sec/batch\n",
      "Epoch 19/20  Iteration 3256/3560 Training loss: 1.2670 1.3853 sec/batch\n",
      "Epoch 19/20  Iteration 3257/3560 Training loss: 1.2669 1.3816 sec/batch\n",
      "Epoch 19/20  Iteration 3258/3560 Training loss: 1.2672 1.3902 sec/batch\n",
      "Epoch 19/20  Iteration 3259/3560 Training loss: 1.2668 1.4197 sec/batch\n",
      "Epoch 19/20  Iteration 3260/3560 Training loss: 1.2668 1.4706 sec/batch\n",
      "Epoch 19/20  Iteration 3261/3560 Training loss: 1.2671 1.5144 sec/batch\n",
      "Epoch 19/20  Iteration 3262/3560 Training loss: 1.2669 1.4620 sec/batch\n",
      "Epoch 19/20  Iteration 3263/3560 Training loss: 1.2666 1.4465 sec/batch\n",
      "Epoch 19/20  Iteration 3264/3560 Training loss: 1.2672 1.4601 sec/batch\n",
      "Epoch 19/20  Iteration 3265/3560 Training loss: 1.2671 1.4771 sec/batch\n",
      "Epoch 19/20  Iteration 3266/3560 Training loss: 1.2680 1.4769 sec/batch\n",
      "Epoch 19/20  Iteration 3267/3560 Training loss: 1.2682 1.4358 sec/batch\n",
      "Epoch 19/20  Iteration 3268/3560 Training loss: 1.2683 1.4589 sec/batch\n",
      "Epoch 19/20  Iteration 3269/3560 Training loss: 1.2682 1.4438 sec/batch\n",
      "Epoch 19/20  Iteration 3270/3560 Training loss: 1.2686 1.5249 sec/batch\n",
      "Epoch 19/20  Iteration 3271/3560 Training loss: 1.2689 1.4689 sec/batch\n",
      "Epoch 19/20  Iteration 3272/3560 Training loss: 1.2685 1.3880 sec/batch\n",
      "Epoch 19/20  Iteration 3273/3560 Training loss: 1.2686 1.4695 sec/batch\n",
      "Epoch 19/20  Iteration 3274/3560 Training loss: 1.2686 1.3637 sec/batch\n",
      "Epoch 19/20  Iteration 3275/3560 Training loss: 1.2692 1.4225 sec/batch\n",
      "Epoch 19/20  Iteration 3276/3560 Training loss: 1.2696 1.3476 sec/batch\n",
      "Epoch 19/20  Iteration 3277/3560 Training loss: 1.2700 1.3452 sec/batch\n",
      "Epoch 19/20  Iteration 3278/3560 Training loss: 1.2696 1.3849 sec/batch\n",
      "Epoch 19/20  Iteration 3279/3560 Training loss: 1.2696 1.4317 sec/batch\n",
      "Epoch 19/20  Iteration 3280/3560 Training loss: 1.2696 1.4244 sec/batch\n",
      "Epoch 19/20  Iteration 3281/3560 Training loss: 1.2694 1.4509 sec/batch\n",
      "Epoch 19/20  Iteration 3282/3560 Training loss: 1.2694 1.4304 sec/batch\n",
      "Epoch 19/20  Iteration 3283/3560 Training loss: 1.2688 1.4239 sec/batch\n",
      "Epoch 19/20  Iteration 3284/3560 Training loss: 1.2689 1.5776 sec/batch\n",
      "Epoch 19/20  Iteration 3285/3560 Training loss: 1.2685 1.5101 sec/batch\n",
      "Epoch 19/20  Iteration 3286/3560 Training loss: 1.2685 1.4796 sec/batch\n",
      "Epoch 19/20  Iteration 3287/3560 Training loss: 1.2681 1.4370 sec/batch\n",
      "Epoch 19/20  Iteration 3288/3560 Training loss: 1.2680 1.3726 sec/batch\n",
      "Epoch 19/20  Iteration 3289/3560 Training loss: 1.2677 1.3350 sec/batch\n",
      "Epoch 19/20  Iteration 3290/3560 Training loss: 1.2676 1.3614 sec/batch\n",
      "Epoch 19/20  Iteration 3291/3560 Training loss: 1.2673 1.4063 sec/batch\n",
      "Epoch 19/20  Iteration 3292/3560 Training loss: 1.2671 1.4231 sec/batch\n",
      "Epoch 19/20  Iteration 3293/3560 Training loss: 1.2668 1.3905 sec/batch\n",
      "Epoch 19/20  Iteration 3294/3560 Training loss: 1.2668 1.4221 sec/batch\n",
      "Epoch 19/20  Iteration 3295/3560 Training loss: 1.2667 1.4453 sec/batch\n",
      "Epoch 19/20  Iteration 3296/3560 Training loss: 1.2666 1.4610 sec/batch\n",
      "Epoch 19/20  Iteration 3297/3560 Training loss: 1.2663 1.3956 sec/batch\n",
      "Epoch 19/20  Iteration 3298/3560 Training loss: 1.2659 1.4747 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3299/3560 Training loss: 1.2657 1.4675 sec/batch\n",
      "Epoch 19/20  Iteration 3300/3560 Training loss: 1.2657 1.4915 sec/batch\n",
      "Epoch 19/20  Iteration 3301/3560 Training loss: 1.2658 1.4487 sec/batch\n",
      "Epoch 19/20  Iteration 3302/3560 Training loss: 1.2654 1.4340 sec/batch\n",
      "Epoch 19/20  Iteration 3303/3560 Training loss: 1.2651 1.4749 sec/batch\n",
      "Epoch 19/20  Iteration 3304/3560 Training loss: 1.2648 1.4557 sec/batch\n",
      "Epoch 19/20  Iteration 3305/3560 Training loss: 1.2647 1.4558 sec/batch\n",
      "Epoch 19/20  Iteration 3306/3560 Training loss: 1.2645 1.4246 sec/batch\n",
      "Epoch 19/20  Iteration 3307/3560 Training loss: 1.2644 1.4657 sec/batch\n",
      "Epoch 19/20  Iteration 3308/3560 Training loss: 1.2643 1.4484 sec/batch\n",
      "Epoch 19/20  Iteration 3309/3560 Training loss: 1.2642 1.5317 sec/batch\n",
      "Epoch 19/20  Iteration 3310/3560 Training loss: 1.2641 1.3968 sec/batch\n",
      "Epoch 19/20  Iteration 3311/3560 Training loss: 1.2640 1.4603 sec/batch\n",
      "Epoch 19/20  Iteration 3312/3560 Training loss: 1.2641 1.4734 sec/batch\n",
      "Epoch 19/20  Iteration 3313/3560 Training loss: 1.2639 1.4355 sec/batch\n",
      "Epoch 19/20  Iteration 3314/3560 Training loss: 1.2640 1.4666 sec/batch\n",
      "Epoch 19/20  Iteration 3315/3560 Training loss: 1.2638 1.4280 sec/batch\n",
      "Epoch 19/20  Iteration 3316/3560 Training loss: 1.2637 1.4677 sec/batch\n",
      "Epoch 19/20  Iteration 3317/3560 Training loss: 1.2637 1.3688 sec/batch\n",
      "Epoch 19/20  Iteration 3318/3560 Training loss: 1.2635 1.3881 sec/batch\n",
      "Epoch 19/20  Iteration 3319/3560 Training loss: 1.2633 1.3354 sec/batch\n",
      "Epoch 19/20  Iteration 3320/3560 Training loss: 1.2629 1.3636 sec/batch\n",
      "Epoch 19/20  Iteration 3321/3560 Training loss: 1.2629 1.3823 sec/batch\n",
      "Epoch 19/20  Iteration 3322/3560 Training loss: 1.2629 1.3762 sec/batch\n",
      "Epoch 19/20  Iteration 3323/3560 Training loss: 1.2629 1.4391 sec/batch\n",
      "Epoch 19/20  Iteration 3324/3560 Training loss: 1.2629 1.4199 sec/batch\n",
      "Epoch 19/20  Iteration 3325/3560 Training loss: 1.2628 1.4076 sec/batch\n",
      "Epoch 19/20  Iteration 3326/3560 Training loss: 1.2625 1.4482 sec/batch\n",
      "Epoch 19/20  Iteration 3327/3560 Training loss: 1.2622 1.3476 sec/batch\n",
      "Epoch 19/20  Iteration 3328/3560 Training loss: 1.2621 1.4537 sec/batch\n",
      "Epoch 19/20  Iteration 3329/3560 Training loss: 1.2621 1.4116 sec/batch\n",
      "Epoch 19/20  Iteration 3330/3560 Training loss: 1.2617 1.4406 sec/batch\n",
      "Epoch 19/20  Iteration 3331/3560 Training loss: 1.2618 1.4341 sec/batch\n",
      "Epoch 19/20  Iteration 3332/3560 Training loss: 1.2619 1.4303 sec/batch\n",
      "Epoch 19/20  Iteration 3333/3560 Training loss: 1.2617 1.4047 sec/batch\n",
      "Epoch 19/20  Iteration 3334/3560 Training loss: 1.2614 1.4003 sec/batch\n",
      "Epoch 19/20  Iteration 3335/3560 Training loss: 1.2609 1.3781 sec/batch\n",
      "Epoch 19/20  Iteration 3336/3560 Training loss: 1.2608 1.3946 sec/batch\n",
      "Epoch 19/20  Iteration 3337/3560 Training loss: 1.2609 1.3725 sec/batch\n",
      "Epoch 19/20  Iteration 3338/3560 Training loss: 1.2609 1.3959 sec/batch\n",
      "Epoch 19/20  Iteration 3339/3560 Training loss: 1.2609 1.3394 sec/batch\n",
      "Epoch 19/20  Iteration 3340/3560 Training loss: 1.2609 1.3639 sec/batch\n",
      "Epoch 19/20  Iteration 3341/3560 Training loss: 1.2611 1.3462 sec/batch\n",
      "Epoch 19/20  Iteration 3342/3560 Training loss: 1.2613 1.4184 sec/batch\n",
      "Epoch 19/20  Iteration 3343/3560 Training loss: 1.2612 1.3396 sec/batch\n",
      "Epoch 19/20  Iteration 3344/3560 Training loss: 1.2613 1.3686 sec/batch\n",
      "Epoch 19/20  Iteration 3345/3560 Training loss: 1.2616 1.3602 sec/batch\n",
      "Epoch 19/20  Iteration 3346/3560 Training loss: 1.2617 1.3775 sec/batch\n",
      "Epoch 19/20  Iteration 3347/3560 Training loss: 1.2617 1.4226 sec/batch\n",
      "Epoch 19/20  Iteration 3348/3560 Training loss: 1.2619 1.3973 sec/batch\n",
      "Epoch 19/20  Iteration 3349/3560 Training loss: 1.2618 1.4435 sec/batch\n",
      "Epoch 19/20  Iteration 3350/3560 Training loss: 1.2620 1.4709 sec/batch\n",
      "Epoch 19/20  Iteration 3351/3560 Training loss: 1.2620 1.4801 sec/batch\n",
      "Epoch 19/20  Iteration 3352/3560 Training loss: 1.2622 1.4314 sec/batch\n",
      "Epoch 19/20  Iteration 3353/3560 Training loss: 1.2623 1.4362 sec/batch\n",
      "Epoch 19/20  Iteration 3354/3560 Training loss: 1.2622 1.4117 sec/batch\n",
      "Epoch 19/20  Iteration 3355/3560 Training loss: 1.2619 1.4355 sec/batch\n",
      "Epoch 19/20  Iteration 3356/3560 Training loss: 1.2617 1.3983 sec/batch\n",
      "Epoch 19/20  Iteration 3357/3560 Training loss: 1.2618 1.4221 sec/batch\n",
      "Epoch 19/20  Iteration 3358/3560 Training loss: 1.2617 1.4183 sec/batch\n",
      "Epoch 19/20  Iteration 3359/3560 Training loss: 1.2617 1.3681 sec/batch\n",
      "Epoch 19/20  Iteration 3360/3560 Training loss: 1.2617 1.4737 sec/batch\n",
      "Epoch 19/20  Iteration 3361/3560 Training loss: 1.2617 1.4295 sec/batch\n",
      "Epoch 19/20  Iteration 3362/3560 Training loss: 1.2616 1.4271 sec/batch\n",
      "Epoch 19/20  Iteration 3363/3560 Training loss: 1.2614 1.4226 sec/batch\n",
      "Epoch 19/20  Iteration 3364/3560 Training loss: 1.2614 1.4258 sec/batch\n",
      "Epoch 19/20  Iteration 3365/3560 Training loss: 1.2616 1.3935 sec/batch\n",
      "Epoch 19/20  Iteration 3366/3560 Training loss: 1.2616 1.4877 sec/batch\n",
      "Epoch 19/20  Iteration 3367/3560 Training loss: 1.2616 2.2195 sec/batch\n",
      "Epoch 19/20  Iteration 3368/3560 Training loss: 1.2615 1.6013 sec/batch\n",
      "Epoch 19/20  Iteration 3369/3560 Training loss: 1.2616 1.8717 sec/batch\n",
      "Epoch 19/20  Iteration 3370/3560 Training loss: 1.2615 1.6035 sec/batch\n",
      "Epoch 19/20  Iteration 3371/3560 Training loss: 1.2617 1.9644 sec/batch\n",
      "Epoch 19/20  Iteration 3372/3560 Training loss: 1.2621 1.7788 sec/batch\n",
      "Epoch 19/20  Iteration 3373/3560 Training loss: 1.2621 1.8010 sec/batch\n",
      "Epoch 19/20  Iteration 3374/3560 Training loss: 1.2621 1.4286 sec/batch\n",
      "Epoch 19/20  Iteration 3375/3560 Training loss: 1.2621 1.4840 sec/batch\n",
      "Epoch 19/20  Iteration 3376/3560 Training loss: 1.2620 1.5555 sec/batch\n",
      "Epoch 19/20  Iteration 3377/3560 Training loss: 1.2621 1.7000 sec/batch\n",
      "Epoch 19/20  Iteration 3378/3560 Training loss: 1.2621 2.1865 sec/batch\n",
      "Epoch 19/20  Iteration 3379/3560 Training loss: 1.2621 1.5154 sec/batch\n",
      "Epoch 19/20  Iteration 3380/3560 Training loss: 1.2620 1.4356 sec/batch\n",
      "Epoch 19/20  Iteration 3381/3560 Training loss: 1.2619 1.4749 sec/batch\n",
      "Epoch 19/20  Iteration 3382/3560 Training loss: 1.2621 1.4429 sec/batch\n",
      "Epoch 20/20  Iteration 3383/3560 Training loss: 1.3835 1.5026 sec/batch\n",
      "Epoch 20/20  Iteration 3384/3560 Training loss: 1.3280 1.9292 sec/batch\n",
      "Epoch 20/20  Iteration 3385/3560 Training loss: 1.3066 2.0401 sec/batch\n",
      "Epoch 20/20  Iteration 3386/3560 Training loss: 1.2984 1.4385 sec/batch\n",
      "Epoch 20/20  Iteration 3387/3560 Training loss: 1.2861 1.4198 sec/batch\n",
      "Epoch 20/20  Iteration 3388/3560 Training loss: 1.2736 1.4413 sec/batch\n",
      "Epoch 20/20  Iteration 3389/3560 Training loss: 1.2723 1.4333 sec/batch\n",
      "Epoch 20/20  Iteration 3390/3560 Training loss: 1.2695 1.4802 sec/batch\n",
      "Epoch 20/20  Iteration 3391/3560 Training loss: 1.2677 1.4506 sec/batch\n",
      "Epoch 20/20  Iteration 3392/3560 Training loss: 1.2659 1.4842 sec/batch\n",
      "Epoch 20/20  Iteration 3393/3560 Training loss: 1.2621 1.4528 sec/batch\n",
      "Epoch 20/20  Iteration 3394/3560 Training loss: 1.2618 1.3613 sec/batch\n",
      "Epoch 20/20  Iteration 3395/3560 Training loss: 1.2614 1.3640 sec/batch\n",
      "Epoch 20/20  Iteration 3396/3560 Training loss: 1.2616 1.3509 sec/batch\n",
      "Epoch 20/20  Iteration 3397/3560 Training loss: 1.2602 1.3840 sec/batch\n",
      "Epoch 20/20  Iteration 3398/3560 Training loss: 1.2586 1.3893 sec/batch\n",
      "Epoch 20/20  Iteration 3399/3560 Training loss: 1.2586 1.5146 sec/batch\n",
      "Epoch 20/20  Iteration 3400/3560 Training loss: 1.2596 1.4807 sec/batch\n",
      "Epoch 20/20  Iteration 3401/3560 Training loss: 1.2593 1.7276 sec/batch\n",
      "Epoch 20/20  Iteration 3402/3560 Training loss: 1.2604 1.5402 sec/batch\n",
      "Epoch 20/20  Iteration 3403/3560 Training loss: 1.2602 1.4873 sec/batch\n",
      "Epoch 20/20  Iteration 3404/3560 Training loss: 1.2608 1.6155 sec/batch\n",
      "Epoch 20/20  Iteration 3405/3560 Training loss: 1.2599 2.0918 sec/batch\n",
      "Epoch 20/20  Iteration 3406/3560 Training loss: 1.2602 1.9475 sec/batch\n",
      "Epoch 20/20  Iteration 3407/3560 Training loss: 1.2602 1.4709 sec/batch\n",
      "Epoch 20/20  Iteration 3408/3560 Training loss: 1.2582 1.4643 sec/batch\n",
      "Epoch 20/20  Iteration 3409/3560 Training loss: 1.2569 1.4241 sec/batch\n",
      "Epoch 20/20  Iteration 3410/3560 Training loss: 1.2576 1.4088 sec/batch\n",
      "Epoch 20/20  Iteration 3411/3560 Training loss: 1.2575 1.4088 sec/batch\n",
      "Epoch 20/20  Iteration 3412/3560 Training loss: 1.2581 1.3982 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3413/3560 Training loss: 1.2572 1.4187 sec/batch\n",
      "Epoch 20/20  Iteration 3414/3560 Training loss: 1.2563 1.3647 sec/batch\n",
      "Epoch 20/20  Iteration 3415/3560 Training loss: 1.2568 1.4094 sec/batch\n",
      "Epoch 20/20  Iteration 3416/3560 Training loss: 1.2569 1.4114 sec/batch\n",
      "Epoch 20/20  Iteration 3417/3560 Training loss: 1.2566 1.4449 sec/batch\n",
      "Epoch 20/20  Iteration 3418/3560 Training loss: 1.2565 1.4120 sec/batch\n",
      "Epoch 20/20  Iteration 3419/3560 Training loss: 1.2559 1.4553 sec/batch\n",
      "Epoch 20/20  Iteration 3420/3560 Training loss: 1.2549 1.4402 sec/batch\n",
      "Epoch 20/20  Iteration 3421/3560 Training loss: 1.2535 1.4465 sec/batch\n",
      "Epoch 20/20  Iteration 3422/3560 Training loss: 1.2531 1.4478 sec/batch\n",
      "Epoch 20/20  Iteration 3423/3560 Training loss: 1.2525 1.4594 sec/batch\n",
      "Epoch 20/20  Iteration 3424/3560 Training loss: 1.2535 1.6174 sec/batch\n",
      "Epoch 20/20  Iteration 3425/3560 Training loss: 1.2533 2.1747 sec/batch\n",
      "Epoch 20/20  Iteration 3426/3560 Training loss: 1.2525 1.6906 sec/batch\n",
      "Epoch 20/20  Iteration 3427/3560 Training loss: 1.2527 1.3793 sec/batch\n",
      "Epoch 20/20  Iteration 3428/3560 Training loss: 1.2520 1.4043 sec/batch\n",
      "Epoch 20/20  Iteration 3429/3560 Training loss: 1.2518 1.4312 sec/batch\n",
      "Epoch 20/20  Iteration 3430/3560 Training loss: 1.2514 1.4318 sec/batch\n",
      "Epoch 20/20  Iteration 3431/3560 Training loss: 1.2515 1.4239 sec/batch\n",
      "Epoch 20/20  Iteration 3432/3560 Training loss: 1.2518 1.4579 sec/batch\n",
      "Epoch 20/20  Iteration 3433/3560 Training loss: 1.2512 1.4527 sec/batch\n",
      "Epoch 20/20  Iteration 3434/3560 Training loss: 1.2519 1.4386 sec/batch\n",
      "Epoch 20/20  Iteration 3435/3560 Training loss: 1.2518 1.4210 sec/batch\n",
      "Epoch 20/20  Iteration 3436/3560 Training loss: 1.2521 1.4476 sec/batch\n",
      "Epoch 20/20  Iteration 3437/3560 Training loss: 1.2521 1.4353 sec/batch\n",
      "Epoch 20/20  Iteration 3438/3560 Training loss: 1.2523 1.4275 sec/batch\n",
      "Epoch 20/20  Iteration 3439/3560 Training loss: 1.2527 1.5117 sec/batch\n",
      "Epoch 20/20  Iteration 3440/3560 Training loss: 1.2524 1.4343 sec/batch\n",
      "Epoch 20/20  Iteration 3441/3560 Training loss: 1.2520 1.4114 sec/batch\n",
      "Epoch 20/20  Iteration 3442/3560 Training loss: 1.2527 1.3937 sec/batch\n",
      "Epoch 20/20  Iteration 3443/3560 Training loss: 1.2529 1.3987 sec/batch\n",
      "Epoch 20/20  Iteration 3444/3560 Training loss: 1.2536 1.3880 sec/batch\n",
      "Epoch 20/20  Iteration 3445/3560 Training loss: 1.2537 1.7211 sec/batch\n",
      "Epoch 20/20  Iteration 3446/3560 Training loss: 1.2539 2.2496 sec/batch\n",
      "Epoch 20/20  Iteration 3447/3560 Training loss: 1.2539 1.4926 sec/batch\n",
      "Epoch 20/20  Iteration 3448/3560 Training loss: 1.2541 1.4649 sec/batch\n",
      "Epoch 20/20  Iteration 3449/3560 Training loss: 1.2542 1.4559 sec/batch\n",
      "Epoch 20/20  Iteration 3450/3560 Training loss: 1.2541 1.4315 sec/batch\n",
      "Epoch 20/20  Iteration 3451/3560 Training loss: 1.2543 1.3885 sec/batch\n",
      "Epoch 20/20  Iteration 3452/3560 Training loss: 1.2543 1.3325 sec/batch\n",
      "Epoch 20/20  Iteration 3453/3560 Training loss: 1.2550 1.4283 sec/batch\n",
      "Epoch 20/20  Iteration 3454/3560 Training loss: 1.2554 1.5801 sec/batch\n",
      "Epoch 20/20  Iteration 3455/3560 Training loss: 1.2559 1.5024 sec/batch\n",
      "Epoch 20/20  Iteration 3456/3560 Training loss: 1.2556 1.4551 sec/batch\n",
      "Epoch 20/20  Iteration 3457/3560 Training loss: 1.2556 2.1358 sec/batch\n",
      "Epoch 20/20  Iteration 3458/3560 Training loss: 1.2558 2.0442 sec/batch\n",
      "Epoch 20/20  Iteration 3459/3560 Training loss: 1.2557 1.8103 sec/batch\n",
      "Epoch 20/20  Iteration 3460/3560 Training loss: 1.2555 1.7991 sec/batch\n",
      "Epoch 20/20  Iteration 3461/3560 Training loss: 1.2551 1.5111 sec/batch\n",
      "Epoch 20/20  Iteration 3462/3560 Training loss: 1.2551 1.4909 sec/batch\n",
      "Epoch 20/20  Iteration 3463/3560 Training loss: 1.2547 1.8222 sec/batch\n",
      "Epoch 20/20  Iteration 3464/3560 Training loss: 1.2546 2.4120 sec/batch\n",
      "Epoch 20/20  Iteration 3465/3560 Training loss: 1.2542 1.9246 sec/batch\n",
      "Epoch 20/20  Iteration 3466/3560 Training loss: 1.2543 1.7588 sec/batch\n",
      "Epoch 20/20  Iteration 3467/3560 Training loss: 1.2540 1.4148 sec/batch\n",
      "Epoch 20/20  Iteration 3468/3560 Training loss: 1.2540 1.4557 sec/batch\n",
      "Epoch 20/20  Iteration 3469/3560 Training loss: 1.2537 1.6302 sec/batch\n",
      "Epoch 20/20  Iteration 3470/3560 Training loss: 1.2535 1.6244 sec/batch\n",
      "Epoch 20/20  Iteration 3471/3560 Training loss: 1.2531 1.7147 sec/batch\n",
      "Epoch 20/20  Iteration 3472/3560 Training loss: 1.2531 1.5575 sec/batch\n",
      "Epoch 20/20  Iteration 3473/3560 Training loss: 1.2529 1.7593 sec/batch\n",
      "Epoch 20/20  Iteration 3474/3560 Training loss: 1.2528 1.3737 sec/batch\n",
      "Epoch 20/20  Iteration 3475/3560 Training loss: 1.2525 1.3895 sec/batch\n",
      "Epoch 20/20  Iteration 3476/3560 Training loss: 1.2521 1.3385 sec/batch\n",
      "Epoch 20/20  Iteration 3477/3560 Training loss: 1.2519 1.6160 sec/batch\n",
      "Epoch 20/20  Iteration 3478/3560 Training loss: 1.2520 1.6402 sec/batch\n",
      "Epoch 20/20  Iteration 3479/3560 Training loss: 1.2520 1.5476 sec/batch\n",
      "Epoch 20/20  Iteration 3480/3560 Training loss: 1.2517 1.4287 sec/batch\n",
      "Epoch 20/20  Iteration 3481/3560 Training loss: 1.2513 1.4560 sec/batch\n",
      "Epoch 20/20  Iteration 3482/3560 Training loss: 1.2510 2.0357 sec/batch\n",
      "Epoch 20/20  Iteration 3483/3560 Training loss: 1.2510 1.9126 sec/batch\n",
      "Epoch 20/20  Iteration 3484/3560 Training loss: 1.2509 1.4062 sec/batch\n",
      "Epoch 20/20  Iteration 3485/3560 Training loss: 1.2508 1.3507 sec/batch\n",
      "Epoch 20/20  Iteration 3486/3560 Training loss: 1.2506 1.3724 sec/batch\n",
      "Epoch 20/20  Iteration 3487/3560 Training loss: 1.2505 1.4056 sec/batch\n",
      "Epoch 20/20  Iteration 3488/3560 Training loss: 1.2504 1.3793 sec/batch\n",
      "Epoch 20/20  Iteration 3489/3560 Training loss: 1.2503 1.3828 sec/batch\n",
      "Epoch 20/20  Iteration 3490/3560 Training loss: 1.2504 1.4417 sec/batch\n",
      "Epoch 20/20  Iteration 3491/3560 Training loss: 1.2502 1.3908 sec/batch\n",
      "Epoch 20/20  Iteration 3492/3560 Training loss: 1.2503 1.3196 sec/batch\n",
      "Epoch 20/20  Iteration 3493/3560 Training loss: 1.2501 1.3313 sec/batch\n",
      "Epoch 20/20  Iteration 3494/3560 Training loss: 1.2500 1.3278 sec/batch\n",
      "Epoch 20/20  Iteration 3495/3560 Training loss: 1.2498 1.3262 sec/batch\n",
      "Epoch 20/20  Iteration 3496/3560 Training loss: 1.2497 1.3228 sec/batch\n",
      "Epoch 20/20  Iteration 3497/3560 Training loss: 1.2495 1.3367 sec/batch\n",
      "Epoch 20/20  Iteration 3498/3560 Training loss: 1.2491 1.4519 sec/batch\n",
      "Epoch 20/20  Iteration 3499/3560 Training loss: 1.2492 1.4261 sec/batch\n",
      "Epoch 20/20  Iteration 3500/3560 Training loss: 1.2491 1.4317 sec/batch\n",
      "Epoch 20/20  Iteration 3501/3560 Training loss: 1.2490 1.4255 sec/batch\n",
      "Epoch 20/20  Iteration 3502/3560 Training loss: 1.2490 1.6238 sec/batch\n",
      "Epoch 20/20  Iteration 3503/3560 Training loss: 1.2489 2.1296 sec/batch\n",
      "Epoch 20/20  Iteration 3504/3560 Training loss: 1.2486 1.6600 sec/batch\n",
      "Epoch 20/20  Iteration 3505/3560 Training loss: 1.2482 1.3875 sec/batch\n",
      "Epoch 20/20  Iteration 3506/3560 Training loss: 1.2482 1.3701 sec/batch\n",
      "Epoch 20/20  Iteration 3507/3560 Training loss: 1.2480 1.3415 sec/batch\n",
      "Epoch 20/20  Iteration 3508/3560 Training loss: 1.2476 1.3813 sec/batch\n",
      "Epoch 20/20  Iteration 3509/3560 Training loss: 1.2476 1.3509 sec/batch\n",
      "Epoch 20/20  Iteration 3510/3560 Training loss: 1.2476 1.3541 sec/batch\n",
      "Epoch 20/20  Iteration 3511/3560 Training loss: 1.2475 1.4120 sec/batch\n",
      "Epoch 20/20  Iteration 3512/3560 Training loss: 1.2471 1.3526 sec/batch\n",
      "Epoch 20/20  Iteration 3513/3560 Training loss: 1.2467 1.3243 sec/batch\n",
      "Epoch 20/20  Iteration 3514/3560 Training loss: 1.2465 1.3440 sec/batch\n",
      "Epoch 20/20  Iteration 3515/3560 Training loss: 1.2466 1.3196 sec/batch\n",
      "Epoch 20/20  Iteration 3516/3560 Training loss: 1.2466 1.3467 sec/batch\n",
      "Epoch 20/20  Iteration 3517/3560 Training loss: 1.2466 1.3368 sec/batch\n",
      "Epoch 20/20  Iteration 3518/3560 Training loss: 1.2468 1.3246 sec/batch\n",
      "Epoch 20/20  Iteration 3519/3560 Training loss: 1.2469 1.4344 sec/batch\n",
      "Epoch 20/20  Iteration 3520/3560 Training loss: 1.2471 1.3392 sec/batch\n",
      "Epoch 20/20  Iteration 3521/3560 Training loss: 1.2470 1.3354 sec/batch\n",
      "Epoch 20/20  Iteration 3522/3560 Training loss: 1.2471 1.3309 sec/batch\n",
      "Epoch 20/20  Iteration 3523/3560 Training loss: 1.2474 1.3577 sec/batch\n",
      "Epoch 20/20  Iteration 3524/3560 Training loss: 1.2475 1.8738 sec/batch\n",
      "Epoch 20/20  Iteration 3525/3560 Training loss: 1.2475 1.8487 sec/batch\n",
      "Epoch 20/20  Iteration 3526/3560 Training loss: 1.2477 1.4436 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3527/3560 Training loss: 1.2477 1.3963 sec/batch\n",
      "Epoch 20/20  Iteration 3528/3560 Training loss: 1.2478 1.4006 sec/batch\n",
      "Epoch 20/20  Iteration 3529/3560 Training loss: 1.2478 1.4190 sec/batch\n",
      "Epoch 20/20  Iteration 3530/3560 Training loss: 1.2480 1.4066 sec/batch\n",
      "Epoch 20/20  Iteration 3531/3560 Training loss: 1.2482 1.3520 sec/batch\n",
      "Epoch 20/20  Iteration 3532/3560 Training loss: 1.2480 1.4352 sec/batch\n",
      "Epoch 20/20  Iteration 3533/3560 Training loss: 1.2478 1.3908 sec/batch\n",
      "Epoch 20/20  Iteration 3534/3560 Training loss: 1.2477 1.3308 sec/batch\n",
      "Epoch 20/20  Iteration 3535/3560 Training loss: 1.2478 1.3772 sec/batch\n",
      "Epoch 20/20  Iteration 3536/3560 Training loss: 1.2479 1.3291 sec/batch\n",
      "Epoch 20/20  Iteration 3537/3560 Training loss: 1.2479 1.3151 sec/batch\n",
      "Epoch 20/20  Iteration 3538/3560 Training loss: 1.2478 1.3227 sec/batch\n",
      "Epoch 20/20  Iteration 3539/3560 Training loss: 1.2478 1.3413 sec/batch\n",
      "Epoch 20/20  Iteration 3540/3560 Training loss: 1.2477 1.4028 sec/batch\n",
      "Epoch 20/20  Iteration 3541/3560 Training loss: 1.2475 1.3566 sec/batch\n",
      "Epoch 20/20  Iteration 3542/3560 Training loss: 1.2476 1.3357 sec/batch\n",
      "Epoch 20/20  Iteration 3543/3560 Training loss: 1.2477 1.3675 sec/batch\n",
      "Epoch 20/20  Iteration 3544/3560 Training loss: 1.2476 1.3951 sec/batch\n",
      "Epoch 20/20  Iteration 3545/3560 Training loss: 1.2477 1.8480 sec/batch\n",
      "Epoch 20/20  Iteration 3546/3560 Training loss: 1.2477 1.9084 sec/batch\n",
      "Epoch 20/20  Iteration 3547/3560 Training loss: 1.2477 1.3796 sec/batch\n",
      "Epoch 20/20  Iteration 3548/3560 Training loss: 1.2476 1.3317 sec/batch\n",
      "Epoch 20/20  Iteration 3549/3560 Training loss: 1.2477 1.3601 sec/batch\n",
      "Epoch 20/20  Iteration 3550/3560 Training loss: 1.2481 1.3409 sec/batch\n",
      "Epoch 20/20  Iteration 3551/3560 Training loss: 1.2481 1.3535 sec/batch\n",
      "Epoch 20/20  Iteration 3552/3560 Training loss: 1.2481 1.3593 sec/batch\n",
      "Epoch 20/20  Iteration 3553/3560 Training loss: 1.2480 1.3975 sec/batch\n",
      "Epoch 20/20  Iteration 3554/3560 Training loss: 1.2479 1.4282 sec/batch\n",
      "Epoch 20/20  Iteration 3555/3560 Training loss: 1.2481 1.3426 sec/batch\n",
      "Epoch 20/20  Iteration 3556/3560 Training loss: 1.2482 1.3659 sec/batch\n",
      "Epoch 20/20  Iteration 3557/3560 Training loss: 1.2481 1.3318 sec/batch\n",
      "Epoch 20/20  Iteration 3558/3560 Training loss: 1.2480 1.3652 sec/batch\n",
      "Epoch 20/20  Iteration 3559/3560 Training loss: 1.2479 1.3428 sec/batch\n",
      "Epoch 20/20  Iteration 3560/3560 Training loss: 1.2481 1.3965 sec/batch\n",
      "Epoch 1/20  Iteration 1/3560 Training loss: 4.4204 3.7357 sec/batch\n",
      "Epoch 1/20  Iteration 2/3560 Training loss: 4.3430 4.5977 sec/batch\n",
      "Epoch 1/20  Iteration 3/3560 Training loss: 4.8116 3.7893 sec/batch\n",
      "Epoch 1/20  Iteration 4/3560 Training loss: 4.6031 3.1477 sec/batch\n",
      "Epoch 1/20  Iteration 5/3560 Training loss: 4.4344 3.2711 sec/batch\n",
      "Epoch 1/20  Iteration 6/3560 Training loss: 4.2901 3.3810 sec/batch\n",
      "Epoch 1/20  Iteration 7/3560 Training loss: 4.1691 3.4588 sec/batch\n",
      "Epoch 1/20  Iteration 8/3560 Training loss: 4.0751 3.4173 sec/batch\n",
      "Epoch 1/20  Iteration 9/3560 Training loss: 3.9905 3.4333 sec/batch\n",
      "Epoch 1/20  Iteration 10/3560 Training loss: 3.9192 3.2060 sec/batch\n",
      "Epoch 1/20  Iteration 11/3560 Training loss: 3.8583 4.2317 sec/batch\n",
      "Epoch 1/20  Iteration 12/3560 Training loss: 3.8085 3.3666 sec/batch\n",
      "Epoch 1/20  Iteration 13/3560 Training loss: 3.7663 3.3188 sec/batch\n",
      "Epoch 1/20  Iteration 14/3560 Training loss: 3.7302 3.2818 sec/batch\n",
      "Epoch 1/20  Iteration 15/3560 Training loss: 3.6978 3.3352 sec/batch\n",
      "Epoch 1/20  Iteration 16/3560 Training loss: 3.6686 3.3530 sec/batch\n",
      "Epoch 1/20  Iteration 17/3560 Training loss: 3.6423 3.3600 sec/batch\n",
      "Epoch 1/20  Iteration 18/3560 Training loss: 3.6205 3.3314 sec/batch\n",
      "Epoch 1/20  Iteration 19/3560 Training loss: 3.5987 3.2053 sec/batch\n",
      "Epoch 1/20  Iteration 20/3560 Training loss: 3.5774 4.0669 sec/batch\n",
      "Epoch 1/20  Iteration 21/3560 Training loss: 3.5586 3.3725 sec/batch\n",
      "Epoch 1/20  Iteration 22/3560 Training loss: 3.5420 3.2588 sec/batch\n",
      "Epoch 1/20  Iteration 23/3560 Training loss: 3.5262 3.2897 sec/batch\n",
      "Epoch 1/20  Iteration 24/3560 Training loss: 3.5117 3.2444 sec/batch\n",
      "Epoch 1/20  Iteration 25/3560 Training loss: 3.4981 3.1101 sec/batch\n",
      "Epoch 1/20  Iteration 26/3560 Training loss: 3.4859 3.1878 sec/batch\n",
      "Epoch 1/20  Iteration 27/3560 Training loss: 3.4747 3.3982 sec/batch\n",
      "Epoch 1/20  Iteration 28/3560 Training loss: 3.4634 3.6552 sec/batch\n",
      "Epoch 1/20  Iteration 29/3560 Training loss: 3.4531 4.0983 sec/batch\n",
      "Epoch 1/20  Iteration 30/3560 Training loss: 3.4437 3.4118 sec/batch\n",
      "Epoch 1/20  Iteration 31/3560 Training loss: 3.4354 3.2726 sec/batch\n",
      "Epoch 1/20  Iteration 32/3560 Training loss: 3.4263 3.1752 sec/batch\n",
      "Epoch 1/20  Iteration 33/3560 Training loss: 3.4178 3.1371 sec/batch\n",
      "Epoch 1/20  Iteration 34/3560 Training loss: 3.4104 3.1759 sec/batch\n",
      "Epoch 1/20  Iteration 35/3560 Training loss: 3.4025 3.1812 sec/batch\n",
      "Epoch 1/20  Iteration 36/3560 Training loss: 3.3956 3.2011 sec/batch\n",
      "Epoch 1/20  Iteration 37/3560 Training loss: 3.3881 3.4703 sec/batch\n",
      "Epoch 1/20  Iteration 38/3560 Training loss: 3.3812 3.9523 sec/batch\n",
      "Epoch 1/20  Iteration 39/3560 Training loss: 3.3743 3.3573 sec/batch\n",
      "Epoch 1/20  Iteration 40/3560 Training loss: 3.3679 3.1282 sec/batch\n",
      "Epoch 1/20  Iteration 41/3560 Training loss: 3.3619 3.1477 sec/batch\n",
      "Epoch 1/20  Iteration 42/3560 Training loss: 3.3561 3.1686 sec/batch\n",
      "Epoch 1/20  Iteration 43/3560 Training loss: 3.3504 3.4625 sec/batch\n",
      "Epoch 1/20  Iteration 44/3560 Training loss: 3.3448 3.2881 sec/batch\n",
      "Epoch 1/20  Iteration 45/3560 Training loss: 3.3392 3.2354 sec/batch\n",
      "Epoch 1/20  Iteration 46/3560 Training loss: 3.3342 3.3825 sec/batch\n",
      "Epoch 1/20  Iteration 47/3560 Training loss: 3.3295 3.9662 sec/batch\n",
      "Epoch 1/20  Iteration 48/3560 Training loss: 3.3250 3.4714 sec/batch\n",
      "Epoch 1/20  Iteration 49/3560 Training loss: 3.3205 3.3607 sec/batch\n",
      "Epoch 1/20  Iteration 50/3560 Training loss: 3.3160 3.2288 sec/batch\n",
      "Epoch 1/20  Iteration 51/3560 Training loss: 3.3116 3.2627 sec/batch\n",
      "Epoch 1/20  Iteration 52/3560 Training loss: 3.3072 3.2671 sec/batch\n",
      "Epoch 1/20  Iteration 53/3560 Training loss: 3.3030 3.2331 sec/batch\n",
      "Epoch 1/20  Iteration 54/3560 Training loss: 3.2985 3.4293 sec/batch\n",
      "Epoch 1/20  Iteration 55/3560 Training loss: 3.4453 4.1955 sec/batch\n",
      "Epoch 1/20  Iteration 56/3560 Training loss: 3.4787 3.2680 sec/batch\n",
      "Epoch 1/20  Iteration 57/3560 Training loss: 3.4761 3.1422 sec/batch\n",
      "Epoch 1/20  Iteration 58/3560 Training loss: 3.4715 3.1286 sec/batch\n",
      "Epoch 1/20  Iteration 59/3560 Training loss: 3.4671 3.2119 sec/batch\n",
      "Epoch 1/20  Iteration 60/3560 Training loss: 3.4611 3.3682 sec/batch\n",
      "Epoch 1/20  Iteration 61/3560 Training loss: 3.4553 3.2467 sec/batch\n",
      "Epoch 1/20  Iteration 62/3560 Training loss: 3.4502 3.2423 sec/batch\n",
      "Epoch 1/20  Iteration 63/3560 Training loss: 3.4452 3.1468 sec/batch\n",
      "Epoch 1/20  Iteration 64/3560 Training loss: 3.4397 3.9851 sec/batch\n",
      "Epoch 1/20  Iteration 65/3560 Training loss: 3.4344 3.3005 sec/batch\n",
      "Epoch 1/20  Iteration 66/3560 Training loss: 3.4296 3.1819 sec/batch\n",
      "Epoch 1/20  Iteration 67/3560 Training loss: 3.4248 3.1743 sec/batch\n",
      "Epoch 1/20  Iteration 68/3560 Training loss: 3.4195 3.1983 sec/batch\n",
      "Epoch 1/20  Iteration 69/3560 Training loss: 3.4147 3.2373 sec/batch\n",
      "Epoch 1/20  Iteration 70/3560 Training loss: 3.4102 3.3912 sec/batch\n",
      "Epoch 1/20  Iteration 71/3560 Training loss: 3.4057 3.4812 sec/batch\n",
      "Epoch 1/20  Iteration 72/3560 Training loss: 3.4016 3.4410 sec/batch\n",
      "Epoch 1/20  Iteration 73/3560 Training loss: 3.3971 4.3495 sec/batch\n",
      "Epoch 1/20  Iteration 74/3560 Training loss: 3.3928 3.3107 sec/batch\n",
      "Epoch 1/20  Iteration 75/3560 Training loss: 3.3887 3.3443 sec/batch\n",
      "Epoch 1/20  Iteration 76/3560 Training loss: 3.3847 3.1815 sec/batch\n",
      "Epoch 1/20  Iteration 77/3560 Training loss: 3.3807 3.1216 sec/batch\n",
      "Epoch 1/20  Iteration 78/3560 Training loss: 3.3767 3.3235 sec/batch\n",
      "Epoch 1/20  Iteration 79/3560 Training loss: 3.3727 3.2797 sec/batch\n",
      "Epoch 1/20  Iteration 80/3560 Training loss: 3.3685 3.4010 sec/batch\n",
      "Epoch 1/20  Iteration 81/3560 Training loss: 3.3645 3.3697 sec/batch\n",
      "Epoch 1/20  Iteration 82/3560 Training loss: 3.3607 4.0730 sec/batch\n",
      "Epoch 1/20  Iteration 83/3560 Training loss: 3.3569 3.3770 sec/batch\n",
      "Epoch 1/20  Iteration 84/3560 Training loss: 3.3532 3.3291 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 85/3560 Training loss: 3.3492 3.2999 sec/batch\n",
      "Epoch 1/20  Iteration 86/3560 Training loss: 3.3453 3.3871 sec/batch\n",
      "Epoch 1/20  Iteration 87/3560 Training loss: 3.3414 3.3563 sec/batch\n",
      "Epoch 1/20  Iteration 88/3560 Training loss: 3.3375 3.3257 sec/batch\n",
      "Epoch 1/20  Iteration 89/3560 Training loss: 3.3338 3.3445 sec/batch\n",
      "Epoch 1/20  Iteration 90/3560 Training loss: 3.3303 3.4398 sec/batch\n",
      "Epoch 1/20  Iteration 91/3560 Training loss: 3.3267 4.3050 sec/batch\n",
      "Epoch 1/20  Iteration 92/3560 Training loss: 3.3231 3.5031 sec/batch\n",
      "Epoch 1/20  Iteration 93/3560 Training loss: 3.3195 3.3150 sec/batch\n",
      "Epoch 1/20  Iteration 94/3560 Training loss: 3.3159 3.2655 sec/batch\n",
      "Epoch 1/20  Iteration 95/3560 Training loss: 3.3122 3.2167 sec/batch\n",
      "Epoch 1/20  Iteration 96/3560 Training loss: 3.3086 3.1981 sec/batch\n",
      "Epoch 1/20  Iteration 97/3560 Training loss: 3.3051 3.2985 sec/batch\n",
      "Epoch 1/20  Iteration 98/3560 Training loss: 3.3015 3.2942 sec/batch\n",
      "Epoch 1/20  Iteration 99/3560 Training loss: 3.2979 3.6396 sec/batch\n",
      "Epoch 1/20  Iteration 100/3560 Training loss: 3.2943 3.8533 sec/batch\n",
      "Epoch 1/20  Iteration 101/3560 Training loss: 3.2906 3.3135 sec/batch\n",
      "Epoch 1/20  Iteration 102/3560 Training loss: 3.2869 3.2066 sec/batch\n",
      "Epoch 1/20  Iteration 103/3560 Training loss: 3.2832 3.0778 sec/batch\n",
      "Epoch 1/20  Iteration 104/3560 Training loss: 3.2796 3.1597 sec/batch\n",
      "Epoch 1/20  Iteration 105/3560 Training loss: 3.2759 3.1534 sec/batch\n",
      "Epoch 1/20  Iteration 106/3560 Training loss: 3.2722 3.1012 sec/batch\n",
      "Epoch 1/20  Iteration 107/3560 Training loss: 3.2683 3.2659 sec/batch\n",
      "Epoch 1/20  Iteration 108/3560 Training loss: 3.2647 3.9071 sec/batch\n",
      "Epoch 1/20  Iteration 109/3560 Training loss: 3.2610 3.8972 sec/batch\n",
      "Epoch 1/20  Iteration 110/3560 Training loss: 3.2569 3.2081 sec/batch\n",
      "Epoch 1/20  Iteration 111/3560 Training loss: 3.2531 3.1927 sec/batch\n",
      "Epoch 1/20  Iteration 112/3560 Training loss: 3.2493 3.1453 sec/batch\n",
      "Epoch 1/20  Iteration 113/3560 Training loss: 3.2454 3.1044 sec/batch\n",
      "Epoch 1/20  Iteration 114/3560 Training loss: 3.2412 3.4279 sec/batch\n",
      "Epoch 1/20  Iteration 115/3560 Training loss: 3.2371 3.2703 sec/batch\n",
      "Epoch 1/20  Iteration 116/3560 Training loss: 3.2329 3.2117 sec/batch\n",
      "Epoch 1/20  Iteration 117/3560 Training loss: 3.2288 3.8594 sec/batch\n",
      "Epoch 1/20  Iteration 118/3560 Training loss: 3.2248 3.7015 sec/batch\n",
      "Epoch 1/20  Iteration 119/3560 Training loss: 3.2210 3.2052 sec/batch\n",
      "Epoch 1/20  Iteration 120/3560 Training loss: 3.2173 3.2044 sec/batch\n",
      "Epoch 1/20  Iteration 121/3560 Training loss: 3.2134 3.2141 sec/batch\n",
      "Epoch 1/20  Iteration 122/3560 Training loss: 3.2095 3.3391 sec/batch\n",
      "Epoch 1/20  Iteration 123/3560 Training loss: 3.2055 3.3312 sec/batch\n",
      "Epoch 1/20  Iteration 124/3560 Training loss: 3.2014 3.3130 sec/batch\n",
      "Epoch 1/20  Iteration 125/3560 Training loss: 3.1974 3.2319 sec/batch\n",
      "Epoch 1/20  Iteration 126/3560 Training loss: 3.1930 4.0322 sec/batch\n",
      "Epoch 1/20  Iteration 127/3560 Training loss: 3.1890 3.5704 sec/batch\n",
      "Epoch 1/20  Iteration 128/3560 Training loss: 3.1851 3.2764 sec/batch\n",
      "Epoch 1/20  Iteration 129/3560 Training loss: 3.1809 3.3190 sec/batch\n",
      "Epoch 1/20  Iteration 130/3560 Training loss: 3.1767 3.0902 sec/batch\n",
      "Epoch 1/20  Iteration 131/3560 Training loss: 3.1726 3.1819 sec/batch\n",
      "Epoch 1/20  Iteration 132/3560 Training loss: 3.1683 3.1311 sec/batch\n",
      "Epoch 1/20  Iteration 133/3560 Training loss: 3.1642 3.0830 sec/batch\n",
      "Epoch 1/20  Iteration 134/3560 Training loss: 3.1601 3.1881 sec/batch\n",
      "Epoch 1/20  Iteration 135/3560 Training loss: 3.1558 3.5535 sec/batch\n",
      "Epoch 1/20  Iteration 136/3560 Training loss: 3.1516 3.8746 sec/batch\n",
      "Epoch 1/20  Iteration 137/3560 Training loss: 3.1474 3.1141 sec/batch\n",
      "Epoch 1/20  Iteration 138/3560 Training loss: 3.1433 3.1878 sec/batch\n",
      "Epoch 1/20  Iteration 139/3560 Training loss: 3.1392 3.2834 sec/batch\n",
      "Epoch 1/20  Iteration 140/3560 Training loss: 3.1351 3.2610 sec/batch\n",
      "Epoch 1/20  Iteration 141/3560 Training loss: 3.1311 3.4215 sec/batch\n",
      "Epoch 1/20  Iteration 142/3560 Training loss: 3.1269 3.3592 sec/batch\n",
      "Epoch 1/20  Iteration 143/3560 Training loss: 3.1229 3.5627 sec/batch\n",
      "Epoch 1/20  Iteration 144/3560 Training loss: 3.1188 5.4284 sec/batch\n",
      "Epoch 1/20  Iteration 145/3560 Training loss: 3.1148 3.6890 sec/batch\n",
      "Epoch 1/20  Iteration 146/3560 Training loss: 3.1109 3.8728 sec/batch\n",
      "Epoch 1/20  Iteration 147/3560 Training loss: 3.1069 4.4507 sec/batch\n",
      "Epoch 1/20  Iteration 148/3560 Training loss: 3.1032 3.8949 sec/batch\n",
      "Epoch 1/20  Iteration 149/3560 Training loss: 3.0992 3.3496 sec/batch\n",
      "Epoch 1/20  Iteration 150/3560 Training loss: 3.0952 4.2168 sec/batch\n",
      "Epoch 1/20  Iteration 151/3560 Training loss: 3.0915 4.5822 sec/batch\n",
      "Epoch 1/20  Iteration 152/3560 Training loss: 3.0878 5.1867 sec/batch\n",
      "Epoch 1/20  Iteration 153/3560 Training loss: 3.0840 4.5649 sec/batch\n",
      "Epoch 1/20  Iteration 154/3560 Training loss: 3.0802 5.5268 sec/batch\n",
      "Epoch 1/20  Iteration 155/3560 Training loss: 3.0764 3.9237 sec/batch\n",
      "Epoch 1/20  Iteration 156/3560 Training loss: 3.0726 3.6559 sec/batch\n",
      "Epoch 1/20  Iteration 157/3560 Training loss: 3.0687 3.2839 sec/batch\n",
      "Epoch 1/20  Iteration 158/3560 Training loss: 3.0649 4.4521 sec/batch\n",
      "Epoch 1/20  Iteration 159/3560 Training loss: 3.0610 3.8068 sec/batch\n",
      "Epoch 1/20  Iteration 160/3560 Training loss: 3.0574 3.4312 sec/batch\n",
      "Epoch 1/20  Iteration 161/3560 Training loss: 3.0542 3.2545 sec/batch\n",
      "Epoch 1/20  Iteration 162/3560 Training loss: 3.0507 3.3337 sec/batch\n",
      "Epoch 1/20  Iteration 163/3560 Training loss: 3.0470 3.3863 sec/batch\n",
      "Epoch 1/20  Iteration 164/3560 Training loss: 3.0434 3.3778 sec/batch\n",
      "Epoch 1/20  Iteration 165/3560 Training loss: 3.0398 3.5121 sec/batch\n",
      "Epoch 1/20  Iteration 166/3560 Training loss: 3.0362 3.6049 sec/batch\n",
      "Epoch 1/20  Iteration 167/3560 Training loss: 3.0327 4.6358 sec/batch\n",
      "Epoch 1/20  Iteration 168/3560 Training loss: 3.0292 3.3536 sec/batch\n",
      "Epoch 1/20  Iteration 169/3560 Training loss: 3.0258 3.0528 sec/batch\n",
      "Epoch 1/20  Iteration 170/3560 Training loss: 3.0221 3.5750 sec/batch\n",
      "Epoch 1/20  Iteration 171/3560 Training loss: 3.0187 3.2356 sec/batch\n",
      "Epoch 1/20  Iteration 172/3560 Training loss: 3.0154 3.4169 sec/batch\n",
      "Epoch 1/20  Iteration 173/3560 Training loss: 3.0123 3.3855 sec/batch\n",
      "Epoch 1/20  Iteration 174/3560 Training loss: 3.0091 3.3060 sec/batch\n",
      "Epoch 1/20  Iteration 175/3560 Training loss: 3.0060 4.2569 sec/batch\n",
      "Epoch 1/20  Iteration 176/3560 Training loss: 3.0026 3.6142 sec/batch\n",
      "Epoch 1/20  Iteration 177/3560 Training loss: 2.9992 3.1934 sec/batch\n",
      "Epoch 1/20  Iteration 178/3560 Training loss: 2.9958 3.4751 sec/batch\n",
      "Epoch 2/20  Iteration 179/3560 Training loss: 2.4292 3.4216 sec/batch\n",
      "Epoch 2/20  Iteration 180/3560 Training loss: 2.3984 3.2919 sec/batch\n",
      "Epoch 2/20  Iteration 181/3560 Training loss: 2.3888 3.3519 sec/batch\n",
      "Epoch 2/20  Iteration 182/3560 Training loss: 2.3861 3.4002 sec/batch\n",
      "Epoch 2/20  Iteration 183/3560 Training loss: 2.3840 3.2171 sec/batch\n",
      "Epoch 2/20  Iteration 184/3560 Training loss: 2.3807 4.5314 sec/batch\n",
      "Epoch 2/20  Iteration 185/3560 Training loss: 2.3801 3.3450 sec/batch\n",
      "Epoch 2/20  Iteration 186/3560 Training loss: 2.3813 3.3397 sec/batch\n",
      "Epoch 2/20  Iteration 187/3560 Training loss: 2.3821 3.0809 sec/batch\n",
      "Epoch 2/20  Iteration 188/3560 Training loss: 2.3802 3.0578 sec/batch\n",
      "Epoch 2/20  Iteration 189/3560 Training loss: 2.3773 3.0620 sec/batch\n",
      "Epoch 2/20  Iteration 190/3560 Training loss: 2.3760 3.1207 sec/batch\n",
      "Epoch 2/20  Iteration 191/3560 Training loss: 2.3749 3.1153 sec/batch\n",
      "Epoch 2/20  Iteration 192/3560 Training loss: 2.3761 3.0836 sec/batch\n",
      "Epoch 2/20  Iteration 193/3560 Training loss: 2.3750 5.1144 sec/batch\n",
      "Epoch 2/20  Iteration 194/3560 Training loss: 2.3735 4.2310 sec/batch\n",
      "Epoch 2/20  Iteration 195/3560 Training loss: 2.3722 3.7394 sec/batch\n",
      "Epoch 2/20  Iteration 196/3560 Training loss: 2.3730 3.4445 sec/batch\n",
      "Epoch 2/20  Iteration 197/3560 Training loss: 2.3721 3.3581 sec/batch\n",
      "Epoch 2/20  Iteration 198/3560 Training loss: 2.3697 3.3617 sec/batch\n",
      "Epoch 2/20  Iteration 199/3560 Training loss: 2.3669 3.6016 sec/batch\n",
      "Epoch 2/20  Iteration 200/3560 Training loss: 2.3671 3.4446 sec/batch\n",
      "Epoch 2/20  Iteration 201/3560 Training loss: 2.3658 4.8134 sec/batch\n",
      "Epoch 2/20  Iteration 202/3560 Training loss: 2.3634 3.4515 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 203/3560 Training loss: 2.3613 3.3534 sec/batch\n",
      "Epoch 2/20  Iteration 204/3560 Training loss: 2.3592 3.4759 sec/batch\n",
      "Epoch 2/20  Iteration 205/3560 Training loss: 2.3571 3.3576 sec/batch\n",
      "Epoch 2/20  Iteration 206/3560 Training loss: 2.3558 3.3687 sec/batch\n",
      "Epoch 2/20  Iteration 207/3560 Training loss: 2.3547 3.3941 sec/batch\n",
      "Epoch 2/20  Iteration 208/3560 Training loss: 2.3531 3.4505 sec/batch\n",
      "Epoch 2/20  Iteration 209/3560 Training loss: 2.3517 4.5478 sec/batch\n",
      "Epoch 2/20  Iteration 210/3560 Training loss: 2.3495 3.7089 sec/batch\n",
      "Epoch 2/20  Iteration 211/3560 Training loss: 2.3472 3.4712 sec/batch\n",
      "Epoch 2/20  Iteration 212/3560 Training loss: 2.3460 3.4786 sec/batch\n",
      "Epoch 2/20  Iteration 213/3560 Training loss: 2.3442 3.5072 sec/batch\n",
      "Epoch 2/20  Iteration 214/3560 Training loss: 2.3427 3.2322 sec/batch\n",
      "Epoch 2/20  Iteration 215/3560 Training loss: 2.3411 3.1145 sec/batch\n",
      "Epoch 2/20  Iteration 216/3560 Training loss: 2.3387 3.3609 sec/batch\n",
      "Epoch 2/20  Iteration 217/3560 Training loss: 2.3364 3.2424 sec/batch\n",
      "Epoch 2/20  Iteration 218/3560 Training loss: 2.3343 4.3138 sec/batch\n",
      "Epoch 2/20  Iteration 219/3560 Training loss: 2.3325 3.1707 sec/batch\n",
      "Epoch 2/20  Iteration 220/3560 Training loss: 2.3307 3.3199 sec/batch\n",
      "Epoch 2/20  Iteration 221/3560 Training loss: 2.3287 3.5842 sec/batch\n",
      "Epoch 2/20  Iteration 222/3560 Training loss: 2.3265 3.4600 sec/batch\n",
      "Epoch 2/20  Iteration 223/3560 Training loss: 2.3248 3.3716 sec/batch\n",
      "Epoch 2/20  Iteration 224/3560 Training loss: 2.3220 3.3556 sec/batch\n",
      "Epoch 2/20  Iteration 225/3560 Training loss: 2.3205 3.3581 sec/batch\n",
      "Epoch 2/20  Iteration 226/3560 Training loss: 2.3186 3.7579 sec/batch\n",
      "Epoch 2/20  Iteration 227/3560 Training loss: 2.3169 4.1514 sec/batch\n",
      "Epoch 2/20  Iteration 228/3560 Training loss: 2.3157 3.2740 sec/batch\n",
      "Epoch 2/20  Iteration 229/3560 Training loss: 2.3136 3.1423 sec/batch\n",
      "Epoch 2/20  Iteration 230/3560 Training loss: 2.3124 3.2103 sec/batch\n",
      "Epoch 2/20  Iteration 231/3560 Training loss: 2.3106 3.2198 sec/batch\n",
      "Epoch 2/20  Iteration 232/3560 Training loss: 2.3086 3.1324 sec/batch\n",
      "Epoch 2/20  Iteration 233/3560 Training loss: 2.3069 3.2461 sec/batch\n",
      "Epoch 2/20  Iteration 234/3560 Training loss: 2.3056 3.2292 sec/batch\n",
      "Epoch 2/20  Iteration 235/3560 Training loss: 2.3042 3.7707 sec/batch\n",
      "Epoch 2/20  Iteration 236/3560 Training loss: 2.3024 3.8256 sec/batch\n",
      "Epoch 2/20  Iteration 237/3560 Training loss: 2.3006 3.2266 sec/batch\n",
      "Epoch 2/20  Iteration 238/3560 Training loss: 2.2995 3.3501 sec/batch\n",
      "Epoch 2/20  Iteration 239/3560 Training loss: 2.2979 3.5135 sec/batch\n",
      "Epoch 2/20  Iteration 240/3560 Training loss: 2.2968 3.3673 sec/batch\n",
      "Epoch 2/20  Iteration 241/3560 Training loss: 2.2959 3.3270 sec/batch\n",
      "Epoch 2/20  Iteration 242/3560 Training loss: 2.2946 3.2764 sec/batch\n",
      "Epoch 2/20  Iteration 243/3560 Training loss: 2.2932 3.3297 sec/batch\n",
      "Epoch 2/20  Iteration 244/3560 Training loss: 2.2920 4.7485 sec/batch\n",
      "Epoch 2/20  Iteration 245/3560 Training loss: 2.2906 3.7354 sec/batch\n",
      "Epoch 2/20  Iteration 246/3560 Training loss: 2.2887 3.4261 sec/batch\n",
      "Epoch 2/20  Iteration 247/3560 Training loss: 2.2870 3.3828 sec/batch\n",
      "Epoch 2/20  Iteration 248/3560 Training loss: 2.2858 3.3132 sec/batch\n",
      "Epoch 2/20  Iteration 249/3560 Training loss: 2.2847 3.3386 sec/batch\n",
      "Epoch 2/20  Iteration 250/3560 Training loss: 2.2832 3.2834 sec/batch\n",
      "Epoch 2/20  Iteration 251/3560 Training loss: 2.2819 3.3531 sec/batch\n",
      "Epoch 2/20  Iteration 252/3560 Training loss: 2.2802 3.8956 sec/batch\n",
      "Epoch 2/20  Iteration 253/3560 Training loss: 2.2787 3.8721 sec/batch\n",
      "Epoch 2/20  Iteration 254/3560 Training loss: 2.2778 3.2042 sec/batch\n",
      "Epoch 2/20  Iteration 255/3560 Training loss: 2.2763 3.1627 sec/batch\n",
      "Epoch 2/20  Iteration 256/3560 Training loss: 2.2751 3.2313 sec/batch\n",
      "Epoch 2/20  Iteration 257/3560 Training loss: 2.2734 3.2886 sec/batch\n",
      "Epoch 2/20  Iteration 258/3560 Training loss: 2.2719 3.2410 sec/batch\n",
      "Epoch 2/20  Iteration 259/3560 Training loss: 2.2702 3.1625 sec/batch\n",
      "Epoch 2/20  Iteration 260/3560 Training loss: 2.2689 3.3686 sec/batch\n",
      "Epoch 2/20  Iteration 261/3560 Training loss: 2.2672 3.8022 sec/batch\n",
      "Epoch 2/20  Iteration 262/3560 Training loss: 2.2655 4.0700 sec/batch\n",
      "Epoch 2/20  Iteration 263/3560 Training loss: 2.2637 3.4412 sec/batch\n",
      "Epoch 2/20  Iteration 264/3560 Training loss: 2.2622 3.5734 sec/batch\n",
      "Epoch 2/20  Iteration 265/3560 Training loss: 2.2608 3.5547 sec/batch\n",
      "Epoch 2/20  Iteration 266/3560 Training loss: 2.2593 3.4598 sec/batch\n",
      "Epoch 2/20  Iteration 267/3560 Training loss: 2.2576 3.3338 sec/batch\n",
      "Epoch 2/20  Iteration 268/3560 Training loss: 2.2564 3.2952 sec/batch\n",
      "Epoch 2/20  Iteration 269/3560 Training loss: 2.2548 3.5476 sec/batch\n",
      "Epoch 2/20  Iteration 270/3560 Training loss: 2.2535 4.2930 sec/batch\n",
      "Epoch 2/20  Iteration 271/3560 Training loss: 2.2518 3.2852 sec/batch\n",
      "Epoch 2/20  Iteration 272/3560 Training loss: 2.2502 3.2694 sec/batch\n",
      "Epoch 2/20  Iteration 273/3560 Training loss: 2.2486 3.3648 sec/batch\n",
      "Epoch 2/20  Iteration 274/3560 Training loss: 2.2470 3.4732 sec/batch\n",
      "Epoch 2/20  Iteration 275/3560 Training loss: 2.2457 3.3048 sec/batch\n",
      "Epoch 2/20  Iteration 276/3560 Training loss: 2.2442 3.4085 sec/batch\n",
      "Epoch 2/20  Iteration 277/3560 Training loss: 2.2425 3.2608 sec/batch\n",
      "Epoch 2/20  Iteration 278/3560 Training loss: 2.2409 3.8772 sec/batch\n",
      "Epoch 2/20  Iteration 279/3560 Training loss: 2.2397 4.0178 sec/batch\n",
      "Epoch 2/20  Iteration 280/3560 Training loss: 2.2384 3.5251 sec/batch\n",
      "Epoch 2/20  Iteration 281/3560 Training loss: 2.2367 3.4186 sec/batch\n",
      "Epoch 2/20  Iteration 282/3560 Training loss: 2.2353 3.3234 sec/batch\n",
      "Epoch 2/20  Iteration 283/3560 Training loss: 2.2338 3.2237 sec/batch\n",
      "Epoch 2/20  Iteration 284/3560 Training loss: 2.2324 3.1767 sec/batch\n",
      "Epoch 2/20  Iteration 285/3560 Training loss: 2.2311 3.4554 sec/batch\n",
      "Epoch 2/20  Iteration 286/3560 Training loss: 2.2299 3.4853 sec/batch\n",
      "Epoch 2/20  Iteration 287/3560 Training loss: 2.2286 4.3291 sec/batch\n",
      "Epoch 2/20  Iteration 288/3560 Training loss: 2.2272 3.3396 sec/batch\n",
      "Epoch 2/20  Iteration 289/3560 Training loss: 2.2259 3.2294 sec/batch\n",
      "Epoch 2/20  Iteration 290/3560 Training loss: 2.2246 3.2101 sec/batch\n",
      "Epoch 2/20  Iteration 291/3560 Training loss: 2.2233 3.2801 sec/batch\n",
      "Epoch 2/20  Iteration 292/3560 Training loss: 2.2220 3.2687 sec/batch\n",
      "Epoch 2/20  Iteration 293/3560 Training loss: 2.2205 3.3442 sec/batch\n",
      "Epoch 2/20  Iteration 294/3560 Training loss: 2.2189 3.3095 sec/batch\n",
      "Epoch 2/20  Iteration 295/3560 Training loss: 2.2176 3.4171 sec/batch\n",
      "Epoch 2/20  Iteration 296/3560 Training loss: 2.2162 4.6644 sec/batch\n",
      "Epoch 2/20  Iteration 297/3560 Training loss: 2.2150 3.5381 sec/batch\n",
      "Epoch 2/20  Iteration 298/3560 Training loss: 2.2138 3.2145 sec/batch\n",
      "Epoch 2/20  Iteration 299/3560 Training loss: 2.2126 3.2039 sec/batch\n",
      "Epoch 2/20  Iteration 300/3560 Training loss: 2.2112 3.2307 sec/batch\n",
      "Epoch 2/20  Iteration 301/3560 Training loss: 2.2098 3.1422 sec/batch\n",
      "Epoch 2/20  Iteration 302/3560 Training loss: 2.2087 3.1400 sec/batch\n",
      "Epoch 2/20  Iteration 303/3560 Training loss: 2.2075 3.2558 sec/batch\n",
      "Epoch 2/20  Iteration 304/3560 Training loss: 2.2061 3.5944 sec/batch\n",
      "Epoch 2/20  Iteration 305/3560 Training loss: 2.2050 3.9643 sec/batch\n",
      "Epoch 2/20  Iteration 306/3560 Training loss: 2.2039 3.2539 sec/batch\n",
      "Epoch 2/20  Iteration 307/3560 Training loss: 2.2027 3.2507 sec/batch\n",
      "Epoch 2/20  Iteration 308/3560 Training loss: 2.2016 3.2932 sec/batch\n",
      "Epoch 2/20  Iteration 309/3560 Training loss: 2.2003 3.3650 sec/batch\n",
      "Epoch 2/20  Iteration 310/3560 Training loss: 2.1989 3.3153 sec/batch\n",
      "Epoch 2/20  Iteration 311/3560 Training loss: 2.1977 3.3942 sec/batch\n",
      "Epoch 2/20  Iteration 312/3560 Training loss: 2.1966 3.3748 sec/batch\n",
      "Epoch 2/20  Iteration 313/3560 Training loss: 2.1954 4.0410 sec/batch\n",
      "Epoch 2/20  Iteration 314/3560 Training loss: 2.1943 3.4377 sec/batch\n",
      "Epoch 2/20  Iteration 315/3560 Training loss: 2.1932 3.3268 sec/batch\n",
      "Epoch 2/20  Iteration 316/3560 Training loss: 2.1921 3.1528 sec/batch\n",
      "Epoch 2/20  Iteration 317/3560 Training loss: 2.1911 3.1539 sec/batch\n",
      "Epoch 2/20  Iteration 318/3560 Training loss: 2.1899 3.2268 sec/batch\n",
      "Epoch 2/20  Iteration 319/3560 Training loss: 2.1889 3.3744 sec/batch\n",
      "Epoch 2/20  Iteration 320/3560 Training loss: 2.1878 3.3756 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 321/3560 Training loss: 2.1867 3.4652 sec/batch\n",
      "Epoch 2/20  Iteration 322/3560 Training loss: 2.1856 4.7063 sec/batch\n",
      "Epoch 2/20  Iteration 323/3560 Training loss: 2.1844 3.4166 sec/batch\n",
      "Epoch 2/20  Iteration 324/3560 Training loss: 2.1833 3.5036 sec/batch\n",
      "Epoch 2/20  Iteration 325/3560 Training loss: 2.1823 3.3838 sec/batch\n",
      "Epoch 2/20  Iteration 326/3560 Training loss: 2.1813 3.5593 sec/batch\n",
      "Epoch 2/20  Iteration 327/3560 Training loss: 2.1802 3.5987 sec/batch\n",
      "Epoch 2/20  Iteration 328/3560 Training loss: 2.1790 3.4013 sec/batch\n",
      "Epoch 2/20  Iteration 329/3560 Training loss: 2.1778 3.5110 sec/batch\n",
      "Epoch 2/20  Iteration 330/3560 Training loss: 2.1769 3.7823 sec/batch\n",
      "Epoch 2/20  Iteration 331/3560 Training loss: 2.1759 4.4686 sec/batch\n",
      "Epoch 2/20  Iteration 332/3560 Training loss: 2.1748 3.5513 sec/batch\n",
      "Epoch 2/20  Iteration 333/3560 Training loss: 2.1736 3.4053 sec/batch\n",
      "Epoch 2/20  Iteration 334/3560 Training loss: 2.1725 3.3139 sec/batch\n",
      "Epoch 2/20  Iteration 335/3560 Training loss: 2.1714 3.3659 sec/batch\n",
      "Epoch 2/20  Iteration 336/3560 Training loss: 2.1704 3.4144 sec/batch\n",
      "Epoch 2/20  Iteration 337/3560 Training loss: 2.1691 3.4274 sec/batch\n",
      "Epoch 2/20  Iteration 338/3560 Training loss: 2.1682 3.3211 sec/batch\n",
      "Epoch 2/20  Iteration 339/3560 Training loss: 2.1673 4.3570 sec/batch\n",
      "Epoch 2/20  Iteration 340/3560 Training loss: 2.1662 3.3890 sec/batch\n",
      "Epoch 2/20  Iteration 341/3560 Training loss: 2.1652 3.5160 sec/batch\n",
      "Epoch 2/20  Iteration 342/3560 Training loss: 2.1641 3.3178 sec/batch\n",
      "Epoch 2/20  Iteration 343/3560 Training loss: 2.1631 3.4348 sec/batch\n",
      "Epoch 2/20  Iteration 344/3560 Training loss: 2.1620 3.4036 sec/batch\n",
      "Epoch 2/20  Iteration 345/3560 Training loss: 2.1610 3.3541 sec/batch\n",
      "Epoch 2/20  Iteration 346/3560 Training loss: 2.1601 3.2795 sec/batch\n",
      "Epoch 2/20  Iteration 347/3560 Training loss: 2.1591 3.8527 sec/batch\n",
      "Epoch 2/20  Iteration 348/3560 Training loss: 2.1580 4.1392 sec/batch\n",
      "Epoch 2/20  Iteration 349/3560 Training loss: 2.1568 3.2327 sec/batch\n",
      "Epoch 2/20  Iteration 350/3560 Training loss: 2.1558 3.2928 sec/batch\n",
      "Epoch 2/20  Iteration 351/3560 Training loss: 2.1548 3.2506 sec/batch\n",
      "Epoch 2/20  Iteration 352/3560 Training loss: 2.1538 3.2752 sec/batch\n",
      "Epoch 2/20  Iteration 353/3560 Training loss: 2.1529 3.4696 sec/batch\n",
      "Epoch 2/20  Iteration 354/3560 Training loss: 2.1518 3.3804 sec/batch\n",
      "Epoch 2/20  Iteration 355/3560 Training loss: 2.1507 3.2183 sec/batch\n",
      "Epoch 2/20  Iteration 356/3560 Training loss: 2.1497 4.2733 sec/batch\n",
      "Epoch 3/20  Iteration 357/3560 Training loss: 2.0159 3.2155 sec/batch\n",
      "Epoch 3/20  Iteration 358/3560 Training loss: 1.9787 3.2397 sec/batch\n",
      "Epoch 3/20  Iteration 359/3560 Training loss: 1.9657 3.2486 sec/batch\n",
      "Epoch 3/20  Iteration 360/3560 Training loss: 1.9612 3.3107 sec/batch\n",
      "Epoch 3/20  Iteration 361/3560 Training loss: 1.9595 3.3376 sec/batch\n",
      "Epoch 3/20  Iteration 362/3560 Training loss: 1.9510 3.4138 sec/batch\n",
      "Epoch 3/20  Iteration 363/3560 Training loss: 1.9508 3.2865 sec/batch\n",
      "Epoch 3/20  Iteration 364/3560 Training loss: 1.9507 3.1300 sec/batch\n",
      "Epoch 3/20  Iteration 365/3560 Training loss: 1.9540 4.0772 sec/batch\n",
      "Epoch 3/20  Iteration 366/3560 Training loss: 1.9533 3.3938 sec/batch\n",
      "Epoch 3/20  Iteration 367/3560 Training loss: 1.9497 3.1919 sec/batch\n",
      "Epoch 3/20  Iteration 368/3560 Training loss: 1.9473 3.3403 sec/batch\n",
      "Epoch 3/20  Iteration 369/3560 Training loss: 1.9466 3.3536 sec/batch\n",
      "Epoch 3/20  Iteration 370/3560 Training loss: 1.9488 3.4037 sec/batch\n",
      "Epoch 3/20  Iteration 371/3560 Training loss: 1.9474 3.5330 sec/batch\n",
      "Epoch 3/20  Iteration 372/3560 Training loss: 1.9462 3.3625 sec/batch\n",
      "Epoch 3/20  Iteration 373/3560 Training loss: 1.9447 3.2713 sec/batch\n",
      "Epoch 3/20  Iteration 374/3560 Training loss: 1.9466 4.4203 sec/batch\n",
      "Epoch 3/20  Iteration 375/3560 Training loss: 1.9456 3.3410 sec/batch\n",
      "Epoch 3/20  Iteration 376/3560 Training loss: 1.9450 3.2216 sec/batch\n",
      "Epoch 3/20  Iteration 377/3560 Training loss: 1.9439 3.0811 sec/batch\n",
      "Epoch 3/20  Iteration 378/3560 Training loss: 1.9451 3.2169 sec/batch\n",
      "Epoch 3/20  Iteration 379/3560 Training loss: 1.9440 3.2004 sec/batch\n",
      "Epoch 3/20  Iteration 380/3560 Training loss: 1.9426 3.1688 sec/batch\n",
      "Epoch 3/20  Iteration 381/3560 Training loss: 1.9420 3.1923 sec/batch\n",
      "Epoch 3/20  Iteration 382/3560 Training loss: 1.9405 3.2249 sec/batch\n",
      "Epoch 3/20  Iteration 383/3560 Training loss: 1.9387 4.0870 sec/batch\n",
      "Epoch 3/20  Iteration 384/3560 Training loss: 1.9380 3.2871 sec/batch\n",
      "Epoch 3/20  Iteration 385/3560 Training loss: 1.9384 3.2848 sec/batch\n",
      "Epoch 3/20  Iteration 386/3560 Training loss: 1.9381 3.5025 sec/batch\n",
      "Epoch 3/20  Iteration 387/3560 Training loss: 1.9375 3.2738 sec/batch\n",
      "Epoch 3/20  Iteration 388/3560 Training loss: 1.9361 3.3569 sec/batch\n",
      "Epoch 3/20  Iteration 389/3560 Training loss: 1.9354 3.5142 sec/batch\n",
      "Epoch 3/20  Iteration 390/3560 Training loss: 1.9358 3.4940 sec/batch\n",
      "Epoch 3/20  Iteration 391/3560 Training loss: 1.9349 4.4797 sec/batch\n",
      "Epoch 3/20  Iteration 392/3560 Training loss: 1.9338 3.9055 sec/batch\n",
      "Epoch 3/20  Iteration 393/3560 Training loss: 1.9327 3.3592 sec/batch\n",
      "Epoch 3/20  Iteration 394/3560 Training loss: 1.9308 3.4432 sec/batch\n",
      "Epoch 3/20  Iteration 395/3560 Training loss: 1.9288 3.3804 sec/batch\n",
      "Epoch 3/20  Iteration 396/3560 Training loss: 1.9274 3.3245 sec/batch\n",
      "Epoch 3/20  Iteration 397/3560 Training loss: 1.9260 3.2427 sec/batch\n",
      "Epoch 3/20  Iteration 398/3560 Training loss: 1.9253 3.2070 sec/batch\n",
      "Epoch 3/20  Iteration 399/3560 Training loss: 1.9244 3.4058 sec/batch\n",
      "Epoch 3/20  Iteration 400/3560 Training loss: 1.9228 4.8921 sec/batch\n",
      "Epoch 3/20  Iteration 401/3560 Training loss: 1.9223 3.5670 sec/batch\n",
      "Epoch 3/20  Iteration 402/3560 Training loss: 1.9204 3.4261 sec/batch\n",
      "Epoch 3/20  Iteration 403/3560 Training loss: 1.9196 3.4755 sec/batch\n",
      "Epoch 3/20  Iteration 404/3560 Training loss: 1.9183 3.4193 sec/batch\n",
      "Epoch 3/20  Iteration 405/3560 Training loss: 1.9174 3.3851 sec/batch\n",
      "Epoch 3/20  Iteration 406/3560 Training loss: 1.9173 3.4943 sec/batch\n",
      "Epoch 3/20  Iteration 407/3560 Training loss: 1.9161 3.3670 sec/batch\n",
      "Epoch 3/20  Iteration 408/3560 Training loss: 1.9163 4.4474 sec/batch\n",
      "Epoch 3/20  Iteration 409/3560 Training loss: 1.9154 3.4434 sec/batch\n",
      "Epoch 3/20  Iteration 410/3560 Training loss: 1.9144 3.3012 sec/batch\n",
      "Epoch 3/20  Iteration 411/3560 Training loss: 1.9137 3.3698 sec/batch\n",
      "Epoch 3/20  Iteration 412/3560 Training loss: 1.9131 3.4356 sec/batch\n",
      "Epoch 3/20  Iteration 413/3560 Training loss: 1.9128 3.4314 sec/batch\n",
      "Epoch 3/20  Iteration 414/3560 Training loss: 1.9119 3.4089 sec/batch\n",
      "Epoch 3/20  Iteration 415/3560 Training loss: 1.9108 3.3988 sec/batch\n",
      "Epoch 3/20  Iteration 416/3560 Training loss: 1.9109 3.4034 sec/batch\n",
      "Epoch 3/20  Iteration 417/3560 Training loss: 1.9100 4.1863 sec/batch\n",
      "Epoch 3/20  Iteration 418/3560 Training loss: 1.9100 3.4596 sec/batch\n",
      "Epoch 3/20  Iteration 419/3560 Training loss: 1.9098 3.4895 sec/batch\n",
      "Epoch 3/20  Iteration 420/3560 Training loss: 1.9093 3.3898 sec/batch\n",
      "Epoch 3/20  Iteration 421/3560 Training loss: 1.9086 3.4125 sec/batch\n",
      "Epoch 3/20  Iteration 422/3560 Training loss: 1.9083 3.2853 sec/batch\n",
      "Epoch 3/20  Iteration 423/3560 Training loss: 1.9077 3.3035 sec/batch\n",
      "Epoch 3/20  Iteration 424/3560 Training loss: 1.9067 3.3239 sec/batch\n",
      "Epoch 3/20  Iteration 425/3560 Training loss: 1.9059 4.2718 sec/batch\n",
      "Epoch 3/20  Iteration 426/3560 Training loss: 1.9051 3.4921 sec/batch\n",
      "Epoch 3/20  Iteration 427/3560 Training loss: 1.9049 3.4215 sec/batch\n",
      "Epoch 3/20  Iteration 428/3560 Training loss: 1.9043 3.2712 sec/batch\n",
      "Epoch 3/20  Iteration 429/3560 Training loss: 1.9040 3.3949 sec/batch\n",
      "Epoch 3/20  Iteration 430/3560 Training loss: 1.9030 3.4807 sec/batch\n",
      "Epoch 3/20  Iteration 431/3560 Training loss: 1.9023 3.3030 sec/batch\n",
      "Epoch 3/20  Iteration 432/3560 Training loss: 1.9019 3.2331 sec/batch\n",
      "Epoch 3/20  Iteration 433/3560 Training loss: 1.9012 3.4210 sec/batch\n",
      "Epoch 3/20  Iteration 434/3560 Training loss: 1.9005 4.5918 sec/batch\n",
      "Epoch 3/20  Iteration 435/3560 Training loss: 1.8994 3.5550 sec/batch\n",
      "Epoch 3/20  Iteration 436/3560 Training loss: 1.8985 3.4870 sec/batch\n",
      "Epoch 3/20  Iteration 437/3560 Training loss: 1.8974 3.4434 sec/batch\n",
      "Epoch 3/20  Iteration 438/3560 Training loss: 1.8968 3.5246 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 439/3560 Training loss: 1.8957 3.3739 sec/batch\n",
      "Epoch 3/20  Iteration 440/3560 Training loss: 1.8950 3.4329 sec/batch\n",
      "Epoch 3/20  Iteration 441/3560 Training loss: 1.8938 3.3465 sec/batch\n",
      "Epoch 3/20  Iteration 442/3560 Training loss: 1.8928 4.1098 sec/batch\n",
      "Epoch 3/20  Iteration 443/3560 Training loss: 1.8920 3.4001 sec/batch\n",
      "Epoch 3/20  Iteration 444/3560 Training loss: 1.8912 3.3934 sec/batch\n",
      "Epoch 3/20  Iteration 445/3560 Training loss: 1.8900 3.5482 sec/batch\n",
      "Epoch 3/20  Iteration 446/3560 Training loss: 1.8895 3.5406 sec/batch\n",
      "Epoch 3/20  Iteration 447/3560 Training loss: 1.8885 3.5980 sec/batch\n",
      "Epoch 3/20  Iteration 448/3560 Training loss: 1.8877 3.4290 sec/batch\n",
      "Epoch 3/20  Iteration 449/3560 Training loss: 1.8866 3.5349 sec/batch\n",
      "Epoch 3/20  Iteration 450/3560 Training loss: 1.8857 3.8172 sec/batch\n",
      "Epoch 3/20  Iteration 451/3560 Training loss: 1.8847 4.7716 sec/batch\n",
      "Epoch 3/20  Iteration 452/3560 Training loss: 1.8839 3.5917 sec/batch\n",
      "Epoch 3/20  Iteration 453/3560 Training loss: 1.8833 3.4782 sec/batch\n",
      "Epoch 3/20  Iteration 454/3560 Training loss: 1.8823 3.3841 sec/batch\n",
      "Epoch 3/20  Iteration 455/3560 Training loss: 1.8814 3.6474 sec/batch\n",
      "Epoch 3/20  Iteration 456/3560 Training loss: 1.8802 3.4718 sec/batch\n",
      "Epoch 3/20  Iteration 457/3560 Training loss: 1.8795 3.5489 sec/batch\n",
      "Epoch 3/20  Iteration 458/3560 Training loss: 1.8789 3.5202 sec/batch\n",
      "Epoch 3/20  Iteration 459/3560 Training loss: 1.8780 4.2675 sec/batch\n",
      "Epoch 3/20  Iteration 460/3560 Training loss: 1.8771 3.4322 sec/batch\n",
      "Epoch 3/20  Iteration 461/3560 Training loss: 1.8763 3.3787 sec/batch\n",
      "Epoch 3/20  Iteration 462/3560 Training loss: 1.8756 3.1892 sec/batch\n",
      "Epoch 3/20  Iteration 463/3560 Training loss: 1.8749 3.2002 sec/batch\n",
      "Epoch 3/20  Iteration 464/3560 Training loss: 1.8743 3.2960 sec/batch\n",
      "Epoch 3/20  Iteration 465/3560 Training loss: 1.8738 3.3547 sec/batch\n",
      "Epoch 3/20  Iteration 466/3560 Training loss: 1.8733 3.2403 sec/batch\n",
      "Epoch 3/20  Iteration 467/3560 Training loss: 1.8726 3.8164 sec/batch\n",
      "Epoch 3/20  Iteration 468/3560 Training loss: 1.8718 4.1429 sec/batch\n",
      "Epoch 3/20  Iteration 469/3560 Training loss: 1.8711 3.3237 sec/batch\n",
      "Epoch 3/20  Iteration 470/3560 Training loss: 1.8704 3.3932 sec/batch\n",
      "Epoch 3/20  Iteration 471/3560 Training loss: 1.8696 3.2750 sec/batch\n",
      "Epoch 3/20  Iteration 472/3560 Training loss: 1.8686 3.5050 sec/batch\n",
      "Epoch 3/20  Iteration 473/3560 Training loss: 1.8679 3.5467 sec/batch\n",
      "Epoch 3/20  Iteration 474/3560 Training loss: 1.8672 3.3926 sec/batch\n",
      "Epoch 3/20  Iteration 475/3560 Training loss: 1.8665 3.2274 sec/batch\n",
      "Epoch 3/20  Iteration 476/3560 Training loss: 1.8658 4.3584 sec/batch\n",
      "Epoch 3/20  Iteration 477/3560 Training loss: 1.8652 3.3383 sec/batch\n",
      "Epoch 3/20  Iteration 478/3560 Training loss: 1.8642 3.5869 sec/batch\n",
      "Epoch 3/20  Iteration 479/3560 Training loss: 1.8634 3.5853 sec/batch\n",
      "Epoch 3/20  Iteration 480/3560 Training loss: 1.8629 3.4578 sec/batch\n",
      "Epoch 3/20  Iteration 481/3560 Training loss: 1.8622 3.5105 sec/batch\n",
      "Epoch 3/20  Iteration 482/3560 Training loss: 1.8613 3.3766 sec/batch\n",
      "Epoch 3/20  Iteration 483/3560 Training loss: 1.8609 3.3655 sec/batch\n",
      "Epoch 3/20  Iteration 484/3560 Training loss: 1.8605 4.0463 sec/batch\n",
      "Epoch 3/20  Iteration 485/3560 Training loss: 1.8597 3.5331 sec/batch\n",
      "Epoch 3/20  Iteration 486/3560 Training loss: 1.8592 3.2026 sec/batch\n",
      "Epoch 3/20  Iteration 487/3560 Training loss: 1.8583 3.4329 sec/batch\n",
      "Epoch 3/20  Iteration 488/3560 Training loss: 1.8575 3.2724 sec/batch\n",
      "Epoch 3/20  Iteration 489/3560 Training loss: 1.8570 3.3159 sec/batch\n",
      "Epoch 3/20  Iteration 490/3560 Training loss: 1.8565 3.4499 sec/batch\n",
      "Epoch 3/20  Iteration 491/3560 Training loss: 1.8560 3.4560 sec/batch\n",
      "Epoch 3/20  Iteration 492/3560 Training loss: 1.8556 3.3748 sec/batch\n",
      "Epoch 3/20  Iteration 493/3560 Training loss: 1.8551 4.5068 sec/batch\n",
      "Epoch 3/20  Iteration 494/3560 Training loss: 1.8546 3.2460 sec/batch\n",
      "Epoch 3/20  Iteration 495/3560 Training loss: 1.8541 3.2935 sec/batch\n",
      "Epoch 3/20  Iteration 496/3560 Training loss: 1.8535 3.4986 sec/batch\n",
      "Epoch 3/20  Iteration 497/3560 Training loss: 1.8532 3.4860 sec/batch\n",
      "Epoch 3/20  Iteration 498/3560 Training loss: 1.8526 3.4754 sec/batch\n",
      "Epoch 3/20  Iteration 499/3560 Training loss: 1.8520 3.6840 sec/batch\n",
      "Epoch 3/20  Iteration 500/3560 Training loss: 1.8514 3.4723 sec/batch\n",
      "Epoch 3/20  Iteration 501/3560 Training loss: 1.8507 3.4208 sec/batch\n",
      "Epoch 3/20  Iteration 502/3560 Training loss: 1.8502 4.8411 sec/batch\n",
      "Epoch 3/20  Iteration 503/3560 Training loss: 1.8496 3.4300 sec/batch\n",
      "Epoch 3/20  Iteration 504/3560 Training loss: 1.8492 3.6195 sec/batch\n",
      "Epoch 3/20  Iteration 505/3560 Training loss: 1.8487 3.4155 sec/batch\n",
      "Epoch 3/20  Iteration 506/3560 Training loss: 1.8481 3.4017 sec/batch\n",
      "Epoch 3/20  Iteration 507/3560 Training loss: 1.8474 3.6283 sec/batch\n",
      "Epoch 3/20  Iteration 508/3560 Training loss: 1.8470 3.4567 sec/batch\n",
      "Epoch 3/20  Iteration 509/3560 Training loss: 1.8465 3.4329 sec/batch\n",
      "Epoch 3/20  Iteration 510/3560 Training loss: 1.8460 4.5455 sec/batch\n",
      "Epoch 3/20  Iteration 511/3560 Training loss: 1.8454 3.2799 sec/batch\n",
      "Epoch 3/20  Iteration 512/3560 Training loss: 1.8448 3.2970 sec/batch\n",
      "Epoch 3/20  Iteration 513/3560 Training loss: 1.8443 3.3705 sec/batch\n",
      "Epoch 3/20  Iteration 514/3560 Training loss: 1.8437 3.4190 sec/batch\n",
      "Epoch 3/20  Iteration 515/3560 Training loss: 1.8429 3.5326 sec/batch\n",
      "Epoch 3/20  Iteration 516/3560 Training loss: 1.8425 3.6306 sec/batch\n",
      "Epoch 3/20  Iteration 517/3560 Training loss: 1.8420 3.4399 sec/batch\n",
      "Epoch 3/20  Iteration 518/3560 Training loss: 1.8415 4.3859 sec/batch\n",
      "Epoch 3/20  Iteration 519/3560 Training loss: 1.8410 3.3035 sec/batch\n",
      "Epoch 3/20  Iteration 520/3560 Training loss: 1.8404 3.2789 sec/batch\n",
      "Epoch 3/20  Iteration 521/3560 Training loss: 1.8398 3.4503 sec/batch\n",
      "Epoch 3/20  Iteration 522/3560 Training loss: 1.8391 3.3063 sec/batch\n",
      "Epoch 3/20  Iteration 523/3560 Training loss: 1.8386 3.4354 sec/batch\n",
      "Epoch 3/20  Iteration 524/3560 Training loss: 1.8385 3.4653 sec/batch\n",
      "Epoch 3/20  Iteration 525/3560 Training loss: 1.8378 3.4039 sec/batch\n",
      "Epoch 3/20  Iteration 526/3560 Training loss: 1.8372 3.4342 sec/batch\n",
      "Epoch 3/20  Iteration 527/3560 Training loss: 1.8366 4.6823 sec/batch\n",
      "Epoch 3/20  Iteration 528/3560 Training loss: 1.8359 3.4558 sec/batch\n",
      "Epoch 3/20  Iteration 529/3560 Training loss: 1.8354 3.4121 sec/batch\n",
      "Epoch 3/20  Iteration 530/3560 Training loss: 1.8349 3.4256 sec/batch\n",
      "Epoch 3/20  Iteration 531/3560 Training loss: 1.8345 3.4219 sec/batch\n",
      "Epoch 3/20  Iteration 532/3560 Training loss: 1.8338 3.4348 sec/batch\n",
      "Epoch 3/20  Iteration 533/3560 Training loss: 1.8332 3.3598 sec/batch\n",
      "Epoch 3/20  Iteration 534/3560 Training loss: 1.8327 3.5987 sec/batch\n",
      "Epoch 4/20  Iteration 535/3560 Training loss: 1.7997 5.1696 sec/batch\n",
      "Epoch 4/20  Iteration 536/3560 Training loss: 1.7603 3.9248 sec/batch\n",
      "Epoch 4/20  Iteration 537/3560 Training loss: 1.7483 3.8580 sec/batch\n",
      "Epoch 4/20  Iteration 538/3560 Training loss: 1.7432 3.8450 sec/batch\n",
      "Epoch 4/20  Iteration 539/3560 Training loss: 1.7382 3.7515 sec/batch\n",
      "Epoch 4/20  Iteration 540/3560 Training loss: 1.7273 3.7936 sec/batch\n",
      "Epoch 4/20  Iteration 541/3560 Training loss: 1.7288 3.9921 sec/batch\n",
      "Epoch 4/20  Iteration 542/3560 Training loss: 1.7271 4.1210 sec/batch\n",
      "Epoch 4/20  Iteration 543/3560 Training loss: 1.7286 4.9361 sec/batch\n",
      "Epoch 4/20  Iteration 544/3560 Training loss: 1.7274 3.7701 sec/batch\n",
      "Epoch 4/20  Iteration 545/3560 Training loss: 1.7238 3.8145 sec/batch\n",
      "Epoch 4/20  Iteration 546/3560 Training loss: 1.7218 3.7920 sec/batch\n",
      "Epoch 4/20  Iteration 547/3560 Training loss: 1.7212 3.8684 sec/batch\n",
      "Epoch 4/20  Iteration 548/3560 Training loss: 1.7236 4.0054 sec/batch\n",
      "Epoch 4/20  Iteration 549/3560 Training loss: 1.7227 3.7426 sec/batch\n",
      "Epoch 4/20  Iteration 550/3560 Training loss: 1.7204 5.2410 sec/batch\n",
      "Epoch 4/20  Iteration 551/3560 Training loss: 1.7201 4.1041 sec/batch\n",
      "Epoch 4/20  Iteration 552/3560 Training loss: 1.7214 3.9806 sec/batch\n",
      "Epoch 4/20  Iteration 553/3560 Training loss: 1.7208 3.8878 sec/batch\n",
      "Epoch 4/20  Iteration 554/3560 Training loss: 1.7213 3.8528 sec/batch\n",
      "Epoch 4/20  Iteration 555/3560 Training loss: 1.7204 3.9881 sec/batch\n",
      "Epoch 4/20  Iteration 556/3560 Training loss: 1.7217 4.0192 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 557/3560 Training loss: 1.7207 5.0010 sec/batch\n",
      "Epoch 4/20  Iteration 558/3560 Training loss: 1.7202 4.2482 sec/batch\n",
      "Epoch 4/20  Iteration 559/3560 Training loss: 1.7201 3.9054 sec/batch\n",
      "Epoch 4/20  Iteration 560/3560 Training loss: 1.7184 3.8740 sec/batch\n",
      "Epoch 4/20  Iteration 561/3560 Training loss: 1.7167 3.8370 sec/batch\n",
      "Epoch 4/20  Iteration 562/3560 Training loss: 1.7166 3.8503 sec/batch\n",
      "Epoch 4/20  Iteration 563/3560 Training loss: 1.7169 4.0683 sec/batch\n",
      "Epoch 4/20  Iteration 564/3560 Training loss: 1.7169 3.8303 sec/batch\n",
      "Epoch 4/20  Iteration 565/3560 Training loss: 1.7165 4.8595 sec/batch\n",
      "Epoch 4/20  Iteration 566/3560 Training loss: 1.7152 3.6890 sec/batch\n",
      "Epoch 4/20  Iteration 567/3560 Training loss: 1.7151 3.8150 sec/batch\n",
      "Epoch 4/20  Iteration 568/3560 Training loss: 1.7153 3.9455 sec/batch\n",
      "Epoch 4/20  Iteration 569/3560 Training loss: 1.7145 3.8609 sec/batch\n",
      "Epoch 4/20  Iteration 570/3560 Training loss: 1.7138 3.8293 sec/batch\n",
      "Epoch 4/20  Iteration 571/3560 Training loss: 1.7127 3.9153 sec/batch\n",
      "Epoch 4/20  Iteration 572/3560 Training loss: 1.7112 4.9318 sec/batch\n",
      "Epoch 4/20  Iteration 573/3560 Training loss: 1.7095 3.9206 sec/batch\n",
      "Epoch 4/20  Iteration 574/3560 Training loss: 1.7084 3.8642 sec/batch\n",
      "Epoch 4/20  Iteration 575/3560 Training loss: 1.7075 3.8999 sec/batch\n",
      "Epoch 4/20  Iteration 576/3560 Training loss: 1.7076 3.9012 sec/batch\n",
      "Epoch 4/20  Iteration 577/3560 Training loss: 1.7066 3.8006 sec/batch\n",
      "Epoch 4/20  Iteration 578/3560 Training loss: 1.7055 3.9891 sec/batch\n",
      "Epoch 4/20  Iteration 579/3560 Training loss: 1.7055 4.3369 sec/batch\n",
      "Epoch 4/20  Iteration 580/3560 Training loss: 1.7042 4.4841 sec/batch\n",
      "Epoch 4/20  Iteration 581/3560 Training loss: 1.7033 3.8534 sec/batch\n",
      "Epoch 4/20  Iteration 582/3560 Training loss: 1.7025 3.8878 sec/batch\n",
      "Epoch 4/20  Iteration 583/3560 Training loss: 1.7018 3.8828 sec/batch\n",
      "Epoch 4/20  Iteration 584/3560 Training loss: 1.7020 3.8653 sec/batch\n",
      "Epoch 4/20  Iteration 585/3560 Training loss: 1.7013 3.9878 sec/batch\n",
      "Epoch 4/20  Iteration 586/3560 Training loss: 1.7017 4.1208 sec/batch\n",
      "Epoch 4/20  Iteration 587/3560 Training loss: 1.7011 6.4544 sec/batch\n",
      "Epoch 4/20  Iteration 588/3560 Training loss: 1.7007 3.4647 sec/batch\n",
      "Epoch 4/20  Iteration 589/3560 Training loss: 1.7002 3.0206 sec/batch\n",
      "Epoch 4/20  Iteration 590/3560 Training loss: 1.6999 3.0711 sec/batch\n",
      "Epoch 4/20  Iteration 591/3560 Training loss: 1.6998 2.8410 sec/batch\n",
      "Epoch 4/20  Iteration 592/3560 Training loss: 1.6992 3.2672 sec/batch\n",
      "Epoch 4/20  Iteration 593/3560 Training loss: 1.6984 4.5418 sec/batch\n",
      "Epoch 4/20  Iteration 594/3560 Training loss: 1.6987 2.9650 sec/batch\n",
      "Epoch 4/20  Iteration 595/3560 Training loss: 1.6980 3.1805 sec/batch\n",
      "Epoch 4/20  Iteration 596/3560 Training loss: 1.6985 3.5612 sec/batch\n",
      "Epoch 4/20  Iteration 597/3560 Training loss: 1.6985 3.0102 sec/batch\n",
      "Epoch 4/20  Iteration 598/3560 Training loss: 1.6984 3.8129 sec/batch\n",
      "Epoch 4/20  Iteration 599/3560 Training loss: 1.6980 3.8929 sec/batch\n",
      "Epoch 4/20  Iteration 600/3560 Training loss: 1.6979 3.1104 sec/batch\n",
      "Epoch 4/20  Iteration 601/3560 Training loss: 1.6976 2.8587 sec/batch\n",
      "Epoch 4/20  Iteration 602/3560 Training loss: 1.6969 4.2825 sec/batch\n",
      "Epoch 4/20  Iteration 603/3560 Training loss: 1.6966 3.8483 sec/batch\n",
      "Epoch 4/20  Iteration 604/3560 Training loss: 1.6962 3.4191 sec/batch\n",
      "Epoch 4/20  Iteration 605/3560 Training loss: 1.6963 2.8346 sec/batch\n",
      "Epoch 4/20  Iteration 606/3560 Training loss: 1.6961 2.7972 sec/batch\n",
      "Epoch 4/20  Iteration 607/3560 Training loss: 1.6961 2.7262 sec/batch\n",
      "Epoch 4/20  Iteration 608/3560 Training loss: 1.6954 2.7863 sec/batch\n",
      "Epoch 4/20  Iteration 609/3560 Training loss: 1.6950 2.7731 sec/batch\n",
      "Epoch 4/20  Iteration 610/3560 Training loss: 1.6948 2.8059 sec/batch\n",
      "Epoch 4/20  Iteration 611/3560 Training loss: 1.6943 2.9197 sec/batch\n",
      "Epoch 4/20  Iteration 612/3560 Training loss: 1.6939 3.0231 sec/batch\n",
      "Epoch 4/20  Iteration 613/3560 Training loss: 1.6930 2.8045 sec/batch\n",
      "Epoch 4/20  Iteration 614/3560 Training loss: 1.6926 3.5390 sec/batch\n",
      "Epoch 4/20  Iteration 615/3560 Training loss: 1.6918 2.8006 sec/batch\n",
      "Epoch 4/20  Iteration 616/3560 Training loss: 1.6914 2.7935 sec/batch\n",
      "Epoch 4/20  Iteration 617/3560 Training loss: 1.6905 2.7009 sec/batch\n",
      "Epoch 4/20  Iteration 618/3560 Training loss: 1.6901 2.7389 sec/batch\n",
      "Epoch 4/20  Iteration 619/3560 Training loss: 1.6893 2.9713 sec/batch\n",
      "Epoch 4/20  Iteration 620/3560 Training loss: 1.6886 2.7723 sec/batch\n",
      "Epoch 4/20  Iteration 621/3560 Training loss: 1.6879 2.7993 sec/batch\n",
      "Epoch 4/20  Iteration 622/3560 Training loss: 1.6873 3.2147 sec/batch\n",
      "Epoch 4/20  Iteration 623/3560 Training loss: 1.6865 3.0810 sec/batch\n",
      "Epoch 4/20  Iteration 624/3560 Training loss: 1.6863 3.7442 sec/batch\n",
      "Epoch 4/20  Iteration 625/3560 Training loss: 1.6857 2.7453 sec/batch\n",
      "Epoch 4/20  Iteration 626/3560 Training loss: 1.6853 2.7015 sec/batch\n",
      "Epoch 4/20  Iteration 627/3560 Training loss: 1.6845 2.7848 sec/batch\n",
      "Epoch 4/20  Iteration 628/3560 Training loss: 1.6838 2.7180 sec/batch\n",
      "Epoch 4/20  Iteration 629/3560 Training loss: 1.6831 2.7299 sec/batch\n",
      "Epoch 4/20  Iteration 630/3560 Training loss: 1.6827 2.7648 sec/batch\n",
      "Epoch 4/20  Iteration 631/3560 Training loss: 1.6822 2.7034 sec/batch\n",
      "Epoch 4/20  Iteration 632/3560 Training loss: 1.6815 2.7588 sec/batch\n",
      "Epoch 4/20  Iteration 633/3560 Training loss: 1.6808 3.1011 sec/batch\n",
      "Epoch 4/20  Iteration 634/3560 Training loss: 1.6800 2.9000 sec/batch\n",
      "Epoch 4/20  Iteration 635/3560 Training loss: 1.6796 3.7498 sec/batch\n",
      "Epoch 4/20  Iteration 636/3560 Training loss: 1.6789 3.0087 sec/batch\n",
      "Epoch 4/20  Iteration 637/3560 Training loss: 1.6783 2.8694 sec/batch\n",
      "Epoch 4/20  Iteration 638/3560 Training loss: 1.6777 2.8307 sec/batch\n",
      "Epoch 4/20  Iteration 639/3560 Training loss: 1.6772 2.8192 sec/batch\n",
      "Epoch 4/20  Iteration 640/3560 Training loss: 1.6768 2.8081 sec/batch\n",
      "Epoch 4/20  Iteration 641/3560 Training loss: 1.6765 2.8696 sec/batch\n",
      "Epoch 4/20  Iteration 642/3560 Training loss: 1.6760 2.8579 sec/batch\n",
      "Epoch 4/20  Iteration 643/3560 Training loss: 1.6757 2.8634 sec/batch\n",
      "Epoch 4/20  Iteration 644/3560 Training loss: 1.6754 2.8858 sec/batch\n",
      "Epoch 4/20  Iteration 645/3560 Training loss: 1.6749 3.6527 sec/batch\n",
      "Epoch 4/20  Iteration 646/3560 Training loss: 1.6745 2.8309 sec/batch\n",
      "Epoch 4/20  Iteration 647/3560 Training loss: 1.6740 2.8403 sec/batch\n",
      "Epoch 4/20  Iteration 648/3560 Training loss: 1.6735 2.8213 sec/batch\n",
      "Epoch 4/20  Iteration 649/3560 Training loss: 1.6730 2.8850 sec/batch\n",
      "Epoch 4/20  Iteration 650/3560 Training loss: 1.6723 2.8142 sec/batch\n",
      "Epoch 4/20  Iteration 651/3560 Training loss: 1.6718 2.8480 sec/batch\n",
      "Epoch 4/20  Iteration 652/3560 Training loss: 1.6713 2.9124 sec/batch\n",
      "Epoch 4/20  Iteration 653/3560 Training loss: 1.6709 2.8519 sec/batch\n",
      "Epoch 4/20  Iteration 654/3560 Training loss: 1.6705 2.8114 sec/batch\n",
      "Epoch 4/20  Iteration 655/3560 Training loss: 1.6701 3.7121 sec/batch\n",
      "Epoch 4/20  Iteration 656/3560 Training loss: 1.6695 2.9025 sec/batch\n",
      "Epoch 4/20  Iteration 657/3560 Training loss: 1.6688 2.7778 sec/batch\n",
      "Epoch 4/20  Iteration 658/3560 Training loss: 1.6686 2.8912 sec/batch\n",
      "Epoch 4/20  Iteration 659/3560 Training loss: 1.6682 2.8414 sec/batch\n",
      "Epoch 4/20  Iteration 660/3560 Training loss: 1.6675 2.8282 sec/batch\n",
      "Epoch 4/20  Iteration 661/3560 Training loss: 1.6673 2.9373 sec/batch\n",
      "Epoch 4/20  Iteration 662/3560 Training loss: 1.6670 2.7989 sec/batch\n",
      "Epoch 4/20  Iteration 663/3560 Training loss: 1.6666 2.9172 sec/batch\n",
      "Epoch 4/20  Iteration 664/3560 Training loss: 1.6661 2.7949 sec/batch\n",
      "Epoch 4/20  Iteration 665/3560 Training loss: 1.6654 3.5590 sec/batch\n",
      "Epoch 4/20  Iteration 666/3560 Training loss: 1.6648 3.0779 sec/batch\n",
      "Epoch 4/20  Iteration 667/3560 Training loss: 1.6645 2.8246 sec/batch\n",
      "Epoch 4/20  Iteration 668/3560 Training loss: 1.6642 2.8530 sec/batch\n",
      "Epoch 4/20  Iteration 669/3560 Training loss: 1.6638 2.7886 sec/batch\n",
      "Epoch 4/20  Iteration 670/3560 Training loss: 1.6635 2.8247 sec/batch\n",
      "Epoch 4/20  Iteration 671/3560 Training loss: 1.6633 2.8183 sec/batch\n",
      "Epoch 4/20  Iteration 672/3560 Training loss: 1.6630 2.8402 sec/batch\n",
      "Epoch 4/20  Iteration 673/3560 Training loss: 1.6628 2.8793 sec/batch\n",
      "Epoch 4/20  Iteration 674/3560 Training loss: 1.6625 2.7977 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 675/3560 Training loss: 1.6625 2.8521 sec/batch\n",
      "Epoch 4/20  Iteration 676/3560 Training loss: 1.6621 3.7037 sec/batch\n",
      "Epoch 4/20  Iteration 677/3560 Training loss: 1.6617 2.7959 sec/batch\n",
      "Epoch 4/20  Iteration 678/3560 Training loss: 1.6615 2.7914 sec/batch\n",
      "Epoch 4/20  Iteration 679/3560 Training loss: 1.6611 2.8558 sec/batch\n",
      "Epoch 4/20  Iteration 680/3560 Training loss: 1.6609 2.8402 sec/batch\n",
      "Epoch 4/20  Iteration 681/3560 Training loss: 1.6606 2.8131 sec/batch\n",
      "Epoch 4/20  Iteration 682/3560 Training loss: 1.6605 2.9017 sec/batch\n",
      "Epoch 4/20  Iteration 683/3560 Training loss: 1.6602 2.8054 sec/batch\n",
      "Epoch 4/20  Iteration 684/3560 Training loss: 1.6597 2.8213 sec/batch\n",
      "Epoch 4/20  Iteration 685/3560 Training loss: 1.6591 2.8476 sec/batch\n",
      "Epoch 4/20  Iteration 686/3560 Training loss: 1.6589 3.7666 sec/batch\n",
      "Epoch 4/20  Iteration 687/3560 Training loss: 1.6586 2.7954 sec/batch\n",
      "Epoch 4/20  Iteration 688/3560 Training loss: 1.6583 2.8149 sec/batch\n",
      "Epoch 4/20  Iteration 689/3560 Training loss: 1.6579 2.8697 sec/batch\n",
      "Epoch 4/20  Iteration 690/3560 Training loss: 1.6576 2.8202 sec/batch\n",
      "Epoch 4/20  Iteration 691/3560 Training loss: 1.6573 2.7957 sec/batch\n",
      "Epoch 4/20  Iteration 692/3560 Training loss: 1.6570 2.7827 sec/batch\n",
      "Epoch 4/20  Iteration 693/3560 Training loss: 1.6565 2.8462 sec/batch\n",
      "Epoch 4/20  Iteration 694/3560 Training loss: 1.6562 2.8279 sec/batch\n",
      "Epoch 4/20  Iteration 695/3560 Training loss: 1.6561 2.8077 sec/batch\n",
      "Epoch 4/20  Iteration 696/3560 Training loss: 1.6558 3.6455 sec/batch\n",
      "Epoch 4/20  Iteration 697/3560 Training loss: 1.6555 2.9136 sec/batch\n",
      "Epoch 4/20  Iteration 698/3560 Training loss: 1.6552 2.7933 sec/batch\n",
      "Epoch 4/20  Iteration 699/3560 Training loss: 1.6549 2.8938 sec/batch\n",
      "Epoch 4/20  Iteration 700/3560 Training loss: 1.6545 2.8358 sec/batch\n",
      "Epoch 4/20  Iteration 701/3560 Training loss: 1.6543 2.8526 sec/batch\n",
      "Epoch 4/20  Iteration 702/3560 Training loss: 1.6544 2.8656 sec/batch\n",
      "Epoch 4/20  Iteration 703/3560 Training loss: 1.6540 2.8050 sec/batch\n",
      "Epoch 4/20  Iteration 704/3560 Training loss: 1.6536 2.8491 sec/batch\n",
      "Epoch 4/20  Iteration 705/3560 Training loss: 1.6532 2.8371 sec/batch\n",
      "Epoch 4/20  Iteration 706/3560 Training loss: 1.6528 3.4320 sec/batch\n",
      "Epoch 4/20  Iteration 707/3560 Training loss: 1.6526 3.0453 sec/batch\n",
      "Epoch 4/20  Iteration 708/3560 Training loss: 1.6523 2.8117 sec/batch\n",
      "Epoch 4/20  Iteration 709/3560 Training loss: 1.6521 2.8152 sec/batch\n",
      "Epoch 4/20  Iteration 710/3560 Training loss: 1.6516 2.8621 sec/batch\n",
      "Epoch 4/20  Iteration 711/3560 Training loss: 1.6512 2.8021 sec/batch\n",
      "Epoch 4/20  Iteration 712/3560 Training loss: 1.6510 2.7940 sec/batch\n",
      "Epoch 5/20  Iteration 713/3560 Training loss: 1.6662 2.9118 sec/batch\n",
      "Epoch 5/20  Iteration 714/3560 Training loss: 1.6260 2.8355 sec/batch\n",
      "Epoch 5/20  Iteration 715/3560 Training loss: 1.6108 2.8199 sec/batch\n",
      "Epoch 5/20  Iteration 716/3560 Training loss: 1.6064 2.8544 sec/batch\n",
      "Epoch 5/20  Iteration 717/3560 Training loss: 1.6005 3.8248 sec/batch\n",
      "Epoch 5/20  Iteration 718/3560 Training loss: 1.5915 2.8637 sec/batch\n",
      "Epoch 5/20  Iteration 719/3560 Training loss: 1.5907 2.8177 sec/batch\n",
      "Epoch 5/20  Iteration 720/3560 Training loss: 1.5891 2.8890 sec/batch\n",
      "Epoch 5/20  Iteration 721/3560 Training loss: 1.5911 2.8004 sec/batch\n",
      "Epoch 5/20  Iteration 722/3560 Training loss: 1.5900 2.8204 sec/batch\n",
      "Epoch 5/20  Iteration 723/3560 Training loss: 1.5870 2.8037 sec/batch\n",
      "Epoch 5/20  Iteration 724/3560 Training loss: 1.5857 2.9129 sec/batch\n",
      "Epoch 5/20  Iteration 725/3560 Training loss: 1.5860 2.8551 sec/batch\n",
      "Epoch 5/20  Iteration 726/3560 Training loss: 1.5881 2.8374 sec/batch\n",
      "Epoch 5/20  Iteration 727/3560 Training loss: 1.5868 3.8390 sec/batch\n",
      "Epoch 5/20  Iteration 728/3560 Training loss: 1.5854 2.8044 sec/batch\n",
      "Epoch 5/20  Iteration 729/3560 Training loss: 1.5854 2.8489 sec/batch\n",
      "Epoch 5/20  Iteration 730/3560 Training loss: 1.5871 2.8135 sec/batch\n",
      "Epoch 5/20  Iteration 731/3560 Training loss: 1.5867 2.8701 sec/batch\n",
      "Epoch 5/20  Iteration 732/3560 Training loss: 1.5879 2.8221 sec/batch\n",
      "Epoch 5/20  Iteration 733/3560 Training loss: 1.5872 2.7727 sec/batch\n",
      "Epoch 5/20  Iteration 734/3560 Training loss: 1.5879 2.8390 sec/batch\n",
      "Epoch 5/20  Iteration 735/3560 Training loss: 1.5869 2.8680 sec/batch\n",
      "Epoch 5/20  Iteration 736/3560 Training loss: 1.5864 2.7773 sec/batch\n",
      "Epoch 5/20  Iteration 737/3560 Training loss: 1.5862 3.6386 sec/batch\n",
      "Epoch 5/20  Iteration 738/3560 Training loss: 1.5845 2.8158 sec/batch\n",
      "Epoch 5/20  Iteration 739/3560 Training loss: 1.5827 2.8462 sec/batch\n",
      "Epoch 5/20  Iteration 740/3560 Training loss: 1.5827 2.8551 sec/batch\n",
      "Epoch 5/20  Iteration 741/3560 Training loss: 1.5829 2.8654 sec/batch\n",
      "Epoch 5/20  Iteration 742/3560 Training loss: 1.5826 2.8173 sec/batch\n",
      "Epoch 5/20  Iteration 743/3560 Training loss: 1.5820 2.7901 sec/batch\n",
      "Epoch 5/20  Iteration 744/3560 Training loss: 1.5807 2.7883 sec/batch\n",
      "Epoch 5/20  Iteration 745/3560 Training loss: 1.5808 2.9160 sec/batch\n",
      "Epoch 5/20  Iteration 746/3560 Training loss: 1.5809 2.7747 sec/batch\n",
      "Epoch 5/20  Iteration 747/3560 Training loss: 1.5804 3.1820 sec/batch\n",
      "Epoch 5/20  Iteration 748/3560 Training loss: 1.5798 3.4516 sec/batch\n",
      "Epoch 5/20  Iteration 749/3560 Training loss: 1.5788 2.7847 sec/batch\n",
      "Epoch 5/20  Iteration 750/3560 Training loss: 1.5773 2.8141 sec/batch\n",
      "Epoch 5/20  Iteration 751/3560 Training loss: 1.5754 2.8819 sec/batch\n",
      "Epoch 5/20  Iteration 752/3560 Training loss: 1.5746 2.8000 sec/batch\n",
      "Epoch 5/20  Iteration 753/3560 Training loss: 1.5738 2.8339 sec/batch\n",
      "Epoch 5/20  Iteration 754/3560 Training loss: 1.5741 2.8465 sec/batch\n",
      "Epoch 5/20  Iteration 755/3560 Training loss: 1.5734 2.8881 sec/batch\n",
      "Epoch 5/20  Iteration 756/3560 Training loss: 1.5723 2.8209 sec/batch\n",
      "Epoch 5/20  Iteration 757/3560 Training loss: 1.5725 2.8411 sec/batch\n",
      "Epoch 5/20  Iteration 758/3560 Training loss: 1.5714 3.6468 sec/batch\n",
      "Epoch 5/20  Iteration 759/3560 Training loss: 1.5709 2.8705 sec/batch\n",
      "Epoch 5/20  Iteration 760/3560 Training loss: 1.5702 2.8173 sec/batch\n",
      "Epoch 5/20  Iteration 761/3560 Training loss: 1.5699 2.8391 sec/batch\n",
      "Epoch 5/20  Iteration 762/3560 Training loss: 1.5701 2.9022 sec/batch\n",
      "Epoch 5/20  Iteration 763/3560 Training loss: 1.5697 2.8177 sec/batch\n",
      "Epoch 5/20  Iteration 764/3560 Training loss: 1.5704 2.8337 sec/batch\n",
      "Epoch 5/20  Iteration 765/3560 Training loss: 1.5702 2.8881 sec/batch\n",
      "Epoch 5/20  Iteration 766/3560 Training loss: 1.5699 2.8479 sec/batch\n",
      "Epoch 5/20  Iteration 767/3560 Training loss: 1.5695 2.8062 sec/batch\n",
      "Epoch 5/20  Iteration 768/3560 Training loss: 1.5693 3.8334 sec/batch\n",
      "Epoch 5/20  Iteration 769/3560 Training loss: 1.5695 2.8416 sec/batch\n",
      "Epoch 5/20  Iteration 770/3560 Training loss: 1.5690 2.7982 sec/batch\n",
      "Epoch 5/20  Iteration 771/3560 Training loss: 1.5683 2.8013 sec/batch\n",
      "Epoch 5/20  Iteration 772/3560 Training loss: 1.5687 2.8615 sec/batch\n",
      "Epoch 5/20  Iteration 773/3560 Training loss: 1.5684 2.8240 sec/batch\n",
      "Epoch 5/20  Iteration 774/3560 Training loss: 1.5691 2.7542 sec/batch\n",
      "Epoch 5/20  Iteration 775/3560 Training loss: 1.5691 2.8317 sec/batch\n",
      "Epoch 5/20  Iteration 776/3560 Training loss: 1.5690 2.8821 sec/batch\n",
      "Epoch 5/20  Iteration 777/3560 Training loss: 1.5687 2.7839 sec/batch\n",
      "Epoch 5/20  Iteration 778/3560 Training loss: 1.5687 3.4386 sec/batch\n",
      "Epoch 5/20  Iteration 779/3560 Training loss: 1.5685 3.1858 sec/batch\n",
      "Epoch 5/20  Iteration 780/3560 Training loss: 1.5679 2.8184 sec/batch\n",
      "Epoch 5/20  Iteration 781/3560 Training loss: 1.5677 2.8272 sec/batch\n",
      "Epoch 5/20  Iteration 782/3560 Training loss: 1.5674 2.8315 sec/batch\n",
      "Epoch 5/20  Iteration 783/3560 Training loss: 1.5678 2.8108 sec/batch\n",
      "Epoch 5/20  Iteration 784/3560 Training loss: 1.5676 2.8271 sec/batch\n",
      "Epoch 5/20  Iteration 785/3560 Training loss: 1.5678 2.7748 sec/batch\n",
      "Epoch 5/20  Iteration 786/3560 Training loss: 1.5672 2.8616 sec/batch\n",
      "Epoch 5/20  Iteration 787/3560 Training loss: 1.5669 2.8506 sec/batch\n",
      "Epoch 5/20  Iteration 788/3560 Training loss: 1.5670 3.1540 sec/batch\n",
      "Epoch 5/20  Iteration 789/3560 Training loss: 1.5666 3.3281 sec/batch\n",
      "Epoch 5/20  Iteration 790/3560 Training loss: 1.5663 2.7787 sec/batch\n",
      "Epoch 5/20  Iteration 791/3560 Training loss: 1.5656 2.8116 sec/batch\n",
      "Epoch 5/20  Iteration 792/3560 Training loss: 1.5653 2.8437 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 793/3560 Training loss: 1.5645 2.8700 sec/batch\n",
      "Epoch 5/20  Iteration 794/3560 Training loss: 1.5643 2.7985 sec/batch\n",
      "Epoch 5/20  Iteration 795/3560 Training loss: 1.5635 2.8231 sec/batch\n",
      "Epoch 5/20  Iteration 796/3560 Training loss: 1.5633 2.8870 sec/batch\n",
      "Epoch 5/20  Iteration 797/3560 Training loss: 1.5627 2.8371 sec/batch\n",
      "Epoch 5/20  Iteration 798/3560 Training loss: 1.5622 2.7814 sec/batch\n",
      "Epoch 5/20  Iteration 799/3560 Training loss: 1.5617 3.8374 sec/batch\n",
      "Epoch 5/20  Iteration 800/3560 Training loss: 1.5611 2.8469 sec/batch\n",
      "Epoch 5/20  Iteration 801/3560 Training loss: 1.5605 2.7682 sec/batch\n",
      "Epoch 5/20  Iteration 802/3560 Training loss: 1.5604 2.8054 sec/batch\n",
      "Epoch 5/20  Iteration 803/3560 Training loss: 1.5598 2.9625 sec/batch\n",
      "Epoch 5/20  Iteration 804/3560 Training loss: 1.5594 2.8116 sec/batch\n",
      "Epoch 5/20  Iteration 805/3560 Training loss: 1.5589 2.7811 sec/batch\n",
      "Epoch 5/20  Iteration 806/3560 Training loss: 1.5584 2.8599 sec/batch\n",
      "Epoch 5/20  Iteration 807/3560 Training loss: 1.5578 2.8595 sec/batch\n",
      "Epoch 5/20  Iteration 808/3560 Training loss: 1.5576 2.7667 sec/batch\n",
      "Epoch 5/20  Iteration 809/3560 Training loss: 1.5573 3.6261 sec/batch\n",
      "Epoch 5/20  Iteration 810/3560 Training loss: 1.5567 2.9367 sec/batch\n",
      "Epoch 5/20  Iteration 811/3560 Training loss: 1.5560 2.7740 sec/batch\n",
      "Epoch 5/20  Iteration 812/3560 Training loss: 1.5552 2.7855 sec/batch\n",
      "Epoch 5/20  Iteration 813/3560 Training loss: 1.5550 2.7867 sec/batch\n",
      "Epoch 5/20  Iteration 814/3560 Training loss: 1.5546 2.9193 sec/batch\n",
      "Epoch 5/20  Iteration 815/3560 Training loss: 1.5542 2.8057 sec/batch\n",
      "Epoch 5/20  Iteration 816/3560 Training loss: 1.5538 2.8165 sec/batch\n",
      "Epoch 5/20  Iteration 817/3560 Training loss: 1.5533 2.8574 sec/batch\n",
      "Epoch 5/20  Iteration 818/3560 Training loss: 1.5529 2.7832 sec/batch\n",
      "Epoch 5/20  Iteration 819/3560 Training loss: 1.5526 3.3819 sec/batch\n",
      "Epoch 5/20  Iteration 820/3560 Training loss: 1.5523 3.1805 sec/batch\n",
      "Epoch 5/20  Iteration 821/3560 Training loss: 1.5519 2.8296 sec/batch\n",
      "Epoch 5/20  Iteration 822/3560 Training loss: 1.5518 2.8417 sec/batch\n",
      "Epoch 5/20  Iteration 823/3560 Training loss: 1.5513 2.7722 sec/batch\n",
      "Epoch 5/20  Iteration 824/3560 Training loss: 1.5510 2.8693 sec/batch\n",
      "Epoch 5/20  Iteration 825/3560 Training loss: 1.5506 2.7977 sec/batch\n",
      "Epoch 5/20  Iteration 826/3560 Training loss: 1.5502 2.8164 sec/batch\n",
      "Epoch 5/20  Iteration 827/3560 Training loss: 1.5497 2.8523 sec/batch\n",
      "Epoch 5/20  Iteration 828/3560 Training loss: 1.5491 2.8936 sec/batch\n",
      "Epoch 5/20  Iteration 829/3560 Training loss: 1.5487 2.8870 sec/batch\n",
      "Epoch 5/20  Iteration 830/3560 Training loss: 1.5485 3.6761 sec/batch\n",
      "Epoch 5/20  Iteration 831/3560 Training loss: 1.5481 2.9523 sec/batch\n",
      "Epoch 5/20  Iteration 832/3560 Training loss: 1.5477 2.8181 sec/batch\n",
      "Epoch 5/20  Iteration 833/3560 Training loss: 1.5474 2.8364 sec/batch\n",
      "Epoch 5/20  Iteration 834/3560 Training loss: 1.5468 2.9069 sec/batch\n",
      "Epoch 5/20  Iteration 835/3560 Training loss: 1.5462 2.7964 sec/batch\n",
      "Epoch 5/20  Iteration 836/3560 Training loss: 1.5459 2.8443 sec/batch\n",
      "Epoch 5/20  Iteration 837/3560 Training loss: 1.5456 2.8556 sec/batch\n",
      "Epoch 5/20  Iteration 838/3560 Training loss: 1.5450 2.8607 sec/batch\n",
      "Epoch 5/20  Iteration 839/3560 Training loss: 1.5448 2.8998 sec/batch\n",
      "Epoch 5/20  Iteration 840/3560 Training loss: 1.5446 3.8766 sec/batch\n",
      "Epoch 5/20  Iteration 841/3560 Training loss: 1.5442 2.9648 sec/batch\n",
      "Epoch 5/20  Iteration 842/3560 Training loss: 1.5437 2.7764 sec/batch\n",
      "Epoch 5/20  Iteration 843/3560 Training loss: 1.5430 2.7985 sec/batch\n",
      "Epoch 5/20  Iteration 844/3560 Training loss: 1.5425 2.8799 sec/batch\n",
      "Epoch 5/20  Iteration 845/3560 Training loss: 1.5423 2.8564 sec/batch\n",
      "Epoch 5/20  Iteration 846/3560 Training loss: 1.5421 2.8355 sec/batch\n",
      "Epoch 5/20  Iteration 847/3560 Training loss: 1.5418 2.8301 sec/batch\n",
      "Epoch 5/20  Iteration 848/3560 Training loss: 1.5415 2.8761 sec/batch\n",
      "Epoch 5/20  Iteration 849/3560 Training loss: 1.5414 2.8027 sec/batch\n",
      "Epoch 5/20  Iteration 850/3560 Training loss: 1.5412 3.6859 sec/batch\n",
      "Epoch 5/20  Iteration 851/3560 Training loss: 1.5409 2.8644 sec/batch\n",
      "Epoch 5/20  Iteration 852/3560 Training loss: 1.5406 2.9062 sec/batch\n",
      "Epoch 5/20  Iteration 853/3560 Training loss: 1.5407 2.8004 sec/batch\n",
      "Epoch 5/20  Iteration 854/3560 Training loss: 1.5403 2.8236 sec/batch\n",
      "Epoch 5/20  Iteration 855/3560 Training loss: 1.5400 2.8837 sec/batch\n",
      "Epoch 5/20  Iteration 856/3560 Training loss: 1.5399 2.8167 sec/batch\n",
      "Epoch 5/20  Iteration 857/3560 Training loss: 1.5395 2.8081 sec/batch\n",
      "Epoch 5/20  Iteration 858/3560 Training loss: 1.5393 2.8255 sec/batch\n",
      "Epoch 5/20  Iteration 859/3560 Training loss: 1.5391 2.8610 sec/batch\n",
      "Epoch 5/20  Iteration 860/3560 Training loss: 1.5391 3.5010 sec/batch\n",
      "Epoch 5/20  Iteration 861/3560 Training loss: 1.5389 3.0566 sec/batch\n",
      "Epoch 5/20  Iteration 862/3560 Training loss: 1.5385 2.8523 sec/batch\n",
      "Epoch 5/20  Iteration 863/3560 Training loss: 1.5379 2.8306 sec/batch\n",
      "Epoch 5/20  Iteration 864/3560 Training loss: 1.5376 2.8086 sec/batch\n",
      "Epoch 5/20  Iteration 865/3560 Training loss: 1.5374 2.8678 sec/batch\n",
      "Epoch 5/20  Iteration 866/3560 Training loss: 1.5371 2.7696 sec/batch\n",
      "Epoch 5/20  Iteration 867/3560 Training loss: 1.5369 2.7894 sec/batch\n",
      "Epoch 5/20  Iteration 868/3560 Training loss: 1.5366 2.8768 sec/batch\n",
      "Epoch 5/20  Iteration 869/3560 Training loss: 1.5364 2.8610 sec/batch\n",
      "Epoch 5/20  Iteration 870/3560 Training loss: 1.5361 2.9230 sec/batch\n",
      "Epoch 5/20  Iteration 871/3560 Training loss: 1.5355 3.5479 sec/batch\n",
      "Epoch 5/20  Iteration 872/3560 Training loss: 1.5354 2.9029 sec/batch\n",
      "Epoch 5/20  Iteration 873/3560 Training loss: 1.5353 2.8039 sec/batch\n",
      "Epoch 5/20  Iteration 874/3560 Training loss: 1.5351 2.8474 sec/batch\n",
      "Epoch 5/20  Iteration 875/3560 Training loss: 1.5348 2.8572 sec/batch\n",
      "Epoch 5/20  Iteration 876/3560 Training loss: 1.5345 2.8357 sec/batch\n",
      "Epoch 5/20  Iteration 877/3560 Training loss: 1.5343 2.8034 sec/batch\n",
      "Epoch 5/20  Iteration 878/3560 Training loss: 1.5339 2.8260 sec/batch\n",
      "Epoch 5/20  Iteration 879/3560 Training loss: 1.5338 2.8454 sec/batch\n",
      "Epoch 5/20  Iteration 880/3560 Training loss: 1.5340 2.8129 sec/batch\n",
      "Epoch 5/20  Iteration 881/3560 Training loss: 1.5337 3.6467 sec/batch\n",
      "Epoch 5/20  Iteration 882/3560 Training loss: 1.5334 2.8313 sec/batch\n",
      "Epoch 5/20  Iteration 883/3560 Training loss: 1.5331 2.8310 sec/batch\n",
      "Epoch 5/20  Iteration 884/3560 Training loss: 1.5327 2.7688 sec/batch\n",
      "Epoch 5/20  Iteration 885/3560 Training loss: 1.5326 2.8447 sec/batch\n",
      "Epoch 5/20  Iteration 886/3560 Training loss: 1.5323 2.8906 sec/batch\n",
      "Epoch 5/20  Iteration 887/3560 Training loss: 1.5322 2.7948 sec/batch\n",
      "Epoch 5/20  Iteration 888/3560 Training loss: 1.5318 2.8199 sec/batch\n",
      "Epoch 5/20  Iteration 889/3560 Training loss: 1.5314 2.8869 sec/batch\n",
      "Epoch 5/20  Iteration 890/3560 Training loss: 1.5312 2.8858 sec/batch\n",
      "Epoch 6/20  Iteration 891/3560 Training loss: 1.6096 3.4752 sec/batch\n",
      "Epoch 6/20  Iteration 892/3560 Training loss: 1.5554 2.9449 sec/batch\n",
      "Epoch 6/20  Iteration 893/3560 Training loss: 1.5387 2.9010 sec/batch\n",
      "Epoch 6/20  Iteration 894/3560 Training loss: 1.5314 2.8211 sec/batch\n",
      "Epoch 6/20  Iteration 895/3560 Training loss: 1.5197 2.8536 sec/batch\n",
      "Epoch 6/20  Iteration 896/3560 Training loss: 1.5088 2.8387 sec/batch\n",
      "Epoch 6/20  Iteration 897/3560 Training loss: 1.5079 2.8241 sec/batch\n",
      "Epoch 6/20  Iteration 898/3560 Training loss: 1.5052 2.8020 sec/batch\n",
      "Epoch 6/20  Iteration 899/3560 Training loss: 1.5049 2.8374 sec/batch\n",
      "Epoch 6/20  Iteration 900/3560 Training loss: 1.5036 2.7780 sec/batch\n",
      "Epoch 6/20  Iteration 901/3560 Training loss: 1.4988 3.1507 sec/batch\n",
      "Epoch 6/20  Iteration 902/3560 Training loss: 1.4977 3.4894 sec/batch\n",
      "Epoch 6/20  Iteration 903/3560 Training loss: 1.4974 2.7998 sec/batch\n",
      "Epoch 6/20  Iteration 904/3560 Training loss: 1.4987 2.9243 sec/batch\n",
      "Epoch 6/20  Iteration 905/3560 Training loss: 1.4971 2.8392 sec/batch\n",
      "Epoch 6/20  Iteration 906/3560 Training loss: 1.4943 2.7571 sec/batch\n",
      "Epoch 6/20  Iteration 907/3560 Training loss: 1.4938 2.8727 sec/batch\n",
      "Epoch 6/20  Iteration 908/3560 Training loss: 1.4948 2.8018 sec/batch\n",
      "Epoch 6/20  Iteration 909/3560 Training loss: 1.4947 2.8499 sec/batch\n",
      "Epoch 6/20  Iteration 910/3560 Training loss: 1.4951 2.8045 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 911/3560 Training loss: 1.4945 2.8769 sec/batch\n",
      "Epoch 6/20  Iteration 912/3560 Training loss: 1.4948 3.6200 sec/batch\n",
      "Epoch 6/20  Iteration 913/3560 Training loss: 1.4935 2.8311 sec/batch\n",
      "Epoch 6/20  Iteration 914/3560 Training loss: 1.4933 2.8807 sec/batch\n",
      "Epoch 6/20  Iteration 915/3560 Training loss: 1.4927 2.7823 sec/batch\n",
      "Epoch 6/20  Iteration 916/3560 Training loss: 1.4908 2.8234 sec/batch\n",
      "Epoch 6/20  Iteration 917/3560 Training loss: 1.4889 2.8158 sec/batch\n",
      "Epoch 6/20  Iteration 918/3560 Training loss: 1.4891 2.8035 sec/batch\n",
      "Epoch 6/20  Iteration 919/3560 Training loss: 1.4891 2.7945 sec/batch\n",
      "Epoch 6/20  Iteration 920/3560 Training loss: 1.4888 2.9195 sec/batch\n",
      "Epoch 6/20  Iteration 921/3560 Training loss: 1.4877 2.8592 sec/batch\n",
      "Epoch 6/20  Iteration 922/3560 Training loss: 1.4864 3.3073 sec/batch\n",
      "Epoch 6/20  Iteration 923/3560 Training loss: 1.4862 3.1020 sec/batch\n",
      "Epoch 6/20  Iteration 924/3560 Training loss: 1.4860 2.8286 sec/batch\n",
      "Epoch 6/20  Iteration 925/3560 Training loss: 1.4854 2.8295 sec/batch\n",
      "Epoch 6/20  Iteration 926/3560 Training loss: 1.4844 2.7878 sec/batch\n",
      "Epoch 6/20  Iteration 927/3560 Training loss: 1.4831 2.8526 sec/batch\n",
      "Epoch 6/20  Iteration 928/3560 Training loss: 1.4816 2.8152 sec/batch\n",
      "Epoch 6/20  Iteration 929/3560 Training loss: 1.4797 2.9080 sec/batch\n",
      "Epoch 6/20  Iteration 930/3560 Training loss: 1.4787 2.8117 sec/batch\n",
      "Epoch 6/20  Iteration 931/3560 Training loss: 1.4780 2.7851 sec/batch\n",
      "Epoch 6/20  Iteration 932/3560 Training loss: 1.4783 3.4334 sec/batch\n",
      "Epoch 6/20  Iteration 933/3560 Training loss: 1.4773 3.0647 sec/batch\n",
      "Epoch 6/20  Iteration 934/3560 Training loss: 1.4762 2.8021 sec/batch\n",
      "Epoch 6/20  Iteration 935/3560 Training loss: 1.4761 2.8567 sec/batch\n",
      "Epoch 6/20  Iteration 936/3560 Training loss: 1.4749 2.8093 sec/batch\n",
      "Epoch 6/20  Iteration 937/3560 Training loss: 1.4744 2.7918 sec/batch\n",
      "Epoch 6/20  Iteration 938/3560 Training loss: 1.4738 2.7845 sec/batch\n",
      "Epoch 6/20  Iteration 939/3560 Training loss: 1.4733 2.8313 sec/batch\n",
      "Epoch 6/20  Iteration 940/3560 Training loss: 1.4733 2.8269 sec/batch\n",
      "Epoch 6/20  Iteration 941/3560 Training loss: 1.4726 2.7866 sec/batch\n",
      "Epoch 6/20  Iteration 942/3560 Training loss: 1.4732 2.8501 sec/batch\n",
      "Epoch 6/20  Iteration 943/3560 Training loss: 1.4727 3.6720 sec/batch\n",
      "Epoch 6/20  Iteration 944/3560 Training loss: 1.4727 2.8063 sec/batch\n",
      "Epoch 6/20  Iteration 945/3560 Training loss: 1.4722 2.8324 sec/batch\n",
      "Epoch 6/20  Iteration 946/3560 Training loss: 1.4720 2.9513 sec/batch\n",
      "Epoch 6/20  Iteration 947/3560 Training loss: 1.4722 2.8210 sec/batch\n",
      "Epoch 6/20  Iteration 948/3560 Training loss: 1.4715 2.8182 sec/batch\n",
      "Epoch 6/20  Iteration 949/3560 Training loss: 1.4709 2.8471 sec/batch\n",
      "Epoch 6/20  Iteration 950/3560 Training loss: 1.4712 2.7747 sec/batch\n",
      "Epoch 6/20  Iteration 951/3560 Training loss: 1.4710 2.8669 sec/batch\n",
      "Epoch 6/20  Iteration 952/3560 Training loss: 1.4717 2.7870 sec/batch\n",
      "Epoch 6/20  Iteration 953/3560 Training loss: 1.4719 3.6723 sec/batch\n",
      "Epoch 6/20  Iteration 954/3560 Training loss: 1.4717 2.8688 sec/batch\n",
      "Epoch 6/20  Iteration 955/3560 Training loss: 1.4714 2.8008 sec/batch\n",
      "Epoch 6/20  Iteration 956/3560 Training loss: 1.4713 2.8325 sec/batch\n",
      "Epoch 6/20  Iteration 957/3560 Training loss: 1.4711 2.8404 sec/batch\n",
      "Epoch 6/20  Iteration 958/3560 Training loss: 1.4704 2.7994 sec/batch\n",
      "Epoch 6/20  Iteration 959/3560 Training loss: 1.4703 2.7589 sec/batch\n",
      "Epoch 6/20  Iteration 960/3560 Training loss: 1.4700 2.8675 sec/batch\n",
      "Epoch 6/20  Iteration 961/3560 Training loss: 1.4703 2.8970 sec/batch\n",
      "Epoch 6/20  Iteration 962/3560 Training loss: 1.4704 2.8039 sec/batch\n",
      "Epoch 6/20  Iteration 963/3560 Training loss: 1.4708 2.9248 sec/batch\n",
      "Epoch 6/20  Iteration 964/3560 Training loss: 1.4703 3.7341 sec/batch\n",
      "Epoch 6/20  Iteration 965/3560 Training loss: 1.4700 2.8323 sec/batch\n",
      "Epoch 6/20  Iteration 966/3560 Training loss: 1.4700 2.9226 sec/batch\n",
      "Epoch 6/20  Iteration 967/3560 Training loss: 1.4697 2.8329 sec/batch\n",
      "Epoch 6/20  Iteration 968/3560 Training loss: 1.4693 2.8154 sec/batch\n",
      "Epoch 6/20  Iteration 969/3560 Training loss: 1.4685 2.8439 sec/batch\n",
      "Epoch 6/20  Iteration 970/3560 Training loss: 1.4681 2.8697 sec/batch\n",
      "Epoch 6/20  Iteration 971/3560 Training loss: 1.4674 2.8424 sec/batch\n",
      "Epoch 6/20  Iteration 972/3560 Training loss: 1.4672 2.8011 sec/batch\n",
      "Epoch 6/20  Iteration 973/3560 Training loss: 1.4665 3.2212 sec/batch\n",
      "Epoch 6/20  Iteration 974/3560 Training loss: 1.4662 3.3298 sec/batch\n",
      "Epoch 6/20  Iteration 975/3560 Training loss: 1.4656 2.8209 sec/batch\n",
      "Epoch 6/20  Iteration 976/3560 Training loss: 1.4652 2.8657 sec/batch\n",
      "Epoch 6/20  Iteration 977/3560 Training loss: 1.4647 2.9141 sec/batch\n",
      "Epoch 6/20  Iteration 978/3560 Training loss: 1.4642 2.8121 sec/batch\n",
      "Epoch 6/20  Iteration 979/3560 Training loss: 1.4636 2.8157 sec/batch\n",
      "Epoch 6/20  Iteration 980/3560 Training loss: 1.4635 2.8295 sec/batch\n",
      "Epoch 6/20  Iteration 981/3560 Training loss: 1.4631 2.8277 sec/batch\n",
      "Epoch 6/20  Iteration 982/3560 Training loss: 1.4628 2.8150 sec/batch\n",
      "Epoch 6/20  Iteration 983/3560 Training loss: 1.4623 2.7734 sec/batch\n",
      "Epoch 6/20  Iteration 984/3560 Training loss: 1.4617 3.6365 sec/batch\n",
      "Epoch 6/20  Iteration 985/3560 Training loss: 1.4613 2.8441 sec/batch\n",
      "Epoch 6/20  Iteration 986/3560 Training loss: 1.4612 2.7739 sec/batch\n",
      "Epoch 6/20  Iteration 987/3560 Training loss: 1.4609 2.9137 sec/batch\n",
      "Epoch 6/20  Iteration 988/3560 Training loss: 1.4604 2.8358 sec/batch\n",
      "Epoch 6/20  Iteration 989/3560 Training loss: 1.4599 2.8112 sec/batch\n",
      "Epoch 6/20  Iteration 990/3560 Training loss: 1.4593 2.8141 sec/batch\n",
      "Epoch 6/20  Iteration 991/3560 Training loss: 1.4590 2.8241 sec/batch\n",
      "Epoch 6/20  Iteration 992/3560 Training loss: 1.4587 2.8164 sec/batch\n",
      "Epoch 6/20  Iteration 993/3560 Training loss: 1.4584 2.8245 sec/batch\n",
      "Epoch 6/20  Iteration 994/3560 Training loss: 1.4581 3.6913 sec/batch\n",
      "Epoch 6/20  Iteration 995/3560 Training loss: 1.4577 3.0789 sec/batch\n",
      "Epoch 6/20  Iteration 996/3560 Training loss: 1.4574 2.7688 sec/batch\n",
      "Epoch 6/20  Iteration 997/3560 Training loss: 1.4572 2.8302 sec/batch\n",
      "Epoch 6/20  Iteration 998/3560 Training loss: 1.4571 2.8371 sec/batch\n",
      "Epoch 6/20  Iteration 999/3560 Training loss: 1.4568 2.8205 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3560 Training loss: 1.4567 2.8080 sec/batch\n",
      "Epoch 6/20  Iteration 1001/3560 Training loss: 1.4563 2.8162 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3560 Training loss: 1.4561 2.7732 sec/batch\n",
      "Epoch 6/20  Iteration 1003/3560 Training loss: 1.4558 2.8170 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3560 Training loss: 1.4554 2.8368 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3560 Training loss: 1.4550 3.5675 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3560 Training loss: 1.4545 2.7737 sec/batch\n",
      "Epoch 6/20  Iteration 1007/3560 Training loss: 1.4543 2.8285 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3560 Training loss: 1.4541 2.8674 sec/batch\n",
      "Epoch 6/20  Iteration 1009/3560 Training loss: 1.4539 2.7770 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3560 Training loss: 1.4537 2.8027 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3560 Training loss: 1.4535 2.7950 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3560 Training loss: 1.4530 2.8187 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3560 Training loss: 1.4525 2.8134 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3560 Training loss: 1.4523 2.7992 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3560 Training loss: 1.4521 3.7044 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3560 Training loss: 1.4517 2.7814 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3560 Training loss: 1.4516 2.7905 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3560 Training loss: 1.4514 2.9209 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3560 Training loss: 1.4511 2.8014 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3560 Training loss: 1.4507 2.7660 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3560 Training loss: 1.4502 2.7542 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3560 Training loss: 1.4498 2.8532 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3560 Training loss: 1.4498 2.8025 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3560 Training loss: 1.4497 2.7524 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3560 Training loss: 1.4495 3.6185 sec/batch\n",
      "Epoch 6/20  Iteration 1026/3560 Training loss: 1.4493 3.0653 sec/batch\n",
      "Epoch 6/20  Iteration 1027/3560 Training loss: 1.4493 2.7517 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 1028/3560 Training loss: 1.4492 2.8021 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3560 Training loss: 1.4491 2.8431 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3560 Training loss: 1.4489 2.7547 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3560 Training loss: 1.4491 2.7843 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3560 Training loss: 1.4488 2.8197 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3560 Training loss: 1.4486 2.8575 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3560 Training loss: 1.4486 2.8145 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3560 Training loss: 1.4483 3.1147 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3560 Training loss: 1.4483 3.3256 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3560 Training loss: 1.4482 2.8174 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3560 Training loss: 1.4483 2.8024 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3560 Training loss: 1.4483 2.8716 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3560 Training loss: 1.4479 2.7856 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3560 Training loss: 1.4475 2.7737 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3560 Training loss: 1.4472 2.7967 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3560 Training loss: 1.4470 2.8304 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3560 Training loss: 1.4468 2.7691 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3560 Training loss: 1.4467 2.7916 sec/batch\n",
      "Epoch 6/20  Iteration 1046/3560 Training loss: 1.4465 3.7195 sec/batch\n",
      "Epoch 6/20  Iteration 1047/3560 Training loss: 1.4464 2.8288 sec/batch\n",
      "Epoch 6/20  Iteration 1048/3560 Training loss: 1.4462 2.7945 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3560 Training loss: 1.4458 2.7867 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3560 Training loss: 1.4458 2.8335 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3560 Training loss: 1.4458 2.7863 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3560 Training loss: 1.4456 2.7664 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3560 Training loss: 1.4454 2.8570 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3560 Training loss: 1.4452 2.8178 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3560 Training loss: 1.4451 2.7400 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3560 Training loss: 1.4448 3.4408 sec/batch\n",
      "Epoch 6/20  Iteration 1057/3560 Training loss: 1.4448 3.0818 sec/batch\n",
      "Epoch 6/20  Iteration 1058/3560 Training loss: 1.4451 2.7586 sec/batch\n",
      "Epoch 6/20  Iteration 1059/3560 Training loss: 1.4449 2.7546 sec/batch\n",
      "Epoch 6/20  Iteration 1060/3560 Training loss: 1.4448 2.8894 sec/batch\n",
      "Epoch 6/20  Iteration 1061/3560 Training loss: 1.4445 2.8001 sec/batch\n",
      "Epoch 6/20  Iteration 1062/3560 Training loss: 1.4442 2.7689 sec/batch\n",
      "Epoch 6/20  Iteration 1063/3560 Training loss: 1.4442 2.8248 sec/batch\n",
      "Epoch 6/20  Iteration 1064/3560 Training loss: 1.4441 2.8476 sec/batch\n",
      "Epoch 6/20  Iteration 1065/3560 Training loss: 1.4440 2.7867 sec/batch\n",
      "Epoch 6/20  Iteration 1066/3560 Training loss: 1.4437 2.8071 sec/batch\n",
      "Epoch 6/20  Iteration 1067/3560 Training loss: 1.4434 3.6805 sec/batch\n",
      "Epoch 6/20  Iteration 1068/3560 Training loss: 1.4434 2.7908 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3560 Training loss: 1.5736 2.7689 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3560 Training loss: 1.5098 2.8193 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3560 Training loss: 1.4898 2.8225 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3560 Training loss: 1.4792 2.7967 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3560 Training loss: 1.4650 2.8037 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3560 Training loss: 1.4539 2.8457 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3560 Training loss: 1.4533 2.8295 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3560 Training loss: 1.4498 2.7791 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3560 Training loss: 1.4497 3.5614 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3560 Training loss: 1.4477 2.8804 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3560 Training loss: 1.4434 2.7884 sec/batch\n",
      "Epoch 7/20  Iteration 1080/3560 Training loss: 1.4416 2.7698 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3560 Training loss: 1.4409 2.8351 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3560 Training loss: 1.4414 2.7665 sec/batch\n",
      "Epoch 7/20  Iteration 1083/3560 Training loss: 1.4386 2.8014 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3560 Training loss: 1.4362 2.7910 sec/batch\n",
      "Epoch 7/20  Iteration 1085/3560 Training loss: 1.4356 2.8678 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3560 Training loss: 1.4357 2.7546 sec/batch\n",
      "Epoch 7/20  Iteration 1087/3560 Training loss: 1.4351 2.9525 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3560 Training loss: 1.4353 3.7679 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3560 Training loss: 1.4341 2.7556 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3560 Training loss: 1.4335 2.8228 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3560 Training loss: 1.4322 2.8275 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3560 Training loss: 1.4315 2.8217 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3560 Training loss: 1.4307 2.7642 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3560 Training loss: 1.4286 2.7923 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3560 Training loss: 1.4268 2.8660 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3560 Training loss: 1.4268 2.7697 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3560 Training loss: 1.4264 2.7876 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3560 Training loss: 1.4265 3.5888 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3560 Training loss: 1.4254 2.8134 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3560 Training loss: 1.4236 2.8053 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3560 Training loss: 1.4232 2.7798 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3560 Training loss: 1.4229 2.8723 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3560 Training loss: 1.4221 2.7910 sec/batch\n",
      "Epoch 7/20  Iteration 1104/3560 Training loss: 1.4213 2.7512 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3560 Training loss: 1.4202 2.8034 sec/batch\n",
      "Epoch 7/20  Iteration 1106/3560 Training loss: 1.4187 2.9389 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3560 Training loss: 1.4169 2.7950 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3560 Training loss: 1.4160 3.4467 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3560 Training loss: 1.4154 2.8724 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3560 Training loss: 1.4158 2.8714 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3560 Training loss: 1.4151 2.8098 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3560 Training loss: 1.4141 2.8393 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3560 Training loss: 1.4139 2.8638 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3560 Training loss: 1.4126 2.7852 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3560 Training loss: 1.4122 2.7793 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3560 Training loss: 1.4116 2.8060 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3560 Training loss: 1.4112 2.8780 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3560 Training loss: 1.4112 2.9221 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3560 Training loss: 1.4106 3.5409 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3560 Training loss: 1.4109 2.7907 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3560 Training loss: 1.4106 2.8145 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3560 Training loss: 1.4105 2.7969 sec/batch\n",
      "Epoch 7/20  Iteration 1123/3560 Training loss: 1.4102 2.7987 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3560 Training loss: 1.4101 2.8212 sec/batch\n",
      "Epoch 7/20  Iteration 1125/3560 Training loss: 1.4103 2.8130 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3560 Training loss: 1.4096 2.8448 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3560 Training loss: 1.4088 2.7996 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3560 Training loss: 1.4092 2.8014 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3560 Training loss: 1.4091 3.6516 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3560 Training loss: 1.4098 2.8513 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3560 Training loss: 1.4100 2.7998 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3560 Training loss: 1.4100 2.8198 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3560 Training loss: 1.4097 2.8180 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3560 Training loss: 1.4098 2.8393 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3560 Training loss: 1.4097 2.7789 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3560 Training loss: 1.4093 2.8511 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3560 Training loss: 1.4093 2.8199 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3560 Training loss: 1.4090 2.8357 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3560 Training loss: 1.4094 3.3377 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3560 Training loss: 1.4094 3.0612 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3560 Training loss: 1.4098 2.8539 sec/batch\n",
      "Epoch 7/20  Iteration 1142/3560 Training loss: 1.4093 2.7344 sec/batch\n",
      "Epoch 7/20  Iteration 1143/3560 Training loss: 1.4090 2.7812 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1144/3560 Training loss: 1.4090 2.8416 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3560 Training loss: 1.4088 2.8503 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3560 Training loss: 1.4084 2.7739 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3560 Training loss: 1.4076 2.8146 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3560 Training loss: 1.4075 2.8319 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3560 Training loss: 1.4068 2.7893 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3560 Training loss: 1.4067 3.8967 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3560 Training loss: 1.4061 2.9333 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3560 Training loss: 1.4059 2.7855 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3560 Training loss: 1.4055 2.8125 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3560 Training loss: 1.4052 2.8679 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3560 Training loss: 1.4048 2.8690 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3560 Training loss: 1.4044 2.8248 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3560 Training loss: 1.4038 2.7843 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3560 Training loss: 1.4038 2.8609 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3560 Training loss: 1.4034 2.8478 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3560 Training loss: 1.4031 3.5889 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3560 Training loss: 1.4026 2.8184 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3560 Training loss: 1.4021 2.8485 sec/batch\n",
      "Epoch 7/20  Iteration 1163/3560 Training loss: 1.4017 2.7699 sec/batch\n",
      "Epoch 7/20  Iteration 1164/3560 Training loss: 1.4016 2.8628 sec/batch\n",
      "Epoch 7/20  Iteration 1165/3560 Training loss: 1.4013 2.8720 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3560 Training loss: 1.4008 2.8403 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3560 Training loss: 1.4002 2.8189 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3560 Training loss: 1.3996 2.8335 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3560 Training loss: 1.3993 2.8314 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3560 Training loss: 1.3991 3.3915 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3560 Training loss: 1.3987 3.0194 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3560 Training loss: 1.3984 2.8630 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3560 Training loss: 1.3981 2.7699 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3560 Training loss: 1.3979 2.8093 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3560 Training loss: 1.3976 2.8413 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3560 Training loss: 1.3974 2.7910 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3560 Training loss: 1.3972 2.7440 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3560 Training loss: 1.3972 2.7847 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3560 Training loss: 1.3968 2.8166 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3560 Training loss: 1.3966 2.8048 sec/batch\n",
      "Epoch 7/20  Iteration 1181/3560 Training loss: 1.3963 3.7088 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3560 Training loss: 1.3960 2.7868 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3560 Training loss: 1.3956 2.8126 sec/batch\n",
      "Epoch 7/20  Iteration 1184/3560 Training loss: 1.3952 2.7879 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3560 Training loss: 1.3950 2.8428 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3560 Training loss: 1.3948 2.8735 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3560 Training loss: 1.3946 3.2569 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3560 Training loss: 1.3943 3.0222 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3560 Training loss: 1.3941 3.0048 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3560 Training loss: 1.3936 3.1984 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3560 Training loss: 1.3930 3.9121 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3560 Training loss: 1.3929 3.0139 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3560 Training loss: 1.3927 3.0425 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3560 Training loss: 1.3921 2.9765 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3560 Training loss: 1.3921 3.0912 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3560 Training loss: 1.3920 3.0627 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3560 Training loss: 1.3917 2.9471 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3560 Training loss: 1.3913 2.9604 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3560 Training loss: 1.3910 3.0675 sec/batch\n",
      "Epoch 7/20  Iteration 1200/3560 Training loss: 1.3907 3.8088 sec/batch\n",
      "Epoch 7/20  Iteration 1201/3560 Training loss: 1.3906 3.2021 sec/batch\n",
      "Epoch 7/20  Iteration 1202/3560 Training loss: 1.3906 2.9956 sec/batch\n",
      "Epoch 7/20  Iteration 1203/3560 Training loss: 1.3904 3.0425 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3560 Training loss: 1.3903 3.0043 sec/batch\n",
      "Epoch 7/20  Iteration 1205/3560 Training loss: 1.3904 2.9578 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3560 Training loss: 1.3904 3.5910 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3560 Training loss: 1.3903 3.1415 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3560 Training loss: 1.3902 2.9348 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3560 Training loss: 1.3905 3.1943 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3560 Training loss: 1.3904 3.9112 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3560 Training loss: 1.3900 3.0689 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3560 Training loss: 1.3902 2.9904 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3560 Training loss: 1.3899 3.0165 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3560 Training loss: 1.3900 3.0710 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3560 Training loss: 1.3899 3.0349 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3560 Training loss: 1.3900 3.0250 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3560 Training loss: 1.3900 3.0692 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3560 Training loss: 1.3897 3.0460 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3560 Training loss: 1.3893 3.3769 sec/batch\n",
      "Epoch 7/20  Iteration 1220/3560 Training loss: 1.3890 3.5824 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3560 Training loss: 1.3890 3.1229 sec/batch\n",
      "Epoch 7/20  Iteration 1222/3560 Training loss: 1.3888 2.9823 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3560 Training loss: 1.3887 2.9168 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3560 Training loss: 1.3886 3.0882 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3560 Training loss: 1.3885 3.0357 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3560 Training loss: 1.3884 2.9387 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3560 Training loss: 1.3880 3.0106 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3560 Training loss: 1.3879 3.0784 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3560 Training loss: 1.3880 3.9171 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3560 Training loss: 1.3879 3.0210 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3560 Training loss: 1.3877 3.0294 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3560 Training loss: 1.3877 2.9961 sec/batch\n",
      "Epoch 7/20  Iteration 1233/3560 Training loss: 1.3875 2.9675 sec/batch\n",
      "Epoch 7/20  Iteration 1234/3560 Training loss: 1.3874 2.9967 sec/batch\n",
      "Epoch 7/20  Iteration 1235/3560 Training loss: 1.3874 3.0511 sec/batch\n",
      "Epoch 7/20  Iteration 1236/3560 Training loss: 1.3877 3.0236 sec/batch\n",
      "Epoch 7/20  Iteration 1237/3560 Training loss: 1.3875 3.0149 sec/batch\n",
      "Epoch 7/20  Iteration 1238/3560 Training loss: 1.3874 3.6649 sec/batch\n",
      "Epoch 7/20  Iteration 1239/3560 Training loss: 1.3872 3.4085 sec/batch\n",
      "Epoch 7/20  Iteration 1240/3560 Training loss: 1.3869 2.9972 sec/batch\n",
      "Epoch 7/20  Iteration 1241/3560 Training loss: 1.3869 3.0566 sec/batch\n",
      "Epoch 7/20  Iteration 1242/3560 Training loss: 1.3869 2.9975 sec/batch\n",
      "Epoch 7/20  Iteration 1243/3560 Training loss: 1.3868 3.0002 sec/batch\n",
      "Epoch 7/20  Iteration 1244/3560 Training loss: 1.3865 3.0389 sec/batch\n",
      "Epoch 7/20  Iteration 1245/3560 Training loss: 1.3863 3.0339 sec/batch\n",
      "Epoch 7/20  Iteration 1246/3560 Training loss: 1.3863 2.9899 sec/batch\n",
      "Epoch 8/20  Iteration 1247/3560 Training loss: 1.5045 3.0127 sec/batch\n",
      "Epoch 8/20  Iteration 1248/3560 Training loss: 1.4416 3.9935 sec/batch\n",
      "Epoch 8/20  Iteration 1249/3560 Training loss: 1.4183 3.0053 sec/batch\n",
      "Epoch 8/20  Iteration 1250/3560 Training loss: 1.4110 3.0055 sec/batch\n",
      "Epoch 8/20  Iteration 1251/3560 Training loss: 1.3987 3.0081 sec/batch\n",
      "Epoch 8/20  Iteration 1252/3560 Training loss: 1.3887 2.9747 sec/batch\n",
      "Epoch 8/20  Iteration 1253/3560 Training loss: 1.3878 3.0805 sec/batch\n",
      "Epoch 8/20  Iteration 1254/3560 Training loss: 1.3835 2.9880 sec/batch\n",
      "Epoch 8/20  Iteration 1255/3560 Training loss: 1.3843 2.9729 sec/batch\n",
      "Epoch 8/20  Iteration 1256/3560 Training loss: 1.3816 2.9781 sec/batch\n",
      "Epoch 8/20  Iteration 1257/3560 Training loss: 1.3775 3.0365 sec/batch\n",
      "Epoch 8/20  Iteration 1258/3560 Training loss: 1.3764 3.8863 sec/batch\n",
      "Epoch 8/20  Iteration 1259/3560 Training loss: 1.3766 3.0108 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1260/3560 Training loss: 1.3772 3.0251 sec/batch\n",
      "Epoch 8/20  Iteration 1261/3560 Training loss: 1.3758 2.9448 sec/batch\n",
      "Epoch 8/20  Iteration 1262/3560 Training loss: 1.3736 3.0231 sec/batch\n",
      "Epoch 8/20  Iteration 1263/3560 Training loss: 1.3734 2.9847 sec/batch\n",
      "Epoch 8/20  Iteration 1264/3560 Training loss: 1.3743 2.9844 sec/batch\n",
      "Epoch 8/20  Iteration 1265/3560 Training loss: 1.3739 3.0106 sec/batch\n",
      "Epoch 8/20  Iteration 1266/3560 Training loss: 1.3749 3.0215 sec/batch\n",
      "Epoch 8/20  Iteration 1267/3560 Training loss: 1.3744 3.3693 sec/batch\n",
      "Epoch 8/20  Iteration 1268/3560 Training loss: 1.3747 3.6971 sec/batch\n",
      "Epoch 8/20  Iteration 1269/3560 Training loss: 1.3731 3.0008 sec/batch\n",
      "Epoch 8/20  Iteration 1270/3560 Training loss: 1.3728 3.0334 sec/batch\n",
      "Epoch 8/20  Iteration 1271/3560 Training loss: 1.3723 2.9973 sec/batch\n",
      "Epoch 8/20  Iteration 1272/3560 Training loss: 1.3703 2.9308 sec/batch\n",
      "Epoch 8/20  Iteration 1273/3560 Training loss: 1.3688 2.9612 sec/batch\n",
      "Epoch 8/20  Iteration 1274/3560 Training loss: 1.3688 2.9988 sec/batch\n",
      "Epoch 8/20  Iteration 1275/3560 Training loss: 1.3689 2.9647 sec/batch\n",
      "Epoch 8/20  Iteration 1276/3560 Training loss: 1.3691 2.9777 sec/batch\n",
      "Epoch 8/20  Iteration 1277/3560 Training loss: 1.3679 4.3052 sec/batch\n",
      "Epoch 8/20  Iteration 1278/3560 Training loss: 1.3666 2.9587 sec/batch\n",
      "Epoch 8/20  Iteration 1279/3560 Training loss: 1.3665 3.0024 sec/batch\n",
      "Epoch 8/20  Iteration 1280/3560 Training loss: 1.3664 3.0677 sec/batch\n",
      "Epoch 8/20  Iteration 1281/3560 Training loss: 1.3660 2.9575 sec/batch\n",
      "Epoch 8/20  Iteration 1282/3560 Training loss: 1.3655 2.9669 sec/batch\n",
      "Epoch 8/20  Iteration 1283/3560 Training loss: 1.3646 3.0323 sec/batch\n",
      "Epoch 8/20  Iteration 1284/3560 Training loss: 1.3630 2.9570 sec/batch\n",
      "Epoch 8/20  Iteration 1285/3560 Training loss: 1.3616 2.9972 sec/batch\n",
      "Epoch 8/20  Iteration 1286/3560 Training loss: 1.3612 3.0172 sec/batch\n",
      "Epoch 8/20  Iteration 1287/3560 Training loss: 1.3604 3.8129 sec/batch\n",
      "Epoch 8/20  Iteration 1288/3560 Training loss: 1.3610 2.9927 sec/batch\n",
      "Epoch 8/20  Iteration 1289/3560 Training loss: 1.3604 3.0018 sec/batch\n",
      "Epoch 8/20  Iteration 1290/3560 Training loss: 1.3595 3.0451 sec/batch\n",
      "Epoch 8/20  Iteration 1291/3560 Training loss: 1.3594 2.9925 sec/batch\n",
      "Epoch 8/20  Iteration 1292/3560 Training loss: 1.3583 2.9615 sec/batch\n",
      "Epoch 8/20  Iteration 1293/3560 Training loss: 1.3581 2.9317 sec/batch\n",
      "Epoch 8/20  Iteration 1294/3560 Training loss: 1.3575 3.0340 sec/batch\n",
      "Epoch 8/20  Iteration 1295/3560 Training loss: 1.3573 2.9537 sec/batch\n",
      "Epoch 8/20  Iteration 1296/3560 Training loss: 1.3573 3.0269 sec/batch\n",
      "Epoch 8/20  Iteration 1297/3560 Training loss: 1.3566 4.0874 sec/batch\n",
      "Epoch 8/20  Iteration 1298/3560 Training loss: 1.3573 2.9672 sec/batch\n",
      "Epoch 8/20  Iteration 1299/3560 Training loss: 1.3570 3.0793 sec/batch\n",
      "Epoch 8/20  Iteration 1300/3560 Training loss: 1.3570 3.0942 sec/batch\n",
      "Epoch 8/20  Iteration 1301/3560 Training loss: 1.3569 2.9470 sec/batch\n",
      "Epoch 8/20  Iteration 1302/3560 Training loss: 1.3570 2.9924 sec/batch\n",
      "Epoch 8/20  Iteration 1303/3560 Training loss: 1.3573 3.0479 sec/batch\n",
      "Epoch 8/20  Iteration 1304/3560 Training loss: 1.3568 3.0671 sec/batch\n",
      "Epoch 8/20  Iteration 1305/3560 Training loss: 1.3562 2.9594 sec/batch\n",
      "Epoch 8/20  Iteration 1306/3560 Training loss: 1.3567 4.0036 sec/batch\n",
      "Epoch 8/20  Iteration 1307/3560 Training loss: 1.3566 3.0135 sec/batch\n",
      "Epoch 8/20  Iteration 1308/3560 Training loss: 1.3573 2.9753 sec/batch\n",
      "Epoch 8/20  Iteration 1309/3560 Training loss: 1.3575 3.0898 sec/batch\n",
      "Epoch 8/20  Iteration 1310/3560 Training loss: 1.3574 2.9851 sec/batch\n",
      "Epoch 8/20  Iteration 1311/3560 Training loss: 1.3571 2.9982 sec/batch\n",
      "Epoch 8/20  Iteration 1312/3560 Training loss: 1.3574 3.0243 sec/batch\n",
      "Epoch 8/20  Iteration 1313/3560 Training loss: 1.3575 3.1002 sec/batch\n",
      "Epoch 8/20  Iteration 1314/3560 Training loss: 1.3571 2.9905 sec/batch\n",
      "Epoch 8/20  Iteration 1315/3560 Training loss: 1.3570 3.1358 sec/batch\n",
      "Epoch 8/20  Iteration 1316/3560 Training loss: 1.3569 3.7357 sec/batch\n",
      "Epoch 8/20  Iteration 1317/3560 Training loss: 1.3573 3.0342 sec/batch\n",
      "Epoch 8/20  Iteration 1318/3560 Training loss: 1.3575 2.9481 sec/batch\n",
      "Epoch 8/20  Iteration 1319/3560 Training loss: 1.3579 3.1148 sec/batch\n",
      "Epoch 8/20  Iteration 1320/3560 Training loss: 1.3574 3.1021 sec/batch\n",
      "Epoch 8/20  Iteration 1321/3560 Training loss: 1.3572 2.9297 sec/batch\n",
      "Epoch 8/20  Iteration 1322/3560 Training loss: 1.3572 3.0545 sec/batch\n",
      "Epoch 8/20  Iteration 1323/3560 Training loss: 1.3570 2.9935 sec/batch\n",
      "Epoch 8/20  Iteration 1324/3560 Training loss: 1.3568 2.9607 sec/batch\n",
      "Epoch 8/20  Iteration 1325/3560 Training loss: 1.3561 3.6464 sec/batch\n",
      "Epoch 8/20  Iteration 1326/3560 Training loss: 1.3559 3.6609 sec/batch\n",
      "Epoch 8/20  Iteration 1327/3560 Training loss: 1.3552 2.9701 sec/batch\n",
      "Epoch 8/20  Iteration 1328/3560 Training loss: 1.3552 2.9792 sec/batch\n",
      "Epoch 8/20  Iteration 1329/3560 Training loss: 1.3546 3.0250 sec/batch\n",
      "Epoch 8/20  Iteration 1330/3560 Training loss: 1.3545 3.0510 sec/batch\n",
      "Epoch 8/20  Iteration 1331/3560 Training loss: 1.3541 3.0150 sec/batch\n",
      "Epoch 8/20  Iteration 1332/3560 Training loss: 1.3539 2.9426 sec/batch\n",
      "Epoch 8/20  Iteration 1333/3560 Training loss: 1.3534 2.9397 sec/batch\n",
      "Epoch 8/20  Iteration 1334/3560 Training loss: 1.3531 3.0482 sec/batch\n",
      "Epoch 8/20  Iteration 1335/3560 Training loss: 1.3526 3.9637 sec/batch\n",
      "Epoch 8/20  Iteration 1336/3560 Training loss: 1.3526 2.9501 sec/batch\n",
      "Epoch 8/20  Iteration 1337/3560 Training loss: 1.3523 3.0576 sec/batch\n",
      "Epoch 8/20  Iteration 1338/3560 Training loss: 1.3520 2.9602 sec/batch\n",
      "Epoch 8/20  Iteration 1339/3560 Training loss: 1.3516 2.9572 sec/batch\n",
      "Epoch 8/20  Iteration 1340/3560 Training loss: 1.3512 2.9919 sec/batch\n",
      "Epoch 8/20  Iteration 1341/3560 Training loss: 1.3508 2.9474 sec/batch\n",
      "Epoch 8/20  Iteration 1342/3560 Training loss: 1.3508 3.0372 sec/batch\n",
      "Epoch 8/20  Iteration 1343/3560 Training loss: 1.3506 2.9715 sec/batch\n",
      "Epoch 8/20  Iteration 1344/3560 Training loss: 1.3502 2.9351 sec/batch\n",
      "Epoch 8/20  Iteration 1345/3560 Training loss: 1.3497 4.0325 sec/batch\n",
      "Epoch 8/20  Iteration 1346/3560 Training loss: 1.3492 2.9514 sec/batch\n",
      "Epoch 8/20  Iteration 1347/3560 Training loss: 1.3491 2.9980 sec/batch\n",
      "Epoch 8/20  Iteration 1348/3560 Training loss: 1.3488 3.0804 sec/batch\n",
      "Epoch 8/20  Iteration 1349/3560 Training loss: 1.3485 2.9433 sec/batch\n",
      "Epoch 8/20  Iteration 1350/3560 Training loss: 1.3484 2.9899 sec/batch\n",
      "Epoch 8/20  Iteration 1351/3560 Training loss: 1.3480 2.9849 sec/batch\n",
      "Epoch 8/20  Iteration 1352/3560 Training loss: 1.3479 3.1289 sec/batch\n",
      "Epoch 8/20  Iteration 1353/3560 Training loss: 1.3477 2.9329 sec/batch\n",
      "Epoch 8/20  Iteration 1354/3560 Training loss: 1.3476 3.5683 sec/batch\n",
      "Epoch 8/20  Iteration 1355/3560 Training loss: 1.3473 3.4979 sec/batch\n",
      "Epoch 8/20  Iteration 1356/3560 Training loss: 1.3472 3.0167 sec/batch\n",
      "Epoch 8/20  Iteration 1357/3560 Training loss: 1.3470 2.9650 sec/batch\n",
      "Epoch 8/20  Iteration 1358/3560 Training loss: 1.3469 3.0930 sec/batch\n",
      "Epoch 8/20  Iteration 1359/3560 Training loss: 1.3467 3.0283 sec/batch\n",
      "Epoch 8/20  Iteration 1360/3560 Training loss: 1.3464 2.9254 sec/batch\n",
      "Epoch 8/20  Iteration 1361/3560 Training loss: 1.3461 2.9567 sec/batch\n",
      "Epoch 8/20  Iteration 1362/3560 Training loss: 1.3456 3.0976 sec/batch\n",
      "Epoch 8/20  Iteration 1363/3560 Training loss: 1.3456 2.9536 sec/batch\n",
      "Epoch 8/20  Iteration 1364/3560 Training loss: 1.3455 3.7986 sec/batch\n",
      "Epoch 8/20  Iteration 1365/3560 Training loss: 1.3452 3.0084 sec/batch\n",
      "Epoch 8/20  Iteration 1366/3560 Training loss: 1.3451 2.9719 sec/batch\n",
      "Epoch 8/20  Iteration 1367/3560 Training loss: 1.3449 3.0283 sec/batch\n",
      "Epoch 8/20  Iteration 1368/3560 Training loss: 1.3445 2.9493 sec/batch\n",
      "Epoch 8/20  Iteration 1369/3560 Training loss: 1.3439 3.0302 sec/batch\n",
      "Epoch 8/20  Iteration 1370/3560 Training loss: 1.3438 2.9430 sec/batch\n",
      "Epoch 8/20  Iteration 1371/3560 Training loss: 1.3436 2.9375 sec/batch\n",
      "Epoch 8/20  Iteration 1372/3560 Training loss: 1.3431 3.0254 sec/batch\n",
      "Epoch 8/20  Iteration 1373/3560 Training loss: 1.3431 2.9813 sec/batch\n",
      "Epoch 8/20  Iteration 1374/3560 Training loss: 1.3431 4.2402 sec/batch\n",
      "Epoch 8/20  Iteration 1375/3560 Training loss: 1.3428 3.0874 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1376/3560 Training loss: 1.3424 3.0206 sec/batch\n",
      "Epoch 8/20  Iteration 1377/3560 Training loss: 1.3419 2.9974 sec/batch\n",
      "Epoch 8/20  Iteration 1378/3560 Training loss: 1.3417 3.0430 sec/batch\n",
      "Epoch 8/20  Iteration 1379/3560 Training loss: 1.3417 2.9672 sec/batch\n",
      "Epoch 8/20  Iteration 1380/3560 Training loss: 1.3416 2.9351 sec/batch\n",
      "Epoch 8/20  Iteration 1381/3560 Training loss: 1.3415 3.0133 sec/batch\n",
      "Epoch 8/20  Iteration 1382/3560 Training loss: 1.3414 3.0116 sec/batch\n",
      "Epoch 8/20  Iteration 1383/3560 Training loss: 1.3415 3.3040 sec/batch\n",
      "Epoch 8/20  Iteration 1384/3560 Training loss: 1.3415 3.6940 sec/batch\n",
      "Epoch 8/20  Iteration 1385/3560 Training loss: 1.3414 2.9763 sec/batch\n",
      "Epoch 8/20  Iteration 1386/3560 Training loss: 1.3413 3.1207 sec/batch\n",
      "Epoch 8/20  Iteration 1387/3560 Training loss: 1.3416 2.9502 sec/batch\n",
      "Epoch 8/20  Iteration 1388/3560 Training loss: 1.3415 3.0473 sec/batch\n",
      "Epoch 8/20  Iteration 1389/3560 Training loss: 1.3413 2.8927 sec/batch\n",
      "Epoch 8/20  Iteration 1390/3560 Training loss: 1.3414 3.0202 sec/batch\n",
      "Epoch 8/20  Iteration 1391/3560 Training loss: 1.3411 3.0798 sec/batch\n",
      "Epoch 8/20  Iteration 1392/3560 Training loss: 1.3412 2.9703 sec/batch\n",
      "Epoch 8/20  Iteration 1393/3560 Training loss: 1.3412 4.0589 sec/batch\n",
      "Epoch 8/20  Iteration 1394/3560 Training loss: 1.3413 3.0411 sec/batch\n",
      "Epoch 8/20  Iteration 1395/3560 Training loss: 1.3413 3.0036 sec/batch\n",
      "Epoch 8/20  Iteration 1396/3560 Training loss: 1.3412 2.9630 sec/batch\n",
      "Epoch 8/20  Iteration 1397/3560 Training loss: 1.3408 2.9820 sec/batch\n",
      "Epoch 8/20  Iteration 1398/3560 Training loss: 1.3405 3.0416 sec/batch\n",
      "Epoch 8/20  Iteration 1399/3560 Training loss: 1.3405 2.9528 sec/batch\n",
      "Epoch 8/20  Iteration 1400/3560 Training loss: 1.3404 3.0061 sec/batch\n",
      "Epoch 8/20  Iteration 1401/3560 Training loss: 1.3403 3.0823 sec/batch\n",
      "Epoch 8/20  Iteration 1402/3560 Training loss: 1.3401 3.2899 sec/batch\n",
      "Epoch 8/20  Iteration 1403/3560 Training loss: 1.3401 3.5687 sec/batch\n",
      "Epoch 8/20  Iteration 1404/3560 Training loss: 1.3399 3.1203 sec/batch\n",
      "Epoch 8/20  Iteration 1405/3560 Training loss: 1.3396 2.9281 sec/batch\n",
      "Epoch 8/20  Iteration 1406/3560 Training loss: 1.3397 2.9786 sec/batch\n",
      "Epoch 8/20  Iteration 1407/3560 Training loss: 1.3398 2.9768 sec/batch\n",
      "Epoch 8/20  Iteration 1408/3560 Training loss: 1.3398 3.0437 sec/batch\n",
      "Epoch 8/20  Iteration 1409/3560 Training loss: 1.3397 2.9673 sec/batch\n",
      "Epoch 8/20  Iteration 1410/3560 Training loss: 1.3395 2.9810 sec/batch\n",
      "Epoch 8/20  Iteration 1411/3560 Training loss: 1.3395 3.1164 sec/batch\n",
      "Epoch 8/20  Iteration 1412/3560 Training loss: 1.3393 3.7481 sec/batch\n",
      "Epoch 8/20  Iteration 1413/3560 Training loss: 1.3394 3.2220 sec/batch\n",
      "Epoch 8/20  Iteration 1414/3560 Training loss: 1.3398 3.0585 sec/batch\n",
      "Epoch 8/20  Iteration 1415/3560 Training loss: 1.3397 2.9769 sec/batch\n",
      "Epoch 8/20  Iteration 1416/3560 Training loss: 1.3397 2.9525 sec/batch\n",
      "Epoch 8/20  Iteration 1417/3560 Training loss: 1.3396 2.9971 sec/batch\n",
      "Epoch 8/20  Iteration 1418/3560 Training loss: 1.3394 2.9951 sec/batch\n",
      "Epoch 8/20  Iteration 1419/3560 Training loss: 1.3395 2.9581 sec/batch\n",
      "Epoch 8/20  Iteration 1420/3560 Training loss: 1.3394 3.0774 sec/batch\n",
      "Epoch 8/20  Iteration 1421/3560 Training loss: 1.3394 3.0553 sec/batch\n",
      "Epoch 8/20  Iteration 1422/3560 Training loss: 1.3392 3.8880 sec/batch\n",
      "Epoch 8/20  Iteration 1423/3560 Training loss: 1.3389 3.1204 sec/batch\n",
      "Epoch 8/20  Iteration 1424/3560 Training loss: 1.3390 3.0811 sec/batch\n",
      "Epoch 9/20  Iteration 1425/3560 Training loss: 1.4711 3.0015 sec/batch\n",
      "Epoch 9/20  Iteration 1426/3560 Training loss: 1.4046 2.9872 sec/batch\n",
      "Epoch 9/20  Iteration 1427/3560 Training loss: 1.3828 3.1253 sec/batch\n",
      "Epoch 9/20  Iteration 1428/3560 Training loss: 1.3765 2.9241 sec/batch\n",
      "Epoch 9/20  Iteration 1429/3560 Training loss: 1.3655 2.9649 sec/batch\n",
      "Epoch 9/20  Iteration 1430/3560 Training loss: 1.3557 3.0277 sec/batch\n",
      "Epoch 9/20  Iteration 1431/3560 Training loss: 1.3533 3.2825 sec/batch\n",
      "Epoch 9/20  Iteration 1432/3560 Training loss: 1.3494 3.6246 sec/batch\n",
      "Epoch 9/20  Iteration 1433/3560 Training loss: 1.3481 3.0841 sec/batch\n",
      "Epoch 9/20  Iteration 1434/3560 Training loss: 1.3453 3.0092 sec/batch\n",
      "Epoch 9/20  Iteration 1435/3560 Training loss: 1.3414 2.9351 sec/batch\n",
      "Epoch 9/20  Iteration 1436/3560 Training loss: 1.3401 2.9858 sec/batch\n",
      "Epoch 9/20  Iteration 1437/3560 Training loss: 1.3394 2.9917 sec/batch\n",
      "Epoch 9/20  Iteration 1438/3560 Training loss: 1.3399 2.9605 sec/batch\n",
      "Epoch 9/20  Iteration 1439/3560 Training loss: 1.3380 2.9660 sec/batch\n",
      "Epoch 9/20  Iteration 1440/3560 Training loss: 1.3356 3.1218 sec/batch\n",
      "Epoch 9/20  Iteration 1441/3560 Training loss: 1.3358 3.9908 sec/batch\n",
      "Epoch 9/20  Iteration 1442/3560 Training loss: 1.3359 3.0503 sec/batch\n",
      "Epoch 9/20  Iteration 1443/3560 Training loss: 1.3350 3.1117 sec/batch\n",
      "Epoch 9/20  Iteration 1444/3560 Training loss: 1.3363 2.9789 sec/batch\n",
      "Epoch 9/20  Iteration 1445/3560 Training loss: 1.3359 2.9621 sec/batch\n",
      "Epoch 9/20  Iteration 1446/3560 Training loss: 1.3357 2.9695 sec/batch\n",
      "Epoch 9/20  Iteration 1447/3560 Training loss: 1.3344 3.0406 sec/batch\n",
      "Epoch 9/20  Iteration 1448/3560 Training loss: 1.3338 2.9935 sec/batch\n",
      "Epoch 9/20  Iteration 1449/3560 Training loss: 1.3334 2.9782 sec/batch\n",
      "Epoch 9/20  Iteration 1450/3560 Training loss: 1.3315 3.0408 sec/batch\n",
      "Epoch 9/20  Iteration 1451/3560 Training loss: 1.3301 4.0837 sec/batch\n",
      "Epoch 9/20  Iteration 1452/3560 Training loss: 1.3302 2.9834 sec/batch\n",
      "Epoch 9/20  Iteration 1453/3560 Training loss: 1.3298 3.0449 sec/batch\n",
      "Epoch 9/20  Iteration 1454/3560 Training loss: 1.3297 3.0571 sec/batch\n",
      "Epoch 9/20  Iteration 1455/3560 Training loss: 1.3286 2.9701 sec/batch\n",
      "Epoch 9/20  Iteration 1456/3560 Training loss: 1.3273 2.9954 sec/batch\n",
      "Epoch 9/20  Iteration 1457/3560 Training loss: 1.3271 2.9819 sec/batch\n",
      "Epoch 9/20  Iteration 1458/3560 Training loss: 1.3272 3.0117 sec/batch\n",
      "Epoch 9/20  Iteration 1459/3560 Training loss: 1.3266 2.9997 sec/batch\n",
      "Epoch 9/20  Iteration 1460/3560 Training loss: 1.3262 3.0488 sec/batch\n",
      "Epoch 9/20  Iteration 1461/3560 Training loss: 1.3252 3.8915 sec/batch\n",
      "Epoch 9/20  Iteration 1462/3560 Training loss: 1.3237 3.0305 sec/batch\n",
      "Epoch 9/20  Iteration 1463/3560 Training loss: 1.3222 2.9815 sec/batch\n",
      "Epoch 9/20  Iteration 1464/3560 Training loss: 1.3217 2.9514 sec/batch\n",
      "Epoch 9/20  Iteration 1465/3560 Training loss: 1.3210 3.0367 sec/batch\n",
      "Epoch 9/20  Iteration 1466/3560 Training loss: 1.3216 2.9635 sec/batch\n",
      "Epoch 9/20  Iteration 1467/3560 Training loss: 1.3211 3.0490 sec/batch\n",
      "Epoch 9/20  Iteration 1468/3560 Training loss: 1.3205 3.0481 sec/batch\n",
      "Epoch 9/20  Iteration 1469/3560 Training loss: 1.3206 3.0209 sec/batch\n",
      "Epoch 9/20  Iteration 1470/3560 Training loss: 1.3198 3.9790 sec/batch\n",
      "Epoch 9/20  Iteration 1471/3560 Training loss: 1.3194 3.0344 sec/batch\n",
      "Epoch 9/20  Iteration 1472/3560 Training loss: 1.3192 3.0364 sec/batch\n",
      "Epoch 9/20  Iteration 1473/3560 Training loss: 1.3190 3.0629 sec/batch\n",
      "Epoch 9/20  Iteration 1474/3560 Training loss: 1.3190 3.2280 sec/batch\n",
      "Epoch 9/20  Iteration 1475/3560 Training loss: 1.3184 3.3823 sec/batch\n",
      "Epoch 9/20  Iteration 1476/3560 Training loss: 1.3189 3.4280 sec/batch\n",
      "Epoch 9/20  Iteration 1477/3560 Training loss: 1.3185 3.0959 sec/batch\n",
      "Epoch 9/20  Iteration 1478/3560 Training loss: 1.3186 3.1462 sec/batch\n",
      "Epoch 9/20  Iteration 1479/3560 Training loss: 1.3182 3.7471 sec/batch\n",
      "Epoch 9/20  Iteration 1480/3560 Training loss: 1.3182 3.4093 sec/batch\n",
      "Epoch 9/20  Iteration 1481/3560 Training loss: 1.3184 3.1046 sec/batch\n",
      "Epoch 9/20  Iteration 1482/3560 Training loss: 1.3180 3.0559 sec/batch\n",
      "Epoch 9/20  Iteration 1483/3560 Training loss: 1.3175 3.0409 sec/batch\n",
      "Epoch 9/20  Iteration 1484/3560 Training loss: 1.3179 3.0201 sec/batch\n",
      "Epoch 9/20  Iteration 1485/3560 Training loss: 1.3179 3.0827 sec/batch\n",
      "Epoch 9/20  Iteration 1486/3560 Training loss: 1.3188 3.0766 sec/batch\n",
      "Epoch 9/20  Iteration 1487/3560 Training loss: 1.3190 3.0640 sec/batch\n",
      "Epoch 9/20  Iteration 1488/3560 Training loss: 1.3190 3.0688 sec/batch\n",
      "Epoch 9/20  Iteration 1489/3560 Training loss: 1.3189 4.1163 sec/batch\n",
      "Epoch 9/20  Iteration 1490/3560 Training loss: 1.3190 3.0753 sec/batch\n",
      "Epoch 9/20  Iteration 1491/3560 Training loss: 1.3191 3.0126 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1492/3560 Training loss: 1.3188 3.1413 sec/batch\n",
      "Epoch 9/20  Iteration 1493/3560 Training loss: 1.3188 3.0398 sec/batch\n",
      "Epoch 9/20  Iteration 1494/3560 Training loss: 1.3187 3.0708 sec/batch\n",
      "Epoch 9/20  Iteration 1495/3560 Training loss: 1.3191 3.1287 sec/batch\n",
      "Epoch 9/20  Iteration 1496/3560 Training loss: 1.3192 3.1141 sec/batch\n",
      "Epoch 9/20  Iteration 1497/3560 Training loss: 1.3196 3.0751 sec/batch\n",
      "Epoch 9/20  Iteration 1498/3560 Training loss: 1.3191 3.6097 sec/batch\n",
      "Epoch 9/20  Iteration 1499/3560 Training loss: 1.3189 3.7619 sec/batch\n",
      "Epoch 9/20  Iteration 1500/3560 Training loss: 1.3189 3.0417 sec/batch\n",
      "Epoch 9/20  Iteration 1501/3560 Training loss: 1.3187 3.2348 sec/batch\n",
      "Epoch 9/20  Iteration 1502/3560 Training loss: 1.3185 3.0446 sec/batch\n",
      "Epoch 9/20  Iteration 1503/3560 Training loss: 1.3178 3.0096 sec/batch\n",
      "Epoch 9/20  Iteration 1504/3560 Training loss: 1.3176 2.9870 sec/batch\n",
      "Epoch 9/20  Iteration 1505/3560 Training loss: 1.3170 3.1565 sec/batch\n",
      "Epoch 9/20  Iteration 1506/3560 Training loss: 1.3167 3.0504 sec/batch\n",
      "Epoch 9/20  Iteration 1507/3560 Training loss: 1.3161 3.2675 sec/batch\n",
      "Epoch 9/20  Iteration 1508/3560 Training loss: 1.3160 3.6539 sec/batch\n",
      "Epoch 9/20  Iteration 1509/3560 Training loss: 1.3157 3.0537 sec/batch\n",
      "Epoch 9/20  Iteration 1510/3560 Training loss: 1.3155 3.0508 sec/batch\n",
      "Epoch 9/20  Iteration 1511/3560 Training loss: 1.3152 3.0386 sec/batch\n",
      "Epoch 9/20  Iteration 1512/3560 Training loss: 1.3148 3.0230 sec/batch\n",
      "Epoch 9/20  Iteration 1513/3560 Training loss: 1.3144 3.0780 sec/batch\n",
      "Epoch 9/20  Iteration 1514/3560 Training loss: 1.3145 3.1512 sec/batch\n",
      "Epoch 9/20  Iteration 1515/3560 Training loss: 1.3142 2.9896 sec/batch\n",
      "Epoch 9/20  Iteration 1516/3560 Training loss: 1.3141 3.0325 sec/batch\n",
      "Epoch 9/20  Iteration 1517/3560 Training loss: 1.3136 3.9777 sec/batch\n",
      "Epoch 9/20  Iteration 1518/3560 Training loss: 1.3132 3.2146 sec/batch\n",
      "Epoch 9/20  Iteration 1519/3560 Training loss: 1.3128 3.0311 sec/batch\n",
      "Epoch 9/20  Iteration 1520/3560 Training loss: 1.3127 3.0882 sec/batch\n",
      "Epoch 9/20  Iteration 1521/3560 Training loss: 1.3125 3.0259 sec/batch\n",
      "Epoch 9/20  Iteration 1522/3560 Training loss: 1.3121 3.0768 sec/batch\n",
      "Epoch 9/20  Iteration 1523/3560 Training loss: 1.3117 3.0079 sec/batch\n",
      "Epoch 9/20  Iteration 1524/3560 Training loss: 1.3113 3.1612 sec/batch\n",
      "Epoch 9/20  Iteration 1525/3560 Training loss: 1.3112 3.0991 sec/batch\n",
      "Epoch 9/20  Iteration 1526/3560 Training loss: 1.3111 3.2428 sec/batch\n",
      "Epoch 9/20  Iteration 1527/3560 Training loss: 1.3108 3.8624 sec/batch\n",
      "Epoch 9/20  Iteration 1528/3560 Training loss: 1.3106 3.0132 sec/batch\n",
      "Epoch 9/20  Iteration 1529/3560 Training loss: 1.3103 3.0124 sec/batch\n",
      "Epoch 9/20  Iteration 1530/3560 Training loss: 1.3101 3.1123 sec/batch\n",
      "Epoch 9/20  Iteration 1531/3560 Training loss: 1.3100 3.0240 sec/batch\n",
      "Epoch 9/20  Iteration 1532/3560 Training loss: 1.3100 3.1231 sec/batch\n",
      "Epoch 9/20  Iteration 1533/3560 Training loss: 1.3097 3.3287 sec/batch\n",
      "Epoch 9/20  Iteration 1534/3560 Training loss: 1.3097 3.3234 sec/batch\n",
      "Epoch 9/20  Iteration 1535/3560 Training loss: 1.3094 3.2262 sec/batch\n",
      "Epoch 9/20  Iteration 1536/3560 Training loss: 1.3093 4.2594 sec/batch\n",
      "Epoch 9/20  Iteration 1537/3560 Training loss: 1.3092 3.0736 sec/batch\n",
      "Epoch 9/20  Iteration 1538/3560 Training loss: 1.3090 3.0526 sec/batch\n",
      "Epoch 9/20  Iteration 1539/3560 Training loss: 1.3086 3.1297 sec/batch\n",
      "Epoch 9/20  Iteration 1540/3560 Training loss: 1.3082 3.0794 sec/batch\n",
      "Epoch 9/20  Iteration 1541/3560 Training loss: 1.3082 3.0640 sec/batch\n",
      "Epoch 9/20  Iteration 1542/3560 Training loss: 1.3081 3.0813 sec/batch\n",
      "Epoch 9/20  Iteration 1543/3560 Training loss: 1.3079 3.1041 sec/batch\n",
      "Epoch 9/20  Iteration 1544/3560 Training loss: 1.3078 3.0192 sec/batch\n",
      "Epoch 9/20  Iteration 1545/3560 Training loss: 1.3075 3.8880 sec/batch\n",
      "Epoch 9/20  Iteration 1546/3560 Training loss: 1.3072 3.2518 sec/batch\n",
      "Epoch 9/20  Iteration 1547/3560 Training loss: 1.3068 3.0652 sec/batch\n",
      "Epoch 9/20  Iteration 1548/3560 Training loss: 1.3067 3.0490 sec/batch\n",
      "Epoch 9/20  Iteration 1549/3560 Training loss: 1.3065 3.0538 sec/batch\n",
      "Epoch 9/20  Iteration 1550/3560 Training loss: 1.3060 3.0688 sec/batch\n",
      "Epoch 9/20  Iteration 1551/3560 Training loss: 1.3060 3.0063 sec/batch\n",
      "Epoch 9/20  Iteration 1552/3560 Training loss: 1.3060 3.1949 sec/batch\n",
      "Epoch 9/20  Iteration 1553/3560 Training loss: 1.3057 3.0843 sec/batch\n",
      "Epoch 9/20  Iteration 1554/3560 Training loss: 1.3053 3.1715 sec/batch\n",
      "Epoch 9/20  Iteration 1555/3560 Training loss: 1.3048 4.0083 sec/batch\n",
      "Epoch 9/20  Iteration 1556/3560 Training loss: 1.3046 3.1418 sec/batch\n",
      "Epoch 9/20  Iteration 1557/3560 Training loss: 1.3045 3.0073 sec/batch\n",
      "Epoch 9/20  Iteration 1558/3560 Training loss: 1.3045 3.0853 sec/batch\n",
      "Epoch 9/20  Iteration 1559/3560 Training loss: 1.3044 3.0685 sec/batch\n",
      "Epoch 9/20  Iteration 1560/3560 Training loss: 1.3043 2.9181 sec/batch\n",
      "Epoch 9/20  Iteration 1561/3560 Training loss: 1.3044 3.0978 sec/batch\n",
      "Epoch 9/20  Iteration 1562/3560 Training loss: 1.3045 3.0723 sec/batch\n",
      "Epoch 9/20  Iteration 1563/3560 Training loss: 1.3045 2.9765 sec/batch\n",
      "Epoch 9/20  Iteration 1564/3560 Training loss: 1.3045 3.9266 sec/batch\n",
      "Epoch 9/20  Iteration 1565/3560 Training loss: 1.3048 3.3418 sec/batch\n",
      "Epoch 9/20  Iteration 1566/3560 Training loss: 1.3047 2.9922 sec/batch\n",
      "Epoch 9/20  Iteration 1567/3560 Training loss: 1.3046 3.0725 sec/batch\n",
      "Epoch 9/20  Iteration 1568/3560 Training loss: 1.3048 3.0526 sec/batch\n",
      "Epoch 9/20  Iteration 1569/3560 Training loss: 1.3046 3.0868 sec/batch\n",
      "Epoch 9/20  Iteration 1570/3560 Training loss: 1.3047 3.0269 sec/batch\n",
      "Epoch 9/20  Iteration 1571/3560 Training loss: 1.3046 3.0780 sec/batch\n",
      "Epoch 9/20  Iteration 1572/3560 Training loss: 1.3048 3.1327 sec/batch\n",
      "Epoch 9/20  Iteration 1573/3560 Training loss: 1.3049 3.7036 sec/batch\n",
      "Epoch 9/20  Iteration 1574/3560 Training loss: 1.3048 3.8967 sec/batch\n",
      "Epoch 9/20  Iteration 1575/3560 Training loss: 1.3044 3.1000 sec/batch\n",
      "Epoch 9/20  Iteration 1576/3560 Training loss: 1.3043 3.0682 sec/batch\n",
      "Epoch 9/20  Iteration 1577/3560 Training loss: 1.3043 3.0019 sec/batch\n",
      "Epoch 9/20  Iteration 1578/3560 Training loss: 1.3042 3.1106 sec/batch\n",
      "Epoch 9/20  Iteration 1579/3560 Training loss: 1.3042 3.0043 sec/batch\n",
      "Epoch 9/20  Iteration 1580/3560 Training loss: 1.3040 3.0232 sec/batch\n",
      "Epoch 9/20  Iteration 1581/3560 Training loss: 1.3040 3.0845 sec/batch\n",
      "Epoch 9/20  Iteration 1582/3560 Training loss: 1.3038 3.0147 sec/batch\n",
      "Epoch 9/20  Iteration 1583/3560 Training loss: 1.3036 4.1135 sec/batch\n",
      "Epoch 9/20  Iteration 1584/3560 Training loss: 1.3036 3.1020 sec/batch\n",
      "Epoch 9/20  Iteration 1585/3560 Training loss: 1.3037 3.0591 sec/batch\n",
      "Epoch 9/20  Iteration 1586/3560 Training loss: 1.3037 3.0462 sec/batch\n",
      "Epoch 9/20  Iteration 1587/3560 Training loss: 1.3037 3.1416 sec/batch\n",
      "Epoch 9/20  Iteration 1588/3560 Training loss: 1.3036 3.0233 sec/batch\n",
      "Epoch 9/20  Iteration 1589/3560 Training loss: 1.3035 3.0458 sec/batch\n",
      "Epoch 9/20  Iteration 1590/3560 Training loss: 1.3034 2.9850 sec/batch\n",
      "Epoch 9/20  Iteration 1591/3560 Training loss: 1.3036 3.1799 sec/batch\n",
      "Epoch 9/20  Iteration 1592/3560 Training loss: 1.3040 3.7093 sec/batch\n",
      "Epoch 9/20  Iteration 1593/3560 Training loss: 1.3040 3.3918 sec/batch\n",
      "Epoch 9/20  Iteration 1594/3560 Training loss: 1.3040 3.0450 sec/batch\n",
      "Epoch 9/20  Iteration 1595/3560 Training loss: 1.3039 3.1105 sec/batch\n",
      "Epoch 9/20  Iteration 1596/3560 Training loss: 1.3037 3.0359 sec/batch\n",
      "Epoch 9/20  Iteration 1597/3560 Training loss: 1.3038 3.1085 sec/batch\n",
      "Epoch 9/20  Iteration 1598/3560 Training loss: 1.3038 3.0404 sec/batch\n",
      "Epoch 9/20  Iteration 1599/3560 Training loss: 1.3038 3.0270 sec/batch\n",
      "Epoch 9/20  Iteration 1600/3560 Training loss: 1.3036 3.1474 sec/batch\n",
      "Epoch 9/20  Iteration 1601/3560 Training loss: 1.3034 2.9863 sec/batch\n",
      "Epoch 9/20  Iteration 1602/3560 Training loss: 1.3035 3.9418 sec/batch\n",
      "Epoch 10/20  Iteration 1603/3560 Training loss: 1.4262 3.1291 sec/batch\n",
      "Epoch 10/20  Iteration 1604/3560 Training loss: 1.3695 3.0486 sec/batch\n",
      "Epoch 10/20  Iteration 1605/3560 Training loss: 1.3433 3.0841 sec/batch\n",
      "Epoch 10/20  Iteration 1606/3560 Training loss: 1.3320 3.1305 sec/batch\n",
      "Epoch 10/20  Iteration 1607/3560 Training loss: 1.3198 3.0775 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1608/3560 Training loss: 1.3070 3.0521 sec/batch\n",
      "Epoch 10/20  Iteration 1609/3560 Training loss: 1.3058 2.9907 sec/batch\n",
      "Epoch 10/20  Iteration 1610/3560 Training loss: 1.3030 3.1381 sec/batch\n",
      "Epoch 10/20  Iteration 1611/3560 Training loss: 1.3029 3.5062 sec/batch\n",
      "Epoch 10/20  Iteration 1612/3560 Training loss: 1.3015 3.8132 sec/batch\n",
      "Epoch 10/20  Iteration 1613/3560 Training loss: 1.2977 3.1735 sec/batch\n",
      "Epoch 10/20  Iteration 1614/3560 Training loss: 1.2964 3.0449 sec/batch\n",
      "Epoch 10/20  Iteration 1615/3560 Training loss: 1.2957 3.0510 sec/batch\n",
      "Epoch 10/20  Iteration 1616/3560 Training loss: 1.2962 3.0972 sec/batch\n",
      "Epoch 10/20  Iteration 1617/3560 Training loss: 1.2947 3.1227 sec/batch\n",
      "Epoch 10/20  Iteration 1618/3560 Training loss: 1.2918 3.0700 sec/batch\n",
      "Epoch 10/20  Iteration 1619/3560 Training loss: 1.2924 3.0932 sec/batch\n",
      "Epoch 10/20  Iteration 1620/3560 Training loss: 1.2937 3.2042 sec/batch\n",
      "Epoch 10/20  Iteration 1621/3560 Training loss: 1.2936 3.9071 sec/batch\n",
      "Epoch 10/20  Iteration 1622/3560 Training loss: 1.2944 3.0680 sec/batch\n",
      "Epoch 10/20  Iteration 1623/3560 Training loss: 1.2938 3.0410 sec/batch\n",
      "Epoch 10/20  Iteration 1624/3560 Training loss: 1.2941 2.9870 sec/batch\n",
      "Epoch 10/20  Iteration 1625/3560 Training loss: 1.2932 3.0705 sec/batch\n",
      "Epoch 10/20  Iteration 1626/3560 Training loss: 1.2928 2.9962 sec/batch\n",
      "Epoch 10/20  Iteration 1627/3560 Training loss: 1.2928 3.0262 sec/batch\n",
      "Epoch 10/20  Iteration 1628/3560 Training loss: 1.2913 3.0894 sec/batch\n",
      "Epoch 10/20  Iteration 1629/3560 Training loss: 1.2905 3.1183 sec/batch\n",
      "Epoch 10/20  Iteration 1630/3560 Training loss: 1.2910 4.0936 sec/batch\n",
      "Epoch 10/20  Iteration 1631/3560 Training loss: 1.2911 3.1127 sec/batch\n",
      "Epoch 10/20  Iteration 1632/3560 Training loss: 1.2915 3.1347 sec/batch\n",
      "Epoch 10/20  Iteration 1633/3560 Training loss: 1.2908 2.9958 sec/batch\n",
      "Epoch 10/20  Iteration 1634/3560 Training loss: 1.2895 3.0834 sec/batch\n",
      "Epoch 10/20  Iteration 1635/3560 Training loss: 1.2898 3.0895 sec/batch\n",
      "Epoch 10/20  Iteration 1636/3560 Training loss: 1.2901 3.0201 sec/batch\n",
      "Epoch 10/20  Iteration 1637/3560 Training loss: 1.2897 3.0407 sec/batch\n",
      "Epoch 10/20  Iteration 1638/3560 Training loss: 1.2895 3.1818 sec/batch\n",
      "Epoch 10/20  Iteration 1639/3560 Training loss: 1.2886 3.2061 sec/batch\n",
      "Epoch 10/20  Iteration 1640/3560 Training loss: 1.2876 3.7495 sec/batch\n",
      "Epoch 10/20  Iteration 1641/3560 Training loss: 1.2861 3.1037 sec/batch\n",
      "Epoch 10/20  Iteration 1642/3560 Training loss: 1.2858 3.0255 sec/batch\n",
      "Epoch 10/20  Iteration 1643/3560 Training loss: 1.2853 2.9891 sec/batch\n",
      "Epoch 10/20  Iteration 1644/3560 Training loss: 1.2863 3.0561 sec/batch\n",
      "Epoch 10/20  Iteration 1645/3560 Training loss: 1.2859 3.0569 sec/batch\n",
      "Epoch 10/20  Iteration 1646/3560 Training loss: 1.2853 3.0611 sec/batch\n",
      "Epoch 10/20  Iteration 1647/3560 Training loss: 1.2853 3.0230 sec/batch\n",
      "Epoch 10/20  Iteration 1648/3560 Training loss: 1.2846 3.1203 sec/batch\n",
      "Epoch 10/20  Iteration 1649/3560 Training loss: 1.2843 3.7563 sec/batch\n",
      "Epoch 10/20  Iteration 1650/3560 Training loss: 1.2840 3.2410 sec/batch\n",
      "Epoch 10/20  Iteration 1651/3560 Training loss: 1.2839 3.1048 sec/batch\n",
      "Epoch 10/20  Iteration 1652/3560 Training loss: 1.2839 3.0545 sec/batch\n",
      "Epoch 10/20  Iteration 1653/3560 Training loss: 1.2835 2.9851 sec/batch\n",
      "Epoch 10/20  Iteration 1654/3560 Training loss: 1.2842 3.0942 sec/batch\n",
      "Epoch 10/20  Iteration 1655/3560 Training loss: 1.2840 3.0774 sec/batch\n",
      "Epoch 10/20  Iteration 1656/3560 Training loss: 1.2842 2.9960 sec/batch\n",
      "Epoch 10/20  Iteration 1657/3560 Training loss: 1.2838 3.0256 sec/batch\n",
      "Epoch 10/20  Iteration 1658/3560 Training loss: 1.2839 3.2753 sec/batch\n",
      "Epoch 10/20  Iteration 1659/3560 Training loss: 1.2842 3.9335 sec/batch\n",
      "Epoch 10/20  Iteration 1660/3560 Training loss: 1.2839 3.0497 sec/batch\n",
      "Epoch 10/20  Iteration 1661/3560 Training loss: 1.2832 3.0901 sec/batch\n",
      "Epoch 10/20  Iteration 1662/3560 Training loss: 1.2836 3.1527 sec/batch\n",
      "Epoch 10/20  Iteration 1663/3560 Training loss: 1.2837 3.0119 sec/batch\n",
      "Epoch 10/20  Iteration 1664/3560 Training loss: 1.2845 3.0555 sec/batch\n",
      "Epoch 10/20  Iteration 1665/3560 Training loss: 1.2847 3.0490 sec/batch\n",
      "Epoch 10/20  Iteration 1666/3560 Training loss: 1.2847 3.0395 sec/batch\n",
      "Epoch 10/20  Iteration 1667/3560 Training loss: 1.2846 3.0861 sec/batch\n",
      "Epoch 10/20  Iteration 1668/3560 Training loss: 1.2848 4.0883 sec/batch\n",
      "Epoch 10/20  Iteration 1669/3560 Training loss: 1.2850 3.0618 sec/batch\n",
      "Epoch 10/20  Iteration 1670/3560 Training loss: 1.2848 3.1056 sec/batch\n",
      "Epoch 10/20  Iteration 1671/3560 Training loss: 1.2847 3.0477 sec/batch\n",
      "Epoch 10/20  Iteration 1672/3560 Training loss: 1.2846 3.0723 sec/batch\n",
      "Epoch 10/20  Iteration 1673/3560 Training loss: 1.2852 3.0702 sec/batch\n",
      "Epoch 10/20  Iteration 1674/3560 Training loss: 1.2854 2.9760 sec/batch\n",
      "Epoch 10/20  Iteration 1675/3560 Training loss: 1.2858 3.0727 sec/batch\n",
      "Epoch 10/20  Iteration 1676/3560 Training loss: 1.2854 3.0356 sec/batch\n",
      "Epoch 10/20  Iteration 1677/3560 Training loss: 1.2853 3.1662 sec/batch\n",
      "Epoch 10/20  Iteration 1678/3560 Training loss: 1.2855 3.9753 sec/batch\n",
      "Epoch 10/20  Iteration 1679/3560 Training loss: 1.2853 3.0519 sec/batch\n",
      "Epoch 10/20  Iteration 1680/3560 Training loss: 1.2851 3.0545 sec/batch\n",
      "Epoch 10/20  Iteration 1681/3560 Training loss: 1.2844 3.0419 sec/batch\n",
      "Epoch 10/20  Iteration 1682/3560 Training loss: 1.2843 3.0773 sec/batch\n",
      "Epoch 10/20  Iteration 1683/3560 Training loss: 1.2839 3.0342 sec/batch\n",
      "Epoch 10/20  Iteration 1684/3560 Training loss: 1.2838 2.9900 sec/batch\n",
      "Epoch 10/20  Iteration 1685/3560 Training loss: 1.2834 2.9506 sec/batch\n",
      "Epoch 10/20  Iteration 1686/3560 Training loss: 1.2833 3.0929 sec/batch\n",
      "Epoch 10/20  Iteration 1687/3560 Training loss: 1.2831 3.8192 sec/batch\n",
      "Epoch 10/20  Iteration 1688/3560 Training loss: 1.2828 3.2120 sec/batch\n",
      "Epoch 10/20  Iteration 1689/3560 Training loss: 1.2825 3.2059 sec/batch\n",
      "Epoch 10/20  Iteration 1690/3560 Training loss: 1.2822 3.1573 sec/batch\n",
      "Epoch 10/20  Iteration 1691/3560 Training loss: 1.2818 3.0368 sec/batch\n",
      "Epoch 10/20  Iteration 1692/3560 Training loss: 1.2819 3.0334 sec/batch\n",
      "Epoch 10/20  Iteration 1693/3560 Training loss: 1.2816 3.1139 sec/batch\n",
      "Epoch 10/20  Iteration 1694/3560 Training loss: 1.2815 3.0064 sec/batch\n",
      "Epoch 10/20  Iteration 1695/3560 Training loss: 1.2810 3.0336 sec/batch\n",
      "Epoch 10/20  Iteration 1696/3560 Training loss: 1.2806 3.2864 sec/batch\n",
      "Epoch 10/20  Iteration 1697/3560 Training loss: 1.2803 3.7672 sec/batch\n",
      "Epoch 10/20  Iteration 1698/3560 Training loss: 1.2803 3.0752 sec/batch\n",
      "Epoch 10/20  Iteration 1699/3560 Training loss: 1.2802 3.1367 sec/batch\n",
      "Epoch 10/20  Iteration 1700/3560 Training loss: 1.2798 3.0047 sec/batch\n",
      "Epoch 10/20  Iteration 1701/3560 Training loss: 1.2793 3.0484 sec/batch\n",
      "Epoch 10/20  Iteration 1702/3560 Training loss: 1.2789 3.0026 sec/batch\n",
      "Epoch 10/20  Iteration 1703/3560 Training loss: 1.2788 3.1037 sec/batch\n",
      "Epoch 10/20  Iteration 1704/3560 Training loss: 1.2786 3.0402 sec/batch\n",
      "Epoch 10/20  Iteration 1705/3560 Training loss: 1.2785 2.9817 sec/batch\n",
      "Epoch 10/20  Iteration 1706/3560 Training loss: 1.2783 3.9888 sec/batch\n",
      "Epoch 10/20  Iteration 1707/3560 Training loss: 1.2781 3.0911 sec/batch\n",
      "Epoch 10/20  Iteration 1708/3560 Training loss: 1.2779 3.0780 sec/batch\n",
      "Epoch 10/20  Iteration 1709/3560 Training loss: 1.2778 3.0899 sec/batch\n",
      "Epoch 10/20  Iteration 1710/3560 Training loss: 1.2777 3.0091 sec/batch\n",
      "Epoch 10/20  Iteration 1711/3560 Training loss: 1.2775 3.0632 sec/batch\n",
      "Epoch 10/20  Iteration 1712/3560 Training loss: 1.2775 3.0333 sec/batch\n",
      "Epoch 10/20  Iteration 1713/3560 Training loss: 1.2772 3.0191 sec/batch\n",
      "Epoch 10/20  Iteration 1714/3560 Training loss: 1.2771 3.0356 sec/batch\n",
      "Epoch 10/20  Iteration 1715/3560 Training loss: 1.2770 3.2010 sec/batch\n",
      "Epoch 10/20  Iteration 1716/3560 Training loss: 1.2769 3.8929 sec/batch\n",
      "Epoch 10/20  Iteration 1717/3560 Training loss: 1.2766 3.0720 sec/batch\n",
      "Epoch 10/20  Iteration 1718/3560 Training loss: 1.2763 3.0910 sec/batch\n",
      "Epoch 10/20  Iteration 1719/3560 Training loss: 1.2762 3.0793 sec/batch\n",
      "Epoch 10/20  Iteration 1720/3560 Training loss: 1.2763 3.0520 sec/batch\n",
      "Epoch 10/20  Iteration 1721/3560 Training loss: 1.2761 3.0677 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1722/3560 Training loss: 1.2760 3.0971 sec/batch\n",
      "Epoch 10/20  Iteration 1723/3560 Training loss: 1.2758 3.0326 sec/batch\n",
      "Epoch 10/20  Iteration 1724/3560 Training loss: 1.2755 3.1595 sec/batch\n",
      "Epoch 10/20  Iteration 1725/3560 Training loss: 1.2750 3.9378 sec/batch\n",
      "Epoch 10/20  Iteration 1726/3560 Training loss: 1.2749 3.3849 sec/batch\n",
      "Epoch 10/20  Iteration 1727/3560 Training loss: 1.2748 3.0374 sec/batch\n",
      "Epoch 10/20  Iteration 1728/3560 Training loss: 1.2744 3.1080 sec/batch\n",
      "Epoch 10/20  Iteration 1729/3560 Training loss: 1.2744 3.0761 sec/batch\n",
      "Epoch 10/20  Iteration 1730/3560 Training loss: 1.2743 2.9892 sec/batch\n",
      "Epoch 10/20  Iteration 1731/3560 Training loss: 1.2742 3.1932 sec/batch\n",
      "Epoch 10/20  Iteration 1732/3560 Training loss: 1.2738 3.0513 sec/batch\n",
      "Epoch 10/20  Iteration 1733/3560 Training loss: 1.2735 3.0471 sec/batch\n",
      "Epoch 10/20  Iteration 1734/3560 Training loss: 1.2732 3.3741 sec/batch\n",
      "Epoch 10/20  Iteration 1735/3560 Training loss: 1.2733 3.7090 sec/batch\n",
      "Epoch 10/20  Iteration 1736/3560 Training loss: 1.2732 3.0373 sec/batch\n",
      "Epoch 10/20  Iteration 1737/3560 Training loss: 1.2732 3.0204 sec/batch\n",
      "Epoch 10/20  Iteration 1738/3560 Training loss: 1.2732 3.0853 sec/batch\n",
      "Epoch 10/20  Iteration 1739/3560 Training loss: 1.2733 3.0679 sec/batch\n",
      "Epoch 10/20  Iteration 1740/3560 Training loss: 1.2735 3.0273 sec/batch\n",
      "Epoch 10/20  Iteration 1741/3560 Training loss: 1.2734 3.0798 sec/batch\n",
      "Epoch 10/20  Iteration 1742/3560 Training loss: 1.2734 3.0859 sec/batch\n",
      "Epoch 10/20  Iteration 1743/3560 Training loss: 1.2737 3.0393 sec/batch\n",
      "Epoch 10/20  Iteration 1744/3560 Training loss: 1.2737 4.1069 sec/batch\n",
      "Epoch 10/20  Iteration 1745/3560 Training loss: 1.2736 3.0931 sec/batch\n",
      "Epoch 10/20  Iteration 1746/3560 Training loss: 1.2738 2.9847 sec/batch\n",
      "Epoch 10/20  Iteration 1747/3560 Training loss: 1.2736 3.1959 sec/batch\n",
      "Epoch 10/20  Iteration 1748/3560 Training loss: 1.2737 2.9943 sec/batch\n",
      "Epoch 10/20  Iteration 1749/3560 Training loss: 1.2737 3.0372 sec/batch\n",
      "Epoch 10/20  Iteration 1750/3560 Training loss: 1.2740 3.0169 sec/batch\n",
      "Epoch 10/20  Iteration 1751/3560 Training loss: 1.2741 3.0705 sec/batch\n",
      "Epoch 10/20  Iteration 1752/3560 Training loss: 1.2740 3.0355 sec/batch\n",
      "Epoch 10/20  Iteration 1753/3560 Training loss: 1.2737 3.9378 sec/batch\n",
      "Epoch 10/20  Iteration 1754/3560 Training loss: 1.2736 3.5353 sec/batch\n",
      "Epoch 10/20  Iteration 1755/3560 Training loss: 1.2736 2.9840 sec/batch\n",
      "Epoch 10/20  Iteration 1756/3560 Training loss: 1.2735 3.0514 sec/batch\n",
      "Epoch 10/20  Iteration 1757/3560 Training loss: 1.2734 3.0418 sec/batch\n",
      "Epoch 10/20  Iteration 1758/3560 Training loss: 1.2733 2.8946 sec/batch\n",
      "Epoch 10/20  Iteration 1759/3560 Training loss: 1.2733 3.0011 sec/batch\n",
      "Epoch 10/20  Iteration 1760/3560 Training loss: 1.2731 3.0929 sec/batch\n",
      "Epoch 10/20  Iteration 1761/3560 Training loss: 1.2729 3.0157 sec/batch\n",
      "Epoch 10/20  Iteration 1762/3560 Training loss: 1.2730 3.2270 sec/batch\n",
      "Epoch 10/20  Iteration 1763/3560 Training loss: 1.2732 3.9870 sec/batch\n",
      "Epoch 10/20  Iteration 1764/3560 Training loss: 1.2732 3.1122 sec/batch\n",
      "Epoch 10/20  Iteration 1765/3560 Training loss: 1.2731 2.9938 sec/batch\n",
      "Epoch 10/20  Iteration 1766/3560 Training loss: 1.2730 3.1257 sec/batch\n",
      "Epoch 10/20  Iteration 1767/3560 Training loss: 1.2730 3.1749 sec/batch\n",
      "Epoch 10/20  Iteration 1768/3560 Training loss: 1.2729 2.9704 sec/batch\n",
      "Epoch 10/20  Iteration 1769/3560 Training loss: 1.2731 3.1054 sec/batch\n",
      "Epoch 10/20  Iteration 1770/3560 Training loss: 1.2734 3.0844 sec/batch\n",
      "Epoch 10/20  Iteration 1771/3560 Training loss: 1.2734 3.0730 sec/batch\n",
      "Epoch 10/20  Iteration 1772/3560 Training loss: 1.2734 3.6628 sec/batch\n",
      "Epoch 10/20  Iteration 1773/3560 Training loss: 1.2733 3.4481 sec/batch\n",
      "Epoch 10/20  Iteration 1774/3560 Training loss: 1.2732 3.0604 sec/batch\n",
      "Epoch 10/20  Iteration 1775/3560 Training loss: 1.2734 3.0680 sec/batch\n",
      "Epoch 10/20  Iteration 1776/3560 Training loss: 1.2734 3.1684 sec/batch\n",
      "Epoch 10/20  Iteration 1777/3560 Training loss: 1.2734 3.1318 sec/batch\n",
      "Epoch 10/20  Iteration 1778/3560 Training loss: 1.2733 2.9486 sec/batch\n",
      "Epoch 10/20  Iteration 1779/3560 Training loss: 1.2731 3.0434 sec/batch\n",
      "Epoch 10/20  Iteration 1780/3560 Training loss: 1.2733 3.0760 sec/batch\n",
      "Epoch 11/20  Iteration 1781/3560 Training loss: 1.4056 3.1061 sec/batch\n",
      "Epoch 11/20  Iteration 1782/3560 Training loss: 1.3433 4.0939 sec/batch\n",
      "Epoch 11/20  Iteration 1783/3560 Training loss: 1.3191 3.1325 sec/batch\n",
      "Epoch 11/20  Iteration 1784/3560 Training loss: 1.3115 3.1114 sec/batch\n",
      "Epoch 11/20  Iteration 1785/3560 Training loss: 1.2985 3.0393 sec/batch\n",
      "Epoch 11/20  Iteration 1786/3560 Training loss: 1.2861 3.1659 sec/batch\n",
      "Epoch 11/20  Iteration 1787/3560 Training loss: 1.2845 3.0970 sec/batch\n",
      "Epoch 11/20  Iteration 1788/3560 Training loss: 1.2809 3.0358 sec/batch\n",
      "Epoch 11/20  Iteration 1789/3560 Training loss: 1.2796 3.1403 sec/batch\n",
      "Epoch 11/20  Iteration 1790/3560 Training loss: 1.2786 3.1155 sec/batch\n",
      "Epoch 11/20  Iteration 1791/3560 Training loss: 1.2753 3.8734 sec/batch\n",
      "Epoch 11/20  Iteration 1792/3560 Training loss: 1.2742 3.3717 sec/batch\n",
      "Epoch 11/20  Iteration 1793/3560 Training loss: 1.2739 3.0205 sec/batch\n",
      "Epoch 11/20  Iteration 1794/3560 Training loss: 1.2744 3.0851 sec/batch\n",
      "Epoch 11/20  Iteration 1795/3560 Training loss: 1.2726 3.0363 sec/batch\n",
      "Epoch 11/20  Iteration 1796/3560 Training loss: 1.2696 3.0347 sec/batch\n",
      "Epoch 11/20  Iteration 1797/3560 Training loss: 1.2696 3.0224 sec/batch\n",
      "Epoch 11/20  Iteration 1798/3560 Training loss: 1.2711 3.0675 sec/batch\n",
      "Epoch 11/20  Iteration 1799/3560 Training loss: 1.2706 3.1168 sec/batch\n",
      "Epoch 11/20  Iteration 1800/3560 Training loss: 1.2716 2.9755 sec/batch\n",
      "Epoch 11/20  Iteration 1801/3560 Training loss: 1.2707 4.0491 sec/batch\n",
      "Epoch 11/20  Iteration 1802/3560 Training loss: 1.2710 3.0877 sec/batch\n",
      "Epoch 11/20  Iteration 1803/3560 Training loss: 1.2700 2.9529 sec/batch\n",
      "Epoch 11/20  Iteration 1804/3560 Training loss: 1.2700 3.0838 sec/batch\n",
      "Epoch 11/20  Iteration 1805/3560 Training loss: 1.2697 3.1721 sec/batch\n",
      "Epoch 11/20  Iteration 1806/3560 Training loss: 1.2681 2.9537 sec/batch\n",
      "Epoch 11/20  Iteration 1807/3560 Training loss: 1.2670 3.1545 sec/batch\n",
      "Epoch 11/20  Iteration 1808/3560 Training loss: 1.2678 3.1359 sec/batch\n",
      "Epoch 11/20  Iteration 1809/3560 Training loss: 1.2681 3.0450 sec/batch\n",
      "Epoch 11/20  Iteration 1810/3560 Training loss: 1.2684 4.0464 sec/batch\n",
      "Epoch 11/20  Iteration 1811/3560 Training loss: 1.2676 3.1768 sec/batch\n",
      "Epoch 11/20  Iteration 1812/3560 Training loss: 1.2665 3.0077 sec/batch\n",
      "Epoch 11/20  Iteration 1813/3560 Training loss: 1.2668 2.9898 sec/batch\n",
      "Epoch 11/20  Iteration 1814/3560 Training loss: 1.2671 3.0334 sec/batch\n",
      "Epoch 11/20  Iteration 1815/3560 Training loss: 1.2667 3.0986 sec/batch\n",
      "Epoch 11/20  Iteration 1816/3560 Training loss: 1.2667 3.0120 sec/batch\n",
      "Epoch 11/20  Iteration 1817/3560 Training loss: 1.2660 2.9669 sec/batch\n",
      "Epoch 11/20  Iteration 1818/3560 Training loss: 1.2648 3.0318 sec/batch\n",
      "Epoch 11/20  Iteration 1819/3560 Training loss: 1.2638 3.1919 sec/batch\n",
      "Epoch 11/20  Iteration 1820/3560 Training loss: 1.2634 3.6963 sec/batch\n",
      "Epoch 11/20  Iteration 1821/3560 Training loss: 1.2629 3.1504 sec/batch\n",
      "Epoch 11/20  Iteration 1822/3560 Training loss: 1.2635 3.0704 sec/batch\n",
      "Epoch 11/20  Iteration 1823/3560 Training loss: 1.2632 3.0364 sec/batch\n",
      "Epoch 11/20  Iteration 1824/3560 Training loss: 1.2626 3.0911 sec/batch\n",
      "Epoch 11/20  Iteration 1825/3560 Training loss: 1.2627 3.1061 sec/batch\n",
      "Epoch 11/20  Iteration 1826/3560 Training loss: 1.2617 3.0033 sec/batch\n",
      "Epoch 11/20  Iteration 1827/3560 Training loss: 1.2614 3.0698 sec/batch\n",
      "Epoch 11/20  Iteration 1828/3560 Training loss: 1.2611 3.0703 sec/batch\n",
      "Epoch 11/20  Iteration 1829/3560 Training loss: 1.2610 3.9937 sec/batch\n",
      "Epoch 11/20  Iteration 1830/3560 Training loss: 1.2610 3.1960 sec/batch\n",
      "Epoch 11/20  Iteration 1831/3560 Training loss: 1.2607 3.1346 sec/batch\n",
      "Epoch 11/20  Iteration 1832/3560 Training loss: 1.2614 2.9776 sec/batch\n",
      "Epoch 11/20  Iteration 1833/3560 Training loss: 1.2613 3.0889 sec/batch\n",
      "Epoch 11/20  Iteration 1834/3560 Training loss: 1.2612 3.0925 sec/batch\n",
      "Epoch 11/20  Iteration 1835/3560 Training loss: 1.2608 3.0178 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1836/3560 Training loss: 1.2608 3.0281 sec/batch\n",
      "Epoch 11/20  Iteration 1837/3560 Training loss: 1.2611 3.1263 sec/batch\n",
      "Epoch 11/20  Iteration 1838/3560 Training loss: 1.2608 3.0060 sec/batch\n",
      "Epoch 11/20  Iteration 1839/3560 Training loss: 1.2601 4.1560 sec/batch\n",
      "Epoch 11/20  Iteration 1840/3560 Training loss: 1.2606 3.1143 sec/batch\n",
      "Epoch 11/20  Iteration 1841/3560 Training loss: 1.2606 3.0027 sec/batch\n",
      "Epoch 11/20  Iteration 1842/3560 Training loss: 1.2612 3.0544 sec/batch\n",
      "Epoch 11/20  Iteration 1843/3560 Training loss: 1.2615 3.0626 sec/batch\n",
      "Epoch 11/20  Iteration 1844/3560 Training loss: 1.2614 3.0635 sec/batch\n",
      "Epoch 11/20  Iteration 1845/3560 Training loss: 1.2613 3.0432 sec/batch\n",
      "Epoch 11/20  Iteration 1846/3560 Training loss: 1.2614 3.0625 sec/batch\n",
      "Epoch 11/20  Iteration 1847/3560 Training loss: 1.2616 3.3540 sec/batch\n",
      "Epoch 11/20  Iteration 1848/3560 Training loss: 1.2613 3.9132 sec/batch\n",
      "Epoch 11/20  Iteration 1849/3560 Training loss: 1.2614 3.1035 sec/batch\n",
      "Epoch 11/20  Iteration 1850/3560 Training loss: 1.2613 3.0876 sec/batch\n",
      "Epoch 11/20  Iteration 1851/3560 Training loss: 1.2617 3.0205 sec/batch\n",
      "Epoch 11/20  Iteration 1852/3560 Training loss: 1.2620 3.0358 sec/batch\n",
      "Epoch 11/20  Iteration 1853/3560 Training loss: 1.2624 3.0981 sec/batch\n",
      "Epoch 11/20  Iteration 1854/3560 Training loss: 1.2619 2.9725 sec/batch\n",
      "Epoch 11/20  Iteration 1855/3560 Training loss: 1.2618 3.0205 sec/batch\n",
      "Epoch 11/20  Iteration 1856/3560 Training loss: 1.2618 3.0558 sec/batch\n",
      "Epoch 11/20  Iteration 1857/3560 Training loss: 1.2617 3.5231 sec/batch\n",
      "Epoch 11/20  Iteration 1858/3560 Training loss: 1.2616 3.4970 sec/batch\n",
      "Epoch 11/20  Iteration 1859/3560 Training loss: 1.2610 3.1043 sec/batch\n",
      "Epoch 11/20  Iteration 1860/3560 Training loss: 1.2609 3.0821 sec/batch\n",
      "Epoch 11/20  Iteration 1861/3560 Training loss: 1.2604 3.0307 sec/batch\n",
      "Epoch 11/20  Iteration 1862/3560 Training loss: 1.2604 3.0435 sec/batch\n",
      "Epoch 11/20  Iteration 1863/3560 Training loss: 1.2600 3.0530 sec/batch\n",
      "Epoch 11/20  Iteration 1864/3560 Training loss: 1.2600 3.0377 sec/batch\n",
      "Epoch 11/20  Iteration 1865/3560 Training loss: 1.2598 3.1169 sec/batch\n",
      "Epoch 11/20  Iteration 1866/3560 Training loss: 1.2596 3.0950 sec/batch\n",
      "Epoch 11/20  Iteration 1867/3560 Training loss: 1.2594 4.1314 sec/batch\n",
      "Epoch 11/20  Iteration 1868/3560 Training loss: 1.2591 3.0511 sec/batch\n",
      "Epoch 11/20  Iteration 1869/3560 Training loss: 1.2588 3.0473 sec/batch\n",
      "Epoch 11/20  Iteration 1870/3560 Training loss: 1.2588 3.0982 sec/batch\n",
      "Epoch 11/20  Iteration 1871/3560 Training loss: 1.2586 3.0459 sec/batch\n",
      "Epoch 11/20  Iteration 1872/3560 Training loss: 1.2585 3.0501 sec/batch\n",
      "Epoch 11/20  Iteration 1873/3560 Training loss: 1.2580 3.0406 sec/batch\n",
      "Epoch 11/20  Iteration 1874/3560 Training loss: 1.2576 3.1067 sec/batch\n",
      "Epoch 11/20  Iteration 1875/3560 Training loss: 1.2573 3.0479 sec/batch\n",
      "Epoch 11/20  Iteration 1876/3560 Training loss: 1.2574 3.7066 sec/batch\n",
      "Epoch 11/20  Iteration 1877/3560 Training loss: 1.2573 3.4360 sec/batch\n",
      "Epoch 11/20  Iteration 1878/3560 Training loss: 1.2568 3.1529 sec/batch\n",
      "Epoch 11/20  Iteration 1879/3560 Training loss: 1.2564 3.0298 sec/batch\n",
      "Epoch 11/20  Iteration 1880/3560 Training loss: 1.2560 2.9910 sec/batch\n",
      "Epoch 11/20  Iteration 1881/3560 Training loss: 1.2559 3.1893 sec/batch\n",
      "Epoch 11/20  Iteration 1882/3560 Training loss: 1.2558 3.0953 sec/batch\n",
      "Epoch 11/20  Iteration 1883/3560 Training loss: 1.2557 3.0599 sec/batch\n",
      "Epoch 11/20  Iteration 1884/3560 Training loss: 1.2556 3.0210 sec/batch\n",
      "Epoch 11/20  Iteration 1885/3560 Training loss: 1.2553 3.0640 sec/batch\n",
      "Epoch 11/20  Iteration 1886/3560 Training loss: 1.2552 3.9213 sec/batch\n",
      "Epoch 11/20  Iteration 1887/3560 Training loss: 1.2552 3.0542 sec/batch\n",
      "Epoch 11/20  Iteration 1888/3560 Training loss: 1.2551 3.1119 sec/batch\n",
      "Epoch 11/20  Iteration 1889/3560 Training loss: 1.2549 3.1633 sec/batch\n",
      "Epoch 11/20  Iteration 1890/3560 Training loss: 1.2549 3.0027 sec/batch\n",
      "Epoch 11/20  Iteration 1891/3560 Training loss: 1.2547 2.9323 sec/batch\n",
      "Epoch 11/20  Iteration 1892/3560 Training loss: 1.2546 2.8855 sec/batch\n",
      "Epoch 11/20  Iteration 1893/3560 Training loss: 1.2544 2.8988 sec/batch\n",
      "Epoch 11/20  Iteration 1894/3560 Training loss: 1.2543 2.8475 sec/batch\n",
      "Epoch 11/20  Iteration 1895/3560 Training loss: 1.2541 3.1331 sec/batch\n",
      "Epoch 11/20  Iteration 1896/3560 Training loss: 1.2537 3.6551 sec/batch\n",
      "Epoch 11/20  Iteration 1897/3560 Training loss: 1.2537 2.8971 sec/batch\n",
      "Epoch 11/20  Iteration 1898/3560 Training loss: 1.2536 2.9022 sec/batch\n",
      "Epoch 11/20  Iteration 1899/3560 Training loss: 1.2535 2.8852 sec/batch\n",
      "Epoch 11/20  Iteration 1900/3560 Training loss: 1.2535 2.8211 sec/batch\n",
      "Epoch 11/20  Iteration 1901/3560 Training loss: 1.2534 2.9016 sec/batch\n",
      "Epoch 11/20  Iteration 1902/3560 Training loss: 1.2530 2.9355 sec/batch\n",
      "Epoch 11/20  Iteration 1903/3560 Training loss: 1.2526 2.8353 sec/batch\n",
      "Epoch 11/20  Iteration 1904/3560 Training loss: 1.2524 2.8202 sec/batch\n",
      "Epoch 11/20  Iteration 1905/3560 Training loss: 1.2523 2.9608 sec/batch\n",
      "Epoch 11/20  Iteration 1906/3560 Training loss: 1.2518 3.7149 sec/batch\n",
      "Epoch 11/20  Iteration 1907/3560 Training loss: 1.2519 2.8188 sec/batch\n",
      "Epoch 11/20  Iteration 1908/3560 Training loss: 1.2518 2.8526 sec/batch\n",
      "Epoch 11/20  Iteration 1909/3560 Training loss: 1.2516 2.9495 sec/batch\n",
      "Epoch 11/20  Iteration 1910/3560 Training loss: 1.2513 2.8046 sec/batch\n",
      "Epoch 11/20  Iteration 1911/3560 Training loss: 1.2509 2.8858 sec/batch\n",
      "Epoch 11/20  Iteration 1912/3560 Training loss: 1.2507 2.8349 sec/batch\n",
      "Epoch 11/20  Iteration 1913/3560 Training loss: 1.2507 2.8447 sec/batch\n",
      "Epoch 11/20  Iteration 1914/3560 Training loss: 1.2506 2.8723 sec/batch\n",
      "Epoch 11/20  Iteration 1915/3560 Training loss: 1.2505 2.8384 sec/batch\n",
      "Epoch 11/20  Iteration 1916/3560 Training loss: 1.2505 3.7435 sec/batch\n",
      "Epoch 11/20  Iteration 1917/3560 Training loss: 1.2505 2.8501 sec/batch\n",
      "Epoch 11/20  Iteration 1918/3560 Training loss: 1.2506 2.8568 sec/batch\n",
      "Epoch 11/20  Iteration 1919/3560 Training loss: 1.2506 2.8799 sec/batch\n",
      "Epoch 11/20  Iteration 1920/3560 Training loss: 1.2506 2.8697 sec/batch\n",
      "Epoch 11/20  Iteration 1921/3560 Training loss: 1.2509 2.8792 sec/batch\n",
      "Epoch 11/20  Iteration 1922/3560 Training loss: 1.2509 2.8765 sec/batch\n",
      "Epoch 11/20  Iteration 1923/3560 Training loss: 1.2508 2.8932 sec/batch\n",
      "Epoch 11/20  Iteration 1924/3560 Training loss: 1.2511 2.8141 sec/batch\n",
      "Epoch 11/20  Iteration 1925/3560 Training loss: 1.2509 2.8833 sec/batch\n",
      "Epoch 11/20  Iteration 1926/3560 Training loss: 1.2511 3.7784 sec/batch\n",
      "Epoch 11/20  Iteration 1927/3560 Training loss: 1.2511 2.8279 sec/batch\n",
      "Epoch 11/20  Iteration 1928/3560 Training loss: 1.2512 2.9269 sec/batch\n",
      "Epoch 11/20  Iteration 1929/3560 Training loss: 1.2513 2.8756 sec/batch\n",
      "Epoch 11/20  Iteration 1930/3560 Training loss: 1.2512 2.8135 sec/batch\n",
      "Epoch 11/20  Iteration 1931/3560 Training loss: 1.2509 2.8546 sec/batch\n",
      "Epoch 11/20  Iteration 1932/3560 Training loss: 1.2507 2.8491 sec/batch\n",
      "Epoch 11/20  Iteration 1933/3560 Training loss: 1.2507 2.9006 sec/batch\n",
      "Epoch 11/20  Iteration 1934/3560 Training loss: 1.2506 2.8292 sec/batch\n",
      "Epoch 11/20  Iteration 1935/3560 Training loss: 1.2506 2.8053 sec/batch\n",
      "Epoch 11/20  Iteration 1936/3560 Training loss: 1.2505 3.7717 sec/batch\n",
      "Epoch 11/20  Iteration 1937/3560 Training loss: 1.2504 2.9608 sec/batch\n",
      "Epoch 11/20  Iteration 1938/3560 Training loss: 1.2503 2.8666 sec/batch\n",
      "Epoch 11/20  Iteration 1939/3560 Training loss: 1.2500 2.8881 sec/batch\n",
      "Epoch 11/20  Iteration 1940/3560 Training loss: 1.2501 2.8262 sec/batch\n",
      "Epoch 11/20  Iteration 1941/3560 Training loss: 1.2503 2.8579 sec/batch\n",
      "Epoch 11/20  Iteration 1942/3560 Training loss: 1.2503 2.8816 sec/batch\n",
      "Epoch 11/20  Iteration 1943/3560 Training loss: 1.2502 2.9526 sec/batch\n",
      "Epoch 11/20  Iteration 1944/3560 Training loss: 1.2502 2.8533 sec/batch\n",
      "Epoch 11/20  Iteration 1945/3560 Training loss: 1.2502 2.8477 sec/batch\n",
      "Epoch 11/20  Iteration 1946/3560 Training loss: 1.2500 3.5317 sec/batch\n",
      "Epoch 11/20  Iteration 1947/3560 Training loss: 1.2502 3.1256 sec/batch\n",
      "Epoch 11/20  Iteration 1948/3560 Training loss: 1.2506 2.8211 sec/batch\n",
      "Epoch 11/20  Iteration 1949/3560 Training loss: 1.2506 2.8879 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1950/3560 Training loss: 1.2506 2.8497 sec/batch\n",
      "Epoch 11/20  Iteration 1951/3560 Training loss: 1.2505 2.8058 sec/batch\n",
      "Epoch 11/20  Iteration 1952/3560 Training loss: 1.2504 2.8600 sec/batch\n",
      "Epoch 11/20  Iteration 1953/3560 Training loss: 1.2506 2.8773 sec/batch\n",
      "Epoch 11/20  Iteration 1954/3560 Training loss: 1.2506 2.8531 sec/batch\n",
      "Epoch 11/20  Iteration 1955/3560 Training loss: 1.2507 2.8842 sec/batch\n",
      "Epoch 11/20  Iteration 1956/3560 Training loss: 1.2505 3.0465 sec/batch\n",
      "Epoch 11/20  Iteration 1957/3560 Training loss: 1.2504 3.8114 sec/batch\n",
      "Epoch 11/20  Iteration 1958/3560 Training loss: 1.2506 2.9436 sec/batch\n",
      "Epoch 12/20  Iteration 1959/3560 Training loss: 1.3848 2.8915 sec/batch\n",
      "Epoch 12/20  Iteration 1960/3560 Training loss: 1.3262 2.9693 sec/batch\n",
      "Epoch 12/20  Iteration 1961/3560 Training loss: 1.3001 2.8224 sec/batch\n",
      "Epoch 12/20  Iteration 1962/3560 Training loss: 1.2963 2.8344 sec/batch\n",
      "Epoch 12/20  Iteration 1963/3560 Training loss: 1.2860 3.0206 sec/batch\n",
      "Epoch 12/20  Iteration 1964/3560 Training loss: 1.2730 2.8499 sec/batch\n",
      "Epoch 12/20  Iteration 1965/3560 Training loss: 1.2721 2.8199 sec/batch\n",
      "Epoch 12/20  Iteration 1966/3560 Training loss: 1.2679 3.2315 sec/batch\n",
      "Epoch 12/20  Iteration 1967/3560 Training loss: 1.2665 3.4091 sec/batch\n",
      "Epoch 12/20  Iteration 1968/3560 Training loss: 1.2646 2.9590 sec/batch\n",
      "Epoch 12/20  Iteration 1969/3560 Training loss: 1.2606 2.8490 sec/batch\n",
      "Epoch 12/20  Iteration 1970/3560 Training loss: 1.2598 2.8997 sec/batch\n",
      "Epoch 12/20  Iteration 1971/3560 Training loss: 1.2598 2.8512 sec/batch\n",
      "Epoch 12/20  Iteration 1972/3560 Training loss: 1.2595 3.0488 sec/batch\n",
      "Epoch 12/20  Iteration 1973/3560 Training loss: 1.2572 2.8631 sec/batch\n",
      "Epoch 12/20  Iteration 1974/3560 Training loss: 1.2542 2.8822 sec/batch\n",
      "Epoch 12/20  Iteration 1975/3560 Training loss: 1.2540 2.8228 sec/batch\n",
      "Epoch 12/20  Iteration 1976/3560 Training loss: 1.2548 3.1471 sec/batch\n",
      "Epoch 12/20  Iteration 1977/3560 Training loss: 1.2540 3.5840 sec/batch\n",
      "Epoch 12/20  Iteration 1978/3560 Training loss: 1.2549 2.9131 sec/batch\n",
      "Epoch 12/20  Iteration 1979/3560 Training loss: 1.2542 2.9268 sec/batch\n",
      "Epoch 12/20  Iteration 1980/3560 Training loss: 1.2540 2.9386 sec/batch\n",
      "Epoch 12/20  Iteration 1981/3560 Training loss: 1.2529 2.8896 sec/batch\n",
      "Epoch 12/20  Iteration 1982/3560 Training loss: 1.2525 2.8381 sec/batch\n",
      "Epoch 12/20  Iteration 1983/3560 Training loss: 1.2521 2.8208 sec/batch\n",
      "Epoch 12/20  Iteration 1984/3560 Training loss: 1.2503 2.9453 sec/batch\n",
      "Epoch 12/20  Iteration 1985/3560 Training loss: 1.2493 2.8681 sec/batch\n",
      "Epoch 12/20  Iteration 1986/3560 Training loss: 1.2495 2.9985 sec/batch\n",
      "Epoch 12/20  Iteration 1987/3560 Training loss: 1.2491 3.6462 sec/batch\n",
      "Epoch 12/20  Iteration 1988/3560 Training loss: 1.2491 2.8712 sec/batch\n",
      "Epoch 12/20  Iteration 1989/3560 Training loss: 1.2480 2.8326 sec/batch\n",
      "Epoch 12/20  Iteration 1990/3560 Training loss: 1.2467 2.8830 sec/batch\n",
      "Epoch 12/20  Iteration 1991/3560 Training loss: 1.2467 2.8910 sec/batch\n",
      "Epoch 12/20  Iteration 1992/3560 Training loss: 1.2467 2.8477 sec/batch\n",
      "Epoch 12/20  Iteration 1993/3560 Training loss: 1.2462 2.8516 sec/batch\n",
      "Epoch 12/20  Iteration 1994/3560 Training loss: 1.2459 2.8716 sec/batch\n",
      "Epoch 12/20  Iteration 1995/3560 Training loss: 1.2450 2.8590 sec/batch\n",
      "Epoch 12/20  Iteration 1996/3560 Training loss: 1.2436 2.8608 sec/batch\n",
      "Epoch 12/20  Iteration 1997/3560 Training loss: 1.2424 3.7884 sec/batch\n",
      "Epoch 12/20  Iteration 1998/3560 Training loss: 1.2419 2.8350 sec/batch\n",
      "Epoch 12/20  Iteration 1999/3560 Training loss: 1.2413 2.8302 sec/batch\n",
      "Epoch 12/20  Iteration 2000/3560 Training loss: 1.2421 2.8159 sec/batch\n",
      "Epoch 12/20  Iteration 2001/3560 Training loss: 1.2419 2.8670 sec/batch\n",
      "Epoch 12/20  Iteration 2002/3560 Training loss: 1.2414 2.8273 sec/batch\n",
      "Epoch 12/20  Iteration 2003/3560 Training loss: 1.2415 2.8737 sec/batch\n",
      "Epoch 12/20  Iteration 2004/3560 Training loss: 1.2406 2.8664 sec/batch\n",
      "Epoch 12/20  Iteration 2005/3560 Training loss: 1.2403 2.8200 sec/batch\n",
      "Epoch 12/20  Iteration 2006/3560 Training loss: 1.2398 2.8438 sec/batch\n",
      "Epoch 12/20  Iteration 2007/3560 Training loss: 1.2396 3.6467 sec/batch\n",
      "Epoch 12/20  Iteration 2008/3560 Training loss: 1.2397 2.8864 sec/batch\n",
      "Epoch 12/20  Iteration 2009/3560 Training loss: 1.2391 2.8860 sec/batch\n",
      "Epoch 12/20  Iteration 2010/3560 Training loss: 1.2398 2.9181 sec/batch\n",
      "Epoch 12/20  Iteration 2011/3560 Training loss: 1.2396 2.9052 sec/batch\n",
      "Epoch 12/20  Iteration 2012/3560 Training loss: 1.2396 2.8527 sec/batch\n",
      "Epoch 12/20  Iteration 2013/3560 Training loss: 1.2394 2.8316 sec/batch\n",
      "Epoch 12/20  Iteration 2014/3560 Training loss: 1.2394 2.8666 sec/batch\n",
      "Epoch 12/20  Iteration 2015/3560 Training loss: 1.2396 2.8935 sec/batch\n",
      "Epoch 12/20  Iteration 2016/3560 Training loss: 1.2392 2.7998 sec/batch\n",
      "Epoch 12/20  Iteration 2017/3560 Training loss: 1.2385 3.9875 sec/batch\n",
      "Epoch 12/20  Iteration 2018/3560 Training loss: 1.2390 3.1063 sec/batch\n",
      "Epoch 12/20  Iteration 2019/3560 Training loss: 1.2389 2.8333 sec/batch\n",
      "Epoch 12/20  Iteration 2020/3560 Training loss: 1.2398 2.8314 sec/batch\n",
      "Epoch 12/20  Iteration 2021/3560 Training loss: 1.2400 2.9134 sec/batch\n",
      "Epoch 12/20  Iteration 2022/3560 Training loss: 1.2401 2.8852 sec/batch\n",
      "Epoch 12/20  Iteration 2023/3560 Training loss: 1.2399 2.8310 sec/batch\n",
      "Epoch 12/20  Iteration 2024/3560 Training loss: 1.2400 2.8370 sec/batch\n",
      "Epoch 12/20  Iteration 2025/3560 Training loss: 1.2404 2.9674 sec/batch\n",
      "Epoch 12/20  Iteration 2026/3560 Training loss: 1.2403 2.8033 sec/batch\n",
      "Epoch 12/20  Iteration 2027/3560 Training loss: 1.2405 3.5406 sec/batch\n",
      "Epoch 12/20  Iteration 2028/3560 Training loss: 1.2404 3.0287 sec/batch\n",
      "Epoch 12/20  Iteration 2029/3560 Training loss: 1.2410 2.8210 sec/batch\n",
      "Epoch 12/20  Iteration 2030/3560 Training loss: 1.2412 2.8132 sec/batch\n",
      "Epoch 12/20  Iteration 2031/3560 Training loss: 1.2414 2.9056 sec/batch\n",
      "Epoch 12/20  Iteration 2032/3560 Training loss: 1.2411 2.8624 sec/batch\n",
      "Epoch 12/20  Iteration 2033/3560 Training loss: 1.2410 2.8497 sec/batch\n",
      "Epoch 12/20  Iteration 2034/3560 Training loss: 1.2412 2.8319 sec/batch\n",
      "Epoch 12/20  Iteration 2035/3560 Training loss: 1.2410 2.8283 sec/batch\n",
      "Epoch 12/20  Iteration 2036/3560 Training loss: 1.2407 2.8953 sec/batch\n",
      "Epoch 12/20  Iteration 2037/3560 Training loss: 1.2400 2.9208 sec/batch\n",
      "Epoch 12/20  Iteration 2038/3560 Training loss: 1.2399 3.5836 sec/batch\n",
      "Epoch 12/20  Iteration 2039/3560 Training loss: 1.2395 2.9394 sec/batch\n",
      "Epoch 12/20  Iteration 2040/3560 Training loss: 1.2393 2.8354 sec/batch\n",
      "Epoch 12/20  Iteration 2041/3560 Training loss: 1.2389 2.8523 sec/batch\n",
      "Epoch 12/20  Iteration 2042/3560 Training loss: 1.2387 2.8699 sec/batch\n",
      "Epoch 12/20  Iteration 2043/3560 Training loss: 1.2385 2.8675 sec/batch\n",
      "Epoch 12/20  Iteration 2044/3560 Training loss: 1.2384 2.8498 sec/batch\n",
      "Epoch 12/20  Iteration 2045/3560 Training loss: 1.2381 2.8534 sec/batch\n",
      "Epoch 12/20  Iteration 2046/3560 Training loss: 1.2377 2.9071 sec/batch\n",
      "Epoch 12/20  Iteration 2047/3560 Training loss: 1.2374 3.0159 sec/batch\n",
      "Epoch 12/20  Iteration 2048/3560 Training loss: 1.2374 3.5549 sec/batch\n",
      "Epoch 12/20  Iteration 2049/3560 Training loss: 1.2371 2.9659 sec/batch\n",
      "Epoch 12/20  Iteration 2050/3560 Training loss: 1.2369 2.8211 sec/batch\n",
      "Epoch 12/20  Iteration 2051/3560 Training loss: 1.2366 2.8477 sec/batch\n",
      "Epoch 12/20  Iteration 2052/3560 Training loss: 1.2362 2.8325 sec/batch\n",
      "Epoch 12/20  Iteration 2053/3560 Training loss: 1.2359 2.8454 sec/batch\n",
      "Epoch 12/20  Iteration 2054/3560 Training loss: 1.2360 2.8213 sec/batch\n",
      "Epoch 12/20  Iteration 2055/3560 Training loss: 1.2358 2.8897 sec/batch\n",
      "Epoch 12/20  Iteration 2056/3560 Training loss: 1.2354 2.8635 sec/batch\n",
      "Epoch 12/20  Iteration 2057/3560 Training loss: 1.2351 2.7851 sec/batch\n",
      "Epoch 12/20  Iteration 2058/3560 Training loss: 1.2347 3.7687 sec/batch\n",
      "Epoch 12/20  Iteration 2059/3560 Training loss: 1.2347 2.9085 sec/batch\n",
      "Epoch 12/20  Iteration 2060/3560 Training loss: 1.2345 2.8649 sec/batch\n",
      "Epoch 12/20  Iteration 2061/3560 Training loss: 1.2344 2.8245 sec/batch\n",
      "Epoch 12/20  Iteration 2062/3560 Training loss: 1.2343 2.8759 sec/batch\n",
      "Epoch 12/20  Iteration 2063/3560 Training loss: 1.2340 2.8911 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 2064/3560 Training loss: 1.2338 2.8010 sec/batch\n",
      "Epoch 12/20  Iteration 2065/3560 Training loss: 1.2337 2.8154 sec/batch\n",
      "Epoch 12/20  Iteration 2066/3560 Training loss: 1.2338 2.8685 sec/batch\n",
      "Epoch 12/20  Iteration 2067/3560 Training loss: 1.2336 2.9582 sec/batch\n",
      "Epoch 12/20  Iteration 2068/3560 Training loss: 1.2335 3.6528 sec/batch\n",
      "Epoch 12/20  Iteration 2069/3560 Training loss: 1.2333 2.8587 sec/batch\n",
      "Epoch 12/20  Iteration 2070/3560 Training loss: 1.2333 2.8354 sec/batch\n",
      "Epoch 12/20  Iteration 2071/3560 Training loss: 1.2332 2.8326 sec/batch\n",
      "Epoch 12/20  Iteration 2072/3560 Training loss: 1.2330 2.7875 sec/batch\n",
      "Epoch 12/20  Iteration 2073/3560 Training loss: 1.2327 2.8731 sec/batch\n",
      "Epoch 12/20  Iteration 2074/3560 Training loss: 1.2324 2.8574 sec/batch\n",
      "Epoch 12/20  Iteration 2075/3560 Training loss: 1.2324 2.8762 sec/batch\n",
      "Epoch 12/20  Iteration 2076/3560 Training loss: 1.2324 2.8050 sec/batch\n",
      "Epoch 12/20  Iteration 2077/3560 Training loss: 1.2323 2.9842 sec/batch\n",
      "Epoch 12/20  Iteration 2078/3560 Training loss: 1.2322 3.6874 sec/batch\n",
      "Epoch 12/20  Iteration 2079/3560 Training loss: 1.2321 3.1180 sec/batch\n",
      "Epoch 12/20  Iteration 2080/3560 Training loss: 1.2317 2.9530 sec/batch\n",
      "Epoch 12/20  Iteration 2081/3560 Training loss: 1.2313 2.8717 sec/batch\n",
      "Epoch 12/20  Iteration 2082/3560 Training loss: 1.2313 2.8653 sec/batch\n",
      "Epoch 12/20  Iteration 2083/3560 Training loss: 1.2312 2.8595 sec/batch\n",
      "Epoch 12/20  Iteration 2084/3560 Training loss: 1.2308 2.8588 sec/batch\n",
      "Epoch 12/20  Iteration 2085/3560 Training loss: 1.2309 2.8763 sec/batch\n",
      "Epoch 12/20  Iteration 2086/3560 Training loss: 1.2308 2.8775 sec/batch\n",
      "Epoch 12/20  Iteration 2087/3560 Training loss: 1.2307 2.9254 sec/batch\n",
      "Epoch 12/20  Iteration 2088/3560 Training loss: 1.2304 3.6139 sec/batch\n",
      "Epoch 12/20  Iteration 2089/3560 Training loss: 1.2299 3.0525 sec/batch\n",
      "Epoch 12/20  Iteration 2090/3560 Training loss: 1.2298 2.9718 sec/batch\n",
      "Epoch 12/20  Iteration 2091/3560 Training loss: 1.2300 2.8133 sec/batch\n",
      "Epoch 12/20  Iteration 2092/3560 Training loss: 1.2299 2.8766 sec/batch\n",
      "Epoch 12/20  Iteration 2093/3560 Training loss: 1.2299 2.8891 sec/batch\n",
      "Epoch 12/20  Iteration 2094/3560 Training loss: 1.2298 2.8850 sec/batch\n",
      "Epoch 12/20  Iteration 2095/3560 Training loss: 1.2300 2.9047 sec/batch\n",
      "Epoch 12/20  Iteration 2096/3560 Training loss: 1.2301 2.9050 sec/batch\n",
      "Epoch 12/20  Iteration 2097/3560 Training loss: 1.2300 3.0262 sec/batch\n",
      "Epoch 12/20  Iteration 2098/3560 Training loss: 1.2301 4.3823 sec/batch\n",
      "Epoch 12/20  Iteration 2099/3560 Training loss: 1.2304 2.9493 sec/batch\n",
      "Epoch 12/20  Iteration 2100/3560 Training loss: 1.2304 2.8983 sec/batch\n",
      "Epoch 12/20  Iteration 2101/3560 Training loss: 1.2303 2.8515 sec/batch\n",
      "Epoch 12/20  Iteration 2102/3560 Training loss: 1.2306 2.8575 sec/batch\n",
      "Epoch 12/20  Iteration 2103/3560 Training loss: 1.2305 2.8566 sec/batch\n",
      "Epoch 12/20  Iteration 2104/3560 Training loss: 1.2306 2.9819 sec/batch\n",
      "Epoch 12/20  Iteration 2105/3560 Training loss: 1.2306 2.8210 sec/batch\n",
      "Epoch 12/20  Iteration 2106/3560 Training loss: 1.2308 2.8360 sec/batch\n",
      "Epoch 12/20  Iteration 2107/3560 Training loss: 1.2310 2.9328 sec/batch\n",
      "Epoch 12/20  Iteration 2108/3560 Training loss: 1.2309 3.6364 sec/batch\n",
      "Epoch 12/20  Iteration 2109/3560 Training loss: 1.2306 2.9879 sec/batch\n",
      "Epoch 12/20  Iteration 2110/3560 Training loss: 1.2304 2.9220 sec/batch\n",
      "Epoch 12/20  Iteration 2111/3560 Training loss: 1.2305 2.8445 sec/batch\n",
      "Epoch 12/20  Iteration 2112/3560 Training loss: 1.2304 2.8086 sec/batch\n",
      "Epoch 12/20  Iteration 2113/3560 Training loss: 1.2304 2.7853 sec/batch\n",
      "Epoch 12/20  Iteration 2114/3560 Training loss: 1.2304 2.9042 sec/batch\n",
      "Epoch 12/20  Iteration 2115/3560 Training loss: 1.2304 2.8454 sec/batch\n",
      "Epoch 12/20  Iteration 2116/3560 Training loss: 1.2302 2.8248 sec/batch\n",
      "Epoch 12/20  Iteration 2117/3560 Training loss: 1.2301 2.8385 sec/batch\n",
      "Epoch 12/20  Iteration 2118/3560 Training loss: 1.2302 3.1062 sec/batch\n",
      "Epoch 12/20  Iteration 2119/3560 Training loss: 1.2304 3.4486 sec/batch\n",
      "Epoch 12/20  Iteration 2120/3560 Training loss: 1.2305 2.8181 sec/batch\n",
      "Epoch 12/20  Iteration 2121/3560 Training loss: 1.2304 2.9850 sec/batch\n",
      "Epoch 12/20  Iteration 2122/3560 Training loss: 1.2303 2.8862 sec/batch\n",
      "Epoch 12/20  Iteration 2123/3560 Training loss: 1.2303 2.8136 sec/batch\n",
      "Epoch 12/20  Iteration 2124/3560 Training loss: 1.2302 2.8600 sec/batch\n",
      "Epoch 12/20  Iteration 2125/3560 Training loss: 1.2304 2.8944 sec/batch\n",
      "Epoch 12/20  Iteration 2126/3560 Training loss: 1.2307 2.8088 sec/batch\n",
      "Epoch 12/20  Iteration 2127/3560 Training loss: 1.2307 2.8362 sec/batch\n",
      "Epoch 12/20  Iteration 2128/3560 Training loss: 1.2307 3.2151 sec/batch\n",
      "Epoch 12/20  Iteration 2129/3560 Training loss: 1.2306 3.5239 sec/batch\n",
      "Epoch 12/20  Iteration 2130/3560 Training loss: 1.2305 2.8481 sec/batch\n",
      "Epoch 12/20  Iteration 2131/3560 Training loss: 1.2306 2.9166 sec/batch\n",
      "Epoch 12/20  Iteration 2132/3560 Training loss: 1.2307 2.8019 sec/batch\n",
      "Epoch 12/20  Iteration 2133/3560 Training loss: 1.2308 2.8677 sec/batch\n",
      "Epoch 12/20  Iteration 2134/3560 Training loss: 1.2306 2.8627 sec/batch\n",
      "Epoch 12/20  Iteration 2135/3560 Training loss: 1.2305 2.8533 sec/batch\n",
      "Epoch 12/20  Iteration 2136/3560 Training loss: 1.2306 2.8538 sec/batch\n",
      "Epoch 13/20  Iteration 2137/3560 Training loss: 1.3691 2.8650 sec/batch\n",
      "Epoch 13/20  Iteration 2138/3560 Training loss: 1.3087 2.9859 sec/batch\n",
      "Epoch 13/20  Iteration 2139/3560 Training loss: 1.2811 3.6622 sec/batch\n",
      "Epoch 13/20  Iteration 2140/3560 Training loss: 1.2751 2.8295 sec/batch\n",
      "Epoch 13/20  Iteration 2141/3560 Training loss: 1.2615 2.8676 sec/batch\n",
      "Epoch 13/20  Iteration 2142/3560 Training loss: 1.2484 2.8835 sec/batch\n",
      "Epoch 13/20  Iteration 2143/3560 Training loss: 1.2459 2.8487 sec/batch\n",
      "Epoch 13/20  Iteration 2144/3560 Training loss: 1.2429 2.8982 sec/batch\n",
      "Epoch 13/20  Iteration 2145/3560 Training loss: 1.2414 2.8627 sec/batch\n",
      "Epoch 13/20  Iteration 2146/3560 Training loss: 1.2400 2.8272 sec/batch\n",
      "Epoch 13/20  Iteration 2147/3560 Training loss: 1.2355 2.8929 sec/batch\n",
      "Epoch 13/20  Iteration 2148/3560 Training loss: 1.2341 2.9869 sec/batch\n",
      "Epoch 13/20  Iteration 2149/3560 Training loss: 1.2342 3.7785 sec/batch\n",
      "Epoch 13/20  Iteration 2150/3560 Training loss: 1.2340 2.8539 sec/batch\n",
      "Epoch 13/20  Iteration 2151/3560 Training loss: 1.2327 2.8646 sec/batch\n",
      "Epoch 13/20  Iteration 2152/3560 Training loss: 1.2305 2.8882 sec/batch\n",
      "Epoch 13/20  Iteration 2153/3560 Training loss: 1.2305 2.8115 sec/batch\n",
      "Epoch 13/20  Iteration 2154/3560 Training loss: 1.2314 2.8217 sec/batch\n",
      "Epoch 13/20  Iteration 2155/3560 Training loss: 1.2313 2.9180 sec/batch\n",
      "Epoch 13/20  Iteration 2156/3560 Training loss: 1.2326 2.8539 sec/batch\n",
      "Epoch 13/20  Iteration 2157/3560 Training loss: 1.2318 2.8107 sec/batch\n",
      "Epoch 13/20  Iteration 2158/3560 Training loss: 1.2319 2.8921 sec/batch\n",
      "Epoch 13/20  Iteration 2159/3560 Training loss: 1.2310 3.5967 sec/batch\n",
      "Epoch 13/20  Iteration 2160/3560 Training loss: 1.2308 3.0648 sec/batch\n",
      "Epoch 13/20  Iteration 2161/3560 Training loss: 1.2303 2.8521 sec/batch\n",
      "Epoch 13/20  Iteration 2162/3560 Training loss: 1.2286 2.8886 sec/batch\n",
      "Epoch 13/20  Iteration 2163/3560 Training loss: 1.2272 2.8474 sec/batch\n",
      "Epoch 13/20  Iteration 2164/3560 Training loss: 1.2278 2.8034 sec/batch\n",
      "Epoch 13/20  Iteration 2165/3560 Training loss: 1.2276 2.8771 sec/batch\n",
      "Epoch 13/20  Iteration 2166/3560 Training loss: 1.2277 2.9026 sec/batch\n",
      "Epoch 13/20  Iteration 2167/3560 Training loss: 1.2269 2.8242 sec/batch\n",
      "Epoch 13/20  Iteration 2168/3560 Training loss: 1.2255 2.8734 sec/batch\n",
      "Epoch 13/20  Iteration 2169/3560 Training loss: 1.2254 3.7708 sec/batch\n",
      "Epoch 13/20  Iteration 2170/3560 Training loss: 1.2256 2.8313 sec/batch\n",
      "Epoch 13/20  Iteration 2171/3560 Training loss: 1.2250 2.8855 sec/batch\n",
      "Epoch 13/20  Iteration 2172/3560 Training loss: 1.2249 2.8905 sec/batch\n",
      "Epoch 13/20  Iteration 2173/3560 Training loss: 1.2241 2.7882 sec/batch\n",
      "Epoch 13/20  Iteration 2174/3560 Training loss: 1.2229 2.8890 sec/batch\n",
      "Epoch 13/20  Iteration 2175/3560 Training loss: 1.2218 2.8695 sec/batch\n",
      "Epoch 13/20  Iteration 2176/3560 Training loss: 1.2215 2.9302 sec/batch\n",
      "Epoch 13/20  Iteration 2177/3560 Training loss: 1.2211 2.8823 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2178/3560 Training loss: 1.2219 2.8276 sec/batch\n",
      "Epoch 13/20  Iteration 2179/3560 Training loss: 1.2216 3.4449 sec/batch\n",
      "Epoch 13/20  Iteration 2180/3560 Training loss: 1.2210 3.2895 sec/batch\n",
      "Epoch 13/20  Iteration 2181/3560 Training loss: 1.2209 2.8536 sec/batch\n",
      "Epoch 13/20  Iteration 2182/3560 Training loss: 1.2201 3.0119 sec/batch\n",
      "Epoch 13/20  Iteration 2183/3560 Training loss: 1.2198 2.8266 sec/batch\n",
      "Epoch 13/20  Iteration 2184/3560 Training loss: 1.2194 2.8716 sec/batch\n",
      "Epoch 13/20  Iteration 2185/3560 Training loss: 1.2194 2.8876 sec/batch\n",
      "Epoch 13/20  Iteration 2186/3560 Training loss: 1.2194 2.8491 sec/batch\n",
      "Epoch 13/20  Iteration 2187/3560 Training loss: 1.2190 2.8204 sec/batch\n",
      "Epoch 13/20  Iteration 2188/3560 Training loss: 1.2196 2.8499 sec/batch\n",
      "Epoch 13/20  Iteration 2189/3560 Training loss: 1.2194 3.4897 sec/batch\n",
      "Epoch 13/20  Iteration 2190/3560 Training loss: 1.2196 3.0867 sec/batch\n",
      "Epoch 13/20  Iteration 2191/3560 Training loss: 1.2192 2.8757 sec/batch\n",
      "Epoch 13/20  Iteration 2192/3560 Training loss: 1.2192 2.9511 sec/batch\n",
      "Epoch 13/20  Iteration 2193/3560 Training loss: 1.2193 2.8763 sec/batch\n",
      "Epoch 13/20  Iteration 2194/3560 Training loss: 1.2190 2.8078 sec/batch\n",
      "Epoch 13/20  Iteration 2195/3560 Training loss: 1.2185 2.8624 sec/batch\n",
      "Epoch 13/20  Iteration 2196/3560 Training loss: 1.2191 2.9739 sec/batch\n",
      "Epoch 13/20  Iteration 2197/3560 Training loss: 1.2192 2.8719 sec/batch\n",
      "Epoch 13/20  Iteration 2198/3560 Training loss: 1.2198 2.8423 sec/batch\n",
      "Epoch 13/20  Iteration 2199/3560 Training loss: 1.2202 3.1260 sec/batch\n",
      "Epoch 13/20  Iteration 2200/3560 Training loss: 1.2202 3.8425 sec/batch\n",
      "Epoch 13/20  Iteration 2201/3560 Training loss: 1.2200 2.8617 sec/batch\n",
      "Epoch 13/20  Iteration 2202/3560 Training loss: 1.2201 2.8249 sec/batch\n",
      "Epoch 13/20  Iteration 2203/3560 Training loss: 1.2204 2.9396 sec/batch\n",
      "Epoch 13/20  Iteration 2204/3560 Training loss: 1.2203 2.8188 sec/batch\n",
      "Epoch 13/20  Iteration 2205/3560 Training loss: 1.2204 2.8027 sec/batch\n",
      "Epoch 13/20  Iteration 2206/3560 Training loss: 1.2203 2.8811 sec/batch\n",
      "Epoch 13/20  Iteration 2207/3560 Training loss: 1.2209 2.8520 sec/batch\n",
      "Epoch 13/20  Iteration 2208/3560 Training loss: 1.2211 2.8205 sec/batch\n",
      "Epoch 13/20  Iteration 2209/3560 Training loss: 1.2216 3.1183 sec/batch\n",
      "Epoch 13/20  Iteration 2210/3560 Training loss: 1.2212 3.5025 sec/batch\n",
      "Epoch 13/20  Iteration 2211/3560 Training loss: 1.2211 2.8339 sec/batch\n",
      "Epoch 13/20  Iteration 2212/3560 Training loss: 1.2215 2.8567 sec/batch\n",
      "Epoch 13/20  Iteration 2213/3560 Training loss: 1.2213 2.8877 sec/batch\n",
      "Epoch 13/20  Iteration 2214/3560 Training loss: 1.2212 2.8147 sec/batch\n",
      "Epoch 13/20  Iteration 2215/3560 Training loss: 1.2208 2.8398 sec/batch\n",
      "Epoch 13/20  Iteration 2216/3560 Training loss: 1.2207 2.8780 sec/batch\n",
      "Epoch 13/20  Iteration 2217/3560 Training loss: 1.2203 2.8095 sec/batch\n",
      "Epoch 13/20  Iteration 2218/3560 Training loss: 1.2203 2.8535 sec/batch\n",
      "Epoch 13/20  Iteration 2219/3560 Training loss: 1.2200 2.8538 sec/batch\n",
      "Epoch 13/20  Iteration 2220/3560 Training loss: 1.2200 3.7304 sec/batch\n",
      "Epoch 13/20  Iteration 2221/3560 Training loss: 1.2197 2.8086 sec/batch\n",
      "Epoch 13/20  Iteration 2222/3560 Training loss: 1.2196 2.8672 sec/batch\n",
      "Epoch 13/20  Iteration 2223/3560 Training loss: 1.2195 2.8853 sec/batch\n",
      "Epoch 13/20  Iteration 2224/3560 Training loss: 1.2193 2.8057 sec/batch\n",
      "Epoch 13/20  Iteration 2225/3560 Training loss: 1.2190 2.8205 sec/batch\n",
      "Epoch 13/20  Iteration 2226/3560 Training loss: 1.2192 2.8520 sec/batch\n",
      "Epoch 13/20  Iteration 2227/3560 Training loss: 1.2189 2.8529 sec/batch\n",
      "Epoch 13/20  Iteration 2228/3560 Training loss: 1.2189 2.8346 sec/batch\n",
      "Epoch 13/20  Iteration 2229/3560 Training loss: 1.2187 2.8973 sec/batch\n",
      "Epoch 13/20  Iteration 2230/3560 Training loss: 1.2184 3.8124 sec/batch\n",
      "Epoch 13/20  Iteration 2231/3560 Training loss: 1.2181 2.7578 sec/batch\n",
      "Epoch 13/20  Iteration 2232/3560 Training loss: 1.2181 2.8718 sec/batch\n",
      "Epoch 13/20  Iteration 2233/3560 Training loss: 1.2180 2.8910 sec/batch\n",
      "Epoch 13/20  Iteration 2234/3560 Training loss: 1.2177 2.8906 sec/batch\n",
      "Epoch 13/20  Iteration 2235/3560 Training loss: 1.2173 2.7777 sec/batch\n",
      "Epoch 13/20  Iteration 2236/3560 Training loss: 1.2171 2.8468 sec/batch\n",
      "Epoch 13/20  Iteration 2237/3560 Training loss: 1.2170 2.9052 sec/batch\n",
      "Epoch 13/20  Iteration 2238/3560 Training loss: 1.2169 2.8448 sec/batch\n",
      "Epoch 13/20  Iteration 2239/3560 Training loss: 1.2168 2.8369 sec/batch\n",
      "Epoch 13/20  Iteration 2240/3560 Training loss: 1.2166 3.5920 sec/batch\n",
      "Epoch 13/20  Iteration 2241/3560 Training loss: 1.2163 2.9978 sec/batch\n",
      "Epoch 13/20  Iteration 2242/3560 Training loss: 1.2162 2.8136 sec/batch\n",
      "Epoch 13/20  Iteration 2243/3560 Training loss: 1.2162 2.8224 sec/batch\n",
      "Epoch 13/20  Iteration 2244/3560 Training loss: 1.2163 2.9191 sec/batch\n",
      "Epoch 13/20  Iteration 2245/3560 Training loss: 1.2161 2.8469 sec/batch\n",
      "Epoch 13/20  Iteration 2246/3560 Training loss: 1.2161 2.8146 sec/batch\n",
      "Epoch 13/20  Iteration 2247/3560 Training loss: 1.2160 2.8968 sec/batch\n",
      "Epoch 13/20  Iteration 2248/3560 Training loss: 1.2158 2.9064 sec/batch\n",
      "Epoch 13/20  Iteration 2249/3560 Training loss: 1.2157 2.7831 sec/batch\n",
      "Epoch 13/20  Iteration 2250/3560 Training loss: 1.2156 3.1423 sec/batch\n",
      "Epoch 13/20  Iteration 2251/3560 Training loss: 1.2153 3.4266 sec/batch\n",
      "Epoch 13/20  Iteration 2252/3560 Training loss: 1.2149 2.8692 sec/batch\n",
      "Epoch 13/20  Iteration 2253/3560 Training loss: 1.2150 2.8725 sec/batch\n",
      "Epoch 13/20  Iteration 2254/3560 Training loss: 1.2150 2.8328 sec/batch\n",
      "Epoch 13/20  Iteration 2255/3560 Training loss: 1.2148 2.8899 sec/batch\n",
      "Epoch 13/20  Iteration 2256/3560 Training loss: 1.2148 2.8371 sec/batch\n",
      "Epoch 13/20  Iteration 2257/3560 Training loss: 1.2148 2.8157 sec/batch\n",
      "Epoch 13/20  Iteration 2258/3560 Training loss: 1.2144 2.9518 sec/batch\n",
      "Epoch 13/20  Iteration 2259/3560 Training loss: 1.2140 2.8083 sec/batch\n",
      "Epoch 13/20  Iteration 2260/3560 Training loss: 1.2138 2.9928 sec/batch\n",
      "Epoch 13/20  Iteration 2261/3560 Training loss: 1.2136 3.8363 sec/batch\n",
      "Epoch 13/20  Iteration 2262/3560 Training loss: 1.2133 2.8631 sec/batch\n",
      "Epoch 13/20  Iteration 2263/3560 Training loss: 1.2134 2.8810 sec/batch\n",
      "Epoch 13/20  Iteration 2264/3560 Training loss: 1.2133 2.8380 sec/batch\n",
      "Epoch 13/20  Iteration 2265/3560 Training loss: 1.2132 2.8882 sec/batch\n",
      "Epoch 13/20  Iteration 2266/3560 Training loss: 1.2129 2.8500 sec/batch\n",
      "Epoch 13/20  Iteration 2267/3560 Training loss: 1.2125 2.9241 sec/batch\n",
      "Epoch 13/20  Iteration 2268/3560 Training loss: 1.2123 3.0872 sec/batch\n",
      "Epoch 13/20  Iteration 2269/3560 Training loss: 1.2124 2.8871 sec/batch\n",
      "Epoch 13/20  Iteration 2270/3560 Training loss: 1.2123 2.9689 sec/batch\n",
      "Epoch 13/20  Iteration 2271/3560 Training loss: 1.2123 3.6108 sec/batch\n",
      "Epoch 13/20  Iteration 2272/3560 Training loss: 1.2122 2.9056 sec/batch\n",
      "Epoch 13/20  Iteration 2273/3560 Training loss: 1.2124 2.8352 sec/batch\n",
      "Epoch 13/20  Iteration 2274/3560 Training loss: 1.2125 2.8875 sec/batch\n",
      "Epoch 13/20  Iteration 2275/3560 Training loss: 1.2125 2.8654 sec/batch\n",
      "Epoch 13/20  Iteration 2276/3560 Training loss: 1.2125 2.8355 sec/batch\n",
      "Epoch 13/20  Iteration 2277/3560 Training loss: 1.2128 2.8119 sec/batch\n",
      "Epoch 13/20  Iteration 2278/3560 Training loss: 1.2129 2.8794 sec/batch\n",
      "Epoch 13/20  Iteration 2279/3560 Training loss: 1.2128 3.0008 sec/batch\n",
      "Epoch 13/20  Iteration 2280/3560 Training loss: 1.2130 2.8700 sec/batch\n",
      "Epoch 13/20  Iteration 2281/3560 Training loss: 1.2129 3.6592 sec/batch\n",
      "Epoch 13/20  Iteration 2282/3560 Training loss: 1.2131 2.8895 sec/batch\n",
      "Epoch 13/20  Iteration 2283/3560 Training loss: 1.2131 2.8803 sec/batch\n",
      "Epoch 13/20  Iteration 2284/3560 Training loss: 1.2133 2.8162 sec/batch\n",
      "Epoch 13/20  Iteration 2285/3560 Training loss: 1.2135 2.9865 sec/batch\n",
      "Epoch 13/20  Iteration 2286/3560 Training loss: 1.2134 2.9210 sec/batch\n",
      "Epoch 13/20  Iteration 2287/3560 Training loss: 1.2131 2.8576 sec/batch\n",
      "Epoch 13/20  Iteration 2288/3560 Training loss: 1.2130 2.8615 sec/batch\n",
      "Epoch 13/20  Iteration 2289/3560 Training loss: 1.2130 2.9208 sec/batch\n",
      "Epoch 13/20  Iteration 2290/3560 Training loss: 1.2129 2.8843 sec/batch\n",
      "Epoch 13/20  Iteration 2291/3560 Training loss: 1.2129 3.6893 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2292/3560 Training loss: 1.2128 2.9426 sec/batch\n",
      "Epoch 13/20  Iteration 2293/3560 Training loss: 1.2128 2.8784 sec/batch\n",
      "Epoch 13/20  Iteration 2294/3560 Training loss: 1.2127 2.8556 sec/batch\n",
      "Epoch 13/20  Iteration 2295/3560 Training loss: 1.2125 2.8182 sec/batch\n",
      "Epoch 13/20  Iteration 2296/3560 Training loss: 1.2126 2.9171 sec/batch\n",
      "Epoch 13/20  Iteration 2297/3560 Training loss: 1.2128 2.8443 sec/batch\n",
      "Epoch 13/20  Iteration 2298/3560 Training loss: 1.2128 2.8447 sec/batch\n",
      "Epoch 13/20  Iteration 2299/3560 Training loss: 1.2128 2.9008 sec/batch\n",
      "Epoch 13/20  Iteration 2300/3560 Training loss: 1.2127 2.8003 sec/batch\n",
      "Epoch 13/20  Iteration 2301/3560 Training loss: 1.2127 3.7006 sec/batch\n",
      "Epoch 13/20  Iteration 2302/3560 Training loss: 1.2126 2.8775 sec/batch\n",
      "Epoch 13/20  Iteration 2303/3560 Training loss: 1.2128 2.8412 sec/batch\n",
      "Epoch 13/20  Iteration 2304/3560 Training loss: 1.2131 2.8683 sec/batch\n",
      "Epoch 13/20  Iteration 2305/3560 Training loss: 1.2131 2.9658 sec/batch\n",
      "Epoch 13/20  Iteration 2306/3560 Training loss: 1.2131 2.7009 sec/batch\n",
      "Epoch 13/20  Iteration 2307/3560 Training loss: 1.2130 2.7109 sec/batch\n",
      "Epoch 13/20  Iteration 2308/3560 Training loss: 1.2129 2.6544 sec/batch\n",
      "Epoch 13/20  Iteration 2309/3560 Training loss: 1.2131 2.6582 sec/batch\n",
      "Epoch 13/20  Iteration 2310/3560 Training loss: 1.2131 2.6887 sec/batch\n",
      "Epoch 13/20  Iteration 2311/3560 Training loss: 1.2131 2.6783 sec/batch\n",
      "Epoch 13/20  Iteration 2312/3560 Training loss: 1.2130 3.2371 sec/batch\n",
      "Epoch 13/20  Iteration 2313/3560 Training loss: 1.2129 2.6647 sec/batch\n",
      "Epoch 13/20  Iteration 2314/3560 Training loss: 1.2131 2.6720 sec/batch\n",
      "Epoch 14/20  Iteration 2315/3560 Training loss: 1.3580 2.6445 sec/batch\n",
      "Epoch 14/20  Iteration 2316/3560 Training loss: 1.2946 2.6397 sec/batch\n",
      "Epoch 14/20  Iteration 2317/3560 Training loss: 1.2677 2.6659 sec/batch\n",
      "Epoch 14/20  Iteration 2318/3560 Training loss: 1.2605 2.6660 sec/batch\n",
      "Epoch 14/20  Iteration 2319/3560 Training loss: 1.2489 2.6603 sec/batch\n",
      "Epoch 14/20  Iteration 2320/3560 Training loss: 1.2377 2.6307 sec/batch\n",
      "Epoch 14/20  Iteration 2321/3560 Training loss: 1.2364 2.6864 sec/batch\n",
      "Epoch 14/20  Iteration 2322/3560 Training loss: 1.2324 2.6370 sec/batch\n",
      "Epoch 14/20  Iteration 2323/3560 Training loss: 1.2308 3.3560 sec/batch\n",
      "Epoch 14/20  Iteration 2324/3560 Training loss: 1.2298 2.6936 sec/batch\n",
      "Epoch 14/20  Iteration 2325/3560 Training loss: 1.2262 2.6684 sec/batch\n",
      "Epoch 14/20  Iteration 2326/3560 Training loss: 1.2248 2.6366 sec/batch\n",
      "Epoch 14/20  Iteration 2327/3560 Training loss: 1.2238 2.6597 sec/batch\n",
      "Epoch 14/20  Iteration 2328/3560 Training loss: 1.2237 2.6805 sec/batch\n",
      "Epoch 14/20  Iteration 2329/3560 Training loss: 1.2216 2.6951 sec/batch\n",
      "Epoch 14/20  Iteration 2330/3560 Training loss: 1.2190 2.6804 sec/batch\n",
      "Epoch 14/20  Iteration 2331/3560 Training loss: 1.2188 2.6414 sec/batch\n",
      "Epoch 14/20  Iteration 2332/3560 Training loss: 1.2194 2.6555 sec/batch\n",
      "Epoch 14/20  Iteration 2333/3560 Training loss: 1.2195 2.6883 sec/batch\n",
      "Epoch 14/20  Iteration 2334/3560 Training loss: 1.2201 2.6300 sec/batch\n",
      "Epoch 14/20  Iteration 2335/3560 Training loss: 1.2190 2.6628 sec/batch\n",
      "Epoch 14/20  Iteration 2336/3560 Training loss: 1.2192 3.2706 sec/batch\n",
      "Epoch 14/20  Iteration 2337/3560 Training loss: 1.2183 2.6879 sec/batch\n",
      "Epoch 14/20  Iteration 2338/3560 Training loss: 1.2182 2.6423 sec/batch\n",
      "Epoch 14/20  Iteration 2339/3560 Training loss: 1.2177 2.6165 sec/batch\n",
      "Epoch 14/20  Iteration 2340/3560 Training loss: 1.2158 2.7093 sec/batch\n",
      "Epoch 14/20  Iteration 2341/3560 Training loss: 1.2146 2.6695 sec/batch\n",
      "Epoch 14/20  Iteration 2342/3560 Training loss: 1.2150 2.6248 sec/batch\n",
      "Epoch 14/20  Iteration 2343/3560 Training loss: 1.2149 2.6768 sec/batch\n",
      "Epoch 14/20  Iteration 2344/3560 Training loss: 1.2151 2.6912 sec/batch\n",
      "Epoch 14/20  Iteration 2345/3560 Training loss: 1.2140 2.6407 sec/batch\n",
      "Epoch 14/20  Iteration 2346/3560 Training loss: 1.2129 2.6398 sec/batch\n",
      "Epoch 14/20  Iteration 2347/3560 Training loss: 1.2130 2.6894 sec/batch\n",
      "Epoch 14/20  Iteration 2348/3560 Training loss: 1.2129 2.6495 sec/batch\n",
      "Epoch 14/20  Iteration 2349/3560 Training loss: 1.2124 2.6768 sec/batch\n",
      "Epoch 14/20  Iteration 2350/3560 Training loss: 1.2120 2.6898 sec/batch\n",
      "Epoch 14/20  Iteration 2351/3560 Training loss: 1.2110 2.7121 sec/batch\n",
      "Epoch 14/20  Iteration 2352/3560 Training loss: 1.2097 2.6473 sec/batch\n",
      "Epoch 14/20  Iteration 2353/3560 Training loss: 1.2084 2.6449 sec/batch\n",
      "Epoch 14/20  Iteration 2354/3560 Training loss: 1.2080 2.6285 sec/batch\n",
      "Epoch 14/20  Iteration 2355/3560 Training loss: 1.2074 2.7014 sec/batch\n",
      "Epoch 14/20  Iteration 2356/3560 Training loss: 1.2081 2.6320 sec/batch\n",
      "Epoch 14/20  Iteration 2357/3560 Training loss: 1.2078 2.6581 sec/batch\n",
      "Epoch 14/20  Iteration 2358/3560 Training loss: 1.2071 2.6571 sec/batch\n",
      "Epoch 14/20  Iteration 2359/3560 Training loss: 1.2070 2.7474 sec/batch\n",
      "Epoch 14/20  Iteration 2360/3560 Training loss: 1.2062 2.6219 sec/batch\n",
      "Epoch 14/20  Iteration 2361/3560 Training loss: 1.2059 2.6419 sec/batch\n",
      "Epoch 14/20  Iteration 2362/3560 Training loss: 1.2057 3.1098 sec/batch\n",
      "Epoch 14/20  Iteration 2363/3560 Training loss: 1.2055 2.8174 sec/batch\n",
      "Epoch 14/20  Iteration 2364/3560 Training loss: 1.2056 2.6580 sec/batch\n",
      "Epoch 14/20  Iteration 2365/3560 Training loss: 1.2051 2.7099 sec/batch\n",
      "Epoch 14/20  Iteration 2366/3560 Training loss: 1.2055 2.7143 sec/batch\n",
      "Epoch 14/20  Iteration 2367/3560 Training loss: 1.2055 2.6581 sec/batch\n",
      "Epoch 14/20  Iteration 2368/3560 Training loss: 1.2057 2.6641 sec/batch\n",
      "Epoch 14/20  Iteration 2369/3560 Training loss: 1.2053 2.6532 sec/batch\n",
      "Epoch 14/20  Iteration 2370/3560 Training loss: 1.2056 2.6838 sec/batch\n",
      "Epoch 14/20  Iteration 2371/3560 Training loss: 1.2057 2.6392 sec/batch\n",
      "Epoch 14/20  Iteration 2372/3560 Training loss: 1.2055 2.6623 sec/batch\n",
      "Epoch 14/20  Iteration 2373/3560 Training loss: 1.2049 2.6403 sec/batch\n",
      "Epoch 14/20  Iteration 2374/3560 Training loss: 1.2055 2.7203 sec/batch\n",
      "Epoch 14/20  Iteration 2375/3560 Training loss: 1.2055 2.6420 sec/batch\n",
      "Epoch 14/20  Iteration 2376/3560 Training loss: 1.2062 2.6708 sec/batch\n",
      "Epoch 14/20  Iteration 2377/3560 Training loss: 1.2067 2.6893 sec/batch\n",
      "Epoch 14/20  Iteration 2378/3560 Training loss: 1.2069 2.6943 sec/batch\n",
      "Epoch 14/20  Iteration 2379/3560 Training loss: 1.2067 2.6746 sec/batch\n",
      "Epoch 14/20  Iteration 2380/3560 Training loss: 1.2069 2.6665 sec/batch\n",
      "Epoch 14/20  Iteration 2381/3560 Training loss: 1.2070 2.6816 sec/batch\n",
      "Epoch 14/20  Iteration 2382/3560 Training loss: 1.2067 2.7199 sec/batch\n",
      "Epoch 14/20  Iteration 2383/3560 Training loss: 1.2069 2.6578 sec/batch\n",
      "Epoch 14/20  Iteration 2384/3560 Training loss: 1.2067 2.8106 sec/batch\n",
      "Epoch 14/20  Iteration 2385/3560 Training loss: 1.2072 3.1087 sec/batch\n",
      "Epoch 14/20  Iteration 2386/3560 Training loss: 1.2074 2.6624 sec/batch\n",
      "Epoch 14/20  Iteration 2387/3560 Training loss: 1.2078 2.6372 sec/batch\n",
      "Epoch 14/20  Iteration 2388/3560 Training loss: 1.2074 2.6495 sec/batch\n",
      "Epoch 14/20  Iteration 2389/3560 Training loss: 1.2073 2.8186 sec/batch\n",
      "Epoch 14/20  Iteration 2390/3560 Training loss: 1.2073 2.6968 sec/batch\n",
      "Epoch 14/20  Iteration 2391/3560 Training loss: 1.2073 2.6890 sec/batch\n",
      "Epoch 14/20  Iteration 2392/3560 Training loss: 1.2071 2.7099 sec/batch\n",
      "Epoch 14/20  Iteration 2393/3560 Training loss: 1.2064 2.7267 sec/batch\n",
      "Epoch 14/20  Iteration 2394/3560 Training loss: 1.2063 2.7360 sec/batch\n",
      "Epoch 14/20  Iteration 2395/3560 Training loss: 1.2059 2.6256 sec/batch\n",
      "Epoch 14/20  Iteration 2396/3560 Training loss: 1.2058 2.7239 sec/batch\n",
      "Epoch 14/20  Iteration 2397/3560 Training loss: 1.2053 2.6503 sec/batch\n",
      "Epoch 14/20  Iteration 2398/3560 Training loss: 1.2052 2.6331 sec/batch\n",
      "Epoch 14/20  Iteration 2399/3560 Training loss: 1.2050 2.6932 sec/batch\n",
      "Epoch 14/20  Iteration 2400/3560 Training loss: 1.2048 2.7048 sec/batch\n",
      "Epoch 14/20  Iteration 2401/3560 Training loss: 1.2045 2.6643 sec/batch\n",
      "Epoch 14/20  Iteration 2402/3560 Training loss: 1.2043 2.6224 sec/batch\n",
      "Epoch 14/20  Iteration 2403/3560 Training loss: 1.2040 2.6577 sec/batch\n",
      "Epoch 14/20  Iteration 2404/3560 Training loss: 1.2040 2.7264 sec/batch\n",
      "Epoch 14/20  Iteration 2405/3560 Training loss: 1.2037 2.6599 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2406/3560 Training loss: 1.2036 2.6689 sec/batch\n",
      "Epoch 14/20  Iteration 2407/3560 Training loss: 1.2033 2.6921 sec/batch\n",
      "Epoch 14/20  Iteration 2408/3560 Training loss: 1.2029 2.7117 sec/batch\n",
      "Epoch 14/20  Iteration 2409/3560 Training loss: 1.2026 2.6497 sec/batch\n",
      "Epoch 14/20  Iteration 2410/3560 Training loss: 1.2026 2.6602 sec/batch\n",
      "Epoch 14/20  Iteration 2411/3560 Training loss: 1.2025 2.7324 sec/batch\n",
      "Epoch 14/20  Iteration 2412/3560 Training loss: 1.2022 2.6496 sec/batch\n",
      "Epoch 14/20  Iteration 2413/3560 Training loss: 1.2019 2.6665 sec/batch\n",
      "Epoch 14/20  Iteration 2414/3560 Training loss: 1.2015 2.6189 sec/batch\n",
      "Epoch 14/20  Iteration 2415/3560 Training loss: 1.2014 2.7510 sec/batch\n",
      "Epoch 14/20  Iteration 2416/3560 Training loss: 1.2013 2.6887 sec/batch\n",
      "Epoch 14/20  Iteration 2417/3560 Training loss: 1.2012 2.6674 sec/batch\n",
      "Epoch 14/20  Iteration 2418/3560 Training loss: 1.2009 2.6871 sec/batch\n",
      "Epoch 14/20  Iteration 2419/3560 Training loss: 1.2007 2.7440 sec/batch\n",
      "Epoch 14/20  Iteration 2420/3560 Training loss: 1.2006 2.6763 sec/batch\n",
      "Epoch 14/20  Iteration 2421/3560 Training loss: 1.2005 2.6357 sec/batch\n",
      "Epoch 14/20  Iteration 2422/3560 Training loss: 1.2005 2.6827 sec/batch\n",
      "Epoch 14/20  Iteration 2423/3560 Training loss: 1.2003 2.6701 sec/batch\n",
      "Epoch 14/20  Iteration 2424/3560 Training loss: 1.2004 2.6195 sec/batch\n",
      "Epoch 14/20  Iteration 2425/3560 Training loss: 1.2002 2.6617 sec/batch\n",
      "Epoch 14/20  Iteration 2426/3560 Training loss: 1.2001 2.6694 sec/batch\n",
      "Epoch 14/20  Iteration 2427/3560 Training loss: 1.2000 2.7478 sec/batch\n",
      "Epoch 14/20  Iteration 2428/3560 Training loss: 1.1998 2.6917 sec/batch\n",
      "Epoch 14/20  Iteration 2429/3560 Training loss: 1.1995 2.6609 sec/batch\n",
      "Epoch 14/20  Iteration 2430/3560 Training loss: 1.1992 2.6493 sec/batch\n",
      "Epoch 14/20  Iteration 2431/3560 Training loss: 1.1992 2.6719 sec/batch\n",
      "Epoch 14/20  Iteration 2432/3560 Training loss: 1.1992 3.1010 sec/batch\n",
      "Epoch 14/20  Iteration 2433/3560 Training loss: 1.1990 2.7823 sec/batch\n",
      "Epoch 14/20  Iteration 2434/3560 Training loss: 1.1990 2.6861 sec/batch\n",
      "Epoch 14/20  Iteration 2435/3560 Training loss: 1.1989 2.6551 sec/batch\n",
      "Epoch 14/20  Iteration 2436/3560 Training loss: 1.1985 2.6271 sec/batch\n",
      "Epoch 14/20  Iteration 2437/3560 Training loss: 1.1982 2.6084 sec/batch\n",
      "Epoch 14/20  Iteration 2438/3560 Training loss: 1.1981 2.7187 sec/batch\n",
      "Epoch 14/20  Iteration 2439/3560 Training loss: 1.1980 2.6794 sec/batch\n",
      "Epoch 14/20  Iteration 2440/3560 Training loss: 1.1976 2.7012 sec/batch\n",
      "Epoch 14/20  Iteration 2441/3560 Training loss: 1.1977 2.6645 sec/batch\n",
      "Epoch 14/20  Iteration 2442/3560 Training loss: 1.1975 2.7148 sec/batch\n",
      "Epoch 14/20  Iteration 2443/3560 Training loss: 1.1973 2.6573 sec/batch\n",
      "Epoch 14/20  Iteration 2444/3560 Training loss: 1.1969 2.6846 sec/batch\n",
      "Epoch 14/20  Iteration 2445/3560 Training loss: 1.1966 2.7306 sec/batch\n",
      "Epoch 14/20  Iteration 2446/3560 Training loss: 1.1964 2.6595 sec/batch\n",
      "Epoch 14/20  Iteration 2447/3560 Training loss: 1.1964 2.6501 sec/batch\n",
      "Epoch 14/20  Iteration 2448/3560 Training loss: 1.1964 2.6524 sec/batch\n",
      "Epoch 14/20  Iteration 2449/3560 Training loss: 1.1964 2.7078 sec/batch\n",
      "Epoch 14/20  Iteration 2450/3560 Training loss: 1.1964 2.6741 sec/batch\n",
      "Epoch 14/20  Iteration 2451/3560 Training loss: 1.1965 2.6658 sec/batch\n",
      "Epoch 14/20  Iteration 2452/3560 Training loss: 1.1966 2.6528 sec/batch\n",
      "Epoch 14/20  Iteration 2453/3560 Training loss: 1.1966 2.7130 sec/batch\n",
      "Epoch 14/20  Iteration 2454/3560 Training loss: 1.1967 2.6461 sec/batch\n",
      "Epoch 14/20  Iteration 2455/3560 Training loss: 1.1970 2.6046 sec/batch\n",
      "Epoch 14/20  Iteration 2456/3560 Training loss: 1.1970 2.7766 sec/batch\n",
      "Epoch 14/20  Iteration 2457/3560 Training loss: 1.1969 2.7590 sec/batch\n",
      "Epoch 14/20  Iteration 2458/3560 Training loss: 1.1971 2.6616 sec/batch\n",
      "Epoch 14/20  Iteration 2459/3560 Training loss: 1.1969 2.6742 sec/batch\n",
      "Epoch 14/20  Iteration 2460/3560 Training loss: 1.1971 2.6839 sec/batch\n",
      "Epoch 14/20  Iteration 2461/3560 Training loss: 1.1971 2.6890 sec/batch\n",
      "Epoch 14/20  Iteration 2462/3560 Training loss: 1.1974 2.6448 sec/batch\n",
      "Epoch 14/20  Iteration 2463/3560 Training loss: 1.1975 2.6604 sec/batch\n",
      "Epoch 14/20  Iteration 2464/3560 Training loss: 1.1973 2.7042 sec/batch\n",
      "Epoch 14/20  Iteration 2465/3560 Training loss: 1.1970 2.6602 sec/batch\n",
      "Epoch 14/20  Iteration 2466/3560 Training loss: 1.1968 2.6514 sec/batch\n",
      "Epoch 14/20  Iteration 2467/3560 Training loss: 1.1969 2.6870 sec/batch\n",
      "Epoch 14/20  Iteration 2468/3560 Training loss: 1.1968 2.7327 sec/batch\n",
      "Epoch 14/20  Iteration 2469/3560 Training loss: 1.1968 2.6501 sec/batch\n",
      "Epoch 14/20  Iteration 2470/3560 Training loss: 1.1967 2.6439 sec/batch\n",
      "Epoch 14/20  Iteration 2471/3560 Training loss: 1.1967 2.6633 sec/batch\n",
      "Epoch 14/20  Iteration 2472/3560 Training loss: 1.1966 2.7057 sec/batch\n",
      "Epoch 14/20  Iteration 2473/3560 Training loss: 1.1964 2.6385 sec/batch\n",
      "Epoch 14/20  Iteration 2474/3560 Training loss: 1.1965 2.6516 sec/batch\n",
      "Epoch 14/20  Iteration 2475/3560 Training loss: 1.1967 2.6447 sec/batch\n",
      "Epoch 14/20  Iteration 2476/3560 Training loss: 1.1967 2.7080 sec/batch\n",
      "Epoch 14/20  Iteration 2477/3560 Training loss: 1.1967 2.6437 sec/batch\n",
      "Epoch 14/20  Iteration 2478/3560 Training loss: 1.1967 2.6576 sec/batch\n",
      "Epoch 14/20  Iteration 2479/3560 Training loss: 1.1967 2.6583 sec/batch\n",
      "Epoch 14/20  Iteration 2480/3560 Training loss: 1.1966 2.7046 sec/batch\n",
      "Epoch 14/20  Iteration 2481/3560 Training loss: 1.1968 2.6499 sec/batch\n",
      "Epoch 14/20  Iteration 2482/3560 Training loss: 1.1971 2.6647 sec/batch\n",
      "Epoch 14/20  Iteration 2483/3560 Training loss: 1.1972 2.6646 sec/batch\n",
      "Epoch 14/20  Iteration 2484/3560 Training loss: 1.1972 2.6872 sec/batch\n",
      "Epoch 14/20  Iteration 2485/3560 Training loss: 1.1972 2.6076 sec/batch\n",
      "Epoch 14/20  Iteration 2486/3560 Training loss: 1.1971 2.7012 sec/batch\n",
      "Epoch 14/20  Iteration 2487/3560 Training loss: 1.1972 2.7220 sec/batch\n",
      "Epoch 14/20  Iteration 2488/3560 Training loss: 1.1973 2.6649 sec/batch\n",
      "Epoch 14/20  Iteration 2489/3560 Training loss: 1.1973 2.6533 sec/batch\n",
      "Epoch 14/20  Iteration 2490/3560 Training loss: 1.1972 2.6625 sec/batch\n",
      "Epoch 14/20  Iteration 2491/3560 Training loss: 1.1971 2.6867 sec/batch\n",
      "Epoch 14/20  Iteration 2492/3560 Training loss: 1.1973 2.6458 sec/batch\n",
      "Epoch 15/20  Iteration 2493/3560 Training loss: 1.3362 3.2038 sec/batch\n",
      "Epoch 15/20  Iteration 2494/3560 Training loss: 1.2740 2.6817 sec/batch\n",
      "Epoch 15/20  Iteration 2495/3560 Training loss: 1.2459 2.7350 sec/batch\n",
      "Epoch 15/20  Iteration 2496/3560 Training loss: 1.2399 2.6937 sec/batch\n",
      "Epoch 15/20  Iteration 2497/3560 Training loss: 1.2275 2.7066 sec/batch\n",
      "Epoch 15/20  Iteration 2498/3560 Training loss: 1.2143 2.6538 sec/batch\n",
      "Epoch 15/20  Iteration 2499/3560 Training loss: 1.2131 2.7050 sec/batch\n",
      "Epoch 15/20  Iteration 2500/3560 Training loss: 1.2103 2.6457 sec/batch\n",
      "Epoch 15/20  Iteration 2501/3560 Training loss: 1.2099 2.6710 sec/batch\n",
      "Epoch 15/20  Iteration 2502/3560 Training loss: 1.2083 2.6854 sec/batch\n",
      "Epoch 15/20  Iteration 2503/3560 Training loss: 1.2050 2.6583 sec/batch\n",
      "Epoch 15/20  Iteration 2504/3560 Training loss: 1.2037 2.6846 sec/batch\n",
      "Epoch 15/20  Iteration 2505/3560 Training loss: 1.2029 2.6764 sec/batch\n",
      "Epoch 15/20  Iteration 2506/3560 Training loss: 1.2038 2.7053 sec/batch\n",
      "Epoch 15/20  Iteration 2507/3560 Training loss: 1.2019 2.6867 sec/batch\n",
      "Epoch 15/20  Iteration 2508/3560 Training loss: 1.1998 2.6265 sec/batch\n",
      "Epoch 15/20  Iteration 2509/3560 Training loss: 1.1997 2.7048 sec/batch\n",
      "Epoch 15/20  Iteration 2510/3560 Training loss: 1.2003 2.6829 sec/batch\n",
      "Epoch 15/20  Iteration 2511/3560 Training loss: 1.2001 2.6493 sec/batch\n",
      "Epoch 15/20  Iteration 2512/3560 Training loss: 1.2007 2.6641 sec/batch\n",
      "Epoch 15/20  Iteration 2513/3560 Training loss: 1.2003 2.6622 sec/batch\n",
      "Epoch 15/20  Iteration 2514/3560 Training loss: 1.2003 2.6753 sec/batch\n",
      "Epoch 15/20  Iteration 2515/3560 Training loss: 1.1998 2.6705 sec/batch\n",
      "Epoch 15/20  Iteration 2516/3560 Training loss: 1.2002 2.6544 sec/batch\n",
      "Epoch 15/20  Iteration 2517/3560 Training loss: 1.1998 2.6804 sec/batch\n",
      "Epoch 15/20  Iteration 2518/3560 Training loss: 1.1978 2.6757 sec/batch\n",
      "Epoch 15/20  Iteration 2519/3560 Training loss: 1.1966 2.6395 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2520/3560 Training loss: 1.1974 2.6436 sec/batch\n",
      "Epoch 15/20  Iteration 2521/3560 Training loss: 1.1975 2.6709 sec/batch\n",
      "Epoch 15/20  Iteration 2522/3560 Training loss: 1.1980 2.7142 sec/batch\n",
      "Epoch 15/20  Iteration 2523/3560 Training loss: 1.1969 2.7676 sec/batch\n",
      "Epoch 15/20  Iteration 2524/3560 Training loss: 1.1961 2.6998 sec/batch\n",
      "Epoch 15/20  Iteration 2525/3560 Training loss: 1.1961 2.6834 sec/batch\n",
      "Epoch 15/20  Iteration 2526/3560 Training loss: 1.1962 2.6752 sec/batch\n",
      "Epoch 15/20  Iteration 2527/3560 Training loss: 1.1957 2.6710 sec/batch\n",
      "Epoch 15/20  Iteration 2528/3560 Training loss: 1.1955 2.6823 sec/batch\n",
      "Epoch 15/20  Iteration 2529/3560 Training loss: 1.1949 2.6467 sec/batch\n",
      "Epoch 15/20  Iteration 2530/3560 Training loss: 1.1939 2.6620 sec/batch\n",
      "Epoch 15/20  Iteration 2531/3560 Training loss: 1.1928 2.6554 sec/batch\n",
      "Epoch 15/20  Iteration 2532/3560 Training loss: 1.1925 2.6587 sec/batch\n",
      "Epoch 15/20  Iteration 2533/3560 Training loss: 1.1920 2.8181 sec/batch\n",
      "Epoch 15/20  Iteration 2534/3560 Training loss: 1.1929 4.1337 sec/batch\n",
      "Epoch 15/20  Iteration 2535/3560 Training loss: 1.1926 3.5870 sec/batch\n",
      "Epoch 15/20  Iteration 2536/3560 Training loss: 1.1923 3.2532 sec/batch\n",
      "Epoch 15/20  Iteration 2537/3560 Training loss: 1.1923 2.9040 sec/batch\n",
      "Epoch 15/20  Iteration 2538/3560 Training loss: 1.1916 2.8219 sec/batch\n",
      "Epoch 15/20  Iteration 2539/3560 Training loss: 1.1914 3.0286 sec/batch\n",
      "Epoch 15/20  Iteration 2540/3560 Training loss: 1.1911 2.8223 sec/batch\n",
      "Epoch 15/20  Iteration 2541/3560 Training loss: 1.1909 2.8362 sec/batch\n",
      "Epoch 15/20  Iteration 2542/3560 Training loss: 1.1912 2.8371 sec/batch\n",
      "Epoch 15/20  Iteration 2543/3560 Training loss: 1.1907 3.0213 sec/batch\n",
      "Epoch 15/20  Iteration 2544/3560 Training loss: 1.1915 3.4934 sec/batch\n",
      "Epoch 15/20  Iteration 2545/3560 Training loss: 1.1915 2.9170 sec/batch\n",
      "Epoch 15/20  Iteration 2546/3560 Training loss: 1.1918 2.8798 sec/batch\n",
      "Epoch 15/20  Iteration 2547/3560 Training loss: 1.1916 2.8020 sec/batch\n",
      "Epoch 15/20  Iteration 2548/3560 Training loss: 1.1919 2.8042 sec/batch\n",
      "Epoch 15/20  Iteration 2549/3560 Training loss: 1.1922 2.9395 sec/batch\n",
      "Epoch 15/20  Iteration 2550/3560 Training loss: 1.1921 2.9087 sec/batch\n",
      "Epoch 15/20  Iteration 2551/3560 Training loss: 1.1915 2.7953 sec/batch\n",
      "Epoch 15/20  Iteration 2552/3560 Training loss: 1.1922 2.8489 sec/batch\n",
      "Epoch 15/20  Iteration 2553/3560 Training loss: 1.1922 2.9014 sec/batch\n",
      "Epoch 15/20  Iteration 2554/3560 Training loss: 1.1929 3.6202 sec/batch\n",
      "Epoch 15/20  Iteration 2555/3560 Training loss: 1.1932 2.8039 sec/batch\n",
      "Epoch 15/20  Iteration 2556/3560 Training loss: 1.1931 2.9518 sec/batch\n",
      "Epoch 15/20  Iteration 2557/3560 Training loss: 1.1929 2.8031 sec/batch\n",
      "Epoch 15/20  Iteration 2558/3560 Training loss: 1.1929 2.8380 sec/batch\n",
      "Epoch 15/20  Iteration 2559/3560 Training loss: 1.1931 2.8690 sec/batch\n",
      "Epoch 15/20  Iteration 2560/3560 Training loss: 1.1929 2.9712 sec/batch\n",
      "Epoch 15/20  Iteration 2561/3560 Training loss: 1.1931 2.8387 sec/batch\n",
      "Epoch 15/20  Iteration 2562/3560 Training loss: 1.1930 2.8264 sec/batch\n",
      "Epoch 15/20  Iteration 2563/3560 Training loss: 1.1935 2.8512 sec/batch\n",
      "Epoch 15/20  Iteration 2564/3560 Training loss: 1.1938 3.7353 sec/batch\n",
      "Epoch 15/20  Iteration 2565/3560 Training loss: 1.1943 2.8023 sec/batch\n",
      "Epoch 15/20  Iteration 2566/3560 Training loss: 1.1938 2.8524 sec/batch\n",
      "Epoch 15/20  Iteration 2567/3560 Training loss: 1.1936 2.9196 sec/batch\n",
      "Epoch 15/20  Iteration 2568/3560 Training loss: 1.1938 2.8582 sec/batch\n",
      "Epoch 15/20  Iteration 2569/3560 Training loss: 1.1936 2.8416 sec/batch\n",
      "Epoch 15/20  Iteration 2570/3560 Training loss: 1.1934 2.8797 sec/batch\n",
      "Epoch 15/20  Iteration 2571/3560 Training loss: 1.1928 2.8587 sec/batch\n",
      "Epoch 15/20  Iteration 2572/3560 Training loss: 1.1927 2.8302 sec/batch\n",
      "Epoch 15/20  Iteration 2573/3560 Training loss: 1.1922 2.8596 sec/batch\n",
      "Epoch 15/20  Iteration 2574/3560 Training loss: 1.1920 3.2570 sec/batch\n",
      "Epoch 15/20  Iteration 2575/3560 Training loss: 1.1915 3.3097 sec/batch\n",
      "Epoch 15/20  Iteration 2576/3560 Training loss: 1.1914 2.8501 sec/batch\n",
      "Epoch 15/20  Iteration 2577/3560 Training loss: 1.1911 2.8883 sec/batch\n",
      "Epoch 15/20  Iteration 2578/3560 Training loss: 1.1910 2.7842 sec/batch\n",
      "Epoch 15/20  Iteration 2579/3560 Training loss: 1.1907 2.8613 sec/batch\n",
      "Epoch 15/20  Iteration 2580/3560 Training loss: 1.1905 2.9843 sec/batch\n",
      "Epoch 15/20  Iteration 2581/3560 Training loss: 1.1901 2.7981 sec/batch\n",
      "Epoch 15/20  Iteration 2582/3560 Training loss: 1.1902 2.8044 sec/batch\n",
      "Epoch 15/20  Iteration 2583/3560 Training loss: 1.1899 2.8937 sec/batch\n",
      "Epoch 15/20  Iteration 2584/3560 Training loss: 1.1897 3.1374 sec/batch\n",
      "Epoch 15/20  Iteration 2585/3560 Training loss: 1.1894 3.5910 sec/batch\n",
      "Epoch 15/20  Iteration 2586/3560 Training loss: 1.1891 2.9219 sec/batch\n",
      "Epoch 15/20  Iteration 2587/3560 Training loss: 1.1888 2.9015 sec/batch\n",
      "Epoch 15/20  Iteration 2588/3560 Training loss: 1.1888 2.7846 sec/batch\n",
      "Epoch 15/20  Iteration 2589/3560 Training loss: 1.1887 2.8561 sec/batch\n",
      "Epoch 15/20  Iteration 2590/3560 Training loss: 1.1884 2.8905 sec/batch\n",
      "Epoch 15/20  Iteration 2591/3560 Training loss: 1.1880 2.8386 sec/batch\n",
      "Epoch 15/20  Iteration 2592/3560 Training loss: 1.1877 2.8261 sec/batch\n",
      "Epoch 15/20  Iteration 2593/3560 Training loss: 1.1877 2.8549 sec/batch\n",
      "Epoch 15/20  Iteration 2594/3560 Training loss: 1.1875 3.0989 sec/batch\n",
      "Epoch 15/20  Iteration 2595/3560 Training loss: 1.1874 3.5890 sec/batch\n",
      "Epoch 15/20  Iteration 2596/3560 Training loss: 1.1873 2.8178 sec/batch\n",
      "Epoch 15/20  Iteration 2597/3560 Training loss: 1.1871 2.9555 sec/batch\n",
      "Epoch 15/20  Iteration 2598/3560 Training loss: 1.1869 2.8475 sec/batch\n",
      "Epoch 15/20  Iteration 2599/3560 Training loss: 1.1869 2.8418 sec/batch\n",
      "Epoch 15/20  Iteration 2600/3560 Training loss: 1.1869 2.8027 sec/batch\n",
      "Epoch 15/20  Iteration 2601/3560 Training loss: 1.1868 2.9492 sec/batch\n",
      "Epoch 15/20  Iteration 2602/3560 Training loss: 1.1868 2.8335 sec/batch\n",
      "Epoch 15/20  Iteration 2603/3560 Training loss: 1.1866 2.7737 sec/batch\n",
      "Epoch 15/20  Iteration 2604/3560 Training loss: 1.1865 2.9543 sec/batch\n",
      "Epoch 15/20  Iteration 2605/3560 Training loss: 1.1864 3.7133 sec/batch\n",
      "Epoch 15/20  Iteration 2606/3560 Training loss: 1.1863 2.7930 sec/batch\n",
      "Epoch 15/20  Iteration 2607/3560 Training loss: 1.1860 2.8846 sec/batch\n",
      "Epoch 15/20  Iteration 2608/3560 Training loss: 1.1858 2.9056 sec/batch\n",
      "Epoch 15/20  Iteration 2609/3560 Training loss: 1.1857 2.8404 sec/batch\n",
      "Epoch 15/20  Iteration 2610/3560 Training loss: 1.1857 2.8602 sec/batch\n",
      "Epoch 15/20  Iteration 2611/3560 Training loss: 1.1856 2.9575 sec/batch\n",
      "Epoch 15/20  Iteration 2612/3560 Training loss: 1.1854 2.8474 sec/batch\n",
      "Epoch 15/20  Iteration 2613/3560 Training loss: 1.1853 2.8895 sec/batch\n",
      "Epoch 15/20  Iteration 2614/3560 Training loss: 1.1850 2.8480 sec/batch\n",
      "Epoch 15/20  Iteration 2615/3560 Training loss: 1.1846 3.7631 sec/batch\n",
      "Epoch 15/20  Iteration 2616/3560 Training loss: 1.1845 2.9119 sec/batch\n",
      "Epoch 15/20  Iteration 2617/3560 Training loss: 1.1844 2.8782 sec/batch\n",
      "Epoch 15/20  Iteration 2618/3560 Training loss: 1.1840 2.8780 sec/batch\n",
      "Epoch 15/20  Iteration 2619/3560 Training loss: 1.1841 2.8388 sec/batch\n",
      "Epoch 15/20  Iteration 2620/3560 Training loss: 1.1840 2.8333 sec/batch\n",
      "Epoch 15/20  Iteration 2621/3560 Training loss: 1.1838 2.9329 sec/batch\n",
      "Epoch 15/20  Iteration 2622/3560 Training loss: 1.1834 2.7875 sec/batch\n",
      "Epoch 15/20  Iteration 2623/3560 Training loss: 1.1830 2.8512 sec/batch\n",
      "Epoch 15/20  Iteration 2624/3560 Training loss: 1.1829 2.9128 sec/batch\n",
      "Epoch 15/20  Iteration 2625/3560 Training loss: 1.1830 3.7583 sec/batch\n",
      "Epoch 15/20  Iteration 2626/3560 Training loss: 1.1829 2.8941 sec/batch\n",
      "Epoch 15/20  Iteration 2627/3560 Training loss: 1.1829 2.8815 sec/batch\n",
      "Epoch 15/20  Iteration 2628/3560 Training loss: 1.1829 2.9033 sec/batch\n",
      "Epoch 15/20  Iteration 2629/3560 Training loss: 1.1830 2.8260 sec/batch\n",
      "Epoch 15/20  Iteration 2630/3560 Training loss: 1.1831 2.8304 sec/batch\n",
      "Epoch 15/20  Iteration 2631/3560 Training loss: 1.1831 2.9524 sec/batch\n",
      "Epoch 15/20  Iteration 2632/3560 Training loss: 1.1831 2.8768 sec/batch\n",
      "Epoch 15/20  Iteration 2633/3560 Training loss: 1.1834 2.7766 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2634/3560 Training loss: 1.1835 2.8810 sec/batch\n",
      "Epoch 15/20  Iteration 2635/3560 Training loss: 1.1834 3.5899 sec/batch\n",
      "Epoch 15/20  Iteration 2636/3560 Training loss: 1.1836 3.0065 sec/batch\n",
      "Epoch 15/20  Iteration 2637/3560 Training loss: 1.1835 2.8520 sec/batch\n",
      "Epoch 15/20  Iteration 2638/3560 Training loss: 1.1837 2.9439 sec/batch\n",
      "Epoch 15/20  Iteration 2639/3560 Training loss: 1.1837 2.9017 sec/batch\n",
      "Epoch 15/20  Iteration 2640/3560 Training loss: 1.1839 2.9295 sec/batch\n",
      "Epoch 15/20  Iteration 2641/3560 Training loss: 1.1840 2.8304 sec/batch\n",
      "Epoch 15/20  Iteration 2642/3560 Training loss: 1.1839 2.9259 sec/batch\n",
      "Epoch 15/20  Iteration 2643/3560 Training loss: 1.1836 2.9406 sec/batch\n",
      "Epoch 15/20  Iteration 2644/3560 Training loss: 1.1835 2.8844 sec/batch\n",
      "Epoch 15/20  Iteration 2645/3560 Training loss: 1.1836 3.8029 sec/batch\n",
      "Epoch 15/20  Iteration 2646/3560 Training loss: 1.1835 3.0722 sec/batch\n",
      "Epoch 15/20  Iteration 2647/3560 Training loss: 1.1835 2.8970 sec/batch\n",
      "Epoch 15/20  Iteration 2648/3560 Training loss: 1.1834 2.8566 sec/batch\n",
      "Epoch 15/20  Iteration 2649/3560 Training loss: 1.1834 2.8656 sec/batch\n",
      "Epoch 15/20  Iteration 2650/3560 Training loss: 1.1833 2.8318 sec/batch\n",
      "Epoch 15/20  Iteration 2651/3560 Training loss: 1.1831 2.8315 sec/batch\n",
      "Epoch 15/20  Iteration 2652/3560 Training loss: 1.1832 2.8084 sec/batch\n",
      "Epoch 15/20  Iteration 2653/3560 Training loss: 1.1834 2.9013 sec/batch\n",
      "Epoch 15/20  Iteration 2654/3560 Training loss: 1.1834 2.8532 sec/batch\n",
      "Epoch 15/20  Iteration 2655/3560 Training loss: 1.1833 2.9884 sec/batch\n",
      "Epoch 15/20  Iteration 2656/3560 Training loss: 1.1832 3.6293 sec/batch\n",
      "Epoch 15/20  Iteration 2657/3560 Training loss: 1.1832 2.8441 sec/batch\n",
      "Epoch 15/20  Iteration 2658/3560 Training loss: 1.1831 2.8437 sec/batch\n",
      "Epoch 15/20  Iteration 2659/3560 Training loss: 1.1832 2.8460 sec/batch\n",
      "Epoch 15/20  Iteration 2660/3560 Training loss: 1.1835 2.7825 sec/batch\n",
      "Epoch 15/20  Iteration 2661/3560 Training loss: 1.1836 2.8719 sec/batch\n",
      "Epoch 15/20  Iteration 2662/3560 Training loss: 1.1837 2.8738 sec/batch\n",
      "Epoch 15/20  Iteration 2663/3560 Training loss: 1.1836 2.8477 sec/batch\n",
      "Epoch 15/20  Iteration 2664/3560 Training loss: 1.1835 2.8558 sec/batch\n",
      "Epoch 15/20  Iteration 2665/3560 Training loss: 1.1837 3.0725 sec/batch\n",
      "Epoch 15/20  Iteration 2666/3560 Training loss: 1.1837 3.5805 sec/batch\n",
      "Epoch 15/20  Iteration 2667/3560 Training loss: 1.1838 2.8060 sec/batch\n",
      "Epoch 15/20  Iteration 2668/3560 Training loss: 1.1837 2.8569 sec/batch\n",
      "Epoch 15/20  Iteration 2669/3560 Training loss: 1.1836 2.8745 sec/batch\n",
      "Epoch 15/20  Iteration 2670/3560 Training loss: 1.1837 2.8000 sec/batch\n",
      "Epoch 16/20  Iteration 2671/3560 Training loss: 1.3291 2.8250 sec/batch\n",
      "Epoch 16/20  Iteration 2672/3560 Training loss: 1.2671 2.9019 sec/batch\n",
      "Epoch 16/20  Iteration 2673/3560 Training loss: 1.2422 2.8634 sec/batch\n",
      "Epoch 16/20  Iteration 2674/3560 Training loss: 1.2341 2.8248 sec/batch\n",
      "Epoch 16/20  Iteration 2675/3560 Training loss: 1.2235 2.9079 sec/batch\n",
      "Epoch 16/20  Iteration 2676/3560 Training loss: 1.2118 3.7170 sec/batch\n",
      "Epoch 16/20  Iteration 2677/3560 Training loss: 1.2105 2.8343 sec/batch\n",
      "Epoch 16/20  Iteration 2678/3560 Training loss: 1.2074 2.8202 sec/batch\n",
      "Epoch 16/20  Iteration 2679/3560 Training loss: 1.2053 2.9248 sec/batch\n",
      "Epoch 16/20  Iteration 2680/3560 Training loss: 1.2032 2.8487 sec/batch\n",
      "Epoch 16/20  Iteration 2681/3560 Training loss: 1.1997 2.8396 sec/batch\n",
      "Epoch 16/20  Iteration 2682/3560 Training loss: 1.1986 2.7957 sec/batch\n",
      "Epoch 16/20  Iteration 2683/3560 Training loss: 1.1980 2.9548 sec/batch\n",
      "Epoch 16/20  Iteration 2684/3560 Training loss: 1.1982 2.8352 sec/batch\n",
      "Epoch 16/20  Iteration 2685/3560 Training loss: 1.1961 2.7963 sec/batch\n",
      "Epoch 16/20  Iteration 2686/3560 Training loss: 1.1938 3.7809 sec/batch\n",
      "Epoch 16/20  Iteration 2687/3560 Training loss: 1.1936 3.0428 sec/batch\n",
      "Epoch 16/20  Iteration 2688/3560 Training loss: 1.1947 2.8262 sec/batch\n",
      "Epoch 16/20  Iteration 2689/3560 Training loss: 1.1945 2.8513 sec/batch\n",
      "Epoch 16/20  Iteration 2690/3560 Training loss: 1.1951 2.8462 sec/batch\n",
      "Epoch 16/20  Iteration 2691/3560 Training loss: 1.1943 2.8594 sec/batch\n",
      "Epoch 16/20  Iteration 2692/3560 Training loss: 1.1942 2.8282 sec/batch\n",
      "Epoch 16/20  Iteration 2693/3560 Training loss: 1.1931 2.8677 sec/batch\n",
      "Epoch 16/20  Iteration 2694/3560 Training loss: 1.1928 2.8254 sec/batch\n",
      "Epoch 16/20  Iteration 2695/3560 Training loss: 1.1923 2.8354 sec/batch\n",
      "Epoch 16/20  Iteration 2696/3560 Training loss: 1.1904 3.2801 sec/batch\n",
      "Epoch 16/20  Iteration 2697/3560 Training loss: 1.1893 3.4398 sec/batch\n",
      "Epoch 16/20  Iteration 2698/3560 Training loss: 1.1896 2.8937 sec/batch\n",
      "Epoch 16/20  Iteration 2699/3560 Training loss: 1.1894 2.9118 sec/batch\n",
      "Epoch 16/20  Iteration 2700/3560 Training loss: 1.1896 2.8242 sec/batch\n",
      "Epoch 16/20  Iteration 2701/3560 Training loss: 1.1886 2.8203 sec/batch\n",
      "Epoch 16/20  Iteration 2702/3560 Training loss: 1.1875 2.9193 sec/batch\n",
      "Epoch 16/20  Iteration 2703/3560 Training loss: 1.1876 2.8195 sec/batch\n",
      "Epoch 16/20  Iteration 2704/3560 Training loss: 1.1876 2.8296 sec/batch\n",
      "Epoch 16/20  Iteration 2705/3560 Training loss: 1.1871 2.8778 sec/batch\n",
      "Epoch 16/20  Iteration 2706/3560 Training loss: 1.1869 3.3303 sec/batch\n",
      "Epoch 16/20  Iteration 2707/3560 Training loss: 1.1861 3.3369 sec/batch\n",
      "Epoch 16/20  Iteration 2708/3560 Training loss: 1.1850 2.8608 sec/batch\n",
      "Epoch 16/20  Iteration 2709/3560 Training loss: 1.1838 2.8472 sec/batch\n",
      "Epoch 16/20  Iteration 2710/3560 Training loss: 1.1835 2.8635 sec/batch\n",
      "Epoch 16/20  Iteration 2711/3560 Training loss: 1.1827 2.8883 sec/batch\n",
      "Epoch 16/20  Iteration 2712/3560 Training loss: 1.1833 2.8416 sec/batch\n",
      "Epoch 16/20  Iteration 2713/3560 Training loss: 1.1831 2.8848 sec/batch\n",
      "Epoch 16/20  Iteration 2714/3560 Training loss: 1.1824 2.8836 sec/batch\n",
      "Epoch 16/20  Iteration 2715/3560 Training loss: 1.1824 2.8095 sec/batch\n",
      "Epoch 16/20  Iteration 2716/3560 Training loss: 1.1818 3.0207 sec/batch\n",
      "Epoch 16/20  Iteration 2717/3560 Training loss: 1.1815 3.7135 sec/batch\n",
      "Epoch 16/20  Iteration 2718/3560 Training loss: 1.1812 2.8924 sec/batch\n",
      "Epoch 16/20  Iteration 2719/3560 Training loss: 1.1810 2.8161 sec/batch\n",
      "Epoch 16/20  Iteration 2720/3560 Training loss: 1.1811 2.8594 sec/batch\n",
      "Epoch 16/20  Iteration 2721/3560 Training loss: 1.1806 2.8861 sec/batch\n",
      "Epoch 16/20  Iteration 2722/3560 Training loss: 1.1811 2.7969 sec/batch\n",
      "Epoch 16/20  Iteration 2723/3560 Training loss: 1.1811 2.8313 sec/batch\n",
      "Epoch 16/20  Iteration 2724/3560 Training loss: 1.1812 2.9724 sec/batch\n",
      "Epoch 16/20  Iteration 2725/3560 Training loss: 1.1809 2.8306 sec/batch\n",
      "Epoch 16/20  Iteration 2726/3560 Training loss: 1.1809 2.9455 sec/batch\n",
      "Epoch 16/20  Iteration 2727/3560 Training loss: 1.1810 3.8940 sec/batch\n",
      "Epoch 16/20  Iteration 2728/3560 Training loss: 1.1808 2.9634 sec/batch\n",
      "Epoch 16/20  Iteration 2729/3560 Training loss: 1.1802 2.9187 sec/batch\n",
      "Epoch 16/20  Iteration 2730/3560 Training loss: 1.1807 2.8295 sec/batch\n",
      "Epoch 16/20  Iteration 2731/3560 Training loss: 1.1806 3.0289 sec/batch\n",
      "Epoch 16/20  Iteration 2732/3560 Training loss: 1.1814 2.9611 sec/batch\n",
      "Epoch 16/20  Iteration 2733/3560 Training loss: 1.1817 2.9179 sec/batch\n",
      "Epoch 16/20  Iteration 2734/3560 Training loss: 1.1818 2.8548 sec/batch\n",
      "Epoch 16/20  Iteration 2735/3560 Training loss: 1.1817 2.9182 sec/batch\n",
      "Epoch 16/20  Iteration 2736/3560 Training loss: 1.1817 2.8914 sec/batch\n",
      "Epoch 16/20  Iteration 2737/3560 Training loss: 1.1819 3.6570 sec/batch\n",
      "Epoch 16/20  Iteration 2738/3560 Training loss: 1.1816 2.9445 sec/batch\n",
      "Epoch 16/20  Iteration 2739/3560 Training loss: 1.1817 2.9605 sec/batch\n",
      "Epoch 16/20  Iteration 2740/3560 Training loss: 1.1815 2.8755 sec/batch\n",
      "Epoch 16/20  Iteration 2741/3560 Training loss: 1.1819 2.8763 sec/batch\n",
      "Epoch 16/20  Iteration 2742/3560 Training loss: 1.1820 2.8387 sec/batch\n",
      "Epoch 16/20  Iteration 2743/3560 Training loss: 1.1825 2.8256 sec/batch\n",
      "Epoch 16/20  Iteration 2744/3560 Training loss: 1.1821 2.8221 sec/batch\n",
      "Epoch 16/20  Iteration 2745/3560 Training loss: 1.1819 2.9403 sec/batch\n",
      "Epoch 16/20  Iteration 2746/3560 Training loss: 1.1820 2.8827 sec/batch\n",
      "Epoch 16/20  Iteration 2747/3560 Training loss: 1.1820 3.7941 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2748/3560 Training loss: 1.1818 2.8597 sec/batch\n",
      "Epoch 16/20  Iteration 2749/3560 Training loss: 1.1812 2.8209 sec/batch\n",
      "Epoch 16/20  Iteration 2750/3560 Training loss: 1.1811 2.8716 sec/batch\n",
      "Epoch 16/20  Iteration 2751/3560 Training loss: 1.1808 2.8849 sec/batch\n",
      "Epoch 16/20  Iteration 2752/3560 Training loss: 1.1807 2.8626 sec/batch\n",
      "Epoch 16/20  Iteration 2753/3560 Training loss: 1.1802 2.8876 sec/batch\n",
      "Epoch 16/20  Iteration 2754/3560 Training loss: 1.1802 2.9039 sec/batch\n",
      "Epoch 16/20  Iteration 2755/3560 Training loss: 1.1799 2.8649 sec/batch\n",
      "Epoch 16/20  Iteration 2756/3560 Training loss: 1.1797 2.8578 sec/batch\n",
      "Epoch 16/20  Iteration 2757/3560 Training loss: 1.1794 3.7218 sec/batch\n",
      "Epoch 16/20  Iteration 2758/3560 Training loss: 1.1792 2.9033 sec/batch\n",
      "Epoch 16/20  Iteration 2759/3560 Training loss: 1.1789 2.8408 sec/batch\n",
      "Epoch 16/20  Iteration 2760/3560 Training loss: 1.1789 2.8487 sec/batch\n",
      "Epoch 16/20  Iteration 2761/3560 Training loss: 1.1786 2.8336 sec/batch\n",
      "Epoch 16/20  Iteration 2762/3560 Training loss: 1.1786 2.9354 sec/batch\n",
      "Epoch 16/20  Iteration 2763/3560 Training loss: 1.1782 2.8482 sec/batch\n",
      "Epoch 16/20  Iteration 2764/3560 Training loss: 1.1777 2.8014 sec/batch\n",
      "Epoch 16/20  Iteration 2765/3560 Training loss: 1.1775 2.9618 sec/batch\n",
      "Epoch 16/20  Iteration 2766/3560 Training loss: 1.1774 2.8716 sec/batch\n",
      "Epoch 16/20  Iteration 2767/3560 Training loss: 1.1773 3.7949 sec/batch\n",
      "Epoch 16/20  Iteration 2768/3560 Training loss: 1.1770 2.9227 sec/batch\n",
      "Epoch 16/20  Iteration 2769/3560 Training loss: 1.1766 2.8803 sec/batch\n",
      "Epoch 16/20  Iteration 2770/3560 Training loss: 1.1762 2.8429 sec/batch\n",
      "Epoch 16/20  Iteration 2771/3560 Training loss: 1.1761 2.7960 sec/batch\n",
      "Epoch 16/20  Iteration 2772/3560 Training loss: 1.1760 2.9832 sec/batch\n",
      "Epoch 16/20  Iteration 2773/3560 Training loss: 1.1758 2.8416 sec/batch\n",
      "Epoch 16/20  Iteration 2774/3560 Training loss: 1.1756 2.8112 sec/batch\n",
      "Epoch 16/20  Iteration 2775/3560 Training loss: 1.1755 2.8958 sec/batch\n",
      "Epoch 16/20  Iteration 2776/3560 Training loss: 1.1752 2.8558 sec/batch\n",
      "Epoch 16/20  Iteration 2777/3560 Training loss: 1.1752 3.1813 sec/batch\n",
      "Epoch 16/20  Iteration 2778/3560 Training loss: 1.1752 3.4774 sec/batch\n",
      "Epoch 16/20  Iteration 2779/3560 Training loss: 1.1750 2.8759 sec/batch\n",
      "Epoch 16/20  Iteration 2780/3560 Training loss: 1.1750 2.9084 sec/batch\n",
      "Epoch 16/20  Iteration 2781/3560 Training loss: 1.1748 2.8229 sec/batch\n",
      "Epoch 16/20  Iteration 2782/3560 Training loss: 1.1747 2.8369 sec/batch\n",
      "Epoch 16/20  Iteration 2783/3560 Training loss: 1.1746 2.8623 sec/batch\n",
      "Epoch 16/20  Iteration 2784/3560 Training loss: 1.1745 2.8569 sec/batch\n",
      "Epoch 16/20  Iteration 2785/3560 Training loss: 1.1743 2.8231 sec/batch\n",
      "Epoch 16/20  Iteration 2786/3560 Training loss: 1.1740 2.8892 sec/batch\n",
      "Epoch 16/20  Iteration 2787/3560 Training loss: 1.1741 3.3965 sec/batch\n",
      "Epoch 16/20  Iteration 2788/3560 Training loss: 1.1741 3.1027 sec/batch\n",
      "Epoch 16/20  Iteration 2789/3560 Training loss: 1.1738 2.8548 sec/batch\n",
      "Epoch 16/20  Iteration 2790/3560 Training loss: 1.1738 2.8174 sec/batch\n",
      "Epoch 16/20  Iteration 2791/3560 Training loss: 1.1736 2.8744 sec/batch\n",
      "Epoch 16/20  Iteration 2792/3560 Training loss: 1.1733 2.9091 sec/batch\n",
      "Epoch 16/20  Iteration 2793/3560 Training loss: 1.1731 2.8410 sec/batch\n",
      "Epoch 16/20  Iteration 2794/3560 Training loss: 1.1730 2.8393 sec/batch\n",
      "Epoch 16/20  Iteration 2795/3560 Training loss: 1.1729 2.8528 sec/batch\n",
      "Epoch 16/20  Iteration 2796/3560 Training loss: 1.1725 2.8794 sec/batch\n",
      "Epoch 16/20  Iteration 2797/3560 Training loss: 1.1725 3.0318 sec/batch\n",
      "Epoch 16/20  Iteration 2798/3560 Training loss: 1.1725 3.5837 sec/batch\n",
      "Epoch 16/20  Iteration 2799/3560 Training loss: 1.1722 2.9479 sec/batch\n",
      "Epoch 16/20  Iteration 2800/3560 Training loss: 1.1719 2.8283 sec/batch\n",
      "Epoch 16/20  Iteration 2801/3560 Training loss: 1.1715 2.8121 sec/batch\n",
      "Epoch 16/20  Iteration 2802/3560 Training loss: 1.1713 2.9187 sec/batch\n",
      "Epoch 16/20  Iteration 2803/3560 Training loss: 1.1714 2.8833 sec/batch\n",
      "Epoch 16/20  Iteration 2804/3560 Training loss: 1.1714 2.8034 sec/batch\n",
      "Epoch 16/20  Iteration 2805/3560 Training loss: 1.1714 2.8440 sec/batch\n",
      "Epoch 16/20  Iteration 2806/3560 Training loss: 1.1714 2.9357 sec/batch\n",
      "Epoch 16/20  Iteration 2807/3560 Training loss: 1.1714 2.8255 sec/batch\n",
      "Epoch 16/20  Iteration 2808/3560 Training loss: 1.1715 3.6205 sec/batch\n",
      "Epoch 16/20  Iteration 2809/3560 Training loss: 1.1715 2.9105 sec/batch\n",
      "Epoch 16/20  Iteration 2810/3560 Training loss: 1.1715 2.9463 sec/batch\n",
      "Epoch 16/20  Iteration 2811/3560 Training loss: 1.1719 2.8810 sec/batch\n",
      "Epoch 16/20  Iteration 2812/3560 Training loss: 1.1719 2.8249 sec/batch\n",
      "Epoch 16/20  Iteration 2813/3560 Training loss: 1.1718 2.9422 sec/batch\n",
      "Epoch 16/20  Iteration 2814/3560 Training loss: 1.1720 2.8634 sec/batch\n",
      "Epoch 16/20  Iteration 2815/3560 Training loss: 1.1718 2.8507 sec/batch\n",
      "Epoch 16/20  Iteration 2816/3560 Training loss: 1.1719 2.8305 sec/batch\n",
      "Epoch 16/20  Iteration 2817/3560 Training loss: 1.1719 2.8770 sec/batch\n",
      "Epoch 16/20  Iteration 2818/3560 Training loss: 1.1721 3.5753 sec/batch\n",
      "Epoch 16/20  Iteration 2819/3560 Training loss: 1.1722 3.0509 sec/batch\n",
      "Epoch 16/20  Iteration 2820/3560 Training loss: 1.1721 2.9157 sec/batch\n",
      "Epoch 16/20  Iteration 2821/3560 Training loss: 1.1718 2.9010 sec/batch\n",
      "Epoch 16/20  Iteration 2822/3560 Training loss: 1.1717 2.8202 sec/batch\n",
      "Epoch 16/20  Iteration 2823/3560 Training loss: 1.1717 2.8949 sec/batch\n",
      "Epoch 16/20  Iteration 2824/3560 Training loss: 1.1717 2.8890 sec/batch\n",
      "Epoch 16/20  Iteration 2825/3560 Training loss: 1.1717 2.7870 sec/batch\n",
      "Epoch 16/20  Iteration 2826/3560 Training loss: 1.1716 2.8662 sec/batch\n",
      "Epoch 16/20  Iteration 2827/3560 Training loss: 1.1716 2.9413 sec/batch\n",
      "Epoch 16/20  Iteration 2828/3560 Training loss: 1.1715 3.8342 sec/batch\n",
      "Epoch 16/20  Iteration 2829/3560 Training loss: 1.1713 2.8723 sec/batch\n",
      "Epoch 16/20  Iteration 2830/3560 Training loss: 1.1714 2.9274 sec/batch\n",
      "Epoch 16/20  Iteration 2831/3560 Training loss: 1.1715 2.8314 sec/batch\n",
      "Epoch 16/20  Iteration 2832/3560 Training loss: 1.1715 2.8630 sec/batch\n",
      "Epoch 16/20  Iteration 2833/3560 Training loss: 1.1714 2.8920 sec/batch\n",
      "Epoch 16/20  Iteration 2834/3560 Training loss: 1.1715 2.8838 sec/batch\n",
      "Epoch 16/20  Iteration 2835/3560 Training loss: 1.1714 2.7862 sec/batch\n",
      "Epoch 16/20  Iteration 2836/3560 Training loss: 1.1714 2.8503 sec/batch\n",
      "Epoch 16/20  Iteration 2837/3560 Training loss: 1.1715 2.9124 sec/batch\n",
      "Epoch 16/20  Iteration 2838/3560 Training loss: 1.1718 3.5557 sec/batch\n",
      "Epoch 16/20  Iteration 2839/3560 Training loss: 1.1719 2.9772 sec/batch\n",
      "Epoch 16/20  Iteration 2840/3560 Training loss: 1.1720 2.9273 sec/batch\n",
      "Epoch 16/20  Iteration 2841/3560 Training loss: 1.1718 2.8369 sec/batch\n",
      "Epoch 16/20  Iteration 2842/3560 Training loss: 1.1717 2.7626 sec/batch\n",
      "Epoch 16/20  Iteration 2843/3560 Training loss: 1.1719 2.8655 sec/batch\n",
      "Epoch 16/20  Iteration 2844/3560 Training loss: 1.1720 2.8712 sec/batch\n",
      "Epoch 16/20  Iteration 2845/3560 Training loss: 1.1720 2.8449 sec/batch\n",
      "Epoch 16/20  Iteration 2846/3560 Training loss: 1.1719 2.7717 sec/batch\n",
      "Epoch 16/20  Iteration 2847/3560 Training loss: 1.1718 2.9108 sec/batch\n",
      "Epoch 16/20  Iteration 2848/3560 Training loss: 1.1721 3.5757 sec/batch\n",
      "Epoch 17/20  Iteration 2849/3560 Training loss: 1.2987 3.0730 sec/batch\n",
      "Epoch 17/20  Iteration 2850/3560 Training loss: 1.2444 2.8572 sec/batch\n",
      "Epoch 17/20  Iteration 2851/3560 Training loss: 1.2272 2.9397 sec/batch\n",
      "Epoch 17/20  Iteration 2852/3560 Training loss: 1.2169 2.8190 sec/batch\n",
      "Epoch 17/20  Iteration 2853/3560 Training loss: 1.2032 2.8232 sec/batch\n",
      "Epoch 17/20  Iteration 2854/3560 Training loss: 1.1909 2.9441 sec/batch\n",
      "Epoch 17/20  Iteration 2855/3560 Training loss: 1.1901 2.8271 sec/batch\n",
      "Epoch 17/20  Iteration 2856/3560 Training loss: 1.1876 2.8453 sec/batch\n",
      "Epoch 17/20  Iteration 2857/3560 Training loss: 1.1857 2.8484 sec/batch\n",
      "Epoch 17/20  Iteration 2858/3560 Training loss: 1.1837 2.9222 sec/batch\n",
      "Epoch 17/20  Iteration 2859/3560 Training loss: 1.1807 3.6372 sec/batch\n",
      "Epoch 17/20  Iteration 2860/3560 Training loss: 1.1806 2.8646 sec/batch\n",
      "Epoch 17/20  Iteration 2861/3560 Training loss: 1.1811 2.9407 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 2862/3560 Training loss: 1.1809 2.8541 sec/batch\n",
      "Epoch 17/20  Iteration 2863/3560 Training loss: 1.1796 2.8559 sec/batch\n",
      "Epoch 17/20  Iteration 2864/3560 Training loss: 1.1773 2.8600 sec/batch\n",
      "Epoch 17/20  Iteration 2865/3560 Training loss: 1.1777 2.9221 sec/batch\n",
      "Epoch 17/20  Iteration 2866/3560 Training loss: 1.1779 2.8843 sec/batch\n",
      "Epoch 17/20  Iteration 2867/3560 Training loss: 1.1779 2.9234 sec/batch\n",
      "Epoch 17/20  Iteration 2868/3560 Training loss: 1.1792 3.0120 sec/batch\n",
      "Epoch 17/20  Iteration 2869/3560 Training loss: 1.1784 3.7471 sec/batch\n",
      "Epoch 17/20  Iteration 2870/3560 Training loss: 1.1789 2.8652 sec/batch\n",
      "Epoch 17/20  Iteration 2871/3560 Training loss: 1.1783 2.8381 sec/batch\n",
      "Epoch 17/20  Iteration 2872/3560 Training loss: 1.1784 2.8849 sec/batch\n",
      "Epoch 17/20  Iteration 2873/3560 Training loss: 1.1776 2.8356 sec/batch\n",
      "Epoch 17/20  Iteration 2874/3560 Training loss: 1.1759 2.8348 sec/batch\n",
      "Epoch 17/20  Iteration 2875/3560 Training loss: 1.1747 2.9722 sec/batch\n",
      "Epoch 17/20  Iteration 2876/3560 Training loss: 1.1751 2.8484 sec/batch\n",
      "Epoch 17/20  Iteration 2877/3560 Training loss: 1.1747 2.8667 sec/batch\n",
      "Epoch 17/20  Iteration 2878/3560 Training loss: 1.1751 2.9639 sec/batch\n",
      "Epoch 17/20  Iteration 2879/3560 Training loss: 1.1742 3.7157 sec/batch\n",
      "Epoch 17/20  Iteration 2880/3560 Training loss: 1.1730 2.9075 sec/batch\n",
      "Epoch 17/20  Iteration 2881/3560 Training loss: 1.1732 2.9320 sec/batch\n",
      "Epoch 17/20  Iteration 2882/3560 Training loss: 1.1733 2.8587 sec/batch\n",
      "Epoch 17/20  Iteration 2883/3560 Training loss: 1.1726 2.7969 sec/batch\n",
      "Epoch 17/20  Iteration 2884/3560 Training loss: 1.1723 2.8697 sec/batch\n",
      "Epoch 17/20  Iteration 2885/3560 Training loss: 1.1714 2.9074 sec/batch\n",
      "Epoch 17/20  Iteration 2886/3560 Training loss: 1.1701 2.8116 sec/batch\n",
      "Epoch 17/20  Iteration 2887/3560 Training loss: 1.1690 2.8223 sec/batch\n",
      "Epoch 17/20  Iteration 2888/3560 Training loss: 1.1685 2.9102 sec/batch\n",
      "Epoch 17/20  Iteration 2889/3560 Training loss: 1.1680 4.0122 sec/batch\n",
      "Epoch 17/20  Iteration 2890/3560 Training loss: 1.1690 2.8475 sec/batch\n",
      "Epoch 17/20  Iteration 2891/3560 Training loss: 1.1688 2.8756 sec/batch\n",
      "Epoch 17/20  Iteration 2892/3560 Training loss: 1.1683 2.9149 sec/batch\n",
      "Epoch 17/20  Iteration 2893/3560 Training loss: 1.1681 2.8365 sec/batch\n",
      "Epoch 17/20  Iteration 2894/3560 Training loss: 1.1675 2.9042 sec/batch\n",
      "Epoch 17/20  Iteration 2895/3560 Training loss: 1.1672 2.9163 sec/batch\n",
      "Epoch 17/20  Iteration 2896/3560 Training loss: 1.1667 2.8569 sec/batch\n",
      "Epoch 17/20  Iteration 2897/3560 Training loss: 1.1666 2.8673 sec/batch\n",
      "Epoch 17/20  Iteration 2898/3560 Training loss: 1.1669 2.8719 sec/batch\n",
      "Epoch 17/20  Iteration 2899/3560 Training loss: 1.1663 3.6493 sec/batch\n",
      "Epoch 17/20  Iteration 2900/3560 Training loss: 1.1670 3.0854 sec/batch\n",
      "Epoch 17/20  Iteration 2901/3560 Training loss: 1.1670 2.8360 sec/batch\n",
      "Epoch 17/20  Iteration 2902/3560 Training loss: 1.1671 2.9412 sec/batch\n",
      "Epoch 17/20  Iteration 2903/3560 Training loss: 1.1668 2.9023 sec/batch\n",
      "Epoch 17/20  Iteration 2904/3560 Training loss: 1.1668 2.8636 sec/batch\n",
      "Epoch 17/20  Iteration 2905/3560 Training loss: 1.1669 2.8364 sec/batch\n",
      "Epoch 17/20  Iteration 2906/3560 Training loss: 1.1666 2.8588 sec/batch\n",
      "Epoch 17/20  Iteration 2907/3560 Training loss: 1.1661 2.8473 sec/batch\n",
      "Epoch 17/20  Iteration 2908/3560 Training loss: 1.1667 2.8347 sec/batch\n",
      "Epoch 17/20  Iteration 2909/3560 Training loss: 1.1667 3.8026 sec/batch\n",
      "Epoch 17/20  Iteration 2910/3560 Training loss: 1.1673 2.8783 sec/batch\n",
      "Epoch 17/20  Iteration 2911/3560 Training loss: 1.1676 2.8692 sec/batch\n",
      "Epoch 17/20  Iteration 2912/3560 Training loss: 1.1675 2.9157 sec/batch\n",
      "Epoch 17/20  Iteration 2913/3560 Training loss: 1.1674 2.8217 sec/batch\n",
      "Epoch 17/20  Iteration 2914/3560 Training loss: 1.1676 2.8834 sec/batch\n",
      "Epoch 17/20  Iteration 2915/3560 Training loss: 1.1678 2.8795 sec/batch\n",
      "Epoch 17/20  Iteration 2916/3560 Training loss: 1.1677 2.8937 sec/batch\n",
      "Epoch 17/20  Iteration 2917/3560 Training loss: 1.1677 2.8687 sec/batch\n",
      "Epoch 17/20  Iteration 2918/3560 Training loss: 1.1677 2.8609 sec/batch\n",
      "Epoch 17/20  Iteration 2919/3560 Training loss: 1.1683 3.6477 sec/batch\n",
      "Epoch 17/20  Iteration 2920/3560 Training loss: 1.1685 2.9361 sec/batch\n",
      "Epoch 17/20  Iteration 2921/3560 Training loss: 1.1689 2.8991 sec/batch\n",
      "Epoch 17/20  Iteration 2922/3560 Training loss: 1.1687 2.8690 sec/batch\n",
      "Epoch 17/20  Iteration 2923/3560 Training loss: 1.1686 2.8091 sec/batch\n",
      "Epoch 17/20  Iteration 2924/3560 Training loss: 1.1687 2.8734 sec/batch\n",
      "Epoch 17/20  Iteration 2925/3560 Training loss: 1.1685 2.8792 sec/batch\n",
      "Epoch 17/20  Iteration 2926/3560 Training loss: 1.1686 2.9012 sec/batch\n",
      "Epoch 17/20  Iteration 2927/3560 Training loss: 1.1680 2.8392 sec/batch\n",
      "Epoch 17/20  Iteration 2928/3560 Training loss: 1.1679 2.8310 sec/batch\n",
      "Epoch 17/20  Iteration 2929/3560 Training loss: 1.1675 3.4219 sec/batch\n",
      "Epoch 17/20  Iteration 2930/3560 Training loss: 1.1675 3.2211 sec/batch\n",
      "Epoch 17/20  Iteration 2931/3560 Training loss: 1.1670 2.8171 sec/batch\n",
      "Epoch 17/20  Iteration 2932/3560 Training loss: 1.1671 2.9514 sec/batch\n",
      "Epoch 17/20  Iteration 2933/3560 Training loss: 1.1667 2.8959 sec/batch\n",
      "Epoch 17/20  Iteration 2934/3560 Training loss: 1.1665 2.8467 sec/batch\n",
      "Epoch 17/20  Iteration 2935/3560 Training loss: 1.1663 2.8840 sec/batch\n",
      "Epoch 17/20  Iteration 2936/3560 Training loss: 1.1660 2.7143 sec/batch\n",
      "Epoch 17/20  Iteration 2937/3560 Training loss: 1.1657 2.7293 sec/batch\n",
      "Epoch 17/20  Iteration 2938/3560 Training loss: 1.1657 2.6493 sec/batch\n",
      "Epoch 17/20  Iteration 2939/3560 Training loss: 1.1655 2.6756 sec/batch\n",
      "Epoch 17/20  Iteration 2940/3560 Training loss: 1.1655 3.2509 sec/batch\n",
      "Epoch 17/20  Iteration 2941/3560 Training loss: 1.1651 2.7786 sec/batch\n",
      "Epoch 17/20  Iteration 2942/3560 Training loss: 1.1649 2.6792 sec/batch\n",
      "Epoch 17/20  Iteration 2943/3560 Training loss: 1.1646 2.6665 sec/batch\n",
      "Epoch 17/20  Iteration 2944/3560 Training loss: 1.1646 2.7356 sec/batch\n",
      "Epoch 17/20  Iteration 2945/3560 Training loss: 1.1646 2.6270 sec/batch\n",
      "Epoch 17/20  Iteration 2946/3560 Training loss: 1.1642 2.6400 sec/batch\n",
      "Epoch 17/20  Iteration 2947/3560 Training loss: 1.1638 2.6580 sec/batch\n",
      "Epoch 17/20  Iteration 2948/3560 Training loss: 1.1635 2.6984 sec/batch\n",
      "Epoch 17/20  Iteration 2949/3560 Training loss: 1.1634 2.6769 sec/batch\n",
      "Epoch 17/20  Iteration 2950/3560 Training loss: 1.1632 2.6896 sec/batch\n",
      "Epoch 17/20  Iteration 2951/3560 Training loss: 1.1631 2.8527 sec/batch\n",
      "Epoch 17/20  Iteration 2952/3560 Training loss: 1.1629 2.7565 sec/batch\n",
      "Epoch 17/20  Iteration 2953/3560 Training loss: 1.1628 2.8874 sec/batch\n",
      "Epoch 17/20  Iteration 2954/3560 Training loss: 1.1626 2.9813 sec/batch\n",
      "Epoch 17/20  Iteration 2955/3560 Training loss: 1.1626 2.7170 sec/batch\n",
      "Epoch 17/20  Iteration 2956/3560 Training loss: 1.1626 2.6927 sec/batch\n",
      "Epoch 17/20  Iteration 2957/3560 Training loss: 1.1623 2.6647 sec/batch\n",
      "Epoch 17/20  Iteration 2958/3560 Training loss: 1.1623 2.6413 sec/batch\n",
      "Epoch 17/20  Iteration 2959/3560 Training loss: 1.1621 2.6667 sec/batch\n",
      "Epoch 17/20  Iteration 2960/3560 Training loss: 1.1620 2.6439 sec/batch\n",
      "Epoch 17/20  Iteration 2961/3560 Training loss: 1.1620 2.6357 sec/batch\n",
      "Epoch 17/20  Iteration 2962/3560 Training loss: 1.1620 2.6298 sec/batch\n",
      "Epoch 17/20  Iteration 2963/3560 Training loss: 1.1617 2.7194 sec/batch\n",
      "Epoch 17/20  Iteration 2964/3560 Training loss: 1.1614 2.6532 sec/batch\n",
      "Epoch 17/20  Iteration 2965/3560 Training loss: 1.1614 2.6756 sec/batch\n",
      "Epoch 17/20  Iteration 2966/3560 Training loss: 1.1614 3.3310 sec/batch\n",
      "Epoch 17/20  Iteration 2967/3560 Training loss: 1.1613 2.6475 sec/batch\n",
      "Epoch 17/20  Iteration 2968/3560 Training loss: 1.1613 2.6475 sec/batch\n",
      "Epoch 17/20  Iteration 2969/3560 Training loss: 1.1612 2.6642 sec/batch\n",
      "Epoch 17/20  Iteration 2970/3560 Training loss: 1.1608 2.6882 sec/batch\n",
      "Epoch 17/20  Iteration 2971/3560 Training loss: 1.1605 2.6166 sec/batch\n",
      "Epoch 17/20  Iteration 2972/3560 Training loss: 1.1604 2.6234 sec/batch\n",
      "Epoch 17/20  Iteration 2973/3560 Training loss: 1.1602 2.7143 sec/batch\n",
      "Epoch 17/20  Iteration 2974/3560 Training loss: 1.1599 2.7189 sec/batch\n",
      "Epoch 17/20  Iteration 2975/3560 Training loss: 1.1598 2.6724 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 2976/3560 Training loss: 1.1599 2.6291 sec/batch\n",
      "Epoch 17/20  Iteration 2977/3560 Training loss: 1.1597 2.6300 sec/batch\n",
      "Epoch 17/20  Iteration 2978/3560 Training loss: 1.1594 2.7326 sec/batch\n",
      "Epoch 17/20  Iteration 2979/3560 Training loss: 1.1591 2.6859 sec/batch\n",
      "Epoch 17/20  Iteration 2980/3560 Training loss: 1.1589 3.2385 sec/batch\n",
      "Epoch 17/20  Iteration 2981/3560 Training loss: 1.1590 2.7008 sec/batch\n",
      "Epoch 17/20  Iteration 2982/3560 Training loss: 1.1590 2.6654 sec/batch\n",
      "Epoch 17/20  Iteration 2983/3560 Training loss: 1.1590 2.6192 sec/batch\n",
      "Epoch 17/20  Iteration 2984/3560 Training loss: 1.1589 2.6638 sec/batch\n",
      "Epoch 17/20  Iteration 2985/3560 Training loss: 1.1590 2.7053 sec/batch\n",
      "Epoch 17/20  Iteration 2986/3560 Training loss: 1.1591 2.6386 sec/batch\n",
      "Epoch 17/20  Iteration 2987/3560 Training loss: 1.1591 2.6376 sec/batch\n",
      "Epoch 17/20  Iteration 2988/3560 Training loss: 1.1592 2.6349 sec/batch\n",
      "Epoch 17/20  Iteration 2989/3560 Training loss: 1.1595 2.6943 sec/batch\n",
      "Epoch 17/20  Iteration 2990/3560 Training loss: 1.1596 2.6142 sec/batch\n",
      "Epoch 17/20  Iteration 2991/3560 Training loss: 1.1595 2.6600 sec/batch\n",
      "Epoch 17/20  Iteration 2992/3560 Training loss: 1.1598 2.6735 sec/batch\n",
      "Epoch 17/20  Iteration 2993/3560 Training loss: 1.1597 2.7358 sec/batch\n",
      "Epoch 17/20  Iteration 2994/3560 Training loss: 1.1599 2.6507 sec/batch\n",
      "Epoch 17/20  Iteration 2995/3560 Training loss: 1.1600 2.7060 sec/batch\n",
      "Epoch 17/20  Iteration 2996/3560 Training loss: 1.1602 2.6523 sec/batch\n",
      "Epoch 17/20  Iteration 2997/3560 Training loss: 1.1605 2.7754 sec/batch\n",
      "Epoch 17/20  Iteration 2998/3560 Training loss: 1.1603 2.6756 sec/batch\n",
      "Epoch 17/20  Iteration 2999/3560 Training loss: 1.1601 2.6785 sec/batch\n",
      "Epoch 17/20  Iteration 3000/3560 Training loss: 1.1600 2.6941 sec/batch\n",
      "Epoch 17/20  Iteration 3001/3560 Training loss: 1.1601 2.7195 sec/batch\n",
      "Epoch 17/20  Iteration 3002/3560 Training loss: 1.1601 2.5926 sec/batch\n",
      "Epoch 17/20  Iteration 3003/3560 Training loss: 1.1600 2.7156 sec/batch\n",
      "Epoch 17/20  Iteration 3004/3560 Training loss: 1.1601 3.2607 sec/batch\n",
      "Epoch 17/20  Iteration 3005/3560 Training loss: 1.1601 2.6536 sec/batch\n",
      "Epoch 17/20  Iteration 3006/3560 Training loss: 1.1600 2.6553 sec/batch\n",
      "Epoch 17/20  Iteration 3007/3560 Training loss: 1.1598 2.5897 sec/batch\n",
      "Epoch 17/20  Iteration 3008/3560 Training loss: 1.1600 2.6801 sec/batch\n",
      "Epoch 17/20  Iteration 3009/3560 Training loss: 1.1601 2.6281 sec/batch\n",
      "Epoch 17/20  Iteration 3010/3560 Training loss: 1.1601 2.6279 sec/batch\n",
      "Epoch 17/20  Iteration 3011/3560 Training loss: 1.1601 2.6710 sec/batch\n",
      "Epoch 17/20  Iteration 3012/3560 Training loss: 1.1601 2.6725 sec/batch\n",
      "Epoch 17/20  Iteration 3013/3560 Training loss: 1.1600 2.6481 sec/batch\n",
      "Epoch 17/20  Iteration 3014/3560 Training loss: 1.1599 2.6627 sec/batch\n",
      "Epoch 17/20  Iteration 3015/3560 Training loss: 1.1600 2.6793 sec/batch\n",
      "Epoch 17/20  Iteration 3016/3560 Training loss: 1.1603 2.6695 sec/batch\n",
      "Epoch 17/20  Iteration 3017/3560 Training loss: 1.1604 2.8128 sec/batch\n",
      "Epoch 17/20  Iteration 3018/3560 Training loss: 1.1605 2.8147 sec/batch\n",
      "Epoch 17/20  Iteration 3019/3560 Training loss: 1.1604 2.7298 sec/batch\n",
      "Epoch 17/20  Iteration 3020/3560 Training loss: 1.1603 2.6620 sec/batch\n",
      "Epoch 17/20  Iteration 3021/3560 Training loss: 1.1605 2.6836 sec/batch\n",
      "Epoch 17/20  Iteration 3022/3560 Training loss: 1.1605 2.6435 sec/batch\n",
      "Epoch 17/20  Iteration 3023/3560 Training loss: 1.1605 2.7256 sec/batch\n",
      "Epoch 17/20  Iteration 3024/3560 Training loss: 1.1604 2.6985 sec/batch\n",
      "Epoch 17/20  Iteration 3025/3560 Training loss: 1.1603 2.6712 sec/batch\n",
      "Epoch 17/20  Iteration 3026/3560 Training loss: 1.1605 2.6693 sec/batch\n",
      "Epoch 18/20  Iteration 3027/3560 Training loss: 1.3003 2.6734 sec/batch\n",
      "Epoch 18/20  Iteration 3028/3560 Training loss: 1.2350 2.6612 sec/batch\n",
      "Epoch 18/20  Iteration 3029/3560 Training loss: 1.2117 2.6639 sec/batch\n",
      "Epoch 18/20  Iteration 3030/3560 Training loss: 1.2045 2.6698 sec/batch\n",
      "Epoch 18/20  Iteration 3031/3560 Training loss: 1.1924 2.6843 sec/batch\n",
      "Epoch 18/20  Iteration 3032/3560 Training loss: 1.1795 2.6190 sec/batch\n",
      "Epoch 18/20  Iteration 3033/3560 Training loss: 1.1782 2.6935 sec/batch\n",
      "Epoch 18/20  Iteration 3034/3560 Training loss: 1.1754 2.6953 sec/batch\n",
      "Epoch 18/20  Iteration 3035/3560 Training loss: 1.1740 2.6751 sec/batch\n",
      "Epoch 18/20  Iteration 3036/3560 Training loss: 1.1727 2.6362 sec/batch\n",
      "Epoch 18/20  Iteration 3037/3560 Training loss: 1.1695 2.6627 sec/batch\n",
      "Epoch 18/20  Iteration 3038/3560 Training loss: 1.1691 2.7131 sec/batch\n",
      "Epoch 18/20  Iteration 3039/3560 Training loss: 1.1692 2.6941 sec/batch\n",
      "Epoch 18/20  Iteration 3040/3560 Training loss: 1.1689 2.5943 sec/batch\n",
      "Epoch 18/20  Iteration 3041/3560 Training loss: 1.1671 2.6781 sec/batch\n",
      "Epoch 18/20  Iteration 3042/3560 Training loss: 1.1652 2.7297 sec/batch\n",
      "Epoch 18/20  Iteration 3043/3560 Training loss: 1.1649 2.6650 sec/batch\n",
      "Epoch 18/20  Iteration 3044/3560 Training loss: 1.1657 2.6438 sec/batch\n",
      "Epoch 18/20  Iteration 3045/3560 Training loss: 1.1655 2.6955 sec/batch\n",
      "Epoch 18/20  Iteration 3046/3560 Training loss: 1.1661 2.7513 sec/batch\n",
      "Epoch 18/20  Iteration 3047/3560 Training loss: 1.1654 2.8837 sec/batch\n",
      "Epoch 18/20  Iteration 3048/3560 Training loss: 1.1662 3.1424 sec/batch\n",
      "Epoch 18/20  Iteration 3049/3560 Training loss: 1.1655 2.8370 sec/batch\n",
      "Epoch 18/20  Iteration 3050/3560 Training loss: 1.1655 2.7188 sec/batch\n",
      "Epoch 18/20  Iteration 3051/3560 Training loss: 1.1651 2.6751 sec/batch\n",
      "Epoch 18/20  Iteration 3052/3560 Training loss: 1.1632 2.6619 sec/batch\n",
      "Epoch 18/20  Iteration 3053/3560 Training loss: 1.1624 2.7718 sec/batch\n",
      "Epoch 18/20  Iteration 3054/3560 Training loss: 1.1627 2.6582 sec/batch\n",
      "Epoch 18/20  Iteration 3055/3560 Training loss: 1.1624 2.6614 sec/batch\n",
      "Epoch 18/20  Iteration 3056/3560 Training loss: 1.1625 2.6768 sec/batch\n",
      "Epoch 18/20  Iteration 3057/3560 Training loss: 1.1616 2.6819 sec/batch\n",
      "Epoch 18/20  Iteration 3058/3560 Training loss: 1.1606 2.6499 sec/batch\n",
      "Epoch 18/20  Iteration 3059/3560 Training loss: 1.1607 2.6899 sec/batch\n",
      "Epoch 18/20  Iteration 3060/3560 Training loss: 1.1605 2.7020 sec/batch\n",
      "Epoch 18/20  Iteration 3061/3560 Training loss: 1.1602 2.6632 sec/batch\n",
      "Epoch 18/20  Iteration 3062/3560 Training loss: 1.1601 2.6658 sec/batch\n",
      "Epoch 18/20  Iteration 3063/3560 Training loss: 1.1594 2.6528 sec/batch\n",
      "Epoch 18/20  Iteration 3064/3560 Training loss: 1.1584 2.6509 sec/batch\n",
      "Epoch 18/20  Iteration 3065/3560 Training loss: 1.1573 2.7116 sec/batch\n",
      "Epoch 18/20  Iteration 3066/3560 Training loss: 1.1569 2.6211 sec/batch\n",
      "Epoch 18/20  Iteration 3067/3560 Training loss: 1.1562 2.6777 sec/batch\n",
      "Epoch 18/20  Iteration 3068/3560 Training loss: 1.1570 2.7065 sec/batch\n",
      "Epoch 18/20  Iteration 3069/3560 Training loss: 1.1568 2.7016 sec/batch\n",
      "Epoch 18/20  Iteration 3070/3560 Training loss: 1.1562 2.6423 sec/batch\n",
      "Epoch 18/20  Iteration 3071/3560 Training loss: 1.1563 2.6264 sec/batch\n",
      "Epoch 18/20  Iteration 3072/3560 Training loss: 1.1558 2.6569 sec/batch\n",
      "Epoch 18/20  Iteration 3073/3560 Training loss: 1.1555 2.6331 sec/batch\n",
      "Epoch 18/20  Iteration 3074/3560 Training loss: 1.1551 2.6767 sec/batch\n",
      "Epoch 18/20  Iteration 3075/3560 Training loss: 1.1550 2.6505 sec/batch\n",
      "Epoch 18/20  Iteration 3076/3560 Training loss: 1.1551 2.6920 sec/batch\n",
      "Epoch 18/20  Iteration 3077/3560 Training loss: 1.1546 2.7170 sec/batch\n",
      "Epoch 18/20  Iteration 3078/3560 Training loss: 1.1553 2.7091 sec/batch\n",
      "Epoch 18/20  Iteration 3079/3560 Training loss: 1.1552 2.7614 sec/batch\n",
      "Epoch 18/20  Iteration 3080/3560 Training loss: 1.1554 2.6736 sec/batch\n",
      "Epoch 18/20  Iteration 3081/3560 Training loss: 1.1551 2.6790 sec/batch\n",
      "Epoch 18/20  Iteration 3082/3560 Training loss: 1.1551 2.6944 sec/batch\n",
      "Epoch 18/20  Iteration 3083/3560 Training loss: 1.1553 2.7148 sec/batch\n",
      "Epoch 18/20  Iteration 3084/3560 Training loss: 1.1551 2.8161 sec/batch\n",
      "Epoch 18/20  Iteration 3085/3560 Training loss: 1.1546 2.6984 sec/batch\n",
      "Epoch 18/20  Iteration 3086/3560 Training loss: 1.1552 2.6518 sec/batch\n",
      "Epoch 18/20  Iteration 3087/3560 Training loss: 1.1552 2.7083 sec/batch\n",
      "Epoch 18/20  Iteration 3088/3560 Training loss: 1.1558 2.6604 sec/batch\n",
      "Epoch 18/20  Iteration 3089/3560 Training loss: 1.1561 2.6474 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3090/3560 Training loss: 1.1561 2.6778 sec/batch\n",
      "Epoch 18/20  Iteration 3091/3560 Training loss: 1.1560 2.7094 sec/batch\n",
      "Epoch 18/20  Iteration 3092/3560 Training loss: 1.1562 2.6771 sec/batch\n",
      "Epoch 18/20  Iteration 3093/3560 Training loss: 1.1564 2.6284 sec/batch\n",
      "Epoch 18/20  Iteration 3094/3560 Training loss: 1.1561 2.6478 sec/batch\n",
      "Epoch 18/20  Iteration 3095/3560 Training loss: 1.1563 2.7254 sec/batch\n",
      "Epoch 18/20  Iteration 3096/3560 Training loss: 1.1561 2.6228 sec/batch\n",
      "Epoch 18/20  Iteration 3097/3560 Training loss: 1.1566 2.6332 sec/batch\n",
      "Epoch 18/20  Iteration 3098/3560 Training loss: 1.1569 2.6952 sec/batch\n",
      "Epoch 18/20  Iteration 3099/3560 Training loss: 1.1573 2.6654 sec/batch\n",
      "Epoch 18/20  Iteration 3100/3560 Training loss: 1.1569 2.6002 sec/batch\n",
      "Epoch 18/20  Iteration 3101/3560 Training loss: 1.1568 2.6557 sec/batch\n",
      "Epoch 18/20  Iteration 3102/3560 Training loss: 1.1569 2.6692 sec/batch\n",
      "Epoch 18/20  Iteration 3103/3560 Training loss: 1.1567 2.6631 sec/batch\n",
      "Epoch 18/20  Iteration 3104/3560 Training loss: 1.1567 2.6770 sec/batch\n",
      "Epoch 18/20  Iteration 3105/3560 Training loss: 1.1561 2.6360 sec/batch\n",
      "Epoch 18/20  Iteration 3106/3560 Training loss: 1.1560 2.7244 sec/batch\n",
      "Epoch 18/20  Iteration 3107/3560 Training loss: 1.1556 2.6384 sec/batch\n",
      "Epoch 18/20  Iteration 3108/3560 Training loss: 1.1556 2.6318 sec/batch\n",
      "Epoch 18/20  Iteration 3109/3560 Training loss: 1.1551 2.6610 sec/batch\n",
      "Epoch 18/20  Iteration 3110/3560 Training loss: 1.1551 2.7178 sec/batch\n",
      "Epoch 18/20  Iteration 3111/3560 Training loss: 1.1549 2.6807 sec/batch\n",
      "Epoch 18/20  Iteration 3112/3560 Training loss: 1.1547 2.6438 sec/batch\n",
      "Epoch 18/20  Iteration 3113/3560 Training loss: 1.1545 2.6967 sec/batch\n",
      "Epoch 18/20  Iteration 3114/3560 Training loss: 1.1542 2.6892 sec/batch\n",
      "Epoch 18/20  Iteration 3115/3560 Training loss: 1.1539 2.7148 sec/batch\n",
      "Epoch 18/20  Iteration 3116/3560 Training loss: 1.1541 2.6714 sec/batch\n",
      "Epoch 18/20  Iteration 3117/3560 Training loss: 1.1539 2.6730 sec/batch\n",
      "Epoch 18/20  Iteration 3118/3560 Training loss: 1.1538 2.7060 sec/batch\n",
      "Epoch 18/20  Iteration 3119/3560 Training loss: 1.1535 2.6400 sec/batch\n",
      "Epoch 18/20  Iteration 3120/3560 Training loss: 1.1532 2.6357 sec/batch\n",
      "Epoch 18/20  Iteration 3121/3560 Training loss: 1.1530 3.2869 sec/batch\n",
      "Epoch 18/20  Iteration 3122/3560 Training loss: 1.1530 2.6399 sec/batch\n",
      "Epoch 18/20  Iteration 3123/3560 Training loss: 1.1529 2.6518 sec/batch\n",
      "Epoch 18/20  Iteration 3124/3560 Training loss: 1.1526 2.6895 sec/batch\n",
      "Epoch 18/20  Iteration 3125/3560 Training loss: 1.1522 2.7304 sec/batch\n",
      "Epoch 18/20  Iteration 3126/3560 Training loss: 1.1519 2.6630 sec/batch\n",
      "Epoch 18/20  Iteration 3127/3560 Training loss: 1.1518 2.6709 sec/batch\n",
      "Epoch 18/20  Iteration 3128/3560 Training loss: 1.1516 2.6541 sec/batch\n",
      "Epoch 18/20  Iteration 3129/3560 Training loss: 1.1516 2.6815 sec/batch\n",
      "Epoch 18/20  Iteration 3130/3560 Training loss: 1.1515 2.6800 sec/batch\n",
      "Epoch 18/20  Iteration 3131/3560 Training loss: 1.1512 2.6691 sec/batch\n",
      "Epoch 18/20  Iteration 3132/3560 Training loss: 1.1511 2.6906 sec/batch\n",
      "Epoch 18/20  Iteration 3133/3560 Training loss: 1.1510 2.7515 sec/batch\n",
      "Epoch 18/20  Iteration 3134/3560 Training loss: 1.1510 2.9981 sec/batch\n",
      "Epoch 18/20  Iteration 3135/3560 Training loss: 1.1509 2.6843 sec/batch\n",
      "Epoch 18/20  Iteration 3136/3560 Training loss: 1.1510 2.7376 sec/batch\n",
      "Epoch 18/20  Iteration 3137/3560 Training loss: 1.1508 2.6812 sec/batch\n",
      "Epoch 18/20  Iteration 3138/3560 Training loss: 1.1508 2.6448 sec/batch\n",
      "Epoch 18/20  Iteration 3139/3560 Training loss: 1.1506 2.6665 sec/batch\n",
      "Epoch 18/20  Iteration 3140/3560 Training loss: 1.1505 2.7070 sec/batch\n",
      "Epoch 18/20  Iteration 3141/3560 Training loss: 1.1502 2.6493 sec/batch\n",
      "Epoch 18/20  Iteration 3142/3560 Training loss: 1.1499 2.6528 sec/batch\n",
      "Epoch 18/20  Iteration 3143/3560 Training loss: 1.1500 2.6446 sec/batch\n",
      "Epoch 18/20  Iteration 3144/3560 Training loss: 1.1500 2.6957 sec/batch\n",
      "Epoch 18/20  Iteration 3145/3560 Training loss: 1.1499 2.6789 sec/batch\n",
      "Epoch 18/20  Iteration 3146/3560 Training loss: 1.1499 2.6534 sec/batch\n",
      "Epoch 18/20  Iteration 3147/3560 Training loss: 1.1498 2.6867 sec/batch\n",
      "Epoch 18/20  Iteration 3148/3560 Training loss: 1.1495 2.7714 sec/batch\n",
      "Epoch 18/20  Iteration 3149/3560 Training loss: 1.1491 2.6550 sec/batch\n",
      "Epoch 18/20  Iteration 3150/3560 Training loss: 1.1491 2.6849 sec/batch\n",
      "Epoch 18/20  Iteration 3151/3560 Training loss: 1.1491 2.8309 sec/batch\n",
      "Epoch 18/20  Iteration 3152/3560 Training loss: 1.1488 2.6903 sec/batch\n",
      "Epoch 18/20  Iteration 3153/3560 Training loss: 1.1488 2.6485 sec/batch\n",
      "Epoch 18/20  Iteration 3154/3560 Training loss: 1.1487 2.6605 sec/batch\n",
      "Epoch 18/20  Iteration 3155/3560 Training loss: 1.1486 2.7354 sec/batch\n",
      "Epoch 18/20  Iteration 3156/3560 Training loss: 1.1483 2.6554 sec/batch\n",
      "Epoch 18/20  Iteration 3157/3560 Training loss: 1.1479 2.6374 sec/batch\n",
      "Epoch 18/20  Iteration 3158/3560 Training loss: 1.1478 2.6697 sec/batch\n",
      "Epoch 18/20  Iteration 3159/3560 Training loss: 1.1479 2.7410 sec/batch\n",
      "Epoch 18/20  Iteration 3160/3560 Training loss: 1.1479 2.6615 sec/batch\n",
      "Epoch 18/20  Iteration 3161/3560 Training loss: 1.1479 2.6800 sec/batch\n",
      "Epoch 18/20  Iteration 3162/3560 Training loss: 1.1480 2.6935 sec/batch\n",
      "Epoch 18/20  Iteration 3163/3560 Training loss: 1.1482 2.7209 sec/batch\n",
      "Epoch 18/20  Iteration 3164/3560 Training loss: 1.1483 2.6349 sec/batch\n",
      "Epoch 18/20  Iteration 3165/3560 Training loss: 1.1483 2.6528 sec/batch\n",
      "Epoch 18/20  Iteration 3166/3560 Training loss: 1.1484 2.6603 sec/batch\n",
      "Epoch 18/20  Iteration 3167/3560 Training loss: 1.1487 2.6954 sec/batch\n",
      "Epoch 18/20  Iteration 3168/3560 Training loss: 1.1488 2.6721 sec/batch\n",
      "Epoch 18/20  Iteration 3169/3560 Training loss: 1.1488 2.6709 sec/batch\n",
      "Epoch 18/20  Iteration 3170/3560 Training loss: 1.1490 2.6885 sec/batch\n",
      "Epoch 18/20  Iteration 3171/3560 Training loss: 1.1490 2.7213 sec/batch\n",
      "Epoch 18/20  Iteration 3172/3560 Training loss: 1.1492 2.6500 sec/batch\n",
      "Epoch 18/20  Iteration 3173/3560 Training loss: 1.1492 2.6496 sec/batch\n",
      "Epoch 18/20  Iteration 3174/3560 Training loss: 1.1494 2.7042 sec/batch\n",
      "Epoch 18/20  Iteration 3175/3560 Training loss: 1.1496 2.6457 sec/batch\n",
      "Epoch 18/20  Iteration 3176/3560 Training loss: 1.1495 2.6501 sec/batch\n",
      "Epoch 18/20  Iteration 3177/3560 Training loss: 1.1493 2.6578 sec/batch\n",
      "Epoch 18/20  Iteration 3178/3560 Training loss: 1.1491 2.7418 sec/batch\n",
      "Epoch 18/20  Iteration 3179/3560 Training loss: 1.1492 2.6751 sec/batch\n",
      "Epoch 18/20  Iteration 3180/3560 Training loss: 1.1491 2.6501 sec/batch\n",
      "Epoch 18/20  Iteration 3181/3560 Training loss: 1.1491 2.6575 sec/batch\n",
      "Epoch 18/20  Iteration 3182/3560 Training loss: 1.1491 2.6733 sec/batch\n",
      "Epoch 18/20  Iteration 3183/3560 Training loss: 1.1491 2.6298 sec/batch\n",
      "Epoch 18/20  Iteration 3184/3560 Training loss: 1.1491 2.6995 sec/batch\n",
      "Epoch 18/20  Iteration 3185/3560 Training loss: 1.1489 2.6799 sec/batch\n",
      "Epoch 18/20  Iteration 3186/3560 Training loss: 1.1490 2.6575 sec/batch\n",
      "Epoch 18/20  Iteration 3187/3560 Training loss: 1.1491 2.7363 sec/batch\n",
      "Epoch 18/20  Iteration 3188/3560 Training loss: 1.1492 3.2305 sec/batch\n",
      "Epoch 18/20  Iteration 3189/3560 Training loss: 1.1491 2.7155 sec/batch\n",
      "Epoch 18/20  Iteration 3190/3560 Training loss: 1.1491 2.6560 sec/batch\n",
      "Epoch 18/20  Iteration 3191/3560 Training loss: 1.1491 2.6868 sec/batch\n",
      "Epoch 18/20  Iteration 3192/3560 Training loss: 1.1491 2.6617 sec/batch\n",
      "Epoch 18/20  Iteration 3193/3560 Training loss: 1.1492 2.7261 sec/batch\n",
      "Epoch 18/20  Iteration 3194/3560 Training loss: 1.1495 2.6238 sec/batch\n",
      "Epoch 18/20  Iteration 3195/3560 Training loss: 1.1496 2.6427 sec/batch\n",
      "Epoch 18/20  Iteration 3196/3560 Training loss: 1.1496 2.6444 sec/batch\n",
      "Epoch 18/20  Iteration 3197/3560 Training loss: 1.1496 2.6833 sec/batch\n",
      "Epoch 18/20  Iteration 3198/3560 Training loss: 1.1495 2.6506 sec/batch\n",
      "Epoch 18/20  Iteration 3199/3560 Training loss: 1.1496 2.6552 sec/batch\n",
      "Epoch 18/20  Iteration 3200/3560 Training loss: 1.1496 2.7000 sec/batch\n",
      "Epoch 18/20  Iteration 3201/3560 Training loss: 1.1497 2.7060 sec/batch\n",
      "Epoch 18/20  Iteration 3202/3560 Training loss: 1.1495 2.6430 sec/batch\n",
      "Epoch 18/20  Iteration 3203/3560 Training loss: 1.1495 2.6706 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3204/3560 Training loss: 1.1497 2.6686 sec/batch\n",
      "Epoch 19/20  Iteration 3205/3560 Training loss: 1.2770 2.6672 sec/batch\n",
      "Epoch 19/20  Iteration 3206/3560 Training loss: 1.2212 2.6216 sec/batch\n",
      "Epoch 19/20  Iteration 3207/3560 Training loss: 1.1998 2.6519 sec/batch\n",
      "Epoch 19/20  Iteration 3208/3560 Training loss: 1.1925 2.7103 sec/batch\n",
      "Epoch 19/20  Iteration 3209/3560 Training loss: 1.1805 2.6685 sec/batch\n",
      "Epoch 19/20  Iteration 3210/3560 Training loss: 1.1678 2.6455 sec/batch\n",
      "Epoch 19/20  Iteration 3211/3560 Training loss: 1.1653 2.6673 sec/batch\n",
      "Epoch 19/20  Iteration 3212/3560 Training loss: 1.1626 2.7366 sec/batch\n",
      "Epoch 19/20  Iteration 3213/3560 Training loss: 1.1610 2.6710 sec/batch\n",
      "Epoch 19/20  Iteration 3214/3560 Training loss: 1.1599 2.6804 sec/batch\n",
      "Epoch 19/20  Iteration 3215/3560 Training loss: 1.1568 2.6982 sec/batch\n",
      "Epoch 19/20  Iteration 3216/3560 Training loss: 1.1567 2.6964 sec/batch\n",
      "Epoch 19/20  Iteration 3217/3560 Training loss: 1.1570 2.6737 sec/batch\n",
      "Epoch 19/20  Iteration 3218/3560 Training loss: 1.1568 2.8269 sec/batch\n",
      "Epoch 19/20  Iteration 3219/3560 Training loss: 1.1553 2.7143 sec/batch\n",
      "Epoch 19/20  Iteration 3220/3560 Training loss: 1.1535 2.6690 sec/batch\n",
      "Epoch 19/20  Iteration 3221/3560 Training loss: 1.1541 2.6740 sec/batch\n",
      "Epoch 19/20  Iteration 3222/3560 Training loss: 1.1547 2.6735 sec/batch\n",
      "Epoch 19/20  Iteration 3223/3560 Training loss: 1.1546 2.7313 sec/batch\n",
      "Epoch 19/20  Iteration 3224/3560 Training loss: 1.1555 2.6785 sec/batch\n",
      "Epoch 19/20  Iteration 3225/3560 Training loss: 1.1548 2.6608 sec/batch\n",
      "Epoch 19/20  Iteration 3226/3560 Training loss: 1.1550 2.6371 sec/batch\n",
      "Epoch 19/20  Iteration 3227/3560 Training loss: 1.1545 2.6796 sec/batch\n",
      "Epoch 19/20  Iteration 3228/3560 Training loss: 1.1548 2.6809 sec/batch\n",
      "Epoch 19/20  Iteration 3229/3560 Training loss: 1.1542 2.6279 sec/batch\n",
      "Epoch 19/20  Iteration 3230/3560 Training loss: 1.1524 2.6670 sec/batch\n",
      "Epoch 19/20  Iteration 3231/3560 Training loss: 1.1512 2.6898 sec/batch\n",
      "Epoch 19/20  Iteration 3232/3560 Training loss: 1.1519 2.6799 sec/batch\n",
      "Epoch 19/20  Iteration 3233/3560 Training loss: 1.1520 2.6430 sec/batch\n",
      "Epoch 19/20  Iteration 3234/3560 Training loss: 1.1522 2.6383 sec/batch\n",
      "Epoch 19/20  Iteration 3235/3560 Training loss: 1.1512 2.6449 sec/batch\n",
      "Epoch 19/20  Iteration 3236/3560 Training loss: 1.1503 2.6638 sec/batch\n",
      "Epoch 19/20  Iteration 3237/3560 Training loss: 1.1503 2.6622 sec/batch\n",
      "Epoch 19/20  Iteration 3238/3560 Training loss: 1.1503 2.7112 sec/batch\n",
      "Epoch 19/20  Iteration 3239/3560 Training loss: 1.1498 2.6509 sec/batch\n",
      "Epoch 19/20  Iteration 3240/3560 Training loss: 1.1497 2.6639 sec/batch\n",
      "Epoch 19/20  Iteration 3241/3560 Training loss: 1.1491 2.6753 sec/batch\n",
      "Epoch 19/20  Iteration 3242/3560 Training loss: 1.1483 2.7146 sec/batch\n",
      "Epoch 19/20  Iteration 3243/3560 Training loss: 1.1473 2.6620 sec/batch\n",
      "Epoch 19/20  Iteration 3244/3560 Training loss: 1.1471 2.6364 sec/batch\n",
      "Epoch 19/20  Iteration 3245/3560 Training loss: 1.1466 2.6642 sec/batch\n",
      "Epoch 19/20  Iteration 3246/3560 Training loss: 1.1474 2.7373 sec/batch\n",
      "Epoch 19/20  Iteration 3247/3560 Training loss: 1.1472 2.6503 sec/batch\n",
      "Epoch 19/20  Iteration 3248/3560 Training loss: 1.1466 2.6732 sec/batch\n",
      "Epoch 19/20  Iteration 3249/3560 Training loss: 1.1467 2.6341 sec/batch\n",
      "Epoch 19/20  Iteration 3250/3560 Training loss: 1.1460 2.6926 sec/batch\n",
      "Epoch 19/20  Iteration 3251/3560 Training loss: 1.1456 2.6955 sec/batch\n",
      "Epoch 19/20  Iteration 3252/3560 Training loss: 1.1451 2.6685 sec/batch\n",
      "Epoch 19/20  Iteration 3253/3560 Training loss: 1.1452 2.6695 sec/batch\n",
      "Epoch 19/20  Iteration 3254/3560 Training loss: 1.1453 2.7747 sec/batch\n",
      "Epoch 19/20  Iteration 3255/3560 Training loss: 1.1448 3.1746 sec/batch\n",
      "Epoch 19/20  Iteration 3256/3560 Training loss: 1.1454 2.6557 sec/batch\n",
      "Epoch 19/20  Iteration 3257/3560 Training loss: 1.1452 2.7183 sec/batch\n",
      "Epoch 19/20  Iteration 3258/3560 Training loss: 1.1452 2.6415 sec/batch\n",
      "Epoch 19/20  Iteration 3259/3560 Training loss: 1.1450 2.6511 sec/batch\n",
      "Epoch 19/20  Iteration 3260/3560 Training loss: 1.1450 2.6532 sec/batch\n",
      "Epoch 19/20  Iteration 3261/3560 Training loss: 1.1452 2.7145 sec/batch\n",
      "Epoch 19/20  Iteration 3262/3560 Training loss: 1.1449 2.6462 sec/batch\n",
      "Epoch 19/20  Iteration 3263/3560 Training loss: 1.1444 2.6345 sec/batch\n",
      "Epoch 19/20  Iteration 3264/3560 Training loss: 1.1450 2.6900 sec/batch\n",
      "Epoch 19/20  Iteration 3265/3560 Training loss: 1.1450 2.7471 sec/batch\n",
      "Epoch 19/20  Iteration 3266/3560 Training loss: 1.1457 2.6664 sec/batch\n",
      "Epoch 19/20  Iteration 3267/3560 Training loss: 1.1460 2.6577 sec/batch\n",
      "Epoch 19/20  Iteration 3268/3560 Training loss: 1.1460 2.6489 sec/batch\n",
      "Epoch 19/20  Iteration 3269/3560 Training loss: 1.1459 2.7268 sec/batch\n",
      "Epoch 19/20  Iteration 3270/3560 Training loss: 1.1459 2.6779 sec/batch\n",
      "Epoch 19/20  Iteration 3271/3560 Training loss: 1.1462 2.6715 sec/batch\n",
      "Epoch 19/20  Iteration 3272/3560 Training loss: 1.1461 2.6745 sec/batch\n",
      "Epoch 19/20  Iteration 3273/3560 Training loss: 1.1462 2.7388 sec/batch\n",
      "Epoch 19/20  Iteration 3274/3560 Training loss: 1.1461 2.6131 sec/batch\n",
      "Epoch 19/20  Iteration 3275/3560 Training loss: 1.1466 2.6482 sec/batch\n",
      "Epoch 19/20  Iteration 3276/3560 Training loss: 1.1468 2.7327 sec/batch\n",
      "Epoch 19/20  Iteration 3277/3560 Training loss: 1.1473 2.6727 sec/batch\n",
      "Epoch 19/20  Iteration 3278/3560 Training loss: 1.1471 2.6584 sec/batch\n",
      "Epoch 19/20  Iteration 3279/3560 Training loss: 1.1471 2.6519 sec/batch\n",
      "Epoch 19/20  Iteration 3280/3560 Training loss: 1.1472 2.7037 sec/batch\n",
      "Epoch 19/20  Iteration 3281/3560 Training loss: 1.1470 2.6696 sec/batch\n",
      "Epoch 19/20  Iteration 3282/3560 Training loss: 1.1469 2.6459 sec/batch\n",
      "Epoch 19/20  Iteration 3283/3560 Training loss: 1.1464 2.6645 sec/batch\n",
      "Epoch 19/20  Iteration 3284/3560 Training loss: 1.1463 2.6957 sec/batch\n",
      "Epoch 19/20  Iteration 3285/3560 Training loss: 1.1460 2.7538 sec/batch\n",
      "Epoch 19/20  Iteration 3286/3560 Training loss: 1.1460 2.7190 sec/batch\n",
      "Epoch 19/20  Iteration 3287/3560 Training loss: 1.1457 2.6637 sec/batch\n",
      "Epoch 19/20  Iteration 3288/3560 Training loss: 1.1456 2.7169 sec/batch\n",
      "Epoch 19/20  Iteration 3289/3560 Training loss: 1.1454 2.6965 sec/batch\n",
      "Epoch 19/20  Iteration 3290/3560 Training loss: 1.1452 2.6447 sec/batch\n",
      "Epoch 19/20  Iteration 3291/3560 Training loss: 1.1450 2.6624 sec/batch\n",
      "Epoch 19/20  Iteration 3292/3560 Training loss: 1.1449 2.7204 sec/batch\n",
      "Epoch 19/20  Iteration 3293/3560 Training loss: 1.1446 2.6929 sec/batch\n",
      "Epoch 19/20  Iteration 3294/3560 Training loss: 1.1446 2.6758 sec/batch\n",
      "Epoch 19/20  Iteration 3295/3560 Training loss: 1.1443 2.7120 sec/batch\n",
      "Epoch 19/20  Iteration 3296/3560 Training loss: 1.1443 2.6971 sec/batch\n",
      "Epoch 19/20  Iteration 3297/3560 Training loss: 1.1440 2.6633 sec/batch\n",
      "Epoch 19/20  Iteration 3298/3560 Training loss: 1.1437 2.6836 sec/batch\n",
      "Epoch 19/20  Iteration 3299/3560 Training loss: 1.1435 2.7298 sec/batch\n",
      "Epoch 19/20  Iteration 3300/3560 Training loss: 1.1435 2.6562 sec/batch\n",
      "Epoch 19/20  Iteration 3301/3560 Training loss: 1.1435 2.6626 sec/batch\n",
      "Epoch 19/20  Iteration 3302/3560 Training loss: 1.1432 2.6829 sec/batch\n",
      "Epoch 19/20  Iteration 3303/3560 Training loss: 1.1428 2.7221 sec/batch\n",
      "Epoch 19/20  Iteration 3304/3560 Training loss: 1.1425 2.6895 sec/batch\n",
      "Epoch 19/20  Iteration 3305/3560 Training loss: 1.1424 2.6474 sec/batch\n",
      "Epoch 19/20  Iteration 3306/3560 Training loss: 1.1423 2.6719 sec/batch\n",
      "Epoch 19/20  Iteration 3307/3560 Training loss: 1.1423 2.7187 sec/batch\n",
      "Epoch 19/20  Iteration 3308/3560 Training loss: 1.1422 2.6575 sec/batch\n",
      "Epoch 19/20  Iteration 3309/3560 Training loss: 1.1421 2.6536 sec/batch\n",
      "Epoch 19/20  Iteration 3310/3560 Training loss: 1.1419 2.6554 sec/batch\n",
      "Epoch 19/20  Iteration 3311/3560 Training loss: 1.1418 2.6802 sec/batch\n",
      "Epoch 19/20  Iteration 3312/3560 Training loss: 1.1418 2.6492 sec/batch\n",
      "Epoch 19/20  Iteration 3313/3560 Training loss: 1.1416 2.6976 sec/batch\n",
      "Epoch 19/20  Iteration 3314/3560 Training loss: 1.1417 2.7473 sec/batch\n",
      "Epoch 19/20  Iteration 3315/3560 Training loss: 1.1416 2.7272 sec/batch\n",
      "Epoch 19/20  Iteration 3316/3560 Training loss: 1.1415 2.6756 sec/batch\n",
      "Epoch 19/20  Iteration 3317/3560 Training loss: 1.1415 2.7034 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3318/3560 Training loss: 1.1415 2.7357 sec/batch\n",
      "Epoch 19/20  Iteration 3319/3560 Training loss: 1.1412 2.6971 sec/batch\n",
      "Epoch 19/20  Iteration 3320/3560 Training loss: 1.1409 2.7015 sec/batch\n",
      "Epoch 19/20  Iteration 3321/3560 Training loss: 1.1409 2.8226 sec/batch\n",
      "Epoch 19/20  Iteration 3322/3560 Training loss: 1.1409 3.1200 sec/batch\n",
      "Epoch 19/20  Iteration 3323/3560 Training loss: 1.1409 2.6827 sec/batch\n",
      "Epoch 19/20  Iteration 3324/3560 Training loss: 1.1408 2.6835 sec/batch\n",
      "Epoch 19/20  Iteration 3325/3560 Training loss: 1.1408 2.6650 sec/batch\n",
      "Epoch 19/20  Iteration 3326/3560 Training loss: 1.1405 2.6594 sec/batch\n",
      "Epoch 19/20  Iteration 3327/3560 Training loss: 1.1401 2.6928 sec/batch\n",
      "Epoch 19/20  Iteration 3328/3560 Training loss: 1.1401 2.6324 sec/batch\n",
      "Epoch 19/20  Iteration 3329/3560 Training loss: 1.1400 2.7570 sec/batch\n",
      "Epoch 19/20  Iteration 3330/3560 Training loss: 1.1396 2.6528 sec/batch\n",
      "Epoch 19/20  Iteration 3331/3560 Training loss: 1.1396 2.6713 sec/batch\n",
      "Epoch 19/20  Iteration 3332/3560 Training loss: 1.1395 2.6346 sec/batch\n",
      "Epoch 19/20  Iteration 3333/3560 Training loss: 1.1394 2.7015 sec/batch\n",
      "Epoch 19/20  Iteration 3334/3560 Training loss: 1.1391 2.6643 sec/batch\n",
      "Epoch 19/20  Iteration 3335/3560 Training loss: 1.1387 2.6860 sec/batch\n",
      "Epoch 19/20  Iteration 3336/3560 Training loss: 1.1386 2.7002 sec/batch\n",
      "Epoch 19/20  Iteration 3337/3560 Training loss: 1.1387 2.7480 sec/batch\n",
      "Epoch 19/20  Iteration 3338/3560 Training loss: 1.1388 2.6331 sec/batch\n",
      "Epoch 19/20  Iteration 3339/3560 Training loss: 1.1388 2.6821 sec/batch\n",
      "Epoch 19/20  Iteration 3340/3560 Training loss: 1.1388 2.6791 sec/batch\n",
      "Epoch 19/20  Iteration 3341/3560 Training loss: 1.1389 2.7183 sec/batch\n",
      "Epoch 19/20  Iteration 3342/3560 Training loss: 1.1390 2.6710 sec/batch\n",
      "Epoch 19/20  Iteration 3343/3560 Training loss: 1.1390 2.6675 sec/batch\n",
      "Epoch 19/20  Iteration 3344/3560 Training loss: 1.1391 2.6819 sec/batch\n",
      "Epoch 19/20  Iteration 3345/3560 Training loss: 1.1394 2.6910 sec/batch\n",
      "Epoch 19/20  Iteration 3346/3560 Training loss: 1.1394 2.6633 sec/batch\n",
      "Epoch 19/20  Iteration 3347/3560 Training loss: 1.1394 2.6429 sec/batch\n",
      "Epoch 19/20  Iteration 3348/3560 Training loss: 1.1398 2.7241 sec/batch\n",
      "Epoch 19/20  Iteration 3349/3560 Training loss: 1.1397 2.6289 sec/batch\n",
      "Epoch 19/20  Iteration 3350/3560 Training loss: 1.1399 2.6449 sec/batch\n",
      "Epoch 19/20  Iteration 3351/3560 Training loss: 1.1399 2.6773 sec/batch\n",
      "Epoch 19/20  Iteration 3352/3560 Training loss: 1.1401 2.8402 sec/batch\n",
      "Epoch 19/20  Iteration 3353/3560 Training loss: 1.1402 2.6815 sec/batch\n",
      "Epoch 19/20  Iteration 3354/3560 Training loss: 1.1401 2.6794 sec/batch\n",
      "Epoch 19/20  Iteration 3355/3560 Training loss: 1.1399 2.6987 sec/batch\n",
      "Epoch 19/20  Iteration 3356/3560 Training loss: 1.1398 2.6826 sec/batch\n",
      "Epoch 19/20  Iteration 3357/3560 Training loss: 1.1398 2.6707 sec/batch\n",
      "Epoch 19/20  Iteration 3358/3560 Training loss: 1.1398 2.6717 sec/batch\n",
      "Epoch 19/20  Iteration 3359/3560 Training loss: 1.1398 2.6714 sec/batch\n",
      "Epoch 19/20  Iteration 3360/3560 Training loss: 1.1398 2.7586 sec/batch\n",
      "Epoch 19/20  Iteration 3361/3560 Training loss: 1.1398 2.6228 sec/batch\n",
      "Epoch 19/20  Iteration 3362/3560 Training loss: 1.1397 2.6777 sec/batch\n",
      "Epoch 19/20  Iteration 3363/3560 Training loss: 1.1395 2.7206 sec/batch\n",
      "Epoch 19/20  Iteration 3364/3560 Training loss: 1.1397 2.6732 sec/batch\n",
      "Epoch 19/20  Iteration 3365/3560 Training loss: 1.1398 2.6773 sec/batch\n",
      "Epoch 19/20  Iteration 3366/3560 Training loss: 1.1399 2.6465 sec/batch\n",
      "Epoch 19/20  Iteration 3367/3560 Training loss: 1.1398 2.6806 sec/batch\n",
      "Epoch 19/20  Iteration 3368/3560 Training loss: 1.1399 2.6444 sec/batch\n",
      "Epoch 19/20  Iteration 3369/3560 Training loss: 1.1399 2.6662 sec/batch\n",
      "Epoch 19/20  Iteration 3370/3560 Training loss: 1.1397 2.6721 sec/batch\n",
      "Epoch 19/20  Iteration 3371/3560 Training loss: 1.1399 2.7292 sec/batch\n",
      "Epoch 19/20  Iteration 3372/3560 Training loss: 1.1401 2.6594 sec/batch\n",
      "Epoch 19/20  Iteration 3373/3560 Training loss: 1.1402 2.6346 sec/batch\n",
      "Epoch 19/20  Iteration 3374/3560 Training loss: 1.1403 2.6625 sec/batch\n",
      "Epoch 19/20  Iteration 3375/3560 Training loss: 1.1403 2.6512 sec/batch\n",
      "Epoch 19/20  Iteration 3376/3560 Training loss: 1.1402 2.6722 sec/batch\n",
      "Epoch 19/20  Iteration 3377/3560 Training loss: 1.1403 2.6193 sec/batch\n",
      "Epoch 19/20  Iteration 3378/3560 Training loss: 1.1404 2.7288 sec/batch\n",
      "Epoch 19/20  Iteration 3379/3560 Training loss: 1.1404 2.7429 sec/batch\n",
      "Epoch 19/20  Iteration 3380/3560 Training loss: 1.1403 2.6953 sec/batch\n",
      "Epoch 19/20  Iteration 3381/3560 Training loss: 1.1402 2.6968 sec/batch\n",
      "Epoch 19/20  Iteration 3382/3560 Training loss: 1.1404 2.7633 sec/batch\n",
      "Epoch 20/20  Iteration 3383/3560 Training loss: 1.2773 2.6893 sec/batch\n",
      "Epoch 20/20  Iteration 3384/3560 Training loss: 1.2174 2.6658 sec/batch\n",
      "Epoch 20/20  Iteration 3385/3560 Training loss: 1.1974 2.6808 sec/batch\n",
      "Epoch 20/20  Iteration 3386/3560 Training loss: 1.1885 2.6841 sec/batch\n",
      "Epoch 20/20  Iteration 3387/3560 Training loss: 1.1743 2.6712 sec/batch\n",
      "Epoch 20/20  Iteration 3388/3560 Training loss: 1.1611 2.9265 sec/batch\n",
      "Epoch 20/20  Iteration 3389/3560 Training loss: 1.1584 3.0036 sec/batch\n",
      "Epoch 20/20  Iteration 3390/3560 Training loss: 1.1543 2.7031 sec/batch\n",
      "Epoch 20/20  Iteration 3391/3560 Training loss: 1.1521 2.6717 sec/batch\n",
      "Epoch 20/20  Iteration 3392/3560 Training loss: 1.1502 2.6622 sec/batch\n",
      "Epoch 20/20  Iteration 3393/3560 Training loss: 1.1474 2.6545 sec/batch\n",
      "Epoch 20/20  Iteration 3394/3560 Training loss: 1.1471 2.6694 sec/batch\n",
      "Epoch 20/20  Iteration 3395/3560 Training loss: 1.1474 2.6348 sec/batch\n",
      "Epoch 20/20  Iteration 3396/3560 Training loss: 1.1474 2.6615 sec/batch\n",
      "Epoch 20/20  Iteration 3397/3560 Training loss: 1.1456 2.7198 sec/batch\n",
      "Epoch 20/20  Iteration 3398/3560 Training loss: 1.1437 2.6876 sec/batch\n",
      "Epoch 20/20  Iteration 3399/3560 Training loss: 1.1442 2.6052 sec/batch\n",
      "Epoch 20/20  Iteration 3400/3560 Training loss: 1.1453 2.6360 sec/batch\n",
      "Epoch 20/20  Iteration 3401/3560 Training loss: 1.1447 2.6858 sec/batch\n",
      "Epoch 20/20  Iteration 3402/3560 Training loss: 1.1455 2.6499 sec/batch\n",
      "Epoch 20/20  Iteration 3403/3560 Training loss: 1.1450 2.6633 sec/batch\n",
      "Epoch 20/20  Iteration 3404/3560 Training loss: 1.1453 2.6875 sec/batch\n",
      "Epoch 20/20  Iteration 3405/3560 Training loss: 1.1448 2.7264 sec/batch\n",
      "Epoch 20/20  Iteration 3406/3560 Training loss: 1.1450 2.6541 sec/batch\n",
      "Epoch 20/20  Iteration 3407/3560 Training loss: 1.1449 2.6514 sec/batch\n",
      "Epoch 20/20  Iteration 3408/3560 Training loss: 1.1433 2.6630 sec/batch\n",
      "Epoch 20/20  Iteration 3409/3560 Training loss: 1.1422 2.6540 sec/batch\n",
      "Epoch 20/20  Iteration 3410/3560 Training loss: 1.1430 2.6202 sec/batch\n",
      "Epoch 20/20  Iteration 3411/3560 Training loss: 1.1430 2.6474 sec/batch\n",
      "Epoch 20/20  Iteration 3412/3560 Training loss: 1.1434 2.6468 sec/batch\n",
      "Epoch 20/20  Iteration 3413/3560 Training loss: 1.1425 2.6858 sec/batch\n",
      "Epoch 20/20  Iteration 3414/3560 Training loss: 1.1415 2.6082 sec/batch\n",
      "Epoch 20/20  Iteration 3415/3560 Training loss: 1.1416 2.6482 sec/batch\n",
      "Epoch 20/20  Iteration 3416/3560 Training loss: 1.1418 2.6712 sec/batch\n",
      "Epoch 20/20  Iteration 3417/3560 Training loss: 1.1411 2.6920 sec/batch\n",
      "Epoch 20/20  Iteration 3418/3560 Training loss: 1.1408 2.6355 sec/batch\n",
      "Epoch 20/20  Iteration 3419/3560 Training loss: 1.1399 2.7856 sec/batch\n",
      "Epoch 20/20  Iteration 3420/3560 Training loss: 1.1390 2.7200 sec/batch\n",
      "Epoch 20/20  Iteration 3421/3560 Training loss: 1.1379 2.6571 sec/batch\n",
      "Epoch 20/20  Iteration 3422/3560 Training loss: 1.1375 2.6580 sec/batch\n",
      "Epoch 20/20  Iteration 3423/3560 Training loss: 1.1369 2.6596 sec/batch\n",
      "Epoch 20/20  Iteration 3424/3560 Training loss: 1.1378 2.6812 sec/batch\n",
      "Epoch 20/20  Iteration 3425/3560 Training loss: 1.1375 2.6814 sec/batch\n",
      "Epoch 20/20  Iteration 3426/3560 Training loss: 1.1369 2.6600 sec/batch\n",
      "Epoch 20/20  Iteration 3427/3560 Training loss: 1.1368 2.6747 sec/batch\n",
      "Epoch 20/20  Iteration 3428/3560 Training loss: 1.1363 2.7284 sec/batch\n",
      "Epoch 20/20  Iteration 3429/3560 Training loss: 1.1358 2.6487 sec/batch\n",
      "Epoch 20/20  Iteration 3430/3560 Training loss: 1.1354 2.6583 sec/batch\n",
      "Epoch 20/20  Iteration 3431/3560 Training loss: 1.1353 2.6402 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3432/3560 Training loss: 1.1355 2.6347 sec/batch\n",
      "Epoch 20/20  Iteration 3433/3560 Training loss: 1.1348 2.6317 sec/batch\n",
      "Epoch 20/20  Iteration 3434/3560 Training loss: 1.1354 2.6484 sec/batch\n",
      "Epoch 20/20  Iteration 3435/3560 Training loss: 1.1354 2.6733 sec/batch\n",
      "Epoch 20/20  Iteration 3436/3560 Training loss: 1.1354 2.7016 sec/batch\n",
      "Epoch 20/20  Iteration 3437/3560 Training loss: 1.1355 2.6507 sec/batch\n",
      "Epoch 20/20  Iteration 3438/3560 Training loss: 1.1355 2.6662 sec/batch\n",
      "Epoch 20/20  Iteration 3439/3560 Training loss: 1.1356 2.7041 sec/batch\n",
      "Epoch 20/20  Iteration 3440/3560 Training loss: 1.1353 2.6720 sec/batch\n",
      "Epoch 20/20  Iteration 3441/3560 Training loss: 1.1347 2.6373 sec/batch\n",
      "Epoch 20/20  Iteration 3442/3560 Training loss: 1.1352 2.6648 sec/batch\n",
      "Epoch 20/20  Iteration 3443/3560 Training loss: 1.1353 2.6835 sec/batch\n",
      "Epoch 20/20  Iteration 3444/3560 Training loss: 1.1359 2.6473 sec/batch\n",
      "Epoch 20/20  Iteration 3445/3560 Training loss: 1.1363 2.6504 sec/batch\n",
      "Epoch 20/20  Iteration 3446/3560 Training loss: 1.1364 2.6614 sec/batch\n",
      "Epoch 20/20  Iteration 3447/3560 Training loss: 1.1363 2.6762 sec/batch\n",
      "Epoch 20/20  Iteration 3448/3560 Training loss: 1.1362 2.6752 sec/batch\n",
      "Epoch 20/20  Iteration 3449/3560 Training loss: 1.1364 2.6371 sec/batch\n",
      "Epoch 20/20  Iteration 3450/3560 Training loss: 1.1363 2.6577 sec/batch\n",
      "Epoch 20/20  Iteration 3451/3560 Training loss: 1.1364 2.7094 sec/batch\n",
      "Epoch 20/20  Iteration 3452/3560 Training loss: 1.1362 2.7311 sec/batch\n",
      "Epoch 20/20  Iteration 3453/3560 Training loss: 1.1367 2.6590 sec/batch\n",
      "Epoch 20/20  Iteration 3454/3560 Training loss: 1.1369 2.6693 sec/batch\n",
      "Epoch 20/20  Iteration 3455/3560 Training loss: 1.1374 2.7974 sec/batch\n",
      "Epoch 20/20  Iteration 3456/3560 Training loss: 1.1372 3.1114 sec/batch\n",
      "Epoch 20/20  Iteration 3457/3560 Training loss: 1.1372 2.6857 sec/batch\n",
      "Epoch 20/20  Iteration 3458/3560 Training loss: 1.1373 2.6642 sec/batch\n",
      "Epoch 20/20  Iteration 3459/3560 Training loss: 1.1372 2.6614 sec/batch\n",
      "Epoch 20/20  Iteration 3460/3560 Training loss: 1.1370 2.6855 sec/batch\n",
      "Epoch 20/20  Iteration 3461/3560 Training loss: 1.1364 2.6999 sec/batch\n",
      "Epoch 20/20  Iteration 3462/3560 Training loss: 1.1365 2.7257 sec/batch\n",
      "Epoch 20/20  Iteration 3463/3560 Training loss: 1.1361 2.6608 sec/batch\n",
      "Epoch 20/20  Iteration 3464/3560 Training loss: 1.1361 2.6692 sec/batch\n",
      "Epoch 20/20  Iteration 3465/3560 Training loss: 1.1357 2.6763 sec/batch\n",
      "Epoch 20/20  Iteration 3466/3560 Training loss: 1.1358 2.7058 sec/batch\n",
      "Epoch 20/20  Iteration 3467/3560 Training loss: 1.1357 2.6840 sec/batch\n",
      "Epoch 20/20  Iteration 3468/3560 Training loss: 1.1354 2.6910 sec/batch\n",
      "Epoch 20/20  Iteration 3469/3560 Training loss: 1.1353 2.6915 sec/batch\n",
      "Epoch 20/20  Iteration 3470/3560 Training loss: 1.1352 2.6697 sec/batch\n",
      "Epoch 20/20  Iteration 3471/3560 Training loss: 1.1348 2.6993 sec/batch\n",
      "Epoch 20/20  Iteration 3472/3560 Training loss: 1.1349 2.6878 sec/batch\n",
      "Epoch 20/20  Iteration 3473/3560 Training loss: 1.1347 2.7935 sec/batch\n",
      "Epoch 20/20  Iteration 3474/3560 Training loss: 1.1346 2.6557 sec/batch\n",
      "Epoch 20/20  Iteration 3475/3560 Training loss: 1.1343 2.6348 sec/batch\n",
      "Epoch 20/20  Iteration 3476/3560 Training loss: 1.1340 2.7121 sec/batch\n",
      "Epoch 20/20  Iteration 3477/3560 Training loss: 1.1338 2.6616 sec/batch\n",
      "Epoch 20/20  Iteration 3478/3560 Training loss: 1.1339 2.6583 sec/batch\n",
      "Epoch 20/20  Iteration 3479/3560 Training loss: 1.1339 2.6595 sec/batch\n",
      "Epoch 20/20  Iteration 3480/3560 Training loss: 1.1335 2.6753 sec/batch\n",
      "Epoch 20/20  Iteration 3481/3560 Training loss: 1.1332 2.7178 sec/batch\n",
      "Epoch 20/20  Iteration 3482/3560 Training loss: 1.1330 2.6566 sec/batch\n",
      "Epoch 20/20  Iteration 3483/3560 Training loss: 1.1329 2.6712 sec/batch\n",
      "Epoch 20/20  Iteration 3484/3560 Training loss: 1.1327 2.6952 sec/batch\n",
      "Epoch 20/20  Iteration 3485/3560 Training loss: 1.1327 2.6820 sec/batch\n",
      "Epoch 20/20  Iteration 3486/3560 Training loss: 1.1326 2.7939 sec/batch\n",
      "Epoch 20/20  Iteration 3487/3560 Training loss: 1.1324 2.6743 sec/batch\n",
      "Epoch 20/20  Iteration 3488/3560 Training loss: 1.1323 2.7021 sec/batch\n",
      "Epoch 20/20  Iteration 3489/3560 Training loss: 1.1322 2.6950 sec/batch\n",
      "Epoch 20/20  Iteration 3490/3560 Training loss: 1.1323 2.6855 sec/batch\n",
      "Epoch 20/20  Iteration 3491/3560 Training loss: 1.1321 2.6861 sec/batch\n",
      "Epoch 20/20  Iteration 3492/3560 Training loss: 1.1322 2.7449 sec/batch\n",
      "Epoch 20/20  Iteration 3493/3560 Training loss: 1.1321 2.6731 sec/batch\n",
      "Epoch 20/20  Iteration 3494/3560 Training loss: 1.1321 2.6801 sec/batch\n",
      "Epoch 20/20  Iteration 3495/3560 Training loss: 1.1321 2.6831 sec/batch\n",
      "Epoch 20/20  Iteration 3496/3560 Training loss: 1.1320 2.7073 sec/batch\n",
      "Epoch 20/20  Iteration 3497/3560 Training loss: 1.1317 2.6547 sec/batch\n",
      "Epoch 20/20  Iteration 3498/3560 Training loss: 1.1313 2.6605 sec/batch\n",
      "Epoch 20/20  Iteration 3499/3560 Training loss: 1.1313 2.7062 sec/batch\n",
      "Epoch 20/20  Iteration 3500/3560 Training loss: 1.1313 2.7170 sec/batch\n",
      "Epoch 20/20  Iteration 3501/3560 Training loss: 1.1313 2.6475 sec/batch\n",
      "Epoch 20/20  Iteration 3502/3560 Training loss: 1.1312 2.6189 sec/batch\n",
      "Epoch 20/20  Iteration 3503/3560 Training loss: 1.1311 2.6845 sec/batch\n",
      "Epoch 20/20  Iteration 3504/3560 Training loss: 1.1308 2.6725 sec/batch\n",
      "Epoch 20/20  Iteration 3505/3560 Training loss: 1.1304 2.6535 sec/batch\n",
      "Epoch 20/20  Iteration 3506/3560 Training loss: 1.1304 2.6490 sec/batch\n",
      "Epoch 20/20  Iteration 3507/3560 Training loss: 1.1303 2.6660 sec/batch\n",
      "Epoch 20/20  Iteration 3508/3560 Training loss: 1.1300 2.7384 sec/batch\n",
      "Epoch 20/20  Iteration 3509/3560 Training loss: 1.1300 2.6868 sec/batch\n",
      "Epoch 20/20  Iteration 3510/3560 Training loss: 1.1299 2.6681 sec/batch\n",
      "Epoch 20/20  Iteration 3511/3560 Training loss: 1.1298 2.7119 sec/batch\n",
      "Epoch 20/20  Iteration 3512/3560 Training loss: 1.1295 2.6494 sec/batch\n",
      "Epoch 20/20  Iteration 3513/3560 Training loss: 1.1291 2.6229 sec/batch\n",
      "Epoch 20/20  Iteration 3514/3560 Training loss: 1.1290 2.6524 sec/batch\n",
      "Epoch 20/20  Iteration 3515/3560 Training loss: 1.1292 2.7251 sec/batch\n",
      "Epoch 20/20  Iteration 3516/3560 Training loss: 1.1291 2.6276 sec/batch\n",
      "Epoch 20/20  Iteration 3517/3560 Training loss: 1.1291 2.6904 sec/batch\n",
      "Epoch 20/20  Iteration 3518/3560 Training loss: 1.1291 2.6300 sec/batch\n",
      "Epoch 20/20  Iteration 3519/3560 Training loss: 1.1293 2.7102 sec/batch\n",
      "Epoch 20/20  Iteration 3520/3560 Training loss: 1.1294 2.6532 sec/batch\n",
      "Epoch 20/20  Iteration 3521/3560 Training loss: 1.1294 2.6798 sec/batch\n",
      "Epoch 20/20  Iteration 3522/3560 Training loss: 1.1295 2.8562 sec/batch\n",
      "Epoch 20/20  Iteration 3523/3560 Training loss: 1.1298 3.1833 sec/batch\n",
      "Epoch 20/20  Iteration 3524/3560 Training loss: 1.1298 2.6437 sec/batch\n",
      "Epoch 20/20  Iteration 3525/3560 Training loss: 1.1297 2.6843 sec/batch\n",
      "Epoch 20/20  Iteration 3526/3560 Training loss: 1.1300 2.6793 sec/batch\n",
      "Epoch 20/20  Iteration 3527/3560 Training loss: 1.1299 2.6659 sec/batch\n",
      "Epoch 20/20  Iteration 3528/3560 Training loss: 1.1301 2.6735 sec/batch\n",
      "Epoch 20/20  Iteration 3529/3560 Training loss: 1.1302 3.3454 sec/batch\n",
      "Epoch 20/20  Iteration 3530/3560 Training loss: 1.1304 2.9571 sec/batch\n",
      "Epoch 20/20  Iteration 3531/3560 Training loss: 1.1305 2.8401 sec/batch\n",
      "Epoch 20/20  Iteration 3532/3560 Training loss: 1.1305 2.8320 sec/batch\n",
      "Epoch 20/20  Iteration 3533/3560 Training loss: 1.1303 3.7212 sec/batch\n",
      "Epoch 20/20  Iteration 3534/3560 Training loss: 1.1301 2.7884 sec/batch\n",
      "Epoch 20/20  Iteration 3535/3560 Training loss: 1.1302 2.8581 sec/batch\n",
      "Epoch 20/20  Iteration 3536/3560 Training loss: 1.1301 2.8310 sec/batch\n",
      "Epoch 20/20  Iteration 3537/3560 Training loss: 1.1301 2.8952 sec/batch\n",
      "Epoch 20/20  Iteration 3538/3560 Training loss: 1.1300 2.9182 sec/batch\n",
      "Epoch 20/20  Iteration 3539/3560 Training loss: 1.1300 2.8486 sec/batch\n",
      "Epoch 20/20  Iteration 3540/3560 Training loss: 1.1299 2.9139 sec/batch\n",
      "Epoch 20/20  Iteration 3541/3560 Training loss: 1.1297 2.8449 sec/batch\n",
      "Epoch 20/20  Iteration 3542/3560 Training loss: 1.1299 3.0481 sec/batch\n",
      "Epoch 20/20  Iteration 3543/3560 Training loss: 1.1301 3.8926 sec/batch\n",
      "Epoch 20/20  Iteration 3544/3560 Training loss: 1.1301 3.1156 sec/batch\n",
      "Epoch 20/20  Iteration 3545/3560 Training loss: 1.1300 3.0389 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3546/3560 Training loss: 1.1299 2.9147 sec/batch\n",
      "Epoch 20/20  Iteration 3547/3560 Training loss: 1.1298 2.9758 sec/batch\n",
      "Epoch 20/20  Iteration 3548/3560 Training loss: 1.1298 3.0013 sec/batch\n",
      "Epoch 20/20  Iteration 3549/3560 Training loss: 1.1299 2.9257 sec/batch\n",
      "Epoch 20/20  Iteration 3550/3560 Training loss: 1.1303 3.2209 sec/batch\n",
      "Epoch 20/20  Iteration 3551/3560 Training loss: 1.1304 2.9823 sec/batch\n",
      "Epoch 20/20  Iteration 3552/3560 Training loss: 1.1304 2.9467 sec/batch\n",
      "Epoch 20/20  Iteration 3553/3560 Training loss: 1.1303 3.6912 sec/batch\n",
      "Epoch 20/20  Iteration 3554/3560 Training loss: 1.1302 2.8324 sec/batch\n",
      "Epoch 20/20  Iteration 3555/3560 Training loss: 1.1304 2.8258 sec/batch\n",
      "Epoch 20/20  Iteration 3556/3560 Training loss: 1.1305 2.8568 sec/batch\n",
      "Epoch 20/20  Iteration 3557/3560 Training loss: 1.1305 2.9413 sec/batch\n",
      "Epoch 20/20  Iteration 3558/3560 Training loss: 1.1304 2.7931 sec/batch\n",
      "Epoch 20/20  Iteration 3559/3560 Training loss: 1.1304 2.8807 sec/batch\n",
      "Epoch 20/20  Iteration 3560/3560 Training loss: 1.1306 2.8927 sec/batch\n",
      "Epoch 1/20  Iteration 1/3560 Training loss: 4.4226 2.9314 sec/batch\n",
      "Epoch 1/20  Iteration 2/3560 Training loss: 4.3785 2.8745 sec/batch\n",
      "Epoch 1/20  Iteration 3/3560 Training loss: 4.2049 3.8690 sec/batch\n",
      "Epoch 1/20  Iteration 4/3560 Training loss: 4.4371 2.8738 sec/batch\n",
      "Epoch 1/20  Iteration 5/3560 Training loss: 4.4094 2.8837 sec/batch\n",
      "Epoch 1/20  Iteration 6/3560 Training loss: 4.3229 2.9029 sec/batch\n",
      "Epoch 1/20  Iteration 7/3560 Training loss: 4.2346 2.8623 sec/batch\n",
      "Epoch 1/20  Iteration 8/3560 Training loss: 4.1562 2.8345 sec/batch\n",
      "Epoch 1/20  Iteration 9/3560 Training loss: 4.0859 2.8458 sec/batch\n",
      "Epoch 1/20  Iteration 10/3560 Training loss: 4.0260 2.9217 sec/batch\n",
      "Epoch 1/20  Iteration 11/3560 Training loss: 3.9716 2.8315 sec/batch\n",
      "Epoch 1/20  Iteration 12/3560 Training loss: 3.9235 2.9020 sec/batch\n",
      "Epoch 1/20  Iteration 13/3560 Training loss: 3.8808 3.6576 sec/batch\n",
      "Epoch 1/20  Iteration 14/3560 Training loss: 3.8434 2.9274 sec/batch\n",
      "Epoch 1/20  Iteration 15/3560 Training loss: 3.8098 2.8936 sec/batch\n",
      "Epoch 1/20  Iteration 16/3560 Training loss: 3.7791 2.8157 sec/batch\n",
      "Epoch 1/20  Iteration 17/3560 Training loss: 3.7509 2.9138 sec/batch\n",
      "Epoch 1/20  Iteration 18/3560 Training loss: 3.7269 2.8578 sec/batch\n",
      "Epoch 1/20  Iteration 19/3560 Training loss: 3.7044 2.7973 sec/batch\n",
      "Epoch 1/20  Iteration 20/3560 Training loss: 3.6819 2.9212 sec/batch\n",
      "Epoch 1/20  Iteration 21/3560 Training loss: 3.6625 2.8727 sec/batch\n",
      "Epoch 1/20  Iteration 22/3560 Training loss: 3.6440 2.8256 sec/batch\n",
      "Epoch 1/20  Iteration 23/3560 Training loss: 3.6269 3.6006 sec/batch\n",
      "Epoch 1/20  Iteration 24/3560 Training loss: 3.6110 2.9165 sec/batch\n",
      "Epoch 1/20  Iteration 25/3560 Training loss: 3.5959 2.8772 sec/batch\n",
      "Epoch 1/20  Iteration 26/3560 Training loss: 3.5828 2.8970 sec/batch\n",
      "Epoch 1/20  Iteration 27/3560 Training loss: 3.5704 2.8346 sec/batch\n",
      "Epoch 1/20  Iteration 28/3560 Training loss: 3.5581 2.8296 sec/batch\n",
      "Epoch 1/20  Iteration 29/3560 Training loss: 3.5467 2.8537 sec/batch\n",
      "Epoch 1/20  Iteration 30/3560 Training loss: 3.5359 2.7940 sec/batch\n",
      "Epoch 1/20  Iteration 31/3560 Training loss: 3.5268 2.8875 sec/batch\n",
      "Epoch 1/20  Iteration 32/3560 Training loss: 3.5169 2.8647 sec/batch\n",
      "Epoch 1/20  Iteration 33/3560 Training loss: 3.5074 3.7241 sec/batch\n",
      "Epoch 1/20  Iteration 34/3560 Training loss: 3.4988 2.8807 sec/batch\n",
      "Epoch 1/20  Iteration 35/3560 Training loss: 3.4902 2.8302 sec/batch\n",
      "Epoch 1/20  Iteration 36/3560 Training loss: 3.4825 2.9104 sec/batch\n",
      "Epoch 1/20  Iteration 37/3560 Training loss: 3.4743 2.9179 sec/batch\n",
      "Epoch 1/20  Iteration 38/3560 Training loss: 3.4667 2.8113 sec/batch\n",
      "Epoch 1/20  Iteration 39/3560 Training loss: 3.4593 2.8462 sec/batch\n",
      "Epoch 1/20  Iteration 40/3560 Training loss: 3.4523 2.8564 sec/batch\n",
      "Epoch 1/20  Iteration 41/3560 Training loss: 3.4453 2.8743 sec/batch\n",
      "Epoch 1/20  Iteration 42/3560 Training loss: 3.4389 2.8950 sec/batch\n",
      "Epoch 1/20  Iteration 43/3560 Training loss: 3.4326 3.5087 sec/batch\n",
      "Epoch 1/20  Iteration 44/3560 Training loss: 3.4267 3.1468 sec/batch\n",
      "Epoch 1/20  Iteration 45/3560 Training loss: 3.4208 2.8680 sec/batch\n",
      "Epoch 1/20  Iteration 46/3560 Training loss: 3.4154 2.8673 sec/batch\n",
      "Epoch 1/20  Iteration 47/3560 Training loss: 3.4102 2.9078 sec/batch\n",
      "Epoch 1/20  Iteration 48/3560 Training loss: 3.4053 2.8994 sec/batch\n",
      "Epoch 1/20  Iteration 49/3560 Training loss: 3.4007 2.8365 sec/batch\n",
      "Epoch 1/20  Iteration 50/3560 Training loss: 3.3961 3.0543 sec/batch\n",
      "Epoch 1/20  Iteration 51/3560 Training loss: 3.3916 2.9848 sec/batch\n",
      "Epoch 1/20  Iteration 52/3560 Training loss: 3.3871 2.8846 sec/batch\n",
      "Epoch 1/20  Iteration 53/3560 Training loss: 3.3829 3.5019 sec/batch\n",
      "Epoch 1/20  Iteration 54/3560 Training loss: 3.3785 2.9744 sec/batch\n",
      "Epoch 1/20  Iteration 55/3560 Training loss: 3.3745 2.8663 sec/batch\n",
      "Epoch 1/20  Iteration 56/3560 Training loss: 3.3703 2.8660 sec/batch\n",
      "Epoch 1/20  Iteration 57/3560 Training loss: 3.3664 2.8264 sec/batch\n",
      "Epoch 1/20  Iteration 58/3560 Training loss: 3.3628 2.9499 sec/batch\n",
      "Epoch 1/20  Iteration 59/3560 Training loss: 3.3591 2.8786 sec/batch\n",
      "Epoch 1/20  Iteration 60/3560 Training loss: 3.3556 2.8505 sec/batch\n",
      "Epoch 1/20  Iteration 61/3560 Training loss: 3.3522 2.9240 sec/batch\n",
      "Epoch 1/20  Iteration 62/3560 Training loss: 3.3492 2.9427 sec/batch\n",
      "Epoch 1/20  Iteration 63/3560 Training loss: 3.3463 3.2752 sec/batch\n",
      "Epoch 1/20  Iteration 64/3560 Training loss: 3.3429 3.2130 sec/batch\n",
      "Epoch 1/20  Iteration 65/3560 Training loss: 3.3397 2.8927 sec/batch\n",
      "Epoch 1/20  Iteration 66/3560 Training loss: 3.3369 2.9569 sec/batch\n",
      "Epoch 1/20  Iteration 67/3560 Training loss: 3.3342 2.8755 sec/batch\n",
      "Epoch 1/20  Iteration 68/3560 Training loss: 3.3308 2.8523 sec/batch\n",
      "Epoch 1/20  Iteration 69/3560 Training loss: 3.3279 2.8834 sec/batch\n",
      "Epoch 1/20  Iteration 70/3560 Training loss: 3.3253 2.8477 sec/batch\n",
      "Epoch 1/20  Iteration 71/3560 Training loss: 3.3226 2.7773 sec/batch\n",
      "Epoch 1/20  Iteration 72/3560 Training loss: 3.3203 2.9179 sec/batch\n",
      "Epoch 1/20  Iteration 73/3560 Training loss: 3.3177 3.1589 sec/batch\n",
      "Epoch 1/20  Iteration 74/3560 Training loss: 3.3153 3.4782 sec/batch\n",
      "Epoch 1/20  Iteration 75/3560 Training loss: 3.3131 2.8879 sec/batch\n",
      "Epoch 1/20  Iteration 76/3560 Training loss: 3.3108 2.8989 sec/batch\n",
      "Epoch 1/20  Iteration 77/3560 Training loss: 3.3086 2.8711 sec/batch\n",
      "Epoch 1/20  Iteration 78/3560 Training loss: 3.3064 2.9200 sec/batch\n",
      "Epoch 1/20  Iteration 79/3560 Training loss: 3.3042 2.8625 sec/batch\n",
      "Epoch 1/20  Iteration 80/3560 Training loss: 3.3018 2.8532 sec/batch\n",
      "Epoch 1/20  Iteration 81/3560 Training loss: 3.2995 2.8333 sec/batch\n",
      "Epoch 1/20  Iteration 82/3560 Training loss: 3.2976 2.9844 sec/batch\n",
      "Epoch 1/20  Iteration 83/3560 Training loss: 3.2956 2.8210 sec/batch\n",
      "Epoch 1/20  Iteration 84/3560 Training loss: 3.2936 3.5815 sec/batch\n",
      "Epoch 1/20  Iteration 85/3560 Training loss: 3.2914 2.9009 sec/batch\n",
      "Epoch 1/20  Iteration 86/3560 Training loss: 3.2894 2.8443 sec/batch\n",
      "Epoch 1/20  Iteration 87/3560 Training loss: 3.2873 2.7961 sec/batch\n",
      "Epoch 1/20  Iteration 88/3560 Training loss: 3.2853 2.9113 sec/batch\n",
      "Epoch 1/20  Iteration 89/3560 Training loss: 3.2835 2.8992 sec/batch\n",
      "Epoch 1/20  Iteration 90/3560 Training loss: 3.2817 2.8553 sec/batch\n",
      "Epoch 1/20  Iteration 91/3560 Training loss: 3.2799 2.8959 sec/batch\n",
      "Epoch 1/20  Iteration 92/3560 Training loss: 3.2781 2.8858 sec/batch\n",
      "Epoch 1/20  Iteration 93/3560 Training loss: 3.2763 2.8420 sec/batch\n",
      "Epoch 1/20  Iteration 94/3560 Training loss: 3.2745 3.6640 sec/batch\n",
      "Epoch 1/20  Iteration 95/3560 Training loss: 3.2727 2.8353 sec/batch\n",
      "Epoch 1/20  Iteration 96/3560 Training loss: 3.2709 2.9355 sec/batch\n",
      "Epoch 1/20  Iteration 97/3560 Training loss: 3.2693 2.9210 sec/batch\n",
      "Epoch 1/20  Iteration 98/3560 Training loss: 3.2676 2.8521 sec/batch\n",
      "Epoch 1/20  Iteration 99/3560 Training loss: 3.2660 2.9750 sec/batch\n",
      "Epoch 1/20  Iteration 100/3560 Training loss: 3.2643 2.9122 sec/batch\n",
      "Epoch 1/20  Iteration 101/3560 Training loss: 3.2627 2.8190 sec/batch\n",
      "Epoch 1/20  Iteration 102/3560 Training loss: 3.2611 2.8615 sec/batch\n",
      "Epoch 1/20  Iteration 103/3560 Training loss: 3.2595 2.9931 sec/batch\n",
      "Epoch 1/20  Iteration 104/3560 Training loss: 3.2578 3.7168 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 105/3560 Training loss: 3.2561 3.1434 sec/batch\n",
      "Epoch 1/20  Iteration 106/3560 Training loss: 3.2545 3.3375 sec/batch\n",
      "Epoch 1/20  Iteration 107/3560 Training loss: 3.2526 2.9742 sec/batch\n",
      "Epoch 1/20  Iteration 108/3560 Training loss: 3.2509 2.9270 sec/batch\n",
      "Epoch 1/20  Iteration 109/3560 Training loss: 3.2493 3.0306 sec/batch\n",
      "Epoch 1/20  Iteration 110/3560 Training loss: 3.2474 3.0219 sec/batch\n",
      "Epoch 1/20  Iteration 111/3560 Training loss: 3.2457 3.2442 sec/batch\n",
      "Epoch 1/20  Iteration 112/3560 Training loss: 3.2441 3.3407 sec/batch\n",
      "Epoch 1/20  Iteration 113/3560 Training loss: 3.2424 3.8390 sec/batch\n",
      "Epoch 1/20  Iteration 114/3560 Training loss: 3.2406 4.1063 sec/batch\n",
      "Epoch 1/20  Iteration 115/3560 Training loss: 3.2387 3.2580 sec/batch\n",
      "Epoch 1/20  Iteration 116/3560 Training loss: 3.2368 3.2481 sec/batch\n",
      "Epoch 1/20  Iteration 117/3560 Training loss: 3.2350 3.3535 sec/batch\n",
      "Epoch 1/20  Iteration 118/3560 Training loss: 3.2333 3.3400 sec/batch\n",
      "Epoch 1/20  Iteration 119/3560 Training loss: 3.2316 3.2728 sec/batch\n",
      "Epoch 1/20  Iteration 120/3560 Training loss: 3.2297 3.1508 sec/batch\n",
      "Epoch 1/20  Iteration 121/3560 Training loss: 3.2280 3.3492 sec/batch\n",
      "Epoch 1/20  Iteration 122/3560 Training loss: 3.2261 3.2061 sec/batch\n",
      "Epoch 1/20  Iteration 123/3560 Training loss: 3.2241 3.1369 sec/batch\n",
      "Epoch 1/20  Iteration 124/3560 Training loss: 3.2223 3.6391 sec/batch\n",
      "Epoch 1/20  Iteration 125/3560 Training loss: 3.2207 3.4332 sec/batch\n",
      "Epoch 1/20  Iteration 126/3560 Training loss: 3.2187 3.0670 sec/batch\n",
      "Epoch 1/20  Iteration 127/3560 Training loss: 3.2168 3.0709 sec/batch\n",
      "Epoch 1/20  Iteration 128/3560 Training loss: 3.2151 3.0093 sec/batch\n",
      "Epoch 1/20  Iteration 129/3560 Training loss: 3.2131 3.1336 sec/batch\n",
      "Epoch 1/20  Iteration 130/3560 Training loss: 3.2111 3.1937 sec/batch\n",
      "Epoch 1/20  Iteration 131/3560 Training loss: 3.2090 3.0299 sec/batch\n",
      "Epoch 1/20  Iteration 132/3560 Training loss: 3.2068 3.0686 sec/batch\n",
      "Epoch 1/20  Iteration 133/3560 Training loss: 3.2046 3.2359 sec/batch\n",
      "Epoch 1/20  Iteration 134/3560 Training loss: 3.2026 3.1885 sec/batch\n",
      "Epoch 1/20  Iteration 135/3560 Training loss: 3.2003 3.1337 sec/batch\n",
      "Epoch 1/20  Iteration 136/3560 Training loss: 3.1979 3.4150 sec/batch\n",
      "Epoch 1/20  Iteration 137/3560 Training loss: 3.1957 3.1443 sec/batch\n",
      "Epoch 1/20  Iteration 138/3560 Training loss: 3.1933 2.9667 sec/batch\n",
      "Epoch 1/20  Iteration 139/3560 Training loss: 3.1910 3.0501 sec/batch\n",
      "Epoch 1/20  Iteration 140/3560 Training loss: 3.1886 3.2741 sec/batch\n",
      "Epoch 1/20  Iteration 141/3560 Training loss: 3.1862 3.0956 sec/batch\n",
      "Epoch 1/20  Iteration 142/3560 Training loss: 3.1836 3.1869 sec/batch\n",
      "Epoch 1/20  Iteration 143/3560 Training loss: 3.1810 3.6433 sec/batch\n",
      "Epoch 1/20  Iteration 144/3560 Training loss: 3.1784 3.3937 sec/batch\n",
      "Epoch 1/20  Iteration 145/3560 Training loss: 3.1758 2.9813 sec/batch\n",
      "Epoch 1/20  Iteration 146/3560 Training loss: 3.1732 3.0019 sec/batch\n",
      "Epoch 1/20  Iteration 147/3560 Training loss: 3.1705 3.1668 sec/batch\n",
      "Epoch 1/20  Iteration 148/3560 Training loss: 3.1680 3.1370 sec/batch\n",
      "Epoch 1/20  Iteration 149/3560 Training loss: 3.1652 3.2510 sec/batch\n",
      "Epoch 1/20  Iteration 150/3560 Training loss: 3.1628 3.2714 sec/batch\n",
      "Epoch 1/20  Iteration 151/3560 Training loss: 3.1605 3.1516 sec/batch\n",
      "Epoch 1/20  Iteration 152/3560 Training loss: 3.1580 4.3990 sec/batch\n",
      "Epoch 1/20  Iteration 153/3560 Training loss: 3.1555 3.8589 sec/batch\n",
      "Epoch 1/20  Iteration 154/3560 Training loss: 3.1529 3.3135 sec/batch\n",
      "Epoch 1/20  Iteration 155/3560 Training loss: 3.1501 3.3683 sec/batch\n",
      "Epoch 1/20  Iteration 156/3560 Training loss: 3.1473 3.4387 sec/batch\n",
      "Epoch 1/20  Iteration 157/3560 Training loss: 3.1445 3.1955 sec/batch\n",
      "Epoch 1/20  Iteration 158/3560 Training loss: 3.1416 3.6341 sec/batch\n",
      "Epoch 1/20  Iteration 159/3560 Training loss: 3.1386 3.4147 sec/batch\n",
      "Epoch 1/20  Iteration 160/3560 Training loss: 3.1359 3.5681 sec/batch\n",
      "Epoch 1/20  Iteration 161/3560 Training loss: 3.1329 3.5777 sec/batch\n",
      "Epoch 1/20  Iteration 162/3560 Training loss: 3.1299 3.2369 sec/batch\n",
      "Epoch 1/20  Iteration 163/3560 Training loss: 3.1267 3.5698 sec/batch\n",
      "Epoch 1/20  Iteration 164/3560 Training loss: 3.1237 3.4079 sec/batch\n",
      "Epoch 1/20  Iteration 165/3560 Training loss: 3.1207 3.3894 sec/batch\n",
      "Epoch 1/20  Iteration 166/3560 Training loss: 3.1177 3.3210 sec/batch\n",
      "Epoch 1/20  Iteration 167/3560 Training loss: 3.1147 3.6520 sec/batch\n",
      "Epoch 1/20  Iteration 168/3560 Training loss: 3.1118 3.5469 sec/batch\n",
      "Epoch 1/20  Iteration 169/3560 Training loss: 3.1087 3.7699 sec/batch\n",
      "Epoch 1/20  Iteration 170/3560 Training loss: 3.1056 3.9905 sec/batch\n",
      "Epoch 1/20  Iteration 171/3560 Training loss: 3.1027 3.4019 sec/batch\n",
      "Epoch 1/20  Iteration 172/3560 Training loss: 3.0998 3.5663 sec/batch\n",
      "Epoch 1/20  Iteration 173/3560 Training loss: 3.0971 3.5518 sec/batch\n",
      "Epoch 1/20  Iteration 174/3560 Training loss: 3.0943 3.7375 sec/batch\n",
      "Epoch 1/20  Iteration 175/3560 Training loss: 3.0919 3.6673 sec/batch\n",
      "Epoch 1/20  Iteration 176/3560 Training loss: 3.0895 3.5469 sec/batch\n",
      "Epoch 1/20  Iteration 177/3560 Training loss: 3.0865 3.5152 sec/batch\n",
      "Epoch 1/20  Iteration 178/3560 Training loss: 3.0835 3.6722 sec/batch\n",
      "Epoch 2/20  Iteration 179/3560 Training loss: 2.6153 3.5321 sec/batch\n",
      "Epoch 2/20  Iteration 180/3560 Training loss: 2.5652 3.8667 sec/batch\n",
      "Epoch 2/20  Iteration 181/3560 Training loss: 2.5513 3.4596 sec/batch\n",
      "Epoch 2/20  Iteration 182/3560 Training loss: 2.5455 3.4694 sec/batch\n",
      "Epoch 2/20  Iteration 183/3560 Training loss: 2.5410 3.7465 sec/batch\n",
      "Epoch 2/20  Iteration 184/3560 Training loss: 2.5367 3.6951 sec/batch\n",
      "Epoch 2/20  Iteration 185/3560 Training loss: 2.5352 3.7482 sec/batch\n",
      "Epoch 2/20  Iteration 186/3560 Training loss: 2.5334 3.6430 sec/batch\n",
      "Epoch 2/20  Iteration 187/3560 Training loss: 2.5335 3.6204 sec/batch\n",
      "Epoch 2/20  Iteration 188/3560 Training loss: 2.5310 3.7848 sec/batch\n",
      "Epoch 2/20  Iteration 189/3560 Training loss: 2.5280 3.8000 sec/batch\n",
      "Epoch 2/20  Iteration 190/3560 Training loss: 2.5264 3.1989 sec/batch\n",
      "Epoch 2/20  Iteration 191/3560 Training loss: 2.5245 3.5205 sec/batch\n",
      "Epoch 2/20  Iteration 192/3560 Training loss: 2.5250 3.5766 sec/batch\n",
      "Epoch 2/20  Iteration 193/3560 Training loss: 2.5229 3.4183 sec/batch\n",
      "Epoch 2/20  Iteration 194/3560 Training loss: 2.5213 3.2044 sec/batch\n",
      "Epoch 2/20  Iteration 195/3560 Training loss: 2.5195 3.2460 sec/batch\n",
      "Epoch 2/20  Iteration 196/3560 Training loss: 2.5191 3.4738 sec/batch\n",
      "Epoch 2/20  Iteration 197/3560 Training loss: 2.5175 3.2259 sec/batch\n",
      "Epoch 2/20  Iteration 198/3560 Training loss: 2.5143 3.4268 sec/batch\n",
      "Epoch 2/20  Iteration 199/3560 Training loss: 2.5118 3.1682 sec/batch\n",
      "Epoch 2/20  Iteration 200/3560 Training loss: 2.5113 2.9266 sec/batch\n",
      "Epoch 2/20  Iteration 201/3560 Training loss: 2.5091 3.3863 sec/batch\n",
      "Epoch 2/20  Iteration 202/3560 Training loss: 2.5068 2.9354 sec/batch\n",
      "Epoch 2/20  Iteration 203/3560 Training loss: 2.5047 3.0811 sec/batch\n",
      "Epoch 2/20  Iteration 204/3560 Training loss: 2.5031 3.3388 sec/batch\n",
      "Epoch 2/20  Iteration 205/3560 Training loss: 2.5009 3.0733 sec/batch\n",
      "Epoch 2/20  Iteration 206/3560 Training loss: 2.4991 2.9366 sec/batch\n",
      "Epoch 2/20  Iteration 207/3560 Training loss: 2.4980 3.3045 sec/batch\n",
      "Epoch 2/20  Iteration 208/3560 Training loss: 2.4962 3.5047 sec/batch\n",
      "Epoch 2/20  Iteration 209/3560 Training loss: 2.4951 3.2229 sec/batch\n",
      "Epoch 2/20  Iteration 210/3560 Training loss: 2.4931 3.2340 sec/batch\n",
      "Epoch 2/20  Iteration 211/3560 Training loss: 2.4906 3.1479 sec/batch\n",
      "Epoch 2/20  Iteration 212/3560 Training loss: 2.4891 3.0290 sec/batch\n",
      "Epoch 2/20  Iteration 213/3560 Training loss: 2.4871 3.4132 sec/batch\n",
      "Epoch 2/20  Iteration 214/3560 Training loss: 2.4856 3.7282 sec/batch\n",
      "Epoch 2/20  Iteration 215/3560 Training loss: 2.4838 3.2175 sec/batch\n",
      "Epoch 2/20  Iteration 216/3560 Training loss: 2.4815 3.1923 sec/batch\n",
      "Epoch 2/20  Iteration 217/3560 Training loss: 2.4794 3.4668 sec/batch\n",
      "Epoch 2/20  Iteration 218/3560 Training loss: 2.4774 3.4188 sec/batch\n",
      "Epoch 2/20  Iteration 219/3560 Training loss: 2.4755 3.1582 sec/batch\n",
      "Epoch 2/20  Iteration 220/3560 Training loss: 2.4734 3.6837 sec/batch\n",
      "Epoch 2/20  Iteration 221/3560 Training loss: 2.4712 3.5790 sec/batch\n",
      "Epoch 2/20  Iteration 222/3560 Training loss: 2.4690 3.3339 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 223/3560 Training loss: 2.4671 3.7129 sec/batch\n",
      "Epoch 2/20  Iteration 224/3560 Training loss: 2.4644 3.4107 sec/batch\n",
      "Epoch 2/20  Iteration 225/3560 Training loss: 2.4630 3.1814 sec/batch\n",
      "Epoch 2/20  Iteration 226/3560 Training loss: 2.4613 3.4399 sec/batch\n",
      "Epoch 2/20  Iteration 227/3560 Training loss: 2.4596 3.1940 sec/batch\n",
      "Epoch 2/20  Iteration 228/3560 Training loss: 2.4584 3.6746 sec/batch\n",
      "Epoch 2/20  Iteration 229/3560 Training loss: 2.4566 3.3571 sec/batch\n",
      "Epoch 2/20  Iteration 230/3560 Training loss: 2.4554 3.1373 sec/batch\n",
      "Epoch 2/20  Iteration 231/3560 Training loss: 2.4536 3.1678 sec/batch\n",
      "Epoch 2/20  Iteration 232/3560 Training loss: 2.4518 3.3132 sec/batch\n",
      "Epoch 2/20  Iteration 233/3560 Training loss: 2.4501 3.1392 sec/batch\n",
      "Epoch 2/20  Iteration 234/3560 Training loss: 2.4486 3.4933 sec/batch\n",
      "Epoch 2/20  Iteration 235/3560 Training loss: 2.4471 3.8507 sec/batch\n",
      "Epoch 2/20  Iteration 236/3560 Training loss: 2.4454 3.6209 sec/batch\n",
      "Epoch 2/20  Iteration 237/3560 Training loss: 2.4438 3.7402 sec/batch\n",
      "Epoch 2/20  Iteration 238/3560 Training loss: 2.4426 3.7116 sec/batch\n",
      "Epoch 2/20  Iteration 239/3560 Training loss: 2.4411 3.4441 sec/batch\n",
      "Epoch 2/20  Iteration 240/3560 Training loss: 2.4400 3.6622 sec/batch\n",
      "Epoch 2/20  Iteration 241/3560 Training loss: 2.4390 3.4876 sec/batch\n",
      "Epoch 2/20  Iteration 242/3560 Training loss: 2.4376 3.5968 sec/batch\n",
      "Epoch 2/20  Iteration 243/3560 Training loss: 2.4361 3.3868 sec/batch\n",
      "Epoch 2/20  Iteration 244/3560 Training loss: 2.4349 3.6896 sec/batch\n",
      "Epoch 2/20  Iteration 245/3560 Training loss: 2.4335 3.6831 sec/batch\n",
      "Epoch 2/20  Iteration 246/3560 Training loss: 2.4317 3.9320 sec/batch\n",
      "Epoch 2/20  Iteration 247/3560 Training loss: 2.4300 3.4880 sec/batch\n",
      "Epoch 2/20  Iteration 248/3560 Training loss: 2.4288 3.5621 sec/batch\n",
      "Epoch 2/20  Iteration 249/3560 Training loss: 2.4277 3.5142 sec/batch\n",
      "Epoch 2/20  Iteration 250/3560 Training loss: 2.4265 3.5505 sec/batch\n",
      "Epoch 2/20  Iteration 251/3560 Training loss: 2.4253 3.3636 sec/batch\n",
      "Epoch 2/20  Iteration 252/3560 Training loss: 2.4237 3.7860 sec/batch\n",
      "Epoch 2/20  Iteration 253/3560 Training loss: 2.4223 3.4727 sec/batch\n",
      "Epoch 2/20  Iteration 254/3560 Training loss: 2.4213 3.3355 sec/batch\n",
      "Epoch 2/20  Iteration 255/3560 Training loss: 2.4199 3.3681 sec/batch\n",
      "Epoch 2/20  Iteration 256/3560 Training loss: 2.4187 3.3597 sec/batch\n",
      "Epoch 2/20  Iteration 257/3560 Training loss: 2.4173 3.7495 sec/batch\n",
      "Epoch 2/20  Iteration 258/3560 Training loss: 2.4158 4.2858 sec/batch\n",
      "Epoch 2/20  Iteration 259/3560 Training loss: 2.4144 4.1149 sec/batch\n",
      "Epoch 2/20  Iteration 260/3560 Training loss: 2.4133 3.8493 sec/batch\n",
      "Epoch 2/20  Iteration 261/3560 Training loss: 2.4118 3.9520 sec/batch\n",
      "Epoch 2/20  Iteration 262/3560 Training loss: 2.4103 3.6497 sec/batch\n",
      "Epoch 2/20  Iteration 263/3560 Training loss: 2.4086 3.6824 sec/batch\n",
      "Epoch 2/20  Iteration 264/3560 Training loss: 2.4072 4.1967 sec/batch\n",
      "Epoch 2/20  Iteration 265/3560 Training loss: 2.4059 4.1043 sec/batch\n",
      "Epoch 2/20  Iteration 266/3560 Training loss: 2.4046 3.8164 sec/batch\n",
      "Epoch 2/20  Iteration 267/3560 Training loss: 2.4031 3.8739 sec/batch\n",
      "Epoch 2/20  Iteration 268/3560 Training loss: 2.4019 4.1852 sec/batch\n",
      "Epoch 2/20  Iteration 269/3560 Training loss: 2.4005 3.7688 sec/batch\n",
      "Epoch 2/20  Iteration 270/3560 Training loss: 2.3993 3.8190 sec/batch\n",
      "Epoch 2/20  Iteration 271/3560 Training loss: 2.3979 3.6765 sec/batch\n",
      "Epoch 2/20  Iteration 272/3560 Training loss: 2.3964 3.6297 sec/batch\n",
      "Epoch 2/20  Iteration 273/3560 Training loss: 2.3950 3.7644 sec/batch\n",
      "Epoch 2/20  Iteration 274/3560 Training loss: 2.3936 3.7716 sec/batch\n",
      "Epoch 2/20  Iteration 275/3560 Training loss: 2.3923 3.6065 sec/batch\n",
      "Epoch 2/20  Iteration 276/3560 Training loss: 2.3909 3.5273 sec/batch\n",
      "Epoch 2/20  Iteration 277/3560 Training loss: 2.3893 3.4380 sec/batch\n",
      "Epoch 2/20  Iteration 278/3560 Training loss: 2.3879 3.9045 sec/batch\n",
      "Epoch 2/20  Iteration 279/3560 Training loss: 2.3867 3.5914 sec/batch\n",
      "Epoch 2/20  Iteration 280/3560 Training loss: 2.3856 3.4294 sec/batch\n",
      "Epoch 2/20  Iteration 281/3560 Training loss: 2.3840 3.5148 sec/batch\n",
      "Epoch 2/20  Iteration 282/3560 Training loss: 2.3827 3.8880 sec/batch\n",
      "Epoch 2/20  Iteration 283/3560 Training loss: 2.3813 3.6768 sec/batch\n",
      "Epoch 2/20  Iteration 284/3560 Training loss: 2.3800 4.5708 sec/batch\n",
      "Epoch 2/20  Iteration 285/3560 Training loss: 2.3787 3.7700 sec/batch\n",
      "Epoch 2/20  Iteration 286/3560 Training loss: 2.3778 3.8500 sec/batch\n",
      "Epoch 2/20  Iteration 287/3560 Training loss: 2.3768 3.9683 sec/batch\n",
      "Epoch 2/20  Iteration 288/3560 Training loss: 2.3755 3.7936 sec/batch\n",
      "Epoch 2/20  Iteration 289/3560 Training loss: 2.3743 3.6179 sec/batch\n",
      "Epoch 2/20  Iteration 290/3560 Training loss: 2.3732 3.7669 sec/batch\n",
      "Epoch 2/20  Iteration 291/3560 Training loss: 2.3718 3.5076 sec/batch\n",
      "Epoch 2/20  Iteration 292/3560 Training loss: 2.3706 3.5202 sec/batch\n",
      "Epoch 2/20  Iteration 293/3560 Training loss: 2.3693 3.4145 sec/batch\n",
      "Epoch 2/20  Iteration 294/3560 Training loss: 2.3678 3.3769 sec/batch\n",
      "Epoch 2/20  Iteration 295/3560 Training loss: 2.3665 3.3216 sec/batch\n",
      "Epoch 2/20  Iteration 296/3560 Training loss: 2.3653 3.2437 sec/batch\n",
      "Epoch 2/20  Iteration 297/3560 Training loss: 2.3642 3.2687 sec/batch\n",
      "Epoch 2/20  Iteration 298/3560 Training loss: 2.3631 3.4851 sec/batch\n",
      "Epoch 2/20  Iteration 299/3560 Training loss: 2.3622 3.3197 sec/batch\n",
      "Epoch 2/20  Iteration 300/3560 Training loss: 2.3609 3.5940 sec/batch\n",
      "Epoch 2/20  Iteration 301/3560 Training loss: 2.3597 3.4667 sec/batch\n",
      "Epoch 2/20  Iteration 302/3560 Training loss: 2.3586 3.2766 sec/batch\n",
      "Epoch 2/20  Iteration 303/3560 Training loss: 2.3575 3.4884 sec/batch\n",
      "Epoch 2/20  Iteration 304/3560 Training loss: 2.3562 3.3496 sec/batch\n",
      "Epoch 2/20  Iteration 305/3560 Training loss: 2.3551 3.3088 sec/batch\n",
      "Epoch 2/20  Iteration 306/3560 Training loss: 2.3540 3.3380 sec/batch\n",
      "Epoch 2/20  Iteration 307/3560 Training loss: 2.3529 3.4285 sec/batch\n",
      "Epoch 2/20  Iteration 308/3560 Training loss: 2.3518 3.8488 sec/batch\n",
      "Epoch 2/20  Iteration 309/3560 Training loss: 2.3506 3.5924 sec/batch\n",
      "Epoch 2/20  Iteration 310/3560 Training loss: 2.3493 3.6042 sec/batch\n",
      "Epoch 2/20  Iteration 311/3560 Training loss: 2.3482 3.5247 sec/batch\n",
      "Epoch 2/20  Iteration 312/3560 Training loss: 2.3472 3.5722 sec/batch\n",
      "Epoch 2/20  Iteration 313/3560 Training loss: 2.3461 3.6746 sec/batch\n",
      "Epoch 2/20  Iteration 314/3560 Training loss: 2.3451 3.6268 sec/batch\n",
      "Epoch 2/20  Iteration 315/3560 Training loss: 2.3440 3.6301 sec/batch\n",
      "Epoch 2/20  Iteration 316/3560 Training loss: 2.3429 3.6499 sec/batch\n",
      "Epoch 2/20  Iteration 317/3560 Training loss: 2.3421 3.8061 sec/batch\n",
      "Epoch 2/20  Iteration 318/3560 Training loss: 2.3410 3.7203 sec/batch\n",
      "Epoch 2/20  Iteration 319/3560 Training loss: 2.3401 3.7907 sec/batch\n",
      "Epoch 2/20  Iteration 320/3560 Training loss: 2.3389 3.4734 sec/batch\n",
      "Epoch 2/20  Iteration 321/3560 Training loss: 2.3378 3.6028 sec/batch\n",
      "Epoch 2/20  Iteration 322/3560 Training loss: 2.3367 3.4830 sec/batch\n",
      "Epoch 2/20  Iteration 323/3560 Training loss: 2.3356 3.5646 sec/batch\n",
      "Epoch 2/20  Iteration 324/3560 Training loss: 2.3347 3.5476 sec/batch\n",
      "Epoch 2/20  Iteration 325/3560 Training loss: 2.3337 3.7984 sec/batch\n",
      "Epoch 2/20  Iteration 326/3560 Training loss: 2.3328 3.5567 sec/batch\n",
      "Epoch 2/20  Iteration 327/3560 Training loss: 2.3318 3.5625 sec/batch\n",
      "Epoch 2/20  Iteration 328/3560 Training loss: 2.3306 3.5154 sec/batch\n",
      "Epoch 2/20  Iteration 329/3560 Training loss: 2.3297 3.4638 sec/batch\n",
      "Epoch 2/20  Iteration 330/3560 Training loss: 2.3289 3.5274 sec/batch\n",
      "Epoch 2/20  Iteration 331/3560 Training loss: 2.3280 3.5261 sec/batch\n",
      "Epoch 2/20  Iteration 332/3560 Training loss: 2.3270 3.5928 sec/batch\n",
      "Epoch 2/20  Iteration 333/3560 Training loss: 2.3258 3.4865 sec/batch\n",
      "Epoch 2/20  Iteration 334/3560 Training loss: 2.3248 3.5521 sec/batch\n",
      "Epoch 2/20  Iteration 335/3560 Training loss: 2.3238 3.6314 sec/batch\n",
      "Epoch 2/20  Iteration 336/3560 Training loss: 2.3228 3.5342 sec/batch\n",
      "Epoch 2/20  Iteration 337/3560 Training loss: 2.3216 3.4534 sec/batch\n",
      "Epoch 2/20  Iteration 338/3560 Training loss: 2.3208 3.5956 sec/batch\n",
      "Epoch 2/20  Iteration 339/3560 Training loss: 2.3199 3.4671 sec/batch\n",
      "Epoch 2/20  Iteration 340/3560 Training loss: 2.3188 3.4772 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Iteration 341/3560 Training loss: 2.3178 3.5715 sec/batch\n",
      "Epoch 2/20  Iteration 342/3560 Training loss: 2.3167 3.4903 sec/batch\n",
      "Epoch 2/20  Iteration 343/3560 Training loss: 2.3158 3.4177 sec/batch\n",
      "Epoch 2/20  Iteration 344/3560 Training loss: 2.3147 3.6621 sec/batch\n",
      "Epoch 2/20  Iteration 345/3560 Training loss: 2.3138 3.5341 sec/batch\n",
      "Epoch 2/20  Iteration 346/3560 Training loss: 2.3129 3.5372 sec/batch\n",
      "Epoch 2/20  Iteration 347/3560 Training loss: 2.3119 3.8232 sec/batch\n",
      "Epoch 2/20  Iteration 348/3560 Training loss: 2.3109 3.5372 sec/batch\n",
      "Epoch 2/20  Iteration 349/3560 Training loss: 2.3098 3.6307 sec/batch\n",
      "Epoch 2/20  Iteration 350/3560 Training loss: 2.3088 3.6321 sec/batch\n",
      "Epoch 2/20  Iteration 351/3560 Training loss: 2.3080 3.5082 sec/batch\n",
      "Epoch 2/20  Iteration 352/3560 Training loss: 2.3071 3.7551 sec/batch\n",
      "Epoch 2/20  Iteration 353/3560 Training loss: 2.3062 3.5463 sec/batch\n",
      "Epoch 2/20  Iteration 354/3560 Training loss: 2.3052 4.2901 sec/batch\n",
      "Epoch 2/20  Iteration 355/3560 Training loss: 2.3042 4.4266 sec/batch\n",
      "Epoch 2/20  Iteration 356/3560 Training loss: 2.3032 4.3860 sec/batch\n",
      "Epoch 3/20  Iteration 357/3560 Training loss: 2.2108 4.2741 sec/batch\n",
      "Epoch 3/20  Iteration 358/3560 Training loss: 2.1573 4.1613 sec/batch\n",
      "Epoch 3/20  Iteration 359/3560 Training loss: 2.1392 3.9367 sec/batch\n",
      "Epoch 3/20  Iteration 360/3560 Training loss: 2.1306 4.2063 sec/batch\n",
      "Epoch 3/20  Iteration 361/3560 Training loss: 2.1285 4.0436 sec/batch\n",
      "Epoch 3/20  Iteration 362/3560 Training loss: 2.1210 4.0552 sec/batch\n",
      "Epoch 3/20  Iteration 363/3560 Training loss: 2.1218 4.0134 sec/batch\n",
      "Epoch 3/20  Iteration 364/3560 Training loss: 2.1218 4.2377 sec/batch\n",
      "Epoch 3/20  Iteration 365/3560 Training loss: 2.1230 4.4421 sec/batch\n",
      "Epoch 3/20  Iteration 366/3560 Training loss: 2.1225 4.0815 sec/batch\n",
      "Epoch 3/20  Iteration 367/3560 Training loss: 2.1199 4.0425 sec/batch\n",
      "Epoch 3/20  Iteration 368/3560 Training loss: 2.1172 3.9765 sec/batch\n",
      "Epoch 3/20  Iteration 369/3560 Training loss: 2.1163 4.1091 sec/batch\n",
      "Epoch 3/20  Iteration 370/3560 Training loss: 2.1175 3.9244 sec/batch\n",
      "Epoch 3/20  Iteration 371/3560 Training loss: 2.1164 3.9008 sec/batch\n",
      "Epoch 3/20  Iteration 372/3560 Training loss: 2.1150 3.9741 sec/batch\n",
      "Epoch 3/20  Iteration 373/3560 Training loss: 2.1137 3.9645 sec/batch\n",
      "Epoch 3/20  Iteration 374/3560 Training loss: 2.1149 4.1060 sec/batch\n",
      "Epoch 3/20  Iteration 375/3560 Training loss: 2.1145 3.9553 sec/batch\n",
      "Epoch 3/20  Iteration 376/3560 Training loss: 2.1137 3.9365 sec/batch\n",
      "Epoch 3/20  Iteration 377/3560 Training loss: 2.1124 4.1880 sec/batch\n",
      "Epoch 3/20  Iteration 378/3560 Training loss: 2.1133 3.8885 sec/batch\n",
      "Epoch 3/20  Iteration 379/3560 Training loss: 2.1121 3.9232 sec/batch\n",
      "Epoch 3/20  Iteration 380/3560 Training loss: 2.1102 4.0231 sec/batch\n",
      "Epoch 3/20  Iteration 381/3560 Training loss: 2.1093 3.8634 sec/batch\n",
      "Epoch 3/20  Iteration 382/3560 Training loss: 2.1079 4.1898 sec/batch\n",
      "Epoch 3/20  Iteration 383/3560 Training loss: 2.1064 3.9054 sec/batch\n",
      "Epoch 3/20  Iteration 384/3560 Training loss: 2.1059 3.9301 sec/batch\n",
      "Epoch 3/20  Iteration 385/3560 Training loss: 2.1063 4.1853 sec/batch\n",
      "Epoch 3/20  Iteration 386/3560 Training loss: 2.1056 4.2813 sec/batch\n",
      "Epoch 3/20  Iteration 387/3560 Training loss: 2.1047 4.4262 sec/batch\n",
      "Epoch 3/20  Iteration 388/3560 Training loss: 2.1034 3.9530 sec/batch\n",
      "Epoch 3/20  Iteration 389/3560 Training loss: 2.1025 4.3501 sec/batch\n",
      "Epoch 3/20  Iteration 390/3560 Training loss: 2.1027 4.2548 sec/batch\n",
      "Epoch 3/20  Iteration 391/3560 Training loss: 2.1018 4.1040 sec/batch\n",
      "Epoch 3/20  Iteration 392/3560 Training loss: 2.1005 4.2246 sec/batch\n",
      "Epoch 3/20  Iteration 393/3560 Training loss: 2.0995 4.1253 sec/batch\n",
      "Epoch 3/20  Iteration 394/3560 Training loss: 2.0974 4.0094 sec/batch\n",
      "Epoch 3/20  Iteration 395/3560 Training loss: 2.0956 3.9543 sec/batch\n",
      "Epoch 3/20  Iteration 396/3560 Training loss: 2.0940 4.0051 sec/batch\n",
      "Epoch 3/20  Iteration 397/3560 Training loss: 2.0928 4.3486 sec/batch\n",
      "Epoch 3/20  Iteration 398/3560 Training loss: 2.0919 4.1257 sec/batch\n",
      "Epoch 3/20  Iteration 399/3560 Training loss: 2.0908 4.1857 sec/batch\n",
      "Epoch 3/20  Iteration 400/3560 Training loss: 2.0895 3.4878 sec/batch\n",
      "Epoch 3/20  Iteration 401/3560 Training loss: 2.0887 3.5871 sec/batch\n",
      "Epoch 3/20  Iteration 402/3560 Training loss: 2.0869 3.3593 sec/batch\n",
      "Epoch 3/20  Iteration 403/3560 Training loss: 2.0864 3.1226 sec/batch\n",
      "Epoch 3/20  Iteration 404/3560 Training loss: 2.0851 2.8917 sec/batch\n",
      "Epoch 3/20  Iteration 405/3560 Training loss: 2.0842 3.0132 sec/batch\n",
      "Epoch 3/20  Iteration 406/3560 Training loss: 2.0841 2.9991 sec/batch\n",
      "Epoch 3/20  Iteration 407/3560 Training loss: 2.0828 3.1991 sec/batch\n",
      "Epoch 3/20  Iteration 408/3560 Training loss: 2.0827 3.1012 sec/batch\n",
      "Epoch 3/20  Iteration 409/3560 Training loss: 2.0817 3.2090 sec/batch\n",
      "Epoch 3/20  Iteration 410/3560 Training loss: 2.0809 3.4598 sec/batch\n",
      "Epoch 3/20  Iteration 411/3560 Training loss: 2.0798 3.6491 sec/batch\n",
      "Epoch 3/20  Iteration 412/3560 Training loss: 2.0793 3.6446 sec/batch\n",
      "Epoch 3/20  Iteration 413/3560 Training loss: 2.0789 3.0527 sec/batch\n",
      "Epoch 3/20  Iteration 414/3560 Training loss: 2.0781 3.8840 sec/batch\n",
      "Epoch 3/20  Iteration 415/3560 Training loss: 2.0770 4.8238 sec/batch\n",
      "Epoch 3/20  Iteration 416/3560 Training loss: 2.0768 4.6305 sec/batch\n",
      "Epoch 3/20  Iteration 417/3560 Training loss: 2.0761 4.3710 sec/batch\n",
      "Epoch 3/20  Iteration 418/3560 Training loss: 2.0761 4.2764 sec/batch\n",
      "Epoch 3/20  Iteration 419/3560 Training loss: 2.0757 4.5397 sec/batch\n",
      "Epoch 3/20  Iteration 420/3560 Training loss: 2.0752 4.1708 sec/batch\n",
      "Epoch 3/20  Iteration 421/3560 Training loss: 2.0744 3.8113 sec/batch\n",
      "Epoch 3/20  Iteration 422/3560 Training loss: 2.0742 3.0969 sec/batch\n",
      "Epoch 3/20  Iteration 423/3560 Training loss: 2.0737 3.0023 sec/batch\n",
      "Epoch 3/20  Iteration 424/3560 Training loss: 2.0727 3.0572 sec/batch\n",
      "Epoch 3/20  Iteration 425/3560 Training loss: 2.0717 3.1464 sec/batch\n",
      "Epoch 3/20  Iteration 426/3560 Training loss: 2.0711 3.0595 sec/batch\n",
      "Epoch 3/20  Iteration 427/3560 Training loss: 2.0710 3.0129 sec/batch\n",
      "Epoch 3/20  Iteration 428/3560 Training loss: 2.0703 3.1401 sec/batch\n",
      "Epoch 3/20  Iteration 429/3560 Training loss: 2.0700 3.1712 sec/batch\n",
      "Epoch 3/20  Iteration 430/3560 Training loss: 2.0691 3.1232 sec/batch\n",
      "Epoch 3/20  Iteration 431/3560 Training loss: 2.0683 3.2773 sec/batch\n",
      "Epoch 3/20  Iteration 432/3560 Training loss: 2.0680 3.1009 sec/batch\n",
      "Epoch 3/20  Iteration 433/3560 Training loss: 2.0673 3.1678 sec/batch\n",
      "Epoch 3/20  Iteration 434/3560 Training loss: 2.0668 3.2171 sec/batch\n",
      "Epoch 3/20  Iteration 435/3560 Training loss: 2.0657 3.0643 sec/batch\n",
      "Epoch 3/20  Iteration 436/3560 Training loss: 2.0647 3.1777 sec/batch\n",
      "Epoch 3/20  Iteration 437/3560 Training loss: 2.0637 3.1583 sec/batch\n",
      "Epoch 3/20  Iteration 438/3560 Training loss: 2.0632 3.1624 sec/batch\n",
      "Epoch 3/20  Iteration 439/3560 Training loss: 2.0621 3.2044 sec/batch\n",
      "Epoch 3/20  Iteration 440/3560 Training loss: 2.0614 3.2385 sec/batch\n",
      "Epoch 3/20  Iteration 441/3560 Training loss: 2.0603 3.1555 sec/batch\n",
      "Epoch 3/20  Iteration 442/3560 Training loss: 2.0594 3.1875 sec/batch\n",
      "Epoch 3/20  Iteration 443/3560 Training loss: 2.0587 3.2163 sec/batch\n",
      "Epoch 3/20  Iteration 444/3560 Training loss: 2.0577 3.2224 sec/batch\n",
      "Epoch 3/20  Iteration 445/3560 Training loss: 2.0566 3.0774 sec/batch\n",
      "Epoch 3/20  Iteration 446/3560 Training loss: 2.0561 3.1804 sec/batch\n",
      "Epoch 3/20  Iteration 447/3560 Training loss: 2.0552 3.2128 sec/batch\n",
      "Epoch 3/20  Iteration 448/3560 Training loss: 2.0545 3.0188 sec/batch\n",
      "Epoch 3/20  Iteration 449/3560 Training loss: 2.0534 3.0884 sec/batch\n",
      "Epoch 3/20  Iteration 450/3560 Training loss: 2.0525 3.2548 sec/batch\n",
      "Epoch 3/20  Iteration 451/3560 Training loss: 2.0516 3.1520 sec/batch\n",
      "Epoch 3/20  Iteration 452/3560 Training loss: 2.0508 3.1728 sec/batch\n",
      "Epoch 3/20  Iteration 453/3560 Training loss: 2.0501 3.1995 sec/batch\n",
      "Epoch 3/20  Iteration 454/3560 Training loss: 2.0492 3.2785 sec/batch\n",
      "Epoch 3/20  Iteration 455/3560 Training loss: 2.0482 3.1052 sec/batch\n",
      "Epoch 3/20  Iteration 456/3560 Training loss: 2.0472 3.1974 sec/batch\n",
      "Epoch 3/20  Iteration 457/3560 Training loss: 2.0466 3.1919 sec/batch\n",
      "Epoch 3/20  Iteration 458/3560 Training loss: 2.0459 3.0938 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Iteration 459/3560 Training loss: 2.0451 3.1527 sec/batch\n",
      "Epoch 3/20  Iteration 460/3560 Training loss: 2.0442 3.1753 sec/batch\n",
      "Epoch 3/20  Iteration 461/3560 Training loss: 2.0433 2.9839 sec/batch\n",
      "Epoch 3/20  Iteration 462/3560 Training loss: 2.0426 3.0689 sec/batch\n",
      "Epoch 3/20  Iteration 463/3560 Training loss: 2.0419 3.0806 sec/batch\n",
      "Epoch 3/20  Iteration 464/3560 Training loss: 2.0413 2.9736 sec/batch\n",
      "Epoch 3/20  Iteration 465/3560 Training loss: 2.0408 3.1383 sec/batch\n",
      "Epoch 3/20  Iteration 466/3560 Training loss: 2.0401 3.1072 sec/batch\n",
      "Epoch 3/20  Iteration 467/3560 Training loss: 2.0394 3.2107 sec/batch\n",
      "Epoch 3/20  Iteration 468/3560 Training loss: 2.0386 3.0758 sec/batch\n",
      "Epoch 3/20  Iteration 469/3560 Training loss: 2.0380 3.0448 sec/batch\n",
      "Epoch 3/20  Iteration 470/3560 Training loss: 2.0373 3.0639 sec/batch\n",
      "Epoch 3/20  Iteration 471/3560 Training loss: 2.0365 2.9932 sec/batch\n",
      "Epoch 3/20  Iteration 472/3560 Training loss: 2.0355 3.0393 sec/batch\n",
      "Epoch 3/20  Iteration 473/3560 Training loss: 2.0349 3.0451 sec/batch\n",
      "Epoch 3/20  Iteration 474/3560 Training loss: 2.0342 3.0314 sec/batch\n",
      "Epoch 3/20  Iteration 475/3560 Training loss: 2.0335 3.0331 sec/batch\n",
      "Epoch 3/20  Iteration 476/3560 Training loss: 2.0329 3.0174 sec/batch\n",
      "Epoch 3/20  Iteration 477/3560 Training loss: 2.0324 2.9711 sec/batch\n",
      "Epoch 3/20  Iteration 478/3560 Training loss: 2.0317 3.0559 sec/batch\n",
      "Epoch 3/20  Iteration 479/3560 Training loss: 2.0309 3.0495 sec/batch\n",
      "Epoch 3/20  Iteration 480/3560 Training loss: 2.0304 3.0683 sec/batch\n",
      "Epoch 3/20  Iteration 481/3560 Training loss: 2.0298 3.0109 sec/batch\n",
      "Epoch 3/20  Iteration 482/3560 Training loss: 2.0288 3.1045 sec/batch\n",
      "Epoch 3/20  Iteration 483/3560 Training loss: 2.0284 3.2074 sec/batch\n",
      "Epoch 3/20  Iteration 484/3560 Training loss: 2.0279 2.9621 sec/batch\n",
      "Epoch 3/20  Iteration 485/3560 Training loss: 2.0272 3.0439 sec/batch\n",
      "Epoch 3/20  Iteration 486/3560 Training loss: 2.0266 3.1339 sec/batch\n",
      "Epoch 3/20  Iteration 487/3560 Training loss: 2.0258 2.9508 sec/batch\n",
      "Epoch 3/20  Iteration 488/3560 Training loss: 2.0249 3.0313 sec/batch\n",
      "Epoch 3/20  Iteration 489/3560 Training loss: 2.0245 3.0623 sec/batch\n",
      "Epoch 3/20  Iteration 490/3560 Training loss: 2.0239 3.0502 sec/batch\n",
      "Epoch 3/20  Iteration 491/3560 Training loss: 2.0233 3.0226 sec/batch\n",
      "Epoch 3/20  Iteration 492/3560 Training loss: 2.0228 3.0192 sec/batch\n",
      "Epoch 3/20  Iteration 493/3560 Training loss: 2.0223 3.1111 sec/batch\n",
      "Epoch 3/20  Iteration 494/3560 Training loss: 2.0218 3.0442 sec/batch\n",
      "Epoch 3/20  Iteration 495/3560 Training loss: 2.0214 3.0067 sec/batch\n",
      "Epoch 3/20  Iteration 496/3560 Training loss: 2.0208 3.1125 sec/batch\n",
      "Epoch 3/20  Iteration 497/3560 Training loss: 2.0204 3.0072 sec/batch\n",
      "Epoch 3/20  Iteration 498/3560 Training loss: 2.0197 3.0125 sec/batch\n",
      "Epoch 3/20  Iteration 499/3560 Training loss: 2.0192 2.9556 sec/batch\n",
      "Epoch 3/20  Iteration 500/3560 Training loss: 2.0186 3.0441 sec/batch\n",
      "Epoch 3/20  Iteration 501/3560 Training loss: 2.0179 3.1111 sec/batch\n",
      "Epoch 3/20  Iteration 502/3560 Training loss: 2.0174 2.9912 sec/batch\n",
      "Epoch 3/20  Iteration 503/3560 Training loss: 2.0170 3.0956 sec/batch\n",
      "Epoch 3/20  Iteration 504/3560 Training loss: 2.0166 3.0117 sec/batch\n",
      "Epoch 3/20  Iteration 505/3560 Training loss: 2.0161 3.0163 sec/batch\n",
      "Epoch 3/20  Iteration 506/3560 Training loss: 2.0154 3.1078 sec/batch\n",
      "Epoch 3/20  Iteration 507/3560 Training loss: 2.0148 3.0169 sec/batch\n",
      "Epoch 3/20  Iteration 508/3560 Training loss: 2.0144 3.0364 sec/batch\n",
      "Epoch 3/20  Iteration 509/3560 Training loss: 2.0139 3.0602 sec/batch\n",
      "Epoch 3/20  Iteration 510/3560 Training loss: 2.0134 3.0287 sec/batch\n",
      "Epoch 3/20  Iteration 511/3560 Training loss: 2.0128 3.0180 sec/batch\n",
      "Epoch 3/20  Iteration 512/3560 Training loss: 2.0122 3.0847 sec/batch\n",
      "Epoch 3/20  Iteration 513/3560 Training loss: 2.0117 3.0335 sec/batch\n",
      "Epoch 3/20  Iteration 514/3560 Training loss: 2.0111 3.0257 sec/batch\n",
      "Epoch 3/20  Iteration 515/3560 Training loss: 2.0103 3.0913 sec/batch\n",
      "Epoch 3/20  Iteration 516/3560 Training loss: 2.0100 3.1066 sec/batch\n",
      "Epoch 3/20  Iteration 517/3560 Training loss: 2.0096 2.9425 sec/batch\n",
      "Epoch 3/20  Iteration 518/3560 Training loss: 2.0091 3.0005 sec/batch\n",
      "Epoch 3/20  Iteration 519/3560 Training loss: 2.0086 3.0450 sec/batch\n",
      "Epoch 3/20  Iteration 520/3560 Training loss: 2.0080 3.0554 sec/batch\n",
      "Epoch 3/20  Iteration 521/3560 Training loss: 2.0075 3.0122 sec/batch\n",
      "Epoch 3/20  Iteration 522/3560 Training loss: 2.0069 3.1207 sec/batch\n",
      "Epoch 3/20  Iteration 523/3560 Training loss: 2.0065 3.0655 sec/batch\n",
      "Epoch 3/20  Iteration 524/3560 Training loss: 2.0062 3.0344 sec/batch\n",
      "Epoch 3/20  Iteration 525/3560 Training loss: 2.0056 3.1168 sec/batch\n",
      "Epoch 3/20  Iteration 526/3560 Training loss: 2.0050 3.1743 sec/batch\n",
      "Epoch 3/20  Iteration 527/3560 Training loss: 2.0044 3.1689 sec/batch\n",
      "Epoch 3/20  Iteration 528/3560 Training loss: 2.0038 3.0008 sec/batch\n",
      "Epoch 3/20  Iteration 529/3560 Training loss: 2.0034 3.1078 sec/batch\n",
      "Epoch 3/20  Iteration 530/3560 Training loss: 2.0029 3.0493 sec/batch\n",
      "Epoch 3/20  Iteration 531/3560 Training loss: 2.0024 3.0037 sec/batch\n",
      "Epoch 3/20  Iteration 532/3560 Training loss: 2.0018 3.0729 sec/batch\n",
      "Epoch 3/20  Iteration 533/3560 Training loss: 2.0012 3.0660 sec/batch\n",
      "Epoch 3/20  Iteration 534/3560 Training loss: 2.0007 2.9704 sec/batch\n",
      "Epoch 4/20  Iteration 535/3560 Training loss: 1.9932 3.0528 sec/batch\n",
      "Epoch 4/20  Iteration 536/3560 Training loss: 1.9392 3.0926 sec/batch\n",
      "Epoch 4/20  Iteration 537/3560 Training loss: 1.9240 3.0641 sec/batch\n",
      "Epoch 4/20  Iteration 538/3560 Training loss: 1.9163 3.0518 sec/batch\n",
      "Epoch 4/20  Iteration 539/3560 Training loss: 1.9111 3.0947 sec/batch\n",
      "Epoch 4/20  Iteration 540/3560 Training loss: 1.9006 3.0590 sec/batch\n",
      "Epoch 4/20  Iteration 541/3560 Training loss: 1.9003 2.9765 sec/batch\n",
      "Epoch 4/20  Iteration 542/3560 Training loss: 1.8983 3.0231 sec/batch\n",
      "Epoch 4/20  Iteration 543/3560 Training loss: 1.9011 3.1061 sec/batch\n",
      "Epoch 4/20  Iteration 544/3560 Training loss: 1.9006 3.1027 sec/batch\n",
      "Epoch 4/20  Iteration 545/3560 Training loss: 1.8971 2.9891 sec/batch\n",
      "Epoch 4/20  Iteration 546/3560 Training loss: 1.8944 3.0038 sec/batch\n",
      "Epoch 4/20  Iteration 547/3560 Training loss: 1.8942 3.0404 sec/batch\n",
      "Epoch 4/20  Iteration 548/3560 Training loss: 1.8962 3.1377 sec/batch\n",
      "Epoch 4/20  Iteration 549/3560 Training loss: 1.8946 3.0947 sec/batch\n",
      "Epoch 4/20  Iteration 550/3560 Training loss: 1.8929 2.9889 sec/batch\n",
      "Epoch 4/20  Iteration 551/3560 Training loss: 1.8921 3.0999 sec/batch\n",
      "Epoch 4/20  Iteration 552/3560 Training loss: 1.8939 2.9987 sec/batch\n",
      "Epoch 4/20  Iteration 553/3560 Training loss: 1.8932 3.0643 sec/batch\n",
      "Epoch 4/20  Iteration 554/3560 Training loss: 1.8931 3.0815 sec/batch\n",
      "Epoch 4/20  Iteration 555/3560 Training loss: 1.8921 3.0584 sec/batch\n",
      "Epoch 4/20  Iteration 556/3560 Training loss: 1.8932 3.1520 sec/batch\n",
      "Epoch 4/20  Iteration 557/3560 Training loss: 1.8918 2.9496 sec/batch\n",
      "Epoch 4/20  Iteration 558/3560 Training loss: 1.8914 3.0607 sec/batch\n",
      "Epoch 4/20  Iteration 559/3560 Training loss: 1.8906 3.0702 sec/batch\n",
      "Epoch 4/20  Iteration 560/3560 Training loss: 1.8887 3.0196 sec/batch\n",
      "Epoch 4/20  Iteration 561/3560 Training loss: 1.8873 3.0938 sec/batch\n",
      "Epoch 4/20  Iteration 562/3560 Training loss: 1.8875 3.0845 sec/batch\n",
      "Epoch 4/20  Iteration 563/3560 Training loss: 1.8881 3.0690 sec/batch\n",
      "Epoch 4/20  Iteration 564/3560 Training loss: 1.8880 3.0007 sec/batch\n",
      "Epoch 4/20  Iteration 565/3560 Training loss: 1.8875 3.3832 sec/batch\n",
      "Epoch 4/20  Iteration 566/3560 Training loss: 1.8865 3.4183 sec/batch\n",
      "Epoch 4/20  Iteration 567/3560 Training loss: 1.8861 3.3603 sec/batch\n",
      "Epoch 4/20  Iteration 568/3560 Training loss: 1.8864 3.1310 sec/batch\n",
      "Epoch 4/20  Iteration 569/3560 Training loss: 1.8858 3.1174 sec/batch\n",
      "Epoch 4/20  Iteration 570/3560 Training loss: 1.8850 3.7161 sec/batch\n",
      "Epoch 4/20  Iteration 571/3560 Training loss: 1.8844 3.4641 sec/batch\n",
      "Epoch 4/20  Iteration 572/3560 Training loss: 1.8830 3.2741 sec/batch\n",
      "Epoch 4/20  Iteration 573/3560 Training loss: 1.8816 3.1417 sec/batch\n",
      "Epoch 4/20  Iteration 574/3560 Training loss: 1.8803 2.9939 sec/batch\n",
      "Epoch 4/20  Iteration 575/3560 Training loss: 1.8796 3.1371 sec/batch\n",
      "Epoch 4/20  Iteration 576/3560 Training loss: 1.8793 3.2807 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 577/3560 Training loss: 1.8785 3.0316 sec/batch\n",
      "Epoch 4/20  Iteration 578/3560 Training loss: 1.8773 3.1118 sec/batch\n",
      "Epoch 4/20  Iteration 579/3560 Training loss: 1.8775 3.1894 sec/batch\n",
      "Epoch 4/20  Iteration 580/3560 Training loss: 1.8760 3.0153 sec/batch\n",
      "Epoch 4/20  Iteration 581/3560 Training loss: 1.8755 2.9892 sec/batch\n",
      "Epoch 4/20  Iteration 582/3560 Training loss: 1.8745 3.2453 sec/batch\n",
      "Epoch 4/20  Iteration 583/3560 Training loss: 1.8739 3.2288 sec/batch\n",
      "Epoch 4/20  Iteration 584/3560 Training loss: 1.8743 3.2361 sec/batch\n",
      "Epoch 4/20  Iteration 585/3560 Training loss: 1.8736 3.3467 sec/batch\n",
      "Epoch 4/20  Iteration 586/3560 Training loss: 1.8741 3.1921 sec/batch\n",
      "Epoch 4/20  Iteration 587/3560 Training loss: 1.8734 3.0068 sec/batch\n",
      "Epoch 4/20  Iteration 588/3560 Training loss: 1.8729 3.0918 sec/batch\n",
      "Epoch 4/20  Iteration 589/3560 Training loss: 1.8721 3.1391 sec/batch\n",
      "Epoch 4/20  Iteration 590/3560 Training loss: 1.8717 3.0676 sec/batch\n",
      "Epoch 4/20  Iteration 591/3560 Training loss: 1.8715 3.1833 sec/batch\n",
      "Epoch 4/20  Iteration 592/3560 Training loss: 1.8708 3.0802 sec/batch\n",
      "Epoch 4/20  Iteration 593/3560 Training loss: 1.8701 3.1142 sec/batch\n",
      "Epoch 4/20  Iteration 594/3560 Training loss: 1.8702 3.0940 sec/batch\n",
      "Epoch 4/20  Iteration 595/3560 Training loss: 1.8697 3.0543 sec/batch\n",
      "Epoch 4/20  Iteration 596/3560 Training loss: 1.8701 3.1097 sec/batch\n",
      "Epoch 4/20  Iteration 597/3560 Training loss: 1.8699 3.2419 sec/batch\n",
      "Epoch 4/20  Iteration 598/3560 Training loss: 1.8698 3.0507 sec/batch\n",
      "Epoch 4/20  Iteration 599/3560 Training loss: 1.8693 3.0773 sec/batch\n",
      "Epoch 4/20  Iteration 600/3560 Training loss: 1.8692 3.0923 sec/batch\n",
      "Epoch 4/20  Iteration 601/3560 Training loss: 1.8690 3.1124 sec/batch\n",
      "Epoch 4/20  Iteration 602/3560 Training loss: 1.8682 3.1123 sec/batch\n",
      "Epoch 4/20  Iteration 603/3560 Training loss: 1.8678 3.0369 sec/batch\n",
      "Epoch 4/20  Iteration 604/3560 Training loss: 1.8673 3.1058 sec/batch\n",
      "Epoch 4/20  Iteration 605/3560 Training loss: 1.8674 3.0347 sec/batch\n",
      "Epoch 4/20  Iteration 606/3560 Training loss: 1.8671 2.9834 sec/batch\n",
      "Epoch 4/20  Iteration 607/3560 Training loss: 1.8670 3.0419 sec/batch\n",
      "Epoch 4/20  Iteration 608/3560 Training loss: 1.8663 3.2919 sec/batch\n",
      "Epoch 4/20  Iteration 609/3560 Training loss: 1.8659 3.1152 sec/batch\n",
      "Epoch 4/20  Iteration 610/3560 Training loss: 1.8658 3.0354 sec/batch\n",
      "Epoch 4/20  Iteration 611/3560 Training loss: 1.8653 3.1289 sec/batch\n",
      "Epoch 4/20  Iteration 612/3560 Training loss: 1.8651 3.0028 sec/batch\n",
      "Epoch 4/20  Iteration 613/3560 Training loss: 1.8642 3.0209 sec/batch\n",
      "Epoch 4/20  Iteration 614/3560 Training loss: 1.8638 3.1896 sec/batch\n",
      "Epoch 4/20  Iteration 615/3560 Training loss: 1.8630 3.0818 sec/batch\n",
      "Epoch 4/20  Iteration 616/3560 Training loss: 1.8628 3.0600 sec/batch\n",
      "Epoch 4/20  Iteration 617/3560 Training loss: 1.8620 3.0780 sec/batch\n",
      "Epoch 4/20  Iteration 618/3560 Training loss: 1.8615 3.1597 sec/batch\n",
      "Epoch 4/20  Iteration 619/3560 Training loss: 1.8606 3.0385 sec/batch\n",
      "Epoch 4/20  Iteration 620/3560 Training loss: 1.8598 3.0151 sec/batch\n",
      "Epoch 4/20  Iteration 621/3560 Training loss: 1.8592 3.2614 sec/batch\n",
      "Epoch 4/20  Iteration 622/3560 Training loss: 1.8586 3.0909 sec/batch\n",
      "Epoch 4/20  Iteration 623/3560 Training loss: 1.8577 3.0015 sec/batch\n",
      "Epoch 4/20  Iteration 624/3560 Training loss: 1.8574 3.1226 sec/batch\n",
      "Epoch 4/20  Iteration 625/3560 Training loss: 1.8567 3.1249 sec/batch\n",
      "Epoch 4/20  Iteration 626/3560 Training loss: 1.8562 3.0156 sec/batch\n",
      "Epoch 4/20  Iteration 627/3560 Training loss: 1.8553 3.0477 sec/batch\n",
      "Epoch 4/20  Iteration 628/3560 Training loss: 1.8547 3.1451 sec/batch\n",
      "Epoch 4/20  Iteration 629/3560 Training loss: 1.8540 3.0569 sec/batch\n",
      "Epoch 4/20  Iteration 630/3560 Training loss: 1.8535 2.9954 sec/batch\n",
      "Epoch 4/20  Iteration 631/3560 Training loss: 1.8530 3.1507 sec/batch\n",
      "Epoch 4/20  Iteration 632/3560 Training loss: 1.8523 3.1947 sec/batch\n",
      "Epoch 4/20  Iteration 633/3560 Training loss: 1.8515 3.0957 sec/batch\n",
      "Epoch 4/20  Iteration 634/3560 Training loss: 1.8507 3.1527 sec/batch\n",
      "Epoch 4/20  Iteration 635/3560 Training loss: 1.8503 3.0871 sec/batch\n",
      "Epoch 4/20  Iteration 636/3560 Training loss: 1.8498 3.0911 sec/batch\n",
      "Epoch 4/20  Iteration 637/3560 Training loss: 1.8492 3.1155 sec/batch\n",
      "Epoch 4/20  Iteration 638/3560 Training loss: 1.8487 2.9773 sec/batch\n",
      "Epoch 4/20  Iteration 639/3560 Training loss: 1.8481 3.0475 sec/batch\n",
      "Epoch 4/20  Iteration 640/3560 Training loss: 1.8476 3.0222 sec/batch\n",
      "Epoch 4/20  Iteration 641/3560 Training loss: 1.8471 3.2926 sec/batch\n",
      "Epoch 4/20  Iteration 642/3560 Training loss: 1.8467 2.9738 sec/batch\n",
      "Epoch 4/20  Iteration 643/3560 Training loss: 1.8465 3.1638 sec/batch\n",
      "Epoch 4/20  Iteration 644/3560 Training loss: 1.8460 3.1233 sec/batch\n",
      "Epoch 4/20  Iteration 645/3560 Training loss: 1.8456 3.0517 sec/batch\n",
      "Epoch 4/20  Iteration 646/3560 Training loss: 1.8451 3.2905 sec/batch\n",
      "Epoch 4/20  Iteration 647/3560 Training loss: 1.8445 3.1487 sec/batch\n",
      "Epoch 4/20  Iteration 648/3560 Training loss: 1.8440 3.0451 sec/batch\n",
      "Epoch 4/20  Iteration 649/3560 Training loss: 1.8433 3.1113 sec/batch\n",
      "Epoch 4/20  Iteration 650/3560 Training loss: 1.8426 3.1853 sec/batch\n",
      "Epoch 4/20  Iteration 651/3560 Training loss: 1.8421 3.0127 sec/batch\n",
      "Epoch 4/20  Iteration 652/3560 Training loss: 1.8415 3.0283 sec/batch\n",
      "Epoch 4/20  Iteration 653/3560 Training loss: 1.8410 3.1073 sec/batch\n",
      "Epoch 4/20  Iteration 654/3560 Training loss: 1.8406 3.1112 sec/batch\n",
      "Epoch 4/20  Iteration 655/3560 Training loss: 1.8402 3.0009 sec/batch\n",
      "Epoch 4/20  Iteration 656/3560 Training loss: 1.8394 3.1021 sec/batch\n",
      "Epoch 4/20  Iteration 657/3560 Training loss: 1.8387 3.2025 sec/batch\n",
      "Epoch 4/20  Iteration 658/3560 Training loss: 1.8384 3.0596 sec/batch\n",
      "Epoch 4/20  Iteration 659/3560 Training loss: 1.8380 3.2026 sec/batch\n",
      "Epoch 4/20  Iteration 660/3560 Training loss: 1.8372 3.2314 sec/batch\n",
      "Epoch 4/20  Iteration 661/3560 Training loss: 1.8370 3.0473 sec/batch\n",
      "Epoch 4/20  Iteration 662/3560 Training loss: 1.8366 3.0466 sec/batch\n",
      "Epoch 4/20  Iteration 663/3560 Training loss: 1.8362 3.1544 sec/batch\n",
      "Epoch 4/20  Iteration 664/3560 Training loss: 1.8357 3.0426 sec/batch\n",
      "Epoch 4/20  Iteration 665/3560 Training loss: 1.8350 3.0144 sec/batch\n",
      "Epoch 4/20  Iteration 666/3560 Training loss: 1.8344 3.1199 sec/batch\n",
      "Epoch 4/20  Iteration 667/3560 Training loss: 1.8340 3.2846 sec/batch\n",
      "Epoch 4/20  Iteration 668/3560 Training loss: 1.8337 3.0185 sec/batch\n",
      "Epoch 4/20  Iteration 669/3560 Training loss: 1.8333 3.1043 sec/batch\n",
      "Epoch 4/20  Iteration 670/3560 Training loss: 1.8329 3.1119 sec/batch\n",
      "Epoch 4/20  Iteration 671/3560 Training loss: 1.8326 3.0095 sec/batch\n",
      "Epoch 4/20  Iteration 672/3560 Training loss: 1.8323 3.0528 sec/batch\n",
      "Epoch 4/20  Iteration 673/3560 Training loss: 1.8320 3.1675 sec/batch\n",
      "Epoch 4/20  Iteration 674/3560 Training loss: 1.8316 3.1205 sec/batch\n",
      "Epoch 4/20  Iteration 675/3560 Training loss: 1.8315 3.0607 sec/batch\n",
      "Epoch 4/20  Iteration 676/3560 Training loss: 1.8310 3.0735 sec/batch\n",
      "Epoch 4/20  Iteration 677/3560 Training loss: 1.8307 3.0575 sec/batch\n",
      "Epoch 4/20  Iteration 678/3560 Training loss: 1.8303 3.0676 sec/batch\n",
      "Epoch 4/20  Iteration 679/3560 Training loss: 1.8298 3.0493 sec/batch\n",
      "Epoch 4/20  Iteration 680/3560 Training loss: 1.8295 3.1575 sec/batch\n",
      "Epoch 4/20  Iteration 681/3560 Training loss: 1.8292 3.0957 sec/batch\n",
      "Epoch 4/20  Iteration 682/3560 Training loss: 1.8290 3.0837 sec/batch\n",
      "Epoch 4/20  Iteration 683/3560 Training loss: 1.8287 3.2954 sec/batch\n",
      "Epoch 4/20  Iteration 684/3560 Training loss: 1.8282 3.0304 sec/batch\n",
      "Epoch 4/20  Iteration 685/3560 Training loss: 1.8277 3.0384 sec/batch\n",
      "Epoch 4/20  Iteration 686/3560 Training loss: 1.8274 3.1204 sec/batch\n",
      "Epoch 4/20  Iteration 687/3560 Training loss: 1.8272 3.0465 sec/batch\n",
      "Epoch 4/20  Iteration 688/3560 Training loss: 1.8269 2.9630 sec/batch\n",
      "Epoch 4/20  Iteration 689/3560 Training loss: 1.8265 3.1100 sec/batch\n",
      "Epoch 4/20  Iteration 690/3560 Training loss: 1.8260 3.0727 sec/batch\n",
      "Epoch 4/20  Iteration 691/3560 Training loss: 1.8257 3.0057 sec/batch\n",
      "Epoch 4/20  Iteration 692/3560 Training loss: 1.8253 3.1214 sec/batch\n",
      "Epoch 4/20  Iteration 693/3560 Training loss: 1.8247 3.1692 sec/batch\n",
      "Epoch 4/20  Iteration 694/3560 Training loss: 1.8245 2.9603 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Iteration 695/3560 Training loss: 1.8244 3.0389 sec/batch\n",
      "Epoch 4/20  Iteration 696/3560 Training loss: 1.8240 3.2303 sec/batch\n",
      "Epoch 4/20  Iteration 697/3560 Training loss: 1.8238 2.9839 sec/batch\n",
      "Epoch 4/20  Iteration 698/3560 Training loss: 1.8234 3.0397 sec/batch\n",
      "Epoch 4/20  Iteration 699/3560 Training loss: 1.8231 3.1068 sec/batch\n",
      "Epoch 4/20  Iteration 700/3560 Training loss: 1.8227 3.2904 sec/batch\n",
      "Epoch 4/20  Iteration 701/3560 Training loss: 1.8224 3.1416 sec/batch\n",
      "Epoch 4/20  Iteration 702/3560 Training loss: 1.8224 3.1258 sec/batch\n",
      "Epoch 4/20  Iteration 703/3560 Training loss: 1.8220 3.0366 sec/batch\n",
      "Epoch 4/20  Iteration 704/3560 Training loss: 1.8216 3.0845 sec/batch\n",
      "Epoch 4/20  Iteration 705/3560 Training loss: 1.8211 3.1139 sec/batch\n",
      "Epoch 4/20  Iteration 706/3560 Training loss: 1.8206 3.1510 sec/batch\n",
      "Epoch 4/20  Iteration 707/3560 Training loss: 1.8203 3.0968 sec/batch\n",
      "Epoch 4/20  Iteration 708/3560 Training loss: 1.8201 3.0871 sec/batch\n",
      "Epoch 4/20  Iteration 709/3560 Training loss: 1.8197 3.2304 sec/batch\n",
      "Epoch 4/20  Iteration 710/3560 Training loss: 1.8193 3.1173 sec/batch\n",
      "Epoch 4/20  Iteration 711/3560 Training loss: 1.8188 3.0897 sec/batch\n",
      "Epoch 4/20  Iteration 712/3560 Training loss: 1.8185 3.0268 sec/batch\n",
      "Epoch 5/20  Iteration 713/3560 Training loss: 1.8490 3.0606 sec/batch\n",
      "Epoch 5/20  Iteration 714/3560 Training loss: 1.7979 3.0360 sec/batch\n",
      "Epoch 5/20  Iteration 715/3560 Training loss: 1.7802 3.1071 sec/batch\n",
      "Epoch 5/20  Iteration 716/3560 Training loss: 1.7732 3.0995 sec/batch\n",
      "Epoch 5/20  Iteration 717/3560 Training loss: 1.7665 3.0130 sec/batch\n",
      "Epoch 5/20  Iteration 718/3560 Training loss: 1.7555 3.1150 sec/batch\n",
      "Epoch 5/20  Iteration 719/3560 Training loss: 1.7546 3.3125 sec/batch\n",
      "Epoch 5/20  Iteration 720/3560 Training loss: 1.7517 3.0597 sec/batch\n",
      "Epoch 5/20  Iteration 721/3560 Training loss: 1.7528 3.1793 sec/batch\n",
      "Epoch 5/20  Iteration 722/3560 Training loss: 1.7512 3.0282 sec/batch\n",
      "Epoch 5/20  Iteration 723/3560 Training loss: 1.7479 3.0484 sec/batch\n",
      "Epoch 5/20  Iteration 724/3560 Training loss: 1.7457 3.1149 sec/batch\n",
      "Epoch 5/20  Iteration 725/3560 Training loss: 1.7451 3.1179 sec/batch\n",
      "Epoch 5/20  Iteration 726/3560 Training loss: 1.7470 3.0731 sec/batch\n",
      "Epoch 5/20  Iteration 727/3560 Training loss: 1.7458 3.0516 sec/batch\n",
      "Epoch 5/20  Iteration 728/3560 Training loss: 1.7434 3.1202 sec/batch\n",
      "Epoch 5/20  Iteration 729/3560 Training loss: 1.7437 3.0424 sec/batch\n",
      "Epoch 5/20  Iteration 730/3560 Training loss: 1.7454 3.0327 sec/batch\n",
      "Epoch 5/20  Iteration 731/3560 Training loss: 1.7453 3.1450 sec/batch\n",
      "Epoch 5/20  Iteration 732/3560 Training loss: 1.7452 3.0790 sec/batch\n",
      "Epoch 5/20  Iteration 733/3560 Training loss: 1.7441 3.0497 sec/batch\n",
      "Epoch 5/20  Iteration 734/3560 Training loss: 1.7451 3.1092 sec/batch\n",
      "Epoch 5/20  Iteration 735/3560 Training loss: 1.7440 3.1260 sec/batch\n",
      "Epoch 5/20  Iteration 736/3560 Training loss: 1.7436 3.1808 sec/batch\n",
      "Epoch 5/20  Iteration 737/3560 Training loss: 1.7429 3.0791 sec/batch\n",
      "Epoch 5/20  Iteration 738/3560 Training loss: 1.7410 3.0375 sec/batch\n",
      "Epoch 5/20  Iteration 739/3560 Training loss: 1.7397 3.1688 sec/batch\n",
      "Epoch 5/20  Iteration 740/3560 Training loss: 1.7398 3.0219 sec/batch\n",
      "Epoch 5/20  Iteration 741/3560 Training loss: 1.7406 3.1364 sec/batch\n",
      "Epoch 5/20  Iteration 742/3560 Training loss: 1.7410 3.1899 sec/batch\n",
      "Epoch 5/20  Iteration 743/3560 Training loss: 1.7404 3.1107 sec/batch\n",
      "Epoch 5/20  Iteration 744/3560 Training loss: 1.7392 3.1140 sec/batch\n",
      "Epoch 5/20  Iteration 745/3560 Training loss: 1.7389 3.0655 sec/batch\n",
      "Epoch 5/20  Iteration 746/3560 Training loss: 1.7390 3.1480 sec/batch\n",
      "Epoch 5/20  Iteration 747/3560 Training loss: 1.7386 3.2548 sec/batch\n",
      "Epoch 5/20  Iteration 748/3560 Training loss: 1.7382 2.9985 sec/batch\n",
      "Epoch 5/20  Iteration 749/3560 Training loss: 1.7375 3.0923 sec/batch\n",
      "Epoch 5/20  Iteration 750/3560 Training loss: 1.7362 3.1153 sec/batch\n",
      "Epoch 5/20  Iteration 751/3560 Training loss: 1.7344 3.1425 sec/batch\n",
      "Epoch 5/20  Iteration 752/3560 Training loss: 1.7332 3.0382 sec/batch\n",
      "Epoch 5/20  Iteration 753/3560 Training loss: 1.7325 3.1014 sec/batch\n",
      "Epoch 5/20  Iteration 754/3560 Training loss: 1.7326 3.0493 sec/batch\n",
      "Epoch 5/20  Iteration 755/3560 Training loss: 1.7317 3.0633 sec/batch\n",
      "Epoch 5/20  Iteration 756/3560 Training loss: 1.7306 3.1317 sec/batch\n",
      "Epoch 5/20  Iteration 757/3560 Training loss: 1.7304 3.1478 sec/batch\n",
      "Epoch 5/20  Iteration 758/3560 Training loss: 1.7292 3.1136 sec/batch\n",
      "Epoch 5/20  Iteration 759/3560 Training loss: 1.7287 3.2059 sec/batch\n",
      "Epoch 5/20  Iteration 760/3560 Training loss: 1.7281 3.1425 sec/batch\n",
      "Epoch 5/20  Iteration 761/3560 Training loss: 1.7276 3.1895 sec/batch\n",
      "Epoch 5/20  Iteration 762/3560 Training loss: 1.7281 3.1643 sec/batch\n",
      "Epoch 5/20  Iteration 763/3560 Training loss: 1.7273 3.0902 sec/batch\n",
      "Epoch 5/20  Iteration 764/3560 Training loss: 1.7279 3.0780 sec/batch\n",
      "Epoch 5/20  Iteration 765/3560 Training loss: 1.7276 3.0754 sec/batch\n",
      "Epoch 5/20  Iteration 766/3560 Training loss: 1.7274 3.0905 sec/batch\n",
      "Epoch 5/20  Iteration 767/3560 Training loss: 1.7269 3.0583 sec/batch\n",
      "Epoch 5/20  Iteration 768/3560 Training loss: 1.7265 3.0277 sec/batch\n",
      "Epoch 5/20  Iteration 769/3560 Training loss: 1.7267 3.0902 sec/batch\n",
      "Epoch 5/20  Iteration 770/3560 Training loss: 1.7263 3.0538 sec/batch\n",
      "Epoch 5/20  Iteration 771/3560 Training loss: 1.7256 3.1883 sec/batch\n",
      "Epoch 5/20  Iteration 772/3560 Training loss: 1.7259 3.1335 sec/batch\n",
      "Epoch 5/20  Iteration 773/3560 Training loss: 1.7255 3.0379 sec/batch\n",
      "Epoch 5/20  Iteration 774/3560 Training loss: 1.7260 3.0849 sec/batch\n",
      "Epoch 5/20  Iteration 775/3560 Training loss: 1.7261 3.1415 sec/batch\n",
      "Epoch 5/20  Iteration 776/3560 Training loss: 1.7261 3.0870 sec/batch\n",
      "Epoch 5/20  Iteration 777/3560 Training loss: 1.7259 3.0309 sec/batch\n",
      "Epoch 5/20  Iteration 778/3560 Training loss: 1.7258 3.1418 sec/batch\n",
      "Epoch 5/20  Iteration 779/3560 Training loss: 1.7257 3.1634 sec/batch\n",
      "Epoch 5/20  Iteration 780/3560 Training loss: 1.7250 3.0903 sec/batch\n",
      "Epoch 5/20  Iteration 781/3560 Training loss: 1.7247 3.0548 sec/batch\n",
      "Epoch 5/20  Iteration 782/3560 Training loss: 1.7243 3.1553 sec/batch\n",
      "Epoch 5/20  Iteration 783/3560 Training loss: 1.7245 3.0761 sec/batch\n",
      "Epoch 5/20  Iteration 784/3560 Training loss: 1.7244 3.1589 sec/batch\n",
      "Epoch 5/20  Iteration 785/3560 Training loss: 1.7246 3.1899 sec/batch\n",
      "Epoch 5/20  Iteration 786/3560 Training loss: 1.7240 3.1099 sec/batch\n",
      "Epoch 5/20  Iteration 787/3560 Training loss: 1.7237 2.9956 sec/batch\n",
      "Epoch 5/20  Iteration 788/3560 Training loss: 1.7237 3.1698 sec/batch\n",
      "Epoch 5/20  Iteration 789/3560 Training loss: 1.7233 3.1156 sec/batch\n",
      "Epoch 5/20  Iteration 790/3560 Training loss: 1.7231 3.0287 sec/batch\n",
      "Epoch 5/20  Iteration 791/3560 Training loss: 1.7224 3.2239 sec/batch\n",
      "Epoch 5/20  Iteration 792/3560 Training loss: 1.7221 3.0937 sec/batch\n",
      "Epoch 5/20  Iteration 793/3560 Training loss: 1.7213 3.1299 sec/batch\n",
      "Epoch 5/20  Iteration 794/3560 Training loss: 1.7211 3.1078 sec/batch\n",
      "Epoch 5/20  Iteration 795/3560 Training loss: 1.7204 3.1855 sec/batch\n",
      "Epoch 5/20  Iteration 796/3560 Training loss: 1.7201 3.0988 sec/batch\n",
      "Epoch 5/20  Iteration 797/3560 Training loss: 1.7193 3.0886 sec/batch\n",
      "Epoch 5/20  Iteration 798/3560 Training loss: 1.7187 3.2089 sec/batch\n",
      "Epoch 5/20  Iteration 799/3560 Training loss: 1.7183 3.0965 sec/batch\n",
      "Epoch 5/20  Iteration 800/3560 Training loss: 1.7177 3.0653 sec/batch\n",
      "Epoch 5/20  Iteration 801/3560 Training loss: 1.7168 3.0751 sec/batch\n",
      "Epoch 5/20  Iteration 802/3560 Training loss: 1.7167 3.1364 sec/batch\n",
      "Epoch 5/20  Iteration 803/3560 Training loss: 1.7161 3.0972 sec/batch\n",
      "Epoch 5/20  Iteration 804/3560 Training loss: 1.7157 3.2243 sec/batch\n",
      "Epoch 5/20  Iteration 805/3560 Training loss: 1.7151 3.1052 sec/batch\n",
      "Epoch 5/20  Iteration 806/3560 Training loss: 1.7146 3.0986 sec/batch\n",
      "Epoch 5/20  Iteration 807/3560 Training loss: 1.7140 3.0016 sec/batch\n",
      "Epoch 5/20  Iteration 808/3560 Training loss: 1.7138 3.1253 sec/batch\n",
      "Epoch 5/20  Iteration 809/3560 Training loss: 1.7134 3.0897 sec/batch\n",
      "Epoch 5/20  Iteration 810/3560 Training loss: 1.7128 3.0027 sec/batch\n",
      "Epoch 5/20  Iteration 811/3560 Training loss: 1.7122 3.2258 sec/batch\n",
      "Epoch 5/20  Iteration 812/3560 Training loss: 1.7114 3.0715 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Iteration 813/3560 Training loss: 1.7112 3.0659 sec/batch\n",
      "Epoch 5/20  Iteration 814/3560 Training loss: 1.7109 3.1494 sec/batch\n",
      "Epoch 5/20  Iteration 815/3560 Training loss: 1.7104 3.2631 sec/batch\n",
      "Epoch 5/20  Iteration 816/3560 Training loss: 1.7101 3.0175 sec/batch\n",
      "Epoch 5/20  Iteration 817/3560 Training loss: 1.7096 3.1786 sec/batch\n",
      "Epoch 5/20  Iteration 818/3560 Training loss: 1.7092 3.1684 sec/batch\n",
      "Epoch 5/20  Iteration 819/3560 Training loss: 1.7088 3.0660 sec/batch\n",
      "Epoch 5/20  Iteration 820/3560 Training loss: 1.7085 3.0560 sec/batch\n",
      "Epoch 5/20  Iteration 821/3560 Training loss: 1.7083 3.1203 sec/batch\n",
      "Epoch 5/20  Iteration 822/3560 Training loss: 1.7081 3.0621 sec/batch\n",
      "Epoch 5/20  Iteration 823/3560 Training loss: 1.7077 3.0702 sec/batch\n",
      "Epoch 5/20  Iteration 824/3560 Training loss: 1.7073 3.1674 sec/batch\n",
      "Epoch 5/20  Iteration 825/3560 Training loss: 1.7070 3.0330 sec/batch\n",
      "Epoch 5/20  Iteration 826/3560 Training loss: 1.7066 3.0677 sec/batch\n",
      "Epoch 5/20  Iteration 827/3560 Training loss: 1.7060 3.1254 sec/batch\n",
      "Epoch 5/20  Iteration 828/3560 Training loss: 1.7054 3.0893 sec/batch\n",
      "Epoch 5/20  Iteration 829/3560 Training loss: 1.7051 3.0018 sec/batch\n",
      "Epoch 5/20  Iteration 830/3560 Training loss: 1.7047 3.1005 sec/batch\n",
      "Epoch 5/20  Iteration 831/3560 Training loss: 1.7043 3.3044 sec/batch\n",
      "Epoch 5/20  Iteration 832/3560 Training loss: 1.7040 3.0424 sec/batch\n",
      "Epoch 5/20  Iteration 833/3560 Training loss: 1.7037 2.9964 sec/batch\n",
      "Epoch 5/20  Iteration 834/3560 Training loss: 1.7030 3.1588 sec/batch\n",
      "Epoch 5/20  Iteration 835/3560 Training loss: 1.7025 3.0968 sec/batch\n",
      "Epoch 5/20  Iteration 836/3560 Training loss: 1.7023 2.9700 sec/batch\n",
      "Epoch 5/20  Iteration 837/3560 Training loss: 1.7020 3.1909 sec/batch\n",
      "Epoch 5/20  Iteration 838/3560 Training loss: 1.7013 3.1192 sec/batch\n",
      "Epoch 5/20  Iteration 839/3560 Training loss: 1.7011 3.0151 sec/batch\n",
      "Epoch 5/20  Iteration 840/3560 Training loss: 1.7010 3.0636 sec/batch\n",
      "Epoch 5/20  Iteration 841/3560 Training loss: 1.7006 3.3407 sec/batch\n",
      "Epoch 5/20  Iteration 842/3560 Training loss: 1.7001 3.0681 sec/batch\n",
      "Epoch 5/20  Iteration 843/3560 Training loss: 1.6995 3.0737 sec/batch\n",
      "Epoch 5/20  Iteration 844/3560 Training loss: 1.6990 3.1534 sec/batch\n",
      "Epoch 5/20  Iteration 845/3560 Training loss: 1.6989 3.0442 sec/batch\n",
      "Epoch 5/20  Iteration 846/3560 Training loss: 1.6986 3.0595 sec/batch\n",
      "Epoch 5/20  Iteration 847/3560 Training loss: 1.6983 3.1843 sec/batch\n",
      "Epoch 5/20  Iteration 848/3560 Training loss: 1.6980 3.0872 sec/batch\n",
      "Epoch 5/20  Iteration 849/3560 Training loss: 1.6979 3.0833 sec/batch\n",
      "Epoch 5/20  Iteration 850/3560 Training loss: 1.6977 3.1537 sec/batch\n",
      "Epoch 5/20  Iteration 851/3560 Training loss: 1.6975 3.0083 sec/batch\n",
      "Epoch 5/20  Iteration 852/3560 Training loss: 1.6972 3.1384 sec/batch\n",
      "Epoch 5/20  Iteration 853/3560 Training loss: 1.6973 3.0617 sec/batch\n",
      "Epoch 5/20  Iteration 854/3560 Training loss: 1.6970 3.1910 sec/batch\n",
      "Epoch 5/20  Iteration 855/3560 Training loss: 1.6966 3.0280 sec/batch\n",
      "Epoch 5/20  Iteration 856/3560 Training loss: 1.6965 3.1007 sec/batch\n",
      "Epoch 5/20  Iteration 857/3560 Training loss: 1.6961 3.1726 sec/batch\n",
      "Epoch 5/20  Iteration 858/3560 Training loss: 1.6959 3.0050 sec/batch\n",
      "Epoch 5/20  Iteration 859/3560 Training loss: 1.6957 3.0593 sec/batch\n",
      "Epoch 5/20  Iteration 860/3560 Training loss: 1.6956 3.1828 sec/batch\n",
      "Epoch 5/20  Iteration 861/3560 Training loss: 1.6954 3.0340 sec/batch\n",
      "Epoch 5/20  Iteration 862/3560 Training loss: 1.6950 3.0439 sec/batch\n",
      "Epoch 5/20  Iteration 863/3560 Training loss: 1.6946 3.2579 sec/batch\n",
      "Epoch 5/20  Iteration 864/3560 Training loss: 1.6944 3.1429 sec/batch\n",
      "Epoch 5/20  Iteration 865/3560 Training loss: 1.6942 3.0047 sec/batch\n",
      "Epoch 5/20  Iteration 866/3560 Training loss: 1.6940 3.1405 sec/batch\n",
      "Epoch 5/20  Iteration 867/3560 Training loss: 1.6938 3.1239 sec/batch\n",
      "Epoch 5/20  Iteration 868/3560 Training loss: 1.6935 2.9422 sec/batch\n",
      "Epoch 5/20  Iteration 869/3560 Training loss: 1.6933 3.0385 sec/batch\n",
      "Epoch 5/20  Iteration 870/3560 Training loss: 1.6931 3.1869 sec/batch\n",
      "Epoch 5/20  Iteration 871/3560 Training loss: 1.6926 2.9831 sec/batch\n",
      "Epoch 5/20  Iteration 872/3560 Training loss: 1.6925 3.0816 sec/batch\n",
      "Epoch 5/20  Iteration 873/3560 Training loss: 1.6926 3.3664 sec/batch\n",
      "Epoch 5/20  Iteration 874/3560 Training loss: 1.6923 3.1163 sec/batch\n",
      "Epoch 5/20  Iteration 875/3560 Training loss: 1.6921 3.1999 sec/batch\n",
      "Epoch 5/20  Iteration 876/3560 Training loss: 1.6919 3.0883 sec/batch\n",
      "Epoch 5/20  Iteration 877/3560 Training loss: 1.6917 3.0923 sec/batch\n",
      "Epoch 5/20  Iteration 878/3560 Training loss: 1.6914 3.0809 sec/batch\n",
      "Epoch 5/20  Iteration 879/3560 Training loss: 1.6914 3.0907 sec/batch\n",
      "Epoch 5/20  Iteration 880/3560 Training loss: 1.6915 3.1427 sec/batch\n",
      "Epoch 5/20  Iteration 881/3560 Training loss: 1.6912 3.0729 sec/batch\n",
      "Epoch 5/20  Iteration 882/3560 Training loss: 1.6909 3.0941 sec/batch\n",
      "Epoch 5/20  Iteration 883/3560 Training loss: 1.6906 3.1323 sec/batch\n",
      "Epoch 5/20  Iteration 884/3560 Training loss: 1.6902 3.1288 sec/batch\n",
      "Epoch 5/20  Iteration 885/3560 Training loss: 1.6901 3.1723 sec/batch\n",
      "Epoch 5/20  Iteration 886/3560 Training loss: 1.6900 3.1173 sec/batch\n",
      "Epoch 5/20  Iteration 887/3560 Training loss: 1.6898 3.1046 sec/batch\n",
      "Epoch 5/20  Iteration 888/3560 Training loss: 1.6895 3.0395 sec/batch\n",
      "Epoch 5/20  Iteration 889/3560 Training loss: 1.6891 3.0825 sec/batch\n",
      "Epoch 5/20  Iteration 890/3560 Training loss: 1.6890 3.1199 sec/batch\n",
      "Epoch 6/20  Iteration 891/3560 Training loss: 1.7466 3.1141 sec/batch\n",
      "Epoch 6/20  Iteration 892/3560 Training loss: 1.6929 3.1022 sec/batch\n",
      "Epoch 6/20  Iteration 893/3560 Training loss: 1.6732 3.1288 sec/batch\n",
      "Epoch 6/20  Iteration 894/3560 Training loss: 1.6665 3.0085 sec/batch\n",
      "Epoch 6/20  Iteration 895/3560 Training loss: 1.6614 3.0964 sec/batch\n",
      "Epoch 6/20  Iteration 896/3560 Training loss: 1.6495 3.2214 sec/batch\n",
      "Epoch 6/20  Iteration 897/3560 Training loss: 1.6479 3.1658 sec/batch\n",
      "Epoch 6/20  Iteration 898/3560 Training loss: 1.6446 3.1136 sec/batch\n",
      "Epoch 6/20  Iteration 899/3560 Training loss: 1.6457 3.1030 sec/batch\n",
      "Epoch 6/20  Iteration 900/3560 Training loss: 1.6437 2.9899 sec/batch\n",
      "Epoch 6/20  Iteration 901/3560 Training loss: 1.6394 3.1020 sec/batch\n",
      "Epoch 6/20  Iteration 902/3560 Training loss: 1.6377 3.0731 sec/batch\n",
      "Epoch 6/20  Iteration 903/3560 Training loss: 1.6373 3.0348 sec/batch\n",
      "Epoch 6/20  Iteration 904/3560 Training loss: 1.6395 3.0906 sec/batch\n",
      "Epoch 6/20  Iteration 905/3560 Training loss: 1.6386 3.1843 sec/batch\n",
      "Epoch 6/20  Iteration 906/3560 Training loss: 1.6367 3.0555 sec/batch\n",
      "Epoch 6/20  Iteration 907/3560 Training loss: 1.6368 3.0501 sec/batch\n",
      "Epoch 6/20  Iteration 908/3560 Training loss: 1.6381 3.0999 sec/batch\n",
      "Epoch 6/20  Iteration 909/3560 Training loss: 1.6388 3.1325 sec/batch\n",
      "Epoch 6/20  Iteration 910/3560 Training loss: 1.6394 3.2556 sec/batch\n",
      "Epoch 6/20  Iteration 911/3560 Training loss: 1.6383 3.0563 sec/batch\n",
      "Epoch 6/20  Iteration 912/3560 Training loss: 1.6394 3.1531 sec/batch\n",
      "Epoch 6/20  Iteration 913/3560 Training loss: 1.6384 3.1226 sec/batch\n",
      "Epoch 6/20  Iteration 914/3560 Training loss: 1.6385 3.1315 sec/batch\n",
      "Epoch 6/20  Iteration 915/3560 Training loss: 1.6381 3.0730 sec/batch\n",
      "Epoch 6/20  Iteration 916/3560 Training loss: 1.6368 3.1260 sec/batch\n",
      "Epoch 6/20  Iteration 917/3560 Training loss: 1.6353 3.0131 sec/batch\n",
      "Epoch 6/20  Iteration 918/3560 Training loss: 1.6354 3.0439 sec/batch\n",
      "Epoch 6/20  Iteration 919/3560 Training loss: 1.6358 3.2394 sec/batch\n",
      "Epoch 6/20  Iteration 920/3560 Training loss: 1.6356 3.1945 sec/batch\n",
      "Epoch 6/20  Iteration 921/3560 Training loss: 1.6351 3.1028 sec/batch\n",
      "Epoch 6/20  Iteration 922/3560 Training loss: 1.6338 3.0954 sec/batch\n",
      "Epoch 6/20  Iteration 923/3560 Training loss: 1.6339 3.0855 sec/batch\n",
      "Epoch 6/20  Iteration 924/3560 Training loss: 1.6342 3.1417 sec/batch\n",
      "Epoch 6/20  Iteration 925/3560 Training loss: 1.6338 3.0403 sec/batch\n",
      "Epoch 6/20  Iteration 926/3560 Training loss: 1.6334 3.0756 sec/batch\n",
      "Epoch 6/20  Iteration 927/3560 Training loss: 1.6326 3.0884 sec/batch\n",
      "Epoch 6/20  Iteration 928/3560 Training loss: 1.6315 3.1177 sec/batch\n",
      "Epoch 6/20  Iteration 929/3560 Training loss: 1.6300 3.0467 sec/batch\n",
      "Epoch 6/20  Iteration 930/3560 Training loss: 1.6290 3.1772 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 931/3560 Training loss: 1.6283 3.2039 sec/batch\n",
      "Epoch 6/20  Iteration 932/3560 Training loss: 1.6284 3.2690 sec/batch\n",
      "Epoch 6/20  Iteration 933/3560 Training loss: 1.6276 3.1141 sec/batch\n",
      "Epoch 6/20  Iteration 934/3560 Training loss: 1.6266 3.1623 sec/batch\n",
      "Epoch 6/20  Iteration 935/3560 Training loss: 1.6264 3.0897 sec/batch\n",
      "Epoch 6/20  Iteration 936/3560 Training loss: 1.6253 3.1181 sec/batch\n",
      "Epoch 6/20  Iteration 937/3560 Training loss: 1.6248 3.1547 sec/batch\n",
      "Epoch 6/20  Iteration 938/3560 Training loss: 1.6239 3.0647 sec/batch\n",
      "Epoch 6/20  Iteration 939/3560 Training loss: 1.6235 3.0670 sec/batch\n",
      "Epoch 6/20  Iteration 940/3560 Training loss: 1.6238 3.0795 sec/batch\n",
      "Epoch 6/20  Iteration 941/3560 Training loss: 1.6232 3.0118 sec/batch\n",
      "Epoch 6/20  Iteration 942/3560 Training loss: 1.6242 3.4176 sec/batch\n",
      "Epoch 6/20  Iteration 943/3560 Training loss: 1.6236 3.0203 sec/batch\n",
      "Epoch 6/20  Iteration 944/3560 Training loss: 1.6236 3.0948 sec/batch\n",
      "Epoch 6/20  Iteration 945/3560 Training loss: 1.6233 3.0709 sec/batch\n",
      "Epoch 6/20  Iteration 946/3560 Training loss: 1.6233 3.0222 sec/batch\n",
      "Epoch 6/20  Iteration 947/3560 Training loss: 1.6236 3.1207 sec/batch\n",
      "Epoch 6/20  Iteration 948/3560 Training loss: 1.6230 3.0633 sec/batch\n",
      "Epoch 6/20  Iteration 949/3560 Training loss: 1.6224 3.0048 sec/batch\n",
      "Epoch 6/20  Iteration 950/3560 Training loss: 1.6228 3.1786 sec/batch\n",
      "Epoch 6/20  Iteration 951/3560 Training loss: 1.6228 3.0830 sec/batch\n",
      "Epoch 6/20  Iteration 952/3560 Training loss: 1.6236 3.0862 sec/batch\n",
      "Epoch 6/20  Iteration 953/3560 Training loss: 1.6240 3.0625 sec/batch\n",
      "Epoch 6/20  Iteration 954/3560 Training loss: 1.6241 3.1608 sec/batch\n",
      "Epoch 6/20  Iteration 955/3560 Training loss: 1.6240 3.1400 sec/batch\n",
      "Epoch 6/20  Iteration 956/3560 Training loss: 1.6240 3.0994 sec/batch\n",
      "Epoch 6/20  Iteration 957/3560 Training loss: 1.6241 3.1372 sec/batch\n",
      "Epoch 6/20  Iteration 958/3560 Training loss: 1.6236 3.1638 sec/batch\n",
      "Epoch 6/20  Iteration 959/3560 Training loss: 1.6235 3.1099 sec/batch\n",
      "Epoch 6/20  Iteration 960/3560 Training loss: 1.6232 3.0706 sec/batch\n",
      "Epoch 6/20  Iteration 961/3560 Training loss: 1.6235 3.0075 sec/batch\n",
      "Epoch 6/20  Iteration 962/3560 Training loss: 1.6236 3.3190 sec/batch\n",
      "Epoch 6/20  Iteration 963/3560 Training loss: 1.6239 3.1530 sec/batch\n",
      "Epoch 6/20  Iteration 964/3560 Training loss: 1.6235 3.0767 sec/batch\n",
      "Epoch 6/20  Iteration 965/3560 Training loss: 1.6232 3.2543 sec/batch\n",
      "Epoch 6/20  Iteration 966/3560 Training loss: 1.6232 3.0730 sec/batch\n",
      "Epoch 6/20  Iteration 967/3560 Training loss: 1.6229 3.0158 sec/batch\n",
      "Epoch 6/20  Iteration 968/3560 Training loss: 1.6227 3.2080 sec/batch\n",
      "Epoch 6/20  Iteration 969/3560 Training loss: 1.6221 3.1083 sec/batch\n",
      "Epoch 6/20  Iteration 970/3560 Training loss: 1.6219 3.0569 sec/batch\n",
      "Epoch 6/20  Iteration 971/3560 Training loss: 1.6213 2.9509 sec/batch\n",
      "Epoch 6/20  Iteration 972/3560 Training loss: 1.6212 3.1286 sec/batch\n",
      "Epoch 6/20  Iteration 973/3560 Training loss: 1.6205 3.1068 sec/batch\n",
      "Epoch 6/20  Iteration 974/3560 Training loss: 1.6203 3.0068 sec/batch\n",
      "Epoch 6/20  Iteration 975/3560 Training loss: 1.6197 3.1311 sec/batch\n",
      "Epoch 6/20  Iteration 976/3560 Training loss: 1.6192 3.0955 sec/batch\n",
      "Epoch 6/20  Iteration 977/3560 Training loss: 1.6187 3.1030 sec/batch\n",
      "Epoch 6/20  Iteration 978/3560 Training loss: 1.6184 3.1386 sec/batch\n",
      "Epoch 6/20  Iteration 979/3560 Training loss: 1.6179 3.1251 sec/batch\n",
      "Epoch 6/20  Iteration 980/3560 Training loss: 1.6178 3.0247 sec/batch\n",
      "Epoch 6/20  Iteration 981/3560 Training loss: 1.6172 3.0126 sec/batch\n",
      "Epoch 6/20  Iteration 982/3560 Training loss: 1.6169 3.0842 sec/batch\n",
      "Epoch 6/20  Iteration 983/3560 Training loss: 1.6164 3.1490 sec/batch\n",
      "Epoch 6/20  Iteration 984/3560 Training loss: 1.6159 3.0228 sec/batch\n",
      "Epoch 6/20  Iteration 985/3560 Training loss: 1.6153 3.1045 sec/batch\n",
      "Epoch 6/20  Iteration 986/3560 Training loss: 1.6151 3.0866 sec/batch\n",
      "Epoch 6/20  Iteration 987/3560 Training loss: 1.6148 3.0421 sec/batch\n",
      "Epoch 6/20  Iteration 988/3560 Training loss: 1.6142 3.2739 sec/batch\n",
      "Epoch 6/20  Iteration 989/3560 Training loss: 1.6136 3.2365 sec/batch\n",
      "Epoch 6/20  Iteration 990/3560 Training loss: 1.6131 3.0936 sec/batch\n",
      "Epoch 6/20  Iteration 991/3560 Training loss: 1.6129 3.1675 sec/batch\n",
      "Epoch 6/20  Iteration 992/3560 Training loss: 1.6127 3.1024 sec/batch\n",
      "Epoch 6/20  Iteration 993/3560 Training loss: 1.6124 2.9974 sec/batch\n",
      "Epoch 6/20  Iteration 994/3560 Training loss: 1.6120 3.1546 sec/batch\n",
      "Epoch 6/20  Iteration 995/3560 Training loss: 1.6116 3.1698 sec/batch\n",
      "Epoch 6/20  Iteration 996/3560 Training loss: 1.6113 3.0524 sec/batch\n",
      "Epoch 6/20  Iteration 997/3560 Training loss: 1.6109 3.1040 sec/batch\n",
      "Epoch 6/20  Iteration 998/3560 Training loss: 1.6107 3.1087 sec/batch\n",
      "Epoch 6/20  Iteration 999/3560 Training loss: 1.6105 3.0521 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3560 Training loss: 1.6103 3.0476 sec/batch\n",
      "Epoch 6/20  Iteration 1001/3560 Training loss: 1.6102 3.2835 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3560 Training loss: 1.6099 3.0849 sec/batch\n",
      "Epoch 6/20  Iteration 1003/3560 Training loss: 1.6095 3.0151 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3560 Training loss: 1.6092 3.1957 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3560 Training loss: 1.6088 3.1004 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3560 Training loss: 1.6082 3.0235 sec/batch\n",
      "Epoch 6/20  Iteration 1007/3560 Training loss: 1.6079 3.0167 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3560 Training loss: 1.6077 3.1371 sec/batch\n",
      "Epoch 6/20  Iteration 1009/3560 Training loss: 1.6074 3.0774 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3560 Training loss: 1.6071 3.0584 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3560 Training loss: 1.6069 3.1671 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3560 Training loss: 1.6063 3.1099 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3560 Training loss: 1.6057 2.9796 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3560 Training loss: 1.6056 3.2057 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3560 Training loss: 1.6053 3.0673 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3560 Training loss: 1.6047 2.9740 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3560 Training loss: 1.6046 3.1330 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3560 Training loss: 1.6045 3.1611 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3560 Training loss: 1.6042 3.0143 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3560 Training loss: 1.6037 3.0993 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3560 Training loss: 1.6032 3.1013 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3560 Training loss: 1.6027 3.1169 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3560 Training loss: 1.6026 3.0465 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3560 Training loss: 1.6024 3.0798 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3560 Training loss: 1.6022 3.0907 sec/batch\n",
      "Epoch 6/20  Iteration 1026/3560 Training loss: 1.6021 3.0501 sec/batch\n",
      "Epoch 6/20  Iteration 1027/3560 Training loss: 1.6020 3.1561 sec/batch\n",
      "Epoch 6/20  Iteration 1028/3560 Training loss: 1.6019 2.9671 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3560 Training loss: 1.6018 3.0787 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3560 Training loss: 1.6015 3.1257 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3560 Training loss: 1.6018 3.1322 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3560 Training loss: 1.6015 3.0036 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3560 Training loss: 1.6013 3.0378 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3560 Training loss: 1.6012 3.1700 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3560 Training loss: 1.6010 3.1040 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3560 Training loss: 1.6009 3.0266 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3560 Training loss: 1.6007 3.1557 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3560 Training loss: 1.6007 3.0468 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3560 Training loss: 1.6006 3.0268 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3560 Training loss: 1.6003 3.1490 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3560 Training loss: 1.5999 3.1310 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3560 Training loss: 1.5997 2.9736 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3560 Training loss: 1.5996 3.0827 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3560 Training loss: 1.5995 3.4805 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3560 Training loss: 1.5993 3.2730 sec/batch\n",
      "Epoch 6/20  Iteration 1046/3560 Training loss: 1.5991 3.1838 sec/batch\n",
      "Epoch 6/20  Iteration 1047/3560 Training loss: 1.5990 3.4310 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Iteration 1048/3560 Training loss: 1.5988 3.1024 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3560 Training loss: 1.5984 3.0627 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3560 Training loss: 1.5983 3.3042 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3560 Training loss: 1.5984 3.0565 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3560 Training loss: 1.5982 3.0931 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3560 Training loss: 1.5980 3.2600 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3560 Training loss: 1.5978 3.1110 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3560 Training loss: 1.5976 3.0908 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3560 Training loss: 1.5973 3.0753 sec/batch\n",
      "Epoch 6/20  Iteration 1057/3560 Training loss: 1.5973 3.1042 sec/batch\n",
      "Epoch 6/20  Iteration 1058/3560 Training loss: 1.5975 3.1562 sec/batch\n",
      "Epoch 6/20  Iteration 1059/3560 Training loss: 1.5972 3.1252 sec/batch\n",
      "Epoch 6/20  Iteration 1060/3560 Training loss: 1.5969 3.1076 sec/batch\n",
      "Epoch 6/20  Iteration 1061/3560 Training loss: 1.5966 3.0965 sec/batch\n",
      "Epoch 6/20  Iteration 1062/3560 Training loss: 1.5961 3.0922 sec/batch\n",
      "Epoch 6/20  Iteration 1063/3560 Training loss: 1.5960 3.2067 sec/batch\n",
      "Epoch 6/20  Iteration 1064/3560 Training loss: 1.5959 3.0748 sec/batch\n",
      "Epoch 6/20  Iteration 1065/3560 Training loss: 1.5958 3.1237 sec/batch\n",
      "Epoch 6/20  Iteration 1066/3560 Training loss: 1.5956 3.2105 sec/batch\n",
      "Epoch 6/20  Iteration 1067/3560 Training loss: 1.5953 3.0318 sec/batch\n",
      "Epoch 6/20  Iteration 1068/3560 Training loss: 1.5952 3.0530 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3560 Training loss: 1.6654 3.1946 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3560 Training loss: 1.6156 3.0587 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3560 Training loss: 1.6010 3.1032 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3560 Training loss: 1.5943 3.1690 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3560 Training loss: 1.5852 3.1694 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3560 Training loss: 1.5737 3.0011 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3560 Training loss: 1.5732 3.1733 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3560 Training loss: 1.5706 3.1619 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3560 Training loss: 1.5705 2.9982 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3560 Training loss: 1.5690 3.0995 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3560 Training loss: 1.5653 3.2767 sec/batch\n",
      "Epoch 7/20  Iteration 1080/3560 Training loss: 1.5638 3.0147 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3560 Training loss: 1.5625 3.1185 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3560 Training loss: 1.5643 3.1718 sec/batch\n",
      "Epoch 7/20  Iteration 1083/3560 Training loss: 1.5625 3.1683 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3560 Training loss: 1.5609 3.1027 sec/batch\n",
      "Epoch 7/20  Iteration 1085/3560 Training loss: 1.5611 3.1081 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3560 Training loss: 1.5626 3.2009 sec/batch\n",
      "Epoch 7/20  Iteration 1087/3560 Training loss: 1.5624 3.0138 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3560 Training loss: 1.5630 3.0891 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3560 Training loss: 1.5619 3.2056 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3560 Training loss: 1.5621 3.0675 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3560 Training loss: 1.5610 3.0484 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3560 Training loss: 1.5605 3.2726 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3560 Training loss: 1.5602 3.1395 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3560 Training loss: 1.5587 3.0915 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3560 Training loss: 1.5573 3.1140 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3560 Training loss: 1.5574 3.1428 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3560 Training loss: 1.5579 3.1228 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3560 Training loss: 1.5580 3.1514 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3560 Training loss: 1.5575 3.1125 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3560 Training loss: 1.5563 3.1522 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3560 Training loss: 1.5561 3.1614 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3560 Training loss: 1.5562 3.1187 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3560 Training loss: 1.5559 3.1211 sec/batch\n",
      "Epoch 7/20  Iteration 1104/3560 Training loss: 1.5555 3.0860 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3560 Training loss: 1.5544 3.2857 sec/batch\n",
      "Epoch 7/20  Iteration 1106/3560 Training loss: 1.5534 3.2177 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3560 Training loss: 1.5519 3.1804 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3560 Training loss: 1.5510 3.1100 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3560 Training loss: 1.5503 3.1118 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3560 Training loss: 1.5503 3.1825 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3560 Training loss: 1.5495 3.0743 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3560 Training loss: 1.5485 3.1861 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3560 Training loss: 1.5486 3.0871 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3560 Training loss: 1.5474 3.1232 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3560 Training loss: 1.5471 3.0841 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3560 Training loss: 1.5464 3.0213 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3560 Training loss: 1.5459 3.2077 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3560 Training loss: 1.5461 3.1715 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3560 Training loss: 1.5453 3.0169 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3560 Training loss: 1.5460 3.0870 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3560 Training loss: 1.5458 3.1188 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3560 Training loss: 1.5458 3.1072 sec/batch\n",
      "Epoch 7/20  Iteration 1123/3560 Training loss: 1.5454 3.0236 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3560 Training loss: 1.5452 3.1503 sec/batch\n",
      "Epoch 7/20  Iteration 1125/3560 Training loss: 1.5455 3.0823 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3560 Training loss: 1.5448 3.0137 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3560 Training loss: 1.5441 3.1462 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3560 Training loss: 1.5443 3.1474 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3560 Training loss: 1.5440 3.1108 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3560 Training loss: 1.5447 3.1245 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3560 Training loss: 1.5448 3.4370 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3560 Training loss: 1.5447 3.1288 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3560 Training loss: 1.5442 3.0818 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3560 Training loss: 1.5442 3.0569 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3560 Training loss: 1.5442 3.1856 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3560 Training loss: 1.5438 3.0888 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3560 Training loss: 1.5436 3.0035 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3560 Training loss: 1.5435 3.1701 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3560 Training loss: 1.5440 3.0859 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3560 Training loss: 1.5440 3.1157 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3560 Training loss: 1.5442 3.0803 sec/batch\n",
      "Epoch 7/20  Iteration 1142/3560 Training loss: 1.5439 3.1034 sec/batch\n",
      "Epoch 7/20  Iteration 1143/3560 Training loss: 1.5434 3.0572 sec/batch\n",
      "Epoch 7/20  Iteration 1144/3560 Training loss: 1.5435 3.4564 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3560 Training loss: 1.5431 3.4218 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3560 Training loss: 1.5429 3.4210 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3560 Training loss: 1.5422 3.2330 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3560 Training loss: 1.5420 3.1372 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3560 Training loss: 1.5413 3.1964 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3560 Training loss: 1.5412 3.1481 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3560 Training loss: 1.5407 3.1714 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3560 Training loss: 1.5405 3.2553 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3560 Training loss: 1.5400 3.1535 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3560 Training loss: 1.5396 3.1336 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3560 Training loss: 1.5392 3.2262 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3560 Training loss: 1.5387 3.1826 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3560 Training loss: 1.5380 3.0608 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3560 Training loss: 1.5379 3.1104 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3560 Training loss: 1.5374 3.1936 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3560 Training loss: 1.5371 3.1385 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3560 Training loss: 1.5365 3.2430 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3560 Training loss: 1.5360 3.4182 sec/batch\n",
      "Epoch 7/20  Iteration 1163/3560 Training loss: 1.5355 3.2311 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Iteration 1164/3560 Training loss: 1.5353 3.1330 sec/batch\n",
      "Epoch 7/20  Iteration 1165/3560 Training loss: 1.5350 3.4357 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3560 Training loss: 1.5343 3.2072 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3560 Training loss: 1.5337 3.2532 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3560 Training loss: 1.5330 3.1436 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3560 Training loss: 1.5329 3.2650 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3560 Training loss: 1.5325 3.2385 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3560 Training loss: 1.5321 3.1299 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3560 Training loss: 1.5317 3.1934 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3560 Training loss: 1.5313 3.1965 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3560 Training loss: 1.5311 3.1303 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3560 Training loss: 1.5309 3.1507 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3560 Training loss: 1.5307 3.1685 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3560 Training loss: 1.5305 3.1323 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3560 Training loss: 1.5304 3.0722 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3560 Training loss: 1.5301 3.2738 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3560 Training loss: 1.5297 3.1784 sec/batch\n",
      "Epoch 7/20  Iteration 1181/3560 Training loss: 1.5293 3.1111 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3560 Training loss: 1.5290 3.1607 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3560 Training loss: 1.5285 3.2037 sec/batch\n",
      "Epoch 7/20  Iteration 1184/3560 Training loss: 1.5280 3.2642 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3560 Training loss: 1.5277 3.1205 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3560 Training loss: 1.5275 3.2485 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3560 Training loss: 1.5272 3.2645 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3560 Training loss: 1.5269 3.0851 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3560 Training loss: 1.5266 3.2779 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3560 Training loss: 1.5261 3.2189 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3560 Training loss: 1.5255 3.1189 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3560 Training loss: 1.5255 3.1895 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3560 Training loss: 1.5252 3.2139 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3560 Training loss: 1.5246 3.0636 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3560 Training loss: 1.5245 3.2133 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3560 Training loss: 1.5243 3.1403 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3560 Training loss: 1.5240 3.1657 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3560 Training loss: 1.5235 3.1829 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3560 Training loss: 1.5230 3.1405 sec/batch\n",
      "Epoch 7/20  Iteration 1200/3560 Training loss: 1.5225 3.2026 sec/batch\n",
      "Epoch 7/20  Iteration 1201/3560 Training loss: 1.5224 3.1689 sec/batch\n",
      "Epoch 7/20  Iteration 1202/3560 Training loss: 1.5222 3.1700 sec/batch\n",
      "Epoch 7/20  Iteration 1203/3560 Training loss: 1.5220 3.1626 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3560 Training loss: 1.5219 3.1885 sec/batch\n",
      "Epoch 7/20  Iteration 1205/3560 Training loss: 1.5219 3.2083 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3560 Training loss: 1.5218 3.1902 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3560 Training loss: 1.5217 3.1293 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3560 Training loss: 1.5214 3.2726 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3560 Training loss: 1.5216 3.1357 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3560 Training loss: 1.5214 3.1029 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3560 Training loss: 1.5211 3.2498 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3560 Training loss: 1.5211 3.1553 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3560 Training loss: 1.5209 3.0695 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3560 Training loss: 1.5209 3.1402 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3560 Training loss: 1.5207 3.2108 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3560 Training loss: 1.5207 3.0616 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3560 Training loss: 1.5207 3.1811 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3560 Training loss: 1.5204 3.4230 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3560 Training loss: 1.5199 3.0969 sec/batch\n",
      "Epoch 7/20  Iteration 1220/3560 Training loss: 1.5197 3.1074 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3560 Training loss: 1.5196 3.3058 sec/batch\n",
      "Epoch 7/20  Iteration 1222/3560 Training loss: 1.5194 3.1554 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3560 Training loss: 1.5192 3.2104 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3560 Training loss: 1.5190 3.3068 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3560 Training loss: 1.5189 3.2890 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3560 Training loss: 1.5187 3.1587 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3560 Training loss: 1.5183 3.2067 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3560 Training loss: 1.5182 3.1713 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3560 Training loss: 1.5183 3.1561 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3560 Training loss: 1.5181 3.2044 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3560 Training loss: 1.5180 3.1623 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3560 Training loss: 1.5179 3.1675 sec/batch\n",
      "Epoch 7/20  Iteration 1233/3560 Training loss: 1.5177 3.1691 sec/batch\n",
      "Epoch 7/20  Iteration 1234/3560 Training loss: 1.5175 3.2980 sec/batch\n",
      "Epoch 7/20  Iteration 1235/3560 Training loss: 1.5174 3.1194 sec/batch\n",
      "Epoch 7/20  Iteration 1236/3560 Training loss: 1.5176 3.1241 sec/batch\n",
      "Epoch 7/20  Iteration 1237/3560 Training loss: 1.5174 3.2628 sec/batch\n",
      "Epoch 7/20  Iteration 1238/3560 Training loss: 1.5172 3.1714 sec/batch\n",
      "Epoch 7/20  Iteration 1239/3560 Training loss: 1.5170 3.1673 sec/batch\n",
      "Epoch 7/20  Iteration 1240/3560 Training loss: 1.5167 3.2946 sec/batch\n",
      "Epoch 7/20  Iteration 1241/3560 Training loss: 1.5168 3.0804 sec/batch\n",
      "Epoch 7/20  Iteration 1242/3560 Training loss: 1.5167 3.1475 sec/batch\n",
      "Epoch 7/20  Iteration 1243/3560 Training loss: 1.5167 3.3073 sec/batch\n",
      "Epoch 7/20  Iteration 1244/3560 Training loss: 1.5166 3.1837 sec/batch\n",
      "Epoch 7/20  Iteration 1245/3560 Training loss: 1.5163 3.2157 sec/batch\n",
      "Epoch 7/20  Iteration 1246/3560 Training loss: 1.5163 3.2877 sec/batch\n",
      "Epoch 8/20  Iteration 1247/3560 Training loss: 1.6018 3.0064 sec/batch\n",
      "Epoch 8/20  Iteration 1248/3560 Training loss: 1.5443 3.2008 sec/batch\n",
      "Epoch 8/20  Iteration 1249/3560 Training loss: 1.5256 3.2664 sec/batch\n",
      "Epoch 8/20  Iteration 1250/3560 Training loss: 1.5198 3.1548 sec/batch\n",
      "Epoch 8/20  Iteration 1251/3560 Training loss: 1.5114 3.1193 sec/batch\n",
      "Epoch 8/20  Iteration 1252/3560 Training loss: 1.5005 3.2301 sec/batch\n",
      "Epoch 8/20  Iteration 1253/3560 Training loss: 1.5002 3.1646 sec/batch\n",
      "Epoch 8/20  Iteration 1254/3560 Training loss: 1.4976 3.1712 sec/batch\n",
      "Epoch 8/20  Iteration 1255/3560 Training loss: 1.4975 3.1708 sec/batch\n",
      "Epoch 8/20  Iteration 1256/3560 Training loss: 1.4954 3.2372 sec/batch\n",
      "Epoch 8/20  Iteration 1257/3560 Training loss: 1.4916 3.0610 sec/batch\n",
      "Epoch 8/20  Iteration 1258/3560 Training loss: 1.4898 3.1889 sec/batch\n",
      "Epoch 8/20  Iteration 1259/3560 Training loss: 1.4887 3.2253 sec/batch\n",
      "Epoch 8/20  Iteration 1260/3560 Training loss: 1.4897 3.1444 sec/batch\n",
      "Epoch 8/20  Iteration 1261/3560 Training loss: 1.4886 3.0906 sec/batch\n",
      "Epoch 8/20  Iteration 1262/3560 Training loss: 1.4871 3.2224 sec/batch\n",
      "Epoch 8/20  Iteration 1263/3560 Training loss: 1.4870 3.2268 sec/batch\n",
      "Epoch 8/20  Iteration 1264/3560 Training loss: 1.4878 3.1586 sec/batch\n",
      "Epoch 8/20  Iteration 1265/3560 Training loss: 1.4877 3.2813 sec/batch\n",
      "Epoch 8/20  Iteration 1266/3560 Training loss: 1.4887 3.0892 sec/batch\n",
      "Epoch 8/20  Iteration 1267/3560 Training loss: 1.4875 3.0707 sec/batch\n",
      "Epoch 8/20  Iteration 1268/3560 Training loss: 1.4878 3.2420 sec/batch\n",
      "Epoch 8/20  Iteration 1269/3560 Training loss: 1.4866 3.1624 sec/batch\n",
      "Epoch 8/20  Iteration 1270/3560 Training loss: 1.4862 3.0862 sec/batch\n",
      "Epoch 8/20  Iteration 1271/3560 Training loss: 1.4859 3.2155 sec/batch\n",
      "Epoch 8/20  Iteration 1272/3560 Training loss: 1.4841 3.1925 sec/batch\n",
      "Epoch 8/20  Iteration 1273/3560 Training loss: 1.4831 3.1920 sec/batch\n",
      "Epoch 8/20  Iteration 1274/3560 Training loss: 1.4835 3.2069 sec/batch\n",
      "Epoch 8/20  Iteration 1275/3560 Training loss: 1.4839 3.3405 sec/batch\n",
      "Epoch 8/20  Iteration 1276/3560 Training loss: 1.4839 3.1681 sec/batch\n",
      "Epoch 8/20  Iteration 1277/3560 Training loss: 1.4832 3.2037 sec/batch\n",
      "Epoch 8/20  Iteration 1278/3560 Training loss: 1.4819 3.2304 sec/batch\n",
      "Epoch 8/20  Iteration 1279/3560 Training loss: 1.4820 3.0624 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1280/3560 Training loss: 1.4822 3.1949 sec/batch\n",
      "Epoch 8/20  Iteration 1281/3560 Training loss: 1.4819 3.2095 sec/batch\n",
      "Epoch 8/20  Iteration 1282/3560 Training loss: 1.4814 3.1882 sec/batch\n",
      "Epoch 8/20  Iteration 1283/3560 Training loss: 1.4803 3.1634 sec/batch\n",
      "Epoch 8/20  Iteration 1284/3560 Training loss: 1.4790 3.2172 sec/batch\n",
      "Epoch 8/20  Iteration 1285/3560 Training loss: 1.4775 3.2096 sec/batch\n",
      "Epoch 8/20  Iteration 1286/3560 Training loss: 1.4769 3.1563 sec/batch\n",
      "Epoch 8/20  Iteration 1287/3560 Training loss: 1.4762 3.1905 sec/batch\n",
      "Epoch 8/20  Iteration 1288/3560 Training loss: 1.4765 3.2491 sec/batch\n",
      "Epoch 8/20  Iteration 1289/3560 Training loss: 1.4760 3.1308 sec/batch\n",
      "Epoch 8/20  Iteration 1290/3560 Training loss: 1.4750 3.1353 sec/batch\n",
      "Epoch 8/20  Iteration 1291/3560 Training loss: 1.4750 3.2176 sec/batch\n",
      "Epoch 8/20  Iteration 1292/3560 Training loss: 1.4739 3.0688 sec/batch\n",
      "Epoch 8/20  Iteration 1293/3560 Training loss: 1.4735 3.1513 sec/batch\n",
      "Epoch 8/20  Iteration 1294/3560 Training loss: 1.4729 3.2459 sec/batch\n",
      "Epoch 8/20  Iteration 1295/3560 Training loss: 1.4723 3.1151 sec/batch\n",
      "Epoch 8/20  Iteration 1296/3560 Training loss: 1.4724 3.1917 sec/batch\n",
      "Epoch 8/20  Iteration 1297/3560 Training loss: 1.4720 3.2923 sec/batch\n",
      "Epoch 8/20  Iteration 1298/3560 Training loss: 1.4725 3.0687 sec/batch\n",
      "Epoch 8/20  Iteration 1299/3560 Training loss: 1.4724 3.1227 sec/batch\n",
      "Epoch 8/20  Iteration 1300/3560 Training loss: 1.4724 3.2951 sec/batch\n",
      "Epoch 8/20  Iteration 1301/3560 Training loss: 1.4720 3.1229 sec/batch\n",
      "Epoch 8/20  Iteration 1302/3560 Training loss: 1.4720 3.5225 sec/batch\n",
      "Epoch 8/20  Iteration 1303/3560 Training loss: 1.4721 3.4983 sec/batch\n",
      "Epoch 8/20  Iteration 1304/3560 Training loss: 1.4715 3.1833 sec/batch\n",
      "Epoch 8/20  Iteration 1305/3560 Training loss: 1.4709 3.1533 sec/batch\n",
      "Epoch 8/20  Iteration 1306/3560 Training loss: 1.4712 3.2240 sec/batch\n",
      "Epoch 8/20  Iteration 1307/3560 Training loss: 1.4712 3.2634 sec/batch\n",
      "Epoch 8/20  Iteration 1308/3560 Training loss: 1.4719 3.2146 sec/batch\n",
      "Epoch 8/20  Iteration 1309/3560 Training loss: 1.4724 3.2452 sec/batch\n",
      "Epoch 8/20  Iteration 1310/3560 Training loss: 1.4724 3.2984 sec/batch\n",
      "Epoch 8/20  Iteration 1311/3560 Training loss: 1.4721 3.1328 sec/batch\n",
      "Epoch 8/20  Iteration 1312/3560 Training loss: 1.4720 3.1810 sec/batch\n",
      "Epoch 8/20  Iteration 1313/3560 Training loss: 1.4721 3.1811 sec/batch\n",
      "Epoch 8/20  Iteration 1314/3560 Training loss: 1.4717 3.1919 sec/batch\n",
      "Epoch 8/20  Iteration 1315/3560 Training loss: 1.4714 3.1662 sec/batch\n",
      "Epoch 8/20  Iteration 1316/3560 Training loss: 1.4712 3.2300 sec/batch\n",
      "Epoch 8/20  Iteration 1317/3560 Training loss: 1.4717 3.1522 sec/batch\n",
      "Epoch 8/20  Iteration 1318/3560 Training loss: 1.4718 3.1574 sec/batch\n",
      "Epoch 8/20  Iteration 1319/3560 Training loss: 1.4723 3.2451 sec/batch\n",
      "Epoch 8/20  Iteration 1320/3560 Training loss: 1.4719 3.0940 sec/batch\n",
      "Epoch 8/20  Iteration 1321/3560 Training loss: 1.4717 3.1886 sec/batch\n",
      "Epoch 8/20  Iteration 1322/3560 Training loss: 1.4717 3.2844 sec/batch\n",
      "Epoch 8/20  Iteration 1323/3560 Training loss: 1.4714 3.0942 sec/batch\n",
      "Epoch 8/20  Iteration 1324/3560 Training loss: 1.4712 3.1156 sec/batch\n",
      "Epoch 8/20  Iteration 1325/3560 Training loss: 1.4704 3.2075 sec/batch\n",
      "Epoch 8/20  Iteration 1326/3560 Training loss: 1.4703 3.1464 sec/batch\n",
      "Epoch 8/20  Iteration 1327/3560 Training loss: 1.4697 3.1519 sec/batch\n",
      "Epoch 8/20  Iteration 1328/3560 Training loss: 1.4696 3.2035 sec/batch\n",
      "Epoch 8/20  Iteration 1329/3560 Training loss: 1.4690 3.1684 sec/batch\n",
      "Epoch 8/20  Iteration 1330/3560 Training loss: 1.4689 3.0681 sec/batch\n",
      "Epoch 8/20  Iteration 1331/3560 Training loss: 1.4685 3.3626 sec/batch\n",
      "Epoch 8/20  Iteration 1332/3560 Training loss: 1.4682 3.3373 sec/batch\n",
      "Epoch 8/20  Iteration 1333/3560 Training loss: 1.4677 3.1803 sec/batch\n",
      "Epoch 8/20  Iteration 1334/3560 Training loss: 1.4672 3.2053 sec/batch\n",
      "Epoch 8/20  Iteration 1335/3560 Training loss: 1.4667 3.1820 sec/batch\n",
      "Epoch 8/20  Iteration 1336/3560 Training loss: 1.4667 3.1904 sec/batch\n",
      "Epoch 8/20  Iteration 1337/3560 Training loss: 1.4663 3.2471 sec/batch\n",
      "Epoch 8/20  Iteration 1338/3560 Training loss: 1.4660 3.3264 sec/batch\n",
      "Epoch 8/20  Iteration 1339/3560 Training loss: 1.4655 3.1380 sec/batch\n",
      "Epoch 8/20  Iteration 1340/3560 Training loss: 1.4650 3.1062 sec/batch\n",
      "Epoch 8/20  Iteration 1341/3560 Training loss: 1.4646 3.3538 sec/batch\n",
      "Epoch 8/20  Iteration 1342/3560 Training loss: 1.4645 3.1900 sec/batch\n",
      "Epoch 8/20  Iteration 1343/3560 Training loss: 1.4643 3.2371 sec/batch\n",
      "Epoch 8/20  Iteration 1344/3560 Training loss: 1.4637 3.2385 sec/batch\n",
      "Epoch 8/20  Iteration 1345/3560 Training loss: 1.4633 3.1810 sec/batch\n",
      "Epoch 8/20  Iteration 1346/3560 Training loss: 1.4627 3.2026 sec/batch\n",
      "Epoch 8/20  Iteration 1347/3560 Training loss: 1.4625 3.2882 sec/batch\n",
      "Epoch 8/20  Iteration 1348/3560 Training loss: 1.4623 3.2178 sec/batch\n",
      "Epoch 8/20  Iteration 1349/3560 Training loss: 1.4619 3.1567 sec/batch\n",
      "Epoch 8/20  Iteration 1350/3560 Training loss: 1.4616 3.2291 sec/batch\n",
      "Epoch 8/20  Iteration 1351/3560 Training loss: 1.4615 3.1249 sec/batch\n",
      "Epoch 8/20  Iteration 1352/3560 Training loss: 1.4613 3.2213 sec/batch\n",
      "Epoch 8/20  Iteration 1353/3560 Training loss: 1.4612 3.1856 sec/batch\n",
      "Epoch 8/20  Iteration 1354/3560 Training loss: 1.4609 3.2256 sec/batch\n",
      "Epoch 8/20  Iteration 1355/3560 Training loss: 1.4607 3.1457 sec/batch\n",
      "Epoch 8/20  Iteration 1356/3560 Training loss: 1.4606 3.2381 sec/batch\n",
      "Epoch 8/20  Iteration 1357/3560 Training loss: 1.4604 3.1652 sec/batch\n",
      "Epoch 8/20  Iteration 1358/3560 Training loss: 1.4601 3.1630 sec/batch\n",
      "Epoch 8/20  Iteration 1359/3560 Training loss: 1.4598 3.2818 sec/batch\n",
      "Epoch 8/20  Iteration 1360/3560 Training loss: 1.4595 3.1855 sec/batch\n",
      "Epoch 8/20  Iteration 1361/3560 Training loss: 1.4591 3.1560 sec/batch\n",
      "Epoch 8/20  Iteration 1362/3560 Training loss: 1.4587 3.1760 sec/batch\n",
      "Epoch 8/20  Iteration 1363/3560 Training loss: 1.4585 3.2327 sec/batch\n",
      "Epoch 8/20  Iteration 1364/3560 Training loss: 1.4584 3.1174 sec/batch\n",
      "Epoch 8/20  Iteration 1365/3560 Training loss: 1.4581 3.1061 sec/batch\n",
      "Epoch 8/20  Iteration 1366/3560 Training loss: 1.4579 3.1969 sec/batch\n",
      "Epoch 8/20  Iteration 1367/3560 Training loss: 1.4577 3.1809 sec/batch\n",
      "Epoch 8/20  Iteration 1368/3560 Training loss: 1.4572 3.1264 sec/batch\n",
      "Epoch 8/20  Iteration 1369/3560 Training loss: 1.4567 3.1325 sec/batch\n",
      "Epoch 8/20  Iteration 1370/3560 Training loss: 1.4567 3.3044 sec/batch\n",
      "Epoch 8/20  Iteration 1371/3560 Training loss: 1.4565 3.1869 sec/batch\n",
      "Epoch 8/20  Iteration 1372/3560 Training loss: 1.4560 3.1858 sec/batch\n",
      "Epoch 8/20  Iteration 1373/3560 Training loss: 1.4559 3.2574 sec/batch\n",
      "Epoch 8/20  Iteration 1374/3560 Training loss: 1.4559 3.0791 sec/batch\n",
      "Epoch 8/20  Iteration 1375/3560 Training loss: 1.4557 3.1378 sec/batch\n",
      "Epoch 8/20  Iteration 1376/3560 Training loss: 1.4554 3.2850 sec/batch\n",
      "Epoch 8/20  Iteration 1377/3560 Training loss: 1.4549 3.1550 sec/batch\n",
      "Epoch 8/20  Iteration 1378/3560 Training loss: 1.4546 3.1660 sec/batch\n",
      "Epoch 8/20  Iteration 1379/3560 Training loss: 1.4547 3.2014 sec/batch\n",
      "Epoch 8/20  Iteration 1380/3560 Training loss: 1.4546 3.2552 sec/batch\n",
      "Epoch 8/20  Iteration 1381/3560 Training loss: 1.4545 3.1415 sec/batch\n",
      "Epoch 8/20  Iteration 1382/3560 Training loss: 1.4544 3.2397 sec/batch\n",
      "Epoch 8/20  Iteration 1383/3560 Training loss: 1.4545 3.1335 sec/batch\n",
      "Epoch 8/20  Iteration 1384/3560 Training loss: 1.4545 3.1673 sec/batch\n",
      "Epoch 8/20  Iteration 1385/3560 Training loss: 1.4545 3.2043 sec/batch\n",
      "Epoch 8/20  Iteration 1386/3560 Training loss: 1.4544 3.0886 sec/batch\n",
      "Epoch 8/20  Iteration 1387/3560 Training loss: 1.4546 3.6027 sec/batch\n",
      "Epoch 8/20  Iteration 1388/3560 Training loss: 1.4546 3.2410 sec/batch\n",
      "Epoch 8/20  Iteration 1389/3560 Training loss: 1.4544 3.1281 sec/batch\n",
      "Epoch 8/20  Iteration 1390/3560 Training loss: 1.4546 3.1255 sec/batch\n",
      "Epoch 8/20  Iteration 1391/3560 Training loss: 1.4544 3.1553 sec/batch\n",
      "Epoch 8/20  Iteration 1392/3560 Training loss: 1.4544 3.1820 sec/batch\n",
      "Epoch 8/20  Iteration 1393/3560 Training loss: 1.4544 3.2332 sec/batch\n",
      "Epoch 8/20  Iteration 1394/3560 Training loss: 1.4546 3.1737 sec/batch\n",
      "Epoch 8/20  Iteration 1395/3560 Training loss: 1.4545 3.1652 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Iteration 1396/3560 Training loss: 1.4543 3.1474 sec/batch\n",
      "Epoch 8/20  Iteration 1397/3560 Training loss: 1.4539 3.1627 sec/batch\n",
      "Epoch 8/20  Iteration 1398/3560 Training loss: 1.4537 3.2308 sec/batch\n",
      "Epoch 8/20  Iteration 1399/3560 Training loss: 1.4537 3.2308 sec/batch\n",
      "Epoch 8/20  Iteration 1400/3560 Training loss: 1.4536 3.1339 sec/batch\n",
      "Epoch 8/20  Iteration 1401/3560 Training loss: 1.4535 3.1764 sec/batch\n",
      "Epoch 8/20  Iteration 1402/3560 Training loss: 1.4534 3.1618 sec/batch\n",
      "Epoch 8/20  Iteration 1403/3560 Training loss: 1.4534 3.2045 sec/batch\n",
      "Epoch 8/20  Iteration 1404/3560 Training loss: 1.4532 3.1707 sec/batch\n",
      "Epoch 8/20  Iteration 1405/3560 Training loss: 1.4529 3.1696 sec/batch\n",
      "Epoch 8/20  Iteration 1406/3560 Training loss: 1.4529 3.1911 sec/batch\n",
      "Epoch 8/20  Iteration 1407/3560 Training loss: 1.4530 3.1631 sec/batch\n",
      "Epoch 8/20  Iteration 1408/3560 Training loss: 1.4529 3.2771 sec/batch\n",
      "Epoch 8/20  Iteration 1409/3560 Training loss: 1.4528 3.1662 sec/batch\n",
      "Epoch 8/20  Iteration 1410/3560 Training loss: 1.4527 3.1385 sec/batch\n",
      "Epoch 8/20  Iteration 1411/3560 Training loss: 1.4526 3.2223 sec/batch\n",
      "Epoch 8/20  Iteration 1412/3560 Training loss: 1.4524 3.1548 sec/batch\n",
      "Epoch 8/20  Iteration 1413/3560 Training loss: 1.4525 3.1390 sec/batch\n",
      "Epoch 8/20  Iteration 1414/3560 Training loss: 1.4528 3.1831 sec/batch\n",
      "Epoch 8/20  Iteration 1415/3560 Training loss: 1.4527 3.2079 sec/batch\n",
      "Epoch 8/20  Iteration 1416/3560 Training loss: 1.4525 3.1710 sec/batch\n",
      "Epoch 8/20  Iteration 1417/3560 Training loss: 1.4523 3.2930 sec/batch\n",
      "Epoch 8/20  Iteration 1418/3560 Training loss: 1.4520 3.2079 sec/batch\n",
      "Epoch 8/20  Iteration 1419/3560 Training loss: 1.4520 3.0829 sec/batch\n",
      "Epoch 8/20  Iteration 1420/3560 Training loss: 1.4520 3.1614 sec/batch\n",
      "Epoch 8/20  Iteration 1421/3560 Training loss: 1.4519 3.2373 sec/batch\n",
      "Epoch 8/20  Iteration 1422/3560 Training loss: 1.4517 3.1701 sec/batch\n",
      "Epoch 8/20  Iteration 1423/3560 Training loss: 1.4515 3.1467 sec/batch\n",
      "Epoch 8/20  Iteration 1424/3560 Training loss: 1.4515 3.2580 sec/batch\n",
      "Epoch 9/20  Iteration 1425/3560 Training loss: 1.5761 3.1523 sec/batch\n",
      "Epoch 9/20  Iteration 1426/3560 Training loss: 1.5169 3.1487 sec/batch\n",
      "Epoch 9/20  Iteration 1427/3560 Training loss: 1.4921 3.1156 sec/batch\n",
      "Epoch 9/20  Iteration 1428/3560 Training loss: 1.4857 3.2111 sec/batch\n",
      "Epoch 9/20  Iteration 1429/3560 Training loss: 1.4744 3.1938 sec/batch\n",
      "Epoch 9/20  Iteration 1430/3560 Training loss: 1.4609 3.1709 sec/batch\n",
      "Epoch 9/20  Iteration 1431/3560 Training loss: 1.4576 3.1683 sec/batch\n",
      "Epoch 9/20  Iteration 1432/3560 Training loss: 1.4536 3.2347 sec/batch\n",
      "Epoch 9/20  Iteration 1433/3560 Training loss: 1.4531 3.1881 sec/batch\n",
      "Epoch 9/20  Iteration 1434/3560 Training loss: 1.4522 3.2760 sec/batch\n",
      "Epoch 9/20  Iteration 1435/3560 Training loss: 1.4480 3.2315 sec/batch\n",
      "Epoch 9/20  Iteration 1436/3560 Training loss: 1.4453 3.2668 sec/batch\n",
      "Epoch 9/20  Iteration 1437/3560 Training loss: 1.4439 3.1524 sec/batch\n",
      "Epoch 9/20  Iteration 1438/3560 Training loss: 1.4447 3.1927 sec/batch\n",
      "Epoch 9/20  Iteration 1439/3560 Training loss: 1.4426 3.1004 sec/batch\n",
      "Epoch 9/20  Iteration 1440/3560 Training loss: 1.4399 3.2542 sec/batch\n",
      "Epoch 9/20  Iteration 1441/3560 Training loss: 1.4390 3.1689 sec/batch\n",
      "Epoch 9/20  Iteration 1442/3560 Training loss: 1.4398 3.1663 sec/batch\n",
      "Epoch 9/20  Iteration 1443/3560 Training loss: 1.4398 3.2687 sec/batch\n",
      "Epoch 9/20  Iteration 1444/3560 Training loss: 1.4408 3.3765 sec/batch\n",
      "Epoch 9/20  Iteration 1445/3560 Training loss: 1.4393 3.1804 sec/batch\n",
      "Epoch 9/20  Iteration 1446/3560 Training loss: 1.4388 3.2771 sec/batch\n",
      "Epoch 9/20  Iteration 1447/3560 Training loss: 1.4377 3.1925 sec/batch\n",
      "Epoch 9/20  Iteration 1448/3560 Training loss: 1.4372 3.1185 sec/batch\n",
      "Epoch 9/20  Iteration 1449/3560 Training loss: 1.4366 3.2676 sec/batch\n",
      "Epoch 9/20  Iteration 1450/3560 Training loss: 1.4349 3.2089 sec/batch\n",
      "Epoch 9/20  Iteration 1451/3560 Training loss: 1.4333 3.2152 sec/batch\n",
      "Epoch 9/20  Iteration 1452/3560 Training loss: 1.4332 3.2835 sec/batch\n",
      "Epoch 9/20  Iteration 1453/3560 Training loss: 1.4331 3.1460 sec/batch\n",
      "Epoch 9/20  Iteration 1454/3560 Training loss: 1.4332 3.1846 sec/batch\n",
      "Epoch 9/20  Iteration 1455/3560 Training loss: 1.4325 3.2436 sec/batch\n",
      "Epoch 9/20  Iteration 1456/3560 Training loss: 1.4310 3.2329 sec/batch\n",
      "Epoch 9/20  Iteration 1457/3560 Training loss: 1.4309 3.2010 sec/batch\n",
      "Epoch 9/20  Iteration 1458/3560 Training loss: 1.4308 3.1450 sec/batch\n",
      "Epoch 9/20  Iteration 1459/3560 Training loss: 1.4307 3.1620 sec/batch\n",
      "Epoch 9/20  Iteration 1460/3560 Training loss: 1.4303 3.1904 sec/batch\n",
      "Epoch 9/20  Iteration 1461/3560 Training loss: 1.4294 3.1207 sec/batch\n",
      "Epoch 9/20  Iteration 1462/3560 Training loss: 1.4279 3.2671 sec/batch\n",
      "Epoch 9/20  Iteration 1463/3560 Training loss: 1.4264 3.1708 sec/batch\n",
      "Epoch 9/20  Iteration 1464/3560 Training loss: 1.4256 3.1899 sec/batch\n",
      "Epoch 9/20  Iteration 1465/3560 Training loss: 1.4250 3.1798 sec/batch\n",
      "Epoch 9/20  Iteration 1466/3560 Training loss: 1.4255 3.2003 sec/batch\n",
      "Epoch 9/20  Iteration 1467/3560 Training loss: 1.4249 3.0879 sec/batch\n",
      "Epoch 9/20  Iteration 1468/3560 Training loss: 1.4240 3.2297 sec/batch\n",
      "Epoch 9/20  Iteration 1469/3560 Training loss: 1.4242 3.2192 sec/batch\n",
      "Epoch 9/20  Iteration 1470/3560 Training loss: 1.4232 3.1806 sec/batch\n",
      "Epoch 9/20  Iteration 1471/3560 Training loss: 1.4229 3.2404 sec/batch\n",
      "Epoch 9/20  Iteration 1472/3560 Training loss: 1.4225 3.1627 sec/batch\n",
      "Epoch 9/20  Iteration 1473/3560 Training loss: 1.4221 3.1757 sec/batch\n",
      "Epoch 9/20  Iteration 1474/3560 Training loss: 1.4223 3.2479 sec/batch\n",
      "Epoch 9/20  Iteration 1475/3560 Training loss: 1.4218 3.1870 sec/batch\n",
      "Epoch 9/20  Iteration 1476/3560 Training loss: 1.4223 3.1760 sec/batch\n",
      "Epoch 9/20  Iteration 1477/3560 Training loss: 1.4221 3.2689 sec/batch\n",
      "Epoch 9/20  Iteration 1478/3560 Training loss: 1.4221 3.1097 sec/batch\n",
      "Epoch 9/20  Iteration 1479/3560 Training loss: 1.4218 3.1207 sec/batch\n",
      "Epoch 9/20  Iteration 1480/3560 Training loss: 1.4217 3.4535 sec/batch\n",
      "Epoch 9/20  Iteration 1481/3560 Training loss: 1.4220 3.2218 sec/batch\n",
      "Epoch 9/20  Iteration 1482/3560 Training loss: 1.4215 3.1148 sec/batch\n",
      "Epoch 9/20  Iteration 1483/3560 Training loss: 1.4208 3.2228 sec/batch\n",
      "Epoch 9/20  Iteration 1484/3560 Training loss: 1.4212 3.1691 sec/batch\n",
      "Epoch 9/20  Iteration 1485/3560 Training loss: 1.4211 3.1706 sec/batch\n",
      "Epoch 9/20  Iteration 1486/3560 Training loss: 1.4218 3.2064 sec/batch\n",
      "Epoch 9/20  Iteration 1487/3560 Training loss: 1.4222 3.1798 sec/batch\n",
      "Epoch 9/20  Iteration 1488/3560 Training loss: 1.4221 3.1571 sec/batch\n",
      "Epoch 9/20  Iteration 1489/3560 Training loss: 1.4218 3.1256 sec/batch\n",
      "Epoch 9/20  Iteration 1490/3560 Training loss: 1.4218 3.2892 sec/batch\n",
      "Epoch 9/20  Iteration 1491/3560 Training loss: 1.4219 3.1859 sec/batch\n",
      "Epoch 9/20  Iteration 1492/3560 Training loss: 1.4215 3.1483 sec/batch\n",
      "Epoch 9/20  Iteration 1493/3560 Training loss: 1.4214 3.2810 sec/batch\n",
      "Epoch 9/20  Iteration 1494/3560 Training loss: 1.4212 3.2547 sec/batch\n",
      "Epoch 9/20  Iteration 1495/3560 Training loss: 1.4217 3.0847 sec/batch\n",
      "Epoch 9/20  Iteration 1496/3560 Training loss: 1.4219 3.2456 sec/batch\n",
      "Epoch 9/20  Iteration 1497/3560 Training loss: 1.4223 3.2441 sec/batch\n",
      "Epoch 9/20  Iteration 1498/3560 Training loss: 1.4218 3.1320 sec/batch\n",
      "Epoch 9/20  Iteration 1499/3560 Training loss: 1.4216 3.1287 sec/batch\n",
      "Epoch 9/20  Iteration 1500/3560 Training loss: 1.4217 3.5141 sec/batch\n",
      "Epoch 9/20  Iteration 1501/3560 Training loss: 1.4214 3.1499 sec/batch\n",
      "Epoch 9/20  Iteration 1502/3560 Training loss: 1.4212 3.2225 sec/batch\n",
      "Epoch 9/20  Iteration 1503/3560 Training loss: 1.4205 3.3237 sec/batch\n",
      "Epoch 9/20  Iteration 1504/3560 Training loss: 1.4204 3.1315 sec/batch\n",
      "Epoch 9/20  Iteration 1505/3560 Training loss: 1.4198 3.1880 sec/batch\n",
      "Epoch 9/20  Iteration 1506/3560 Training loss: 1.4197 3.2472 sec/batch\n",
      "Epoch 9/20  Iteration 1507/3560 Training loss: 1.4191 3.2980 sec/batch\n",
      "Epoch 9/20  Iteration 1508/3560 Training loss: 1.4190 3.1627 sec/batch\n",
      "Epoch 9/20  Iteration 1509/3560 Training loss: 1.4185 3.1867 sec/batch\n",
      "Epoch 9/20  Iteration 1510/3560 Training loss: 1.4183 3.2316 sec/batch\n",
      "Epoch 9/20  Iteration 1511/3560 Training loss: 1.4179 3.1959 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Iteration 1512/3560 Training loss: 1.4176 3.2364 sec/batch\n",
      "Epoch 9/20  Iteration 1513/3560 Training loss: 1.4172 3.2594 sec/batch\n",
      "Epoch 9/20  Iteration 1514/3560 Training loss: 1.4171 3.1863 sec/batch\n",
      "Epoch 9/20  Iteration 1515/3560 Training loss: 1.4167 3.1984 sec/batch\n",
      "Epoch 9/20  Iteration 1516/3560 Training loss: 1.4165 3.2102 sec/batch\n",
      "Epoch 9/20  Iteration 1517/3560 Training loss: 1.4161 3.1552 sec/batch\n",
      "Epoch 9/20  Iteration 1518/3560 Training loss: 1.4156 3.1533 sec/batch\n",
      "Epoch 9/20  Iteration 1519/3560 Training loss: 1.4152 3.3686 sec/batch\n",
      "Epoch 9/20  Iteration 1520/3560 Training loss: 1.4151 3.0951 sec/batch\n",
      "Epoch 9/20  Iteration 1521/3560 Training loss: 1.4150 3.1794 sec/batch\n",
      "Epoch 9/20  Iteration 1522/3560 Training loss: 1.4146 3.2780 sec/batch\n",
      "Epoch 9/20  Iteration 1523/3560 Training loss: 1.4141 3.1408 sec/batch\n",
      "Epoch 9/20  Iteration 1524/3560 Training loss: 1.4136 3.1743 sec/batch\n",
      "Epoch 9/20  Iteration 1525/3560 Training loss: 1.4135 3.3695 sec/batch\n",
      "Epoch 9/20  Iteration 1526/3560 Training loss: 1.4133 3.0735 sec/batch\n",
      "Epoch 9/20  Iteration 1527/3560 Training loss: 1.4130 3.1342 sec/batch\n",
      "Epoch 9/20  Iteration 1528/3560 Training loss: 1.4128 3.3228 sec/batch\n",
      "Epoch 9/20  Iteration 1529/3560 Training loss: 1.4126 3.0332 sec/batch\n",
      "Epoch 9/20  Iteration 1530/3560 Training loss: 1.4124 3.2087 sec/batch\n",
      "Epoch 9/20  Iteration 1531/3560 Training loss: 1.4123 3.3020 sec/batch\n",
      "Epoch 9/20  Iteration 1532/3560 Training loss: 1.4121 3.1306 sec/batch\n",
      "Epoch 9/20  Iteration 1533/3560 Training loss: 1.4119 3.0729 sec/batch\n",
      "Epoch 9/20  Iteration 1534/3560 Training loss: 1.4118 3.2982 sec/batch\n",
      "Epoch 9/20  Iteration 1535/3560 Training loss: 1.4115 3.1691 sec/batch\n",
      "Epoch 9/20  Iteration 1536/3560 Training loss: 1.4113 3.1677 sec/batch\n",
      "Epoch 9/20  Iteration 1537/3560 Training loss: 1.4110 3.1746 sec/batch\n",
      "Epoch 9/20  Iteration 1538/3560 Training loss: 1.4108 3.2046 sec/batch\n",
      "Epoch 9/20  Iteration 1539/3560 Training loss: 1.4105 3.2372 sec/batch\n",
      "Epoch 9/20  Iteration 1540/3560 Training loss: 1.4100 3.1550 sec/batch\n",
      "Epoch 9/20  Iteration 1541/3560 Training loss: 1.4099 3.2192 sec/batch\n",
      "Epoch 9/20  Iteration 1542/3560 Training loss: 1.4099 3.2143 sec/batch\n",
      "Epoch 9/20  Iteration 1543/3560 Training loss: 1.4097 3.1069 sec/batch\n",
      "Epoch 9/20  Iteration 1544/3560 Training loss: 1.4095 3.3012 sec/batch\n",
      "Epoch 9/20  Iteration 1545/3560 Training loss: 1.4092 3.2440 sec/batch\n",
      "Epoch 9/20  Iteration 1546/3560 Training loss: 1.4088 3.1832 sec/batch\n",
      "Epoch 9/20  Iteration 1547/3560 Training loss: 1.4082 3.2510 sec/batch\n",
      "Epoch 9/20  Iteration 1548/3560 Training loss: 1.4081 3.1380 sec/batch\n",
      "Epoch 9/20  Iteration 1549/3560 Training loss: 1.4079 3.1178 sec/batch\n",
      "Epoch 9/20  Iteration 1550/3560 Training loss: 1.4075 3.2671 sec/batch\n",
      "Epoch 9/20  Iteration 1551/3560 Training loss: 1.4075 3.2064 sec/batch\n",
      "Epoch 9/20  Iteration 1552/3560 Training loss: 1.4074 3.1295 sec/batch\n",
      "Epoch 9/20  Iteration 1553/3560 Training loss: 1.4072 3.2674 sec/batch\n",
      "Epoch 9/20  Iteration 1554/3560 Training loss: 1.4068 3.0681 sec/batch\n",
      "Epoch 9/20  Iteration 1555/3560 Training loss: 1.4063 3.0980 sec/batch\n",
      "Epoch 9/20  Iteration 1556/3560 Training loss: 1.4060 3.5462 sec/batch\n",
      "Epoch 9/20  Iteration 1557/3560 Training loss: 1.4060 3.1039 sec/batch\n",
      "Epoch 9/20  Iteration 1558/3560 Training loss: 1.4060 3.1203 sec/batch\n",
      "Epoch 9/20  Iteration 1559/3560 Training loss: 1.4058 3.2182 sec/batch\n",
      "Epoch 9/20  Iteration 1560/3560 Training loss: 1.4058 3.1829 sec/batch\n",
      "Epoch 9/20  Iteration 1561/3560 Training loss: 1.4060 3.1354 sec/batch\n",
      "Epoch 9/20  Iteration 1562/3560 Training loss: 1.4060 3.2638 sec/batch\n",
      "Epoch 9/20  Iteration 1563/3560 Training loss: 1.4059 3.2288 sec/batch\n",
      "Epoch 9/20  Iteration 1564/3560 Training loss: 1.4058 3.1977 sec/batch\n",
      "Epoch 9/20  Iteration 1565/3560 Training loss: 1.4061 3.2631 sec/batch\n",
      "Epoch 9/20  Iteration 1566/3560 Training loss: 1.4061 3.2365 sec/batch\n",
      "Epoch 9/20  Iteration 1567/3560 Training loss: 1.4059 3.1549 sec/batch\n",
      "Epoch 9/20  Iteration 1568/3560 Training loss: 1.4061 3.1516 sec/batch\n",
      "Epoch 9/20  Iteration 1569/3560 Training loss: 1.4060 3.2875 sec/batch\n",
      "Epoch 9/20  Iteration 1570/3560 Training loss: 1.4060 3.1385 sec/batch\n",
      "Epoch 9/20  Iteration 1571/3560 Training loss: 1.4060 3.1951 sec/batch\n",
      "Epoch 9/20  Iteration 1572/3560 Training loss: 1.4061 3.2957 sec/batch\n",
      "Epoch 9/20  Iteration 1573/3560 Training loss: 1.4061 3.1821 sec/batch\n",
      "Epoch 9/20  Iteration 1574/3560 Training loss: 1.4059 3.2331 sec/batch\n",
      "Epoch 9/20  Iteration 1575/3560 Training loss: 1.4055 3.3459 sec/batch\n",
      "Epoch 9/20  Iteration 1576/3560 Training loss: 1.4053 3.1282 sec/batch\n",
      "Epoch 9/20  Iteration 1577/3560 Training loss: 1.4053 3.0808 sec/batch\n",
      "Epoch 9/20  Iteration 1578/3560 Training loss: 1.4052 3.1488 sec/batch\n",
      "Epoch 9/20  Iteration 1579/3560 Training loss: 1.4051 3.2207 sec/batch\n",
      "Epoch 9/20  Iteration 1580/3560 Training loss: 1.4050 3.1236 sec/batch\n",
      "Epoch 9/20  Iteration 1581/3560 Training loss: 1.4050 3.2158 sec/batch\n",
      "Epoch 9/20  Iteration 1582/3560 Training loss: 1.4048 3.1146 sec/batch\n",
      "Epoch 9/20  Iteration 1583/3560 Training loss: 1.4045 3.1055 sec/batch\n",
      "Epoch 9/20  Iteration 1584/3560 Training loss: 1.4045 3.2136 sec/batch\n",
      "Epoch 9/20  Iteration 1585/3560 Training loss: 1.4046 3.2265 sec/batch\n",
      "Epoch 9/20  Iteration 1586/3560 Training loss: 1.4045 3.1026 sec/batch\n",
      "Epoch 9/20  Iteration 1587/3560 Training loss: 1.4043 3.1811 sec/batch\n",
      "Epoch 9/20  Iteration 1588/3560 Training loss: 1.4042 3.3326 sec/batch\n",
      "Epoch 9/20  Iteration 1589/3560 Training loss: 1.4041 3.2574 sec/batch\n",
      "Epoch 9/20  Iteration 1590/3560 Training loss: 1.4040 3.1745 sec/batch\n",
      "Epoch 9/20  Iteration 1591/3560 Training loss: 1.4041 3.2414 sec/batch\n",
      "Epoch 9/20  Iteration 1592/3560 Training loss: 1.4045 3.1154 sec/batch\n",
      "Epoch 9/20  Iteration 1593/3560 Training loss: 1.4044 3.1127 sec/batch\n",
      "Epoch 9/20  Iteration 1594/3560 Training loss: 1.4044 3.2684 sec/batch\n",
      "Epoch 9/20  Iteration 1595/3560 Training loss: 1.4042 3.1807 sec/batch\n",
      "Epoch 9/20  Iteration 1596/3560 Training loss: 1.4039 3.1751 sec/batch\n",
      "Epoch 9/20  Iteration 1597/3560 Training loss: 1.4040 3.1729 sec/batch\n",
      "Epoch 9/20  Iteration 1598/3560 Training loss: 1.4039 3.1890 sec/batch\n",
      "Epoch 9/20  Iteration 1599/3560 Training loss: 1.4039 3.1859 sec/batch\n",
      "Epoch 9/20  Iteration 1600/3560 Training loss: 1.4038 3.1687 sec/batch\n",
      "Epoch 9/20  Iteration 1601/3560 Training loss: 1.4036 3.2748 sec/batch\n",
      "Epoch 9/20  Iteration 1602/3560 Training loss: 1.4036 3.0597 sec/batch\n",
      "Epoch 10/20  Iteration 1603/3560 Training loss: 1.5260 3.2241 sec/batch\n",
      "Epoch 10/20  Iteration 1604/3560 Training loss: 1.4654 3.2352 sec/batch\n",
      "Epoch 10/20  Iteration 1605/3560 Training loss: 1.4401 3.0928 sec/batch\n",
      "Epoch 10/20  Iteration 1606/3560 Training loss: 1.4335 3.1910 sec/batch\n",
      "Epoch 10/20  Iteration 1607/3560 Training loss: 1.4230 3.2057 sec/batch\n",
      "Epoch 10/20  Iteration 1608/3560 Training loss: 1.4107 3.1817 sec/batch\n",
      "Epoch 10/20  Iteration 1609/3560 Training loss: 1.4103 3.1661 sec/batch\n",
      "Epoch 10/20  Iteration 1610/3560 Training loss: 1.4068 3.1830 sec/batch\n",
      "Epoch 10/20  Iteration 1611/3560 Training loss: 1.4055 3.1596 sec/batch\n",
      "Epoch 10/20  Iteration 1612/3560 Training loss: 1.4036 3.2347 sec/batch\n",
      "Epoch 10/20  Iteration 1613/3560 Training loss: 1.3994 3.2919 sec/batch\n",
      "Epoch 10/20  Iteration 1614/3560 Training loss: 1.3975 3.2523 sec/batch\n",
      "Epoch 10/20  Iteration 1615/3560 Training loss: 1.3964 3.1614 sec/batch\n",
      "Epoch 10/20  Iteration 1616/3560 Training loss: 1.3971 3.0949 sec/batch\n",
      "Epoch 10/20  Iteration 1617/3560 Training loss: 1.3955 3.2555 sec/batch\n",
      "Epoch 10/20  Iteration 1618/3560 Training loss: 1.3937 3.1462 sec/batch\n",
      "Epoch 10/20  Iteration 1619/3560 Training loss: 1.3934 3.2315 sec/batch\n",
      "Epoch 10/20  Iteration 1620/3560 Training loss: 1.3944 3.1576 sec/batch\n",
      "Epoch 10/20  Iteration 1621/3560 Training loss: 1.3941 3.1978 sec/batch\n",
      "Epoch 10/20  Iteration 1622/3560 Training loss: 1.3950 3.1720 sec/batch\n",
      "Epoch 10/20  Iteration 1623/3560 Training loss: 1.3939 3.2544 sec/batch\n",
      "Epoch 10/20  Iteration 1624/3560 Training loss: 1.3940 3.2398 sec/batch\n",
      "Epoch 10/20  Iteration 1625/3560 Training loss: 1.3932 3.1389 sec/batch\n",
      "Epoch 10/20  Iteration 1626/3560 Training loss: 1.3928 3.1548 sec/batch\n",
      "Epoch 10/20  Iteration 1627/3560 Training loss: 1.3927 3.2063 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1628/3560 Training loss: 1.3911 3.2488 sec/batch\n",
      "Epoch 10/20  Iteration 1629/3560 Training loss: 1.3899 3.1637 sec/batch\n",
      "Epoch 10/20  Iteration 1630/3560 Training loss: 1.3901 3.1670 sec/batch\n",
      "Epoch 10/20  Iteration 1631/3560 Training loss: 1.3902 3.1900 sec/batch\n",
      "Epoch 10/20  Iteration 1632/3560 Training loss: 1.3902 3.1408 sec/batch\n",
      "Epoch 10/20  Iteration 1633/3560 Training loss: 1.3898 3.1469 sec/batch\n",
      "Epoch 10/20  Iteration 1634/3560 Training loss: 1.3886 3.1239 sec/batch\n",
      "Epoch 10/20  Iteration 1635/3560 Training loss: 1.3887 3.2612 sec/batch\n",
      "Epoch 10/20  Iteration 1636/3560 Training loss: 1.3887 3.1264 sec/batch\n",
      "Epoch 10/20  Iteration 1637/3560 Training loss: 1.3881 3.1312 sec/batch\n",
      "Epoch 10/20  Iteration 1638/3560 Training loss: 1.3877 3.2258 sec/batch\n",
      "Epoch 10/20  Iteration 1639/3560 Training loss: 1.3869 3.2651 sec/batch\n",
      "Epoch 10/20  Iteration 1640/3560 Training loss: 1.3857 3.1132 sec/batch\n",
      "Epoch 10/20  Iteration 1641/3560 Training loss: 1.3843 3.2492 sec/batch\n",
      "Epoch 10/20  Iteration 1642/3560 Training loss: 1.3838 3.1314 sec/batch\n",
      "Epoch 10/20  Iteration 1643/3560 Training loss: 1.3832 3.1821 sec/batch\n",
      "Epoch 10/20  Iteration 1644/3560 Training loss: 1.3836 3.2278 sec/batch\n",
      "Epoch 10/20  Iteration 1645/3560 Training loss: 1.3830 3.0939 sec/batch\n",
      "Epoch 10/20  Iteration 1646/3560 Training loss: 1.3823 3.2470 sec/batch\n",
      "Epoch 10/20  Iteration 1647/3560 Training loss: 1.3823 3.1704 sec/batch\n",
      "Epoch 10/20  Iteration 1648/3560 Training loss: 1.3815 3.0995 sec/batch\n",
      "Epoch 10/20  Iteration 1649/3560 Training loss: 1.3811 3.3318 sec/batch\n",
      "Epoch 10/20  Iteration 1650/3560 Training loss: 1.3806 3.1546 sec/batch\n",
      "Epoch 10/20  Iteration 1651/3560 Training loss: 1.3804 3.1139 sec/batch\n",
      "Epoch 10/20  Iteration 1652/3560 Training loss: 1.3806 3.2967 sec/batch\n",
      "Epoch 10/20  Iteration 1653/3560 Training loss: 1.3799 3.2655 sec/batch\n",
      "Epoch 10/20  Iteration 1654/3560 Training loss: 1.3808 3.1274 sec/batch\n",
      "Epoch 10/20  Iteration 1655/3560 Training loss: 1.3807 3.2295 sec/batch\n",
      "Epoch 10/20  Iteration 1656/3560 Training loss: 1.3807 3.1727 sec/batch\n",
      "Epoch 10/20  Iteration 1657/3560 Training loss: 1.3802 3.1835 sec/batch\n",
      "Epoch 10/20  Iteration 1658/3560 Training loss: 1.3801 3.2233 sec/batch\n",
      "Epoch 10/20  Iteration 1659/3560 Training loss: 1.3803 3.2185 sec/batch\n",
      "Epoch 10/20  Iteration 1660/3560 Training loss: 1.3800 3.1723 sec/batch\n",
      "Epoch 10/20  Iteration 1661/3560 Training loss: 1.3792 3.0841 sec/batch\n",
      "Epoch 10/20  Iteration 1662/3560 Training loss: 1.3795 3.2195 sec/batch\n",
      "Epoch 10/20  Iteration 1663/3560 Training loss: 1.3794 3.1923 sec/batch\n",
      "Epoch 10/20  Iteration 1664/3560 Training loss: 1.3802 3.1641 sec/batch\n",
      "Epoch 10/20  Iteration 1665/3560 Training loss: 1.3806 3.1643 sec/batch\n",
      "Epoch 10/20  Iteration 1666/3560 Training loss: 1.3806 3.2762 sec/batch\n",
      "Epoch 10/20  Iteration 1667/3560 Training loss: 1.3803 3.1224 sec/batch\n",
      "Epoch 10/20  Iteration 1668/3560 Training loss: 1.3804 3.1034 sec/batch\n",
      "Epoch 10/20  Iteration 1669/3560 Training loss: 1.3805 3.5155 sec/batch\n",
      "Epoch 10/20  Iteration 1670/3560 Training loss: 1.3802 3.1655 sec/batch\n",
      "Epoch 10/20  Iteration 1671/3560 Training loss: 1.3801 3.2060 sec/batch\n",
      "Epoch 10/20  Iteration 1672/3560 Training loss: 1.3800 3.2673 sec/batch\n",
      "Epoch 10/20  Iteration 1673/3560 Training loss: 1.3804 3.1312 sec/batch\n",
      "Epoch 10/20  Iteration 1674/3560 Training loss: 1.3805 3.2022 sec/batch\n",
      "Epoch 10/20  Iteration 1675/3560 Training loss: 1.3809 3.1904 sec/batch\n",
      "Epoch 10/20  Iteration 1676/3560 Training loss: 1.3805 3.1776 sec/batch\n",
      "Epoch 10/20  Iteration 1677/3560 Training loss: 1.3803 3.2584 sec/batch\n",
      "Epoch 10/20  Iteration 1678/3560 Training loss: 1.3802 3.1634 sec/batch\n",
      "Epoch 10/20  Iteration 1679/3560 Training loss: 1.3799 3.1122 sec/batch\n",
      "Epoch 10/20  Iteration 1680/3560 Training loss: 1.3797 3.1792 sec/batch\n",
      "Epoch 10/20  Iteration 1681/3560 Training loss: 1.3791 3.2082 sec/batch\n",
      "Epoch 10/20  Iteration 1682/3560 Training loss: 1.3790 3.2064 sec/batch\n",
      "Epoch 10/20  Iteration 1683/3560 Training loss: 1.3784 3.1560 sec/batch\n",
      "Epoch 10/20  Iteration 1684/3560 Training loss: 1.3783 3.2137 sec/batch\n",
      "Epoch 10/20  Iteration 1685/3560 Training loss: 1.3778 3.1544 sec/batch\n",
      "Epoch 10/20  Iteration 1686/3560 Training loss: 1.3777 3.1931 sec/batch\n",
      "Epoch 10/20  Iteration 1687/3560 Training loss: 1.3774 3.2902 sec/batch\n",
      "Epoch 10/20  Iteration 1688/3560 Training loss: 1.3772 3.1879 sec/batch\n",
      "Epoch 10/20  Iteration 1689/3560 Training loss: 1.3768 3.1319 sec/batch\n",
      "Epoch 10/20  Iteration 1690/3560 Training loss: 1.3764 3.1837 sec/batch\n",
      "Epoch 10/20  Iteration 1691/3560 Training loss: 1.3759 3.3064 sec/batch\n",
      "Epoch 10/20  Iteration 1692/3560 Training loss: 1.3759 3.1945 sec/batch\n",
      "Epoch 10/20  Iteration 1693/3560 Training loss: 1.3755 3.1710 sec/batch\n",
      "Epoch 10/20  Iteration 1694/3560 Training loss: 1.3753 3.2869 sec/batch\n",
      "Epoch 10/20  Iteration 1695/3560 Training loss: 1.3749 3.1327 sec/batch\n",
      "Epoch 10/20  Iteration 1696/3560 Training loss: 1.3746 3.1762 sec/batch\n",
      "Epoch 10/20  Iteration 1697/3560 Training loss: 1.3743 3.3157 sec/batch\n",
      "Epoch 10/20  Iteration 1698/3560 Training loss: 1.3743 3.1537 sec/batch\n",
      "Epoch 10/20  Iteration 1699/3560 Training loss: 1.3742 3.0889 sec/batch\n",
      "Epoch 10/20  Iteration 1700/3560 Training loss: 1.3738 3.2228 sec/batch\n",
      "Epoch 10/20  Iteration 1701/3560 Training loss: 1.3735 3.1658 sec/batch\n",
      "Epoch 10/20  Iteration 1702/3560 Training loss: 1.3731 3.1284 sec/batch\n",
      "Epoch 10/20  Iteration 1703/3560 Training loss: 1.3731 3.3146 sec/batch\n",
      "Epoch 10/20  Iteration 1704/3560 Training loss: 1.3729 3.1243 sec/batch\n",
      "Epoch 10/20  Iteration 1705/3560 Training loss: 1.3728 3.1272 sec/batch\n",
      "Epoch 10/20  Iteration 1706/3560 Training loss: 1.3726 3.1757 sec/batch\n",
      "Epoch 10/20  Iteration 1707/3560 Training loss: 1.3723 3.1469 sec/batch\n",
      "Epoch 10/20  Iteration 1708/3560 Training loss: 1.3721 3.2090 sec/batch\n",
      "Epoch 10/20  Iteration 1709/3560 Training loss: 1.3721 3.1052 sec/batch\n",
      "Epoch 10/20  Iteration 1710/3560 Training loss: 1.3720 3.2235 sec/batch\n",
      "Epoch 10/20  Iteration 1711/3560 Training loss: 1.3717 3.1890 sec/batch\n",
      "Epoch 10/20  Iteration 1712/3560 Training loss: 1.3717 3.2030 sec/batch\n",
      "Epoch 10/20  Iteration 1713/3560 Training loss: 1.3714 3.1771 sec/batch\n",
      "Epoch 10/20  Iteration 1714/3560 Training loss: 1.3712 3.2242 sec/batch\n",
      "Epoch 10/20  Iteration 1715/3560 Training loss: 1.3711 3.0821 sec/batch\n",
      "Epoch 10/20  Iteration 1716/3560 Training loss: 1.3709 3.2426 sec/batch\n",
      "Epoch 10/20  Iteration 1717/3560 Training loss: 1.3707 3.1403 sec/batch\n",
      "Epoch 10/20  Iteration 1718/3560 Training loss: 1.3703 3.1783 sec/batch\n",
      "Epoch 10/20  Iteration 1719/3560 Training loss: 1.3702 3.2158 sec/batch\n",
      "Epoch 10/20  Iteration 1720/3560 Training loss: 1.3701 3.1507 sec/batch\n",
      "Epoch 10/20  Iteration 1721/3560 Training loss: 1.3698 3.1592 sec/batch\n",
      "Epoch 10/20  Iteration 1722/3560 Training loss: 1.3697 3.2972 sec/batch\n",
      "Epoch 10/20  Iteration 1723/3560 Training loss: 1.3695 3.1271 sec/batch\n",
      "Epoch 10/20  Iteration 1724/3560 Training loss: 1.3691 3.1759 sec/batch\n",
      "Epoch 10/20  Iteration 1725/3560 Training loss: 1.3685 3.4673 sec/batch\n",
      "Epoch 10/20  Iteration 1726/3560 Training loss: 1.3684 3.2203 sec/batch\n",
      "Epoch 10/20  Iteration 1727/3560 Training loss: 1.3683 3.0792 sec/batch\n",
      "Epoch 10/20  Iteration 1728/3560 Training loss: 1.3679 3.2138 sec/batch\n",
      "Epoch 10/20  Iteration 1729/3560 Training loss: 1.3679 3.2965 sec/batch\n",
      "Epoch 10/20  Iteration 1730/3560 Training loss: 1.3677 3.0867 sec/batch\n",
      "Epoch 10/20  Iteration 1731/3560 Training loss: 1.3675 3.2415 sec/batch\n",
      "Epoch 10/20  Iteration 1732/3560 Training loss: 1.3671 3.1774 sec/batch\n",
      "Epoch 10/20  Iteration 1733/3560 Training loss: 1.3667 3.1456 sec/batch\n",
      "Epoch 10/20  Iteration 1734/3560 Training loss: 1.3664 3.0705 sec/batch\n",
      "Epoch 10/20  Iteration 1735/3560 Training loss: 1.3665 3.1972 sec/batch\n",
      "Epoch 10/20  Iteration 1736/3560 Training loss: 1.3665 3.2004 sec/batch\n",
      "Epoch 10/20  Iteration 1737/3560 Training loss: 1.3664 3.1315 sec/batch\n",
      "Epoch 10/20  Iteration 1738/3560 Training loss: 1.3665 3.3132 sec/batch\n",
      "Epoch 10/20  Iteration 1739/3560 Training loss: 1.3666 3.1137 sec/batch\n",
      "Epoch 10/20  Iteration 1740/3560 Training loss: 1.3667 3.1851 sec/batch\n",
      "Epoch 10/20  Iteration 1741/3560 Training loss: 1.3667 3.3118 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Iteration 1742/3560 Training loss: 1.3666 3.2157 sec/batch\n",
      "Epoch 10/20  Iteration 1743/3560 Training loss: 1.3669 3.1076 sec/batch\n",
      "Epoch 10/20  Iteration 1744/3560 Training loss: 1.3669 3.1215 sec/batch\n",
      "Epoch 10/20  Iteration 1745/3560 Training loss: 1.3669 3.2996 sec/batch\n",
      "Epoch 10/20  Iteration 1746/3560 Training loss: 1.3671 3.1007 sec/batch\n",
      "Epoch 10/20  Iteration 1747/3560 Training loss: 1.3669 3.1424 sec/batch\n",
      "Epoch 10/20  Iteration 1748/3560 Training loss: 1.3670 3.2283 sec/batch\n",
      "Epoch 10/20  Iteration 1749/3560 Training loss: 1.3670 3.0945 sec/batch\n",
      "Epoch 10/20  Iteration 1750/3560 Training loss: 1.3672 3.2279 sec/batch\n",
      "Epoch 10/20  Iteration 1751/3560 Training loss: 1.3672 3.2670 sec/batch\n",
      "Epoch 10/20  Iteration 1752/3560 Training loss: 1.3671 3.1565 sec/batch\n",
      "Epoch 10/20  Iteration 1753/3560 Training loss: 1.3668 3.2192 sec/batch\n",
      "Epoch 10/20  Iteration 1754/3560 Training loss: 1.3666 3.2100 sec/batch\n",
      "Epoch 10/20  Iteration 1755/3560 Training loss: 1.3666 3.0883 sec/batch\n",
      "Epoch 10/20  Iteration 1756/3560 Training loss: 1.3666 3.1879 sec/batch\n",
      "Epoch 10/20  Iteration 1757/3560 Training loss: 1.3665 3.2220 sec/batch\n",
      "Epoch 10/20  Iteration 1758/3560 Training loss: 1.3665 3.0707 sec/batch\n",
      "Epoch 10/20  Iteration 1759/3560 Training loss: 1.3665 3.1839 sec/batch\n",
      "Epoch 10/20  Iteration 1760/3560 Training loss: 1.3664 3.1905 sec/batch\n",
      "Epoch 10/20  Iteration 1761/3560 Training loss: 1.3661 3.1739 sec/batch\n",
      "Epoch 10/20  Iteration 1762/3560 Training loss: 1.3662 3.1991 sec/batch\n",
      "Epoch 10/20  Iteration 1763/3560 Training loss: 1.3663 3.1429 sec/batch\n",
      "Epoch 10/20  Iteration 1764/3560 Training loss: 1.3662 3.1360 sec/batch\n",
      "Epoch 10/20  Iteration 1765/3560 Training loss: 1.3662 3.1298 sec/batch\n",
      "Epoch 10/20  Iteration 1766/3560 Training loss: 1.3661 3.2889 sec/batch\n",
      "Epoch 10/20  Iteration 1767/3560 Training loss: 1.3660 3.2756 sec/batch\n",
      "Epoch 10/20  Iteration 1768/3560 Training loss: 1.3659 3.0802 sec/batch\n",
      "Epoch 10/20  Iteration 1769/3560 Training loss: 1.3660 3.2274 sec/batch\n",
      "Epoch 10/20  Iteration 1770/3560 Training loss: 1.3664 3.2954 sec/batch\n",
      "Epoch 10/20  Iteration 1771/3560 Training loss: 1.3664 3.5079 sec/batch\n",
      "Epoch 10/20  Iteration 1772/3560 Training loss: 1.3663 3.3990 sec/batch\n",
      "Epoch 10/20  Iteration 1773/3560 Training loss: 1.3662 3.2974 sec/batch\n",
      "Epoch 10/20  Iteration 1774/3560 Training loss: 1.3660 3.1245 sec/batch\n",
      "Epoch 10/20  Iteration 1775/3560 Training loss: 1.3661 3.1333 sec/batch\n",
      "Epoch 10/20  Iteration 1776/3560 Training loss: 1.3661 3.2474 sec/batch\n",
      "Epoch 10/20  Iteration 1777/3560 Training loss: 1.3661 3.1621 sec/batch\n",
      "Epoch 10/20  Iteration 1778/3560 Training loss: 1.3660 3.1029 sec/batch\n",
      "Epoch 10/20  Iteration 1779/3560 Training loss: 1.3658 3.3499 sec/batch\n",
      "Epoch 10/20  Iteration 1780/3560 Training loss: 1.3659 3.1760 sec/batch\n",
      "Epoch 11/20  Iteration 1781/3560 Training loss: 1.4795 3.1342 sec/batch\n",
      "Epoch 11/20  Iteration 1782/3560 Training loss: 1.4219 3.3839 sec/batch\n",
      "Epoch 11/20  Iteration 1783/3560 Training loss: 1.4023 3.2191 sec/batch\n",
      "Epoch 11/20  Iteration 1784/3560 Training loss: 1.3963 3.2273 sec/batch\n",
      "Epoch 11/20  Iteration 1785/3560 Training loss: 1.3858 3.2781 sec/batch\n",
      "Epoch 11/20  Iteration 1786/3560 Training loss: 1.3753 3.1792 sec/batch\n",
      "Epoch 11/20  Iteration 1787/3560 Training loss: 1.3742 3.1971 sec/batch\n",
      "Epoch 11/20  Iteration 1788/3560 Training loss: 1.3716 3.1984 sec/batch\n",
      "Epoch 11/20  Iteration 1789/3560 Training loss: 1.3697 3.2869 sec/batch\n",
      "Epoch 11/20  Iteration 1790/3560 Training loss: 1.3678 3.1702 sec/batch\n",
      "Epoch 11/20  Iteration 1791/3560 Training loss: 1.3640 3.1282 sec/batch\n",
      "Epoch 11/20  Iteration 1792/3560 Training loss: 1.3625 3.2560 sec/batch\n",
      "Epoch 11/20  Iteration 1793/3560 Training loss: 1.3616 3.2855 sec/batch\n",
      "Epoch 11/20  Iteration 1794/3560 Training loss: 1.3627 3.1904 sec/batch\n",
      "Epoch 11/20  Iteration 1795/3560 Training loss: 1.3612 3.1944 sec/batch\n",
      "Epoch 11/20  Iteration 1796/3560 Training loss: 1.3592 3.1802 sec/batch\n",
      "Epoch 11/20  Iteration 1797/3560 Training loss: 1.3592 3.1270 sec/batch\n",
      "Epoch 11/20  Iteration 1798/3560 Training loss: 1.3598 3.3269 sec/batch\n",
      "Epoch 11/20  Iteration 1799/3560 Training loss: 1.3599 3.1678 sec/batch\n",
      "Epoch 11/20  Iteration 1800/3560 Training loss: 1.3608 3.1753 sec/batch\n",
      "Epoch 11/20  Iteration 1801/3560 Training loss: 1.3602 3.2045 sec/batch\n",
      "Epoch 11/20  Iteration 1802/3560 Training loss: 1.3602 3.2171 sec/batch\n",
      "Epoch 11/20  Iteration 1803/3560 Training loss: 1.3591 3.1297 sec/batch\n",
      "Epoch 11/20  Iteration 1804/3560 Training loss: 1.3590 3.2732 sec/batch\n",
      "Epoch 11/20  Iteration 1805/3560 Training loss: 1.3588 3.1180 sec/batch\n",
      "Epoch 11/20  Iteration 1806/3560 Training loss: 1.3570 3.1398 sec/batch\n",
      "Epoch 11/20  Iteration 1807/3560 Training loss: 1.3556 3.3709 sec/batch\n",
      "Epoch 11/20  Iteration 1808/3560 Training loss: 1.3560 3.2871 sec/batch\n",
      "Epoch 11/20  Iteration 1809/3560 Training loss: 1.3561 3.2054 sec/batch\n",
      "Epoch 11/20  Iteration 1810/3560 Training loss: 1.3563 3.2341 sec/batch\n",
      "Epoch 11/20  Iteration 1811/3560 Training loss: 1.3557 3.2320 sec/batch\n",
      "Epoch 11/20  Iteration 1812/3560 Training loss: 1.3547 3.1383 sec/batch\n",
      "Epoch 11/20  Iteration 1813/3560 Training loss: 1.3546 3.1990 sec/batch\n",
      "Epoch 11/20  Iteration 1814/3560 Training loss: 1.3547 3.1907 sec/batch\n",
      "Epoch 11/20  Iteration 1815/3560 Training loss: 1.3545 3.1370 sec/batch\n",
      "Epoch 11/20  Iteration 1816/3560 Training loss: 1.3542 3.1992 sec/batch\n",
      "Epoch 11/20  Iteration 1817/3560 Training loss: 1.3535 3.2013 sec/batch\n",
      "Epoch 11/20  Iteration 1818/3560 Training loss: 1.3523 3.2601 sec/batch\n",
      "Epoch 11/20  Iteration 1819/3560 Training loss: 1.3509 3.1337 sec/batch\n",
      "Epoch 11/20  Iteration 1820/3560 Training loss: 1.3505 3.1856 sec/batch\n",
      "Epoch 11/20  Iteration 1821/3560 Training loss: 1.3499 3.1787 sec/batch\n",
      "Epoch 11/20  Iteration 1822/3560 Training loss: 1.3506 3.0974 sec/batch\n",
      "Epoch 11/20  Iteration 1823/3560 Training loss: 1.3501 3.1843 sec/batch\n",
      "Epoch 11/20  Iteration 1824/3560 Training loss: 1.3494 3.2697 sec/batch\n",
      "Epoch 11/20  Iteration 1825/3560 Training loss: 1.3495 3.1700 sec/batch\n",
      "Epoch 11/20  Iteration 1826/3560 Training loss: 1.3487 3.1943 sec/batch\n",
      "Epoch 11/20  Iteration 1827/3560 Training loss: 1.3481 3.2844 sec/batch\n",
      "Epoch 11/20  Iteration 1828/3560 Training loss: 1.3477 3.1027 sec/batch\n",
      "Epoch 11/20  Iteration 1829/3560 Training loss: 1.3475 3.3154 sec/batch\n",
      "Epoch 11/20  Iteration 1830/3560 Training loss: 1.3476 3.2095 sec/batch\n",
      "Epoch 11/20  Iteration 1831/3560 Training loss: 1.3473 3.1287 sec/batch\n",
      "Epoch 11/20  Iteration 1832/3560 Training loss: 1.3481 3.2234 sec/batch\n",
      "Epoch 11/20  Iteration 1833/3560 Training loss: 1.3480 3.2414 sec/batch\n",
      "Epoch 11/20  Iteration 1834/3560 Training loss: 1.3481 3.1641 sec/batch\n",
      "Epoch 11/20  Iteration 1835/3560 Training loss: 1.3478 3.2030 sec/batch\n",
      "Epoch 11/20  Iteration 1836/3560 Training loss: 1.3476 3.0985 sec/batch\n",
      "Epoch 11/20  Iteration 1837/3560 Training loss: 1.3480 3.2144 sec/batch\n",
      "Epoch 11/20  Iteration 1838/3560 Training loss: 1.3476 3.4409 sec/batch\n",
      "Epoch 11/20  Iteration 1839/3560 Training loss: 1.3470 3.1647 sec/batch\n",
      "Epoch 11/20  Iteration 1840/3560 Training loss: 1.3475 3.2124 sec/batch\n",
      "Epoch 11/20  Iteration 1841/3560 Training loss: 1.3475 3.2576 sec/batch\n",
      "Epoch 11/20  Iteration 1842/3560 Training loss: 1.3483 3.0916 sec/batch\n",
      "Epoch 11/20  Iteration 1843/3560 Training loss: 1.3485 3.2050 sec/batch\n",
      "Epoch 11/20  Iteration 1844/3560 Training loss: 1.3486 3.2281 sec/batch\n",
      "Epoch 11/20  Iteration 1845/3560 Training loss: 1.3486 3.0973 sec/batch\n",
      "Epoch 11/20  Iteration 1846/3560 Training loss: 1.3487 3.2533 sec/batch\n",
      "Epoch 11/20  Iteration 1847/3560 Training loss: 1.3488 3.1891 sec/batch\n",
      "Epoch 11/20  Iteration 1848/3560 Training loss: 1.3486 3.1344 sec/batch\n",
      "Epoch 11/20  Iteration 1849/3560 Training loss: 1.3486 3.2656 sec/batch\n",
      "Epoch 11/20  Iteration 1850/3560 Training loss: 1.3484 3.1396 sec/batch\n",
      "Epoch 11/20  Iteration 1851/3560 Training loss: 1.3489 3.1647 sec/batch\n",
      "Epoch 11/20  Iteration 1852/3560 Training loss: 1.3492 3.2206 sec/batch\n",
      "Epoch 11/20  Iteration 1853/3560 Training loss: 1.3497 3.0804 sec/batch\n",
      "Epoch 11/20  Iteration 1854/3560 Training loss: 1.3494 3.1637 sec/batch\n",
      "Epoch 11/20  Iteration 1855/3560 Training loss: 1.3492 3.1793 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Iteration 1856/3560 Training loss: 1.3494 3.4602 sec/batch\n",
      "Epoch 11/20  Iteration 1857/3560 Training loss: 1.3492 3.1397 sec/batch\n",
      "Epoch 11/20  Iteration 1858/3560 Training loss: 1.3490 3.1327 sec/batch\n",
      "Epoch 11/20  Iteration 1859/3560 Training loss: 1.3484 3.1862 sec/batch\n",
      "Epoch 11/20  Iteration 1860/3560 Training loss: 1.3482 3.1313 sec/batch\n",
      "Epoch 11/20  Iteration 1861/3560 Training loss: 1.3477 3.1982 sec/batch\n",
      "Epoch 11/20  Iteration 1862/3560 Training loss: 1.3477 3.1765 sec/batch\n",
      "Epoch 11/20  Iteration 1863/3560 Training loss: 1.3472 3.1910 sec/batch\n",
      "Epoch 11/20  Iteration 1864/3560 Training loss: 1.3471 3.1454 sec/batch\n",
      "Epoch 11/20  Iteration 1865/3560 Training loss: 1.3469 3.3277 sec/batch\n",
      "Epoch 11/20  Iteration 1866/3560 Training loss: 1.3467 3.2185 sec/batch\n",
      "Epoch 11/20  Iteration 1867/3560 Training loss: 1.3463 3.1407 sec/batch\n",
      "Epoch 11/20  Iteration 1868/3560 Training loss: 1.3461 3.2367 sec/batch\n",
      "Epoch 11/20  Iteration 1869/3560 Training loss: 1.3457 3.2018 sec/batch\n",
      "Epoch 11/20  Iteration 1870/3560 Training loss: 1.3458 3.1322 sec/batch\n",
      "Epoch 11/20  Iteration 1871/3560 Training loss: 1.3454 3.2240 sec/batch\n",
      "Epoch 11/20  Iteration 1872/3560 Training loss: 1.3454 3.3134 sec/batch\n",
      "Epoch 11/20  Iteration 1873/3560 Training loss: 1.3450 3.1448 sec/batch\n",
      "Epoch 11/20  Iteration 1874/3560 Training loss: 1.3446 3.2216 sec/batch\n",
      "Epoch 11/20  Iteration 1875/3560 Training loss: 1.3441 3.1298 sec/batch\n",
      "Epoch 11/20  Iteration 1876/3560 Training loss: 1.3440 3.1987 sec/batch\n",
      "Epoch 11/20  Iteration 1877/3560 Training loss: 1.3441 3.1553 sec/batch\n",
      "Epoch 11/20  Iteration 1878/3560 Training loss: 1.3435 3.4415 sec/batch\n",
      "Epoch 11/20  Iteration 1879/3560 Training loss: 1.3431 3.2815 sec/batch\n",
      "Epoch 11/20  Iteration 1880/3560 Training loss: 1.3427 3.2463 sec/batch\n",
      "Epoch 11/20  Iteration 1881/3560 Training loss: 1.3427 3.2165 sec/batch\n",
      "Epoch 11/20  Iteration 1882/3560 Training loss: 1.3424 3.2295 sec/batch\n",
      "Epoch 11/20  Iteration 1883/3560 Training loss: 1.3422 3.1756 sec/batch\n",
      "Epoch 11/20  Iteration 1884/3560 Training loss: 1.3420 3.2446 sec/batch\n",
      "Epoch 11/20  Iteration 1885/3560 Training loss: 1.3419 3.1703 sec/batch\n",
      "Epoch 11/20  Iteration 1886/3560 Training loss: 1.3417 3.1626 sec/batch\n",
      "Epoch 11/20  Iteration 1887/3560 Training loss: 1.3416 3.1580 sec/batch\n",
      "Epoch 11/20  Iteration 1888/3560 Training loss: 1.3415 3.2042 sec/batch\n",
      "Epoch 11/20  Iteration 1889/3560 Training loss: 1.3412 3.1763 sec/batch\n",
      "Epoch 11/20  Iteration 1890/3560 Training loss: 1.3412 3.1511 sec/batch\n",
      "Epoch 11/20  Iteration 1891/3560 Training loss: 1.3410 3.1523 sec/batch\n",
      "Epoch 11/20  Iteration 1892/3560 Training loss: 1.3409 3.0804 sec/batch\n",
      "Epoch 11/20  Iteration 1893/3560 Training loss: 1.3407 3.2131 sec/batch\n",
      "Epoch 11/20  Iteration 1894/3560 Training loss: 1.3405 3.4462 sec/batch\n",
      "Epoch 11/20  Iteration 1895/3560 Training loss: 1.3402 3.1081 sec/batch\n",
      "Epoch 11/20  Iteration 1896/3560 Training loss: 1.3397 3.2135 sec/batch\n",
      "Epoch 11/20  Iteration 1897/3560 Training loss: 1.3397 3.1574 sec/batch\n",
      "Epoch 11/20  Iteration 1898/3560 Training loss: 1.3396 3.1349 sec/batch\n",
      "Epoch 11/20  Iteration 1899/3560 Training loss: 1.3394 3.1852 sec/batch\n",
      "Epoch 11/20  Iteration 1900/3560 Training loss: 1.3393 3.2501 sec/batch\n",
      "Epoch 11/20  Iteration 1901/3560 Training loss: 1.3390 3.2055 sec/batch\n",
      "Epoch 11/20  Iteration 1902/3560 Training loss: 1.3386 3.1772 sec/batch\n",
      "Epoch 11/20  Iteration 1903/3560 Training loss: 1.3381 3.3111 sec/batch\n",
      "Epoch 11/20  Iteration 1904/3560 Training loss: 1.3380 3.2244 sec/batch\n",
      "Epoch 11/20  Iteration 1905/3560 Training loss: 1.3379 3.1349 sec/batch\n",
      "Epoch 11/20  Iteration 1906/3560 Training loss: 1.3374 3.2263 sec/batch\n",
      "Epoch 11/20  Iteration 1907/3560 Training loss: 1.3375 3.1404 sec/batch\n",
      "Epoch 11/20  Iteration 1908/3560 Training loss: 1.3373 3.1724 sec/batch\n",
      "Epoch 11/20  Iteration 1909/3560 Training loss: 1.3371 3.1139 sec/batch\n",
      "Epoch 11/20  Iteration 1910/3560 Training loss: 1.3368 3.2107 sec/batch\n",
      "Epoch 11/20  Iteration 1911/3560 Training loss: 1.3364 3.2571 sec/batch\n",
      "Epoch 11/20  Iteration 1912/3560 Training loss: 1.3362 3.1253 sec/batch\n",
      "Epoch 11/20  Iteration 1913/3560 Training loss: 1.3363 3.2343 sec/batch\n",
      "Epoch 11/20  Iteration 1914/3560 Training loss: 1.3362 3.2168 sec/batch\n",
      "Epoch 11/20  Iteration 1915/3560 Training loss: 1.3362 3.1395 sec/batch\n",
      "Epoch 11/20  Iteration 1916/3560 Training loss: 1.3362 3.1820 sec/batch\n",
      "Epoch 11/20  Iteration 1917/3560 Training loss: 1.3363 3.1612 sec/batch\n",
      "Epoch 11/20  Iteration 1918/3560 Training loss: 1.3363 3.1751 sec/batch\n",
      "Epoch 11/20  Iteration 1919/3560 Training loss: 1.3363 3.2205 sec/batch\n",
      "Epoch 11/20  Iteration 1920/3560 Training loss: 1.3363 3.1282 sec/batch\n",
      "Epoch 11/20  Iteration 1921/3560 Training loss: 1.3366 3.1045 sec/batch\n",
      "Epoch 11/20  Iteration 1922/3560 Training loss: 1.3366 3.2621 sec/batch\n",
      "Epoch 11/20  Iteration 1923/3560 Training loss: 1.3365 3.1117 sec/batch\n",
      "Epoch 11/20  Iteration 1924/3560 Training loss: 1.3368 3.1372 sec/batch\n",
      "Epoch 11/20  Iteration 1925/3560 Training loss: 1.3367 3.2267 sec/batch\n",
      "Epoch 11/20  Iteration 1926/3560 Training loss: 1.3368 3.2049 sec/batch\n",
      "Epoch 11/20  Iteration 1927/3560 Training loss: 1.3368 3.2328 sec/batch\n",
      "Epoch 11/20  Iteration 1928/3560 Training loss: 1.3370 3.1033 sec/batch\n",
      "Epoch 11/20  Iteration 1929/3560 Training loss: 1.3371 3.2338 sec/batch\n",
      "Epoch 11/20  Iteration 1930/3560 Training loss: 1.3368 3.0995 sec/batch\n",
      "Epoch 11/20  Iteration 1931/3560 Training loss: 1.3365 3.1662 sec/batch\n",
      "Epoch 11/20  Iteration 1932/3560 Training loss: 1.3363 3.2383 sec/batch\n",
      "Epoch 11/20  Iteration 1933/3560 Training loss: 1.3364 3.1871 sec/batch\n",
      "Epoch 11/20  Iteration 1934/3560 Training loss: 1.3362 3.1372 sec/batch\n",
      "Epoch 11/20  Iteration 1935/3560 Training loss: 1.3362 3.2372 sec/batch\n",
      "Epoch 11/20  Iteration 1936/3560 Training loss: 1.3361 3.1574 sec/batch\n",
      "Epoch 11/20  Iteration 1937/3560 Training loss: 1.3361 3.1500 sec/batch\n",
      "Epoch 11/20  Iteration 1938/3560 Training loss: 1.3360 3.3322 sec/batch\n",
      "Epoch 11/20  Iteration 1939/3560 Training loss: 1.3357 3.1294 sec/batch\n",
      "Epoch 11/20  Iteration 1940/3560 Training loss: 1.3358 3.0951 sec/batch\n",
      "Epoch 11/20  Iteration 1941/3560 Training loss: 1.3360 3.1767 sec/batch\n",
      "Epoch 11/20  Iteration 1942/3560 Training loss: 1.3360 3.1794 sec/batch\n",
      "Epoch 11/20  Iteration 1943/3560 Training loss: 1.3359 3.1743 sec/batch\n",
      "Epoch 11/20  Iteration 1944/3560 Training loss: 1.3358 3.0813 sec/batch\n",
      "Epoch 11/20  Iteration 1945/3560 Training loss: 1.3358 3.2764 sec/batch\n",
      "Epoch 11/20  Iteration 1946/3560 Training loss: 1.3357 3.0789 sec/batch\n",
      "Epoch 11/20  Iteration 1947/3560 Training loss: 1.3358 3.1087 sec/batch\n",
      "Epoch 11/20  Iteration 1948/3560 Training loss: 1.3362 3.2433 sec/batch\n",
      "Epoch 11/20  Iteration 1949/3560 Training loss: 1.3361 3.3429 sec/batch\n",
      "Epoch 11/20  Iteration 1950/3560 Training loss: 1.3361 3.4845 sec/batch\n",
      "Epoch 11/20  Iteration 1951/3560 Training loss: 1.3360 3.2747 sec/batch\n",
      "Epoch 11/20  Iteration 1952/3560 Training loss: 1.3358 3.1249 sec/batch\n",
      "Epoch 11/20  Iteration 1953/3560 Training loss: 1.3359 3.1965 sec/batch\n",
      "Epoch 11/20  Iteration 1954/3560 Training loss: 1.3359 3.1625 sec/batch\n",
      "Epoch 11/20  Iteration 1955/3560 Training loss: 1.3359 3.1292 sec/batch\n",
      "Epoch 11/20  Iteration 1956/3560 Training loss: 1.3357 3.1042 sec/batch\n",
      "Epoch 11/20  Iteration 1957/3560 Training loss: 1.3355 3.1800 sec/batch\n",
      "Epoch 11/20  Iteration 1958/3560 Training loss: 1.3356 3.1578 sec/batch\n",
      "Epoch 12/20  Iteration 1959/3560 Training loss: 1.4513 3.2096 sec/batch\n",
      "Epoch 12/20  Iteration 1960/3560 Training loss: 1.3977 3.1734 sec/batch\n",
      "Epoch 12/20  Iteration 1961/3560 Training loss: 1.3745 3.1727 sec/batch\n",
      "Epoch 12/20  Iteration 1962/3560 Training loss: 1.3690 3.2817 sec/batch\n",
      "Epoch 12/20  Iteration 1963/3560 Training loss: 1.3566 3.0701 sec/batch\n",
      "Epoch 12/20  Iteration 1964/3560 Training loss: 1.3435 3.2342 sec/batch\n",
      "Epoch 12/20  Iteration 1965/3560 Training loss: 1.3442 3.1251 sec/batch\n",
      "Epoch 12/20  Iteration 1966/3560 Training loss: 1.3406 3.1845 sec/batch\n",
      "Epoch 12/20  Iteration 1967/3560 Training loss: 1.3395 3.2108 sec/batch\n",
      "Epoch 12/20  Iteration 1968/3560 Training loss: 1.3374 3.1093 sec/batch\n",
      "Epoch 12/20  Iteration 1969/3560 Training loss: 1.3333 3.1716 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 1970/3560 Training loss: 1.3310 3.2201 sec/batch\n",
      "Epoch 12/20  Iteration 1971/3560 Training loss: 1.3300 3.0789 sec/batch\n",
      "Epoch 12/20  Iteration 1972/3560 Training loss: 1.3302 3.1925 sec/batch\n",
      "Epoch 12/20  Iteration 1973/3560 Training loss: 1.3285 3.3075 sec/batch\n",
      "Epoch 12/20  Iteration 1974/3560 Training loss: 1.3273 3.1266 sec/batch\n",
      "Epoch 12/20  Iteration 1975/3560 Training loss: 1.3278 3.0808 sec/batch\n",
      "Epoch 12/20  Iteration 1976/3560 Training loss: 1.3288 3.3493 sec/batch\n",
      "Epoch 12/20  Iteration 1977/3560 Training loss: 1.3286 3.1520 sec/batch\n",
      "Epoch 12/20  Iteration 1978/3560 Training loss: 1.3293 3.0608 sec/batch\n",
      "Epoch 12/20  Iteration 1979/3560 Training loss: 1.3286 3.2480 sec/batch\n",
      "Epoch 12/20  Iteration 1980/3560 Training loss: 1.3291 3.2109 sec/batch\n",
      "Epoch 12/20  Iteration 1981/3560 Training loss: 1.3277 3.1413 sec/batch\n",
      "Epoch 12/20  Iteration 1982/3560 Training loss: 1.3278 3.1599 sec/batch\n",
      "Epoch 12/20  Iteration 1983/3560 Training loss: 1.3279 3.2153 sec/batch\n",
      "Epoch 12/20  Iteration 1984/3560 Training loss: 1.3263 3.1129 sec/batch\n",
      "Epoch 12/20  Iteration 1985/3560 Training loss: 1.3252 3.1230 sec/batch\n",
      "Epoch 12/20  Iteration 1986/3560 Training loss: 1.3256 3.3056 sec/batch\n",
      "Epoch 12/20  Iteration 1987/3560 Training loss: 1.3255 3.1272 sec/batch\n",
      "Epoch 12/20  Iteration 1988/3560 Training loss: 1.3257 3.2091 sec/batch\n",
      "Epoch 12/20  Iteration 1989/3560 Training loss: 1.3251 3.2280 sec/batch\n",
      "Epoch 12/20  Iteration 1990/3560 Training loss: 1.3243 3.1615 sec/batch\n",
      "Epoch 12/20  Iteration 1991/3560 Training loss: 1.3246 3.1626 sec/batch\n",
      "Epoch 12/20  Iteration 1992/3560 Training loss: 1.3249 3.3265 sec/batch\n",
      "Epoch 12/20  Iteration 1993/3560 Training loss: 1.3247 3.1347 sec/batch\n",
      "Epoch 12/20  Iteration 1994/3560 Training loss: 1.3245 3.1857 sec/batch\n",
      "Epoch 12/20  Iteration 1995/3560 Training loss: 1.3238 3.2053 sec/batch\n",
      "Epoch 12/20  Iteration 1996/3560 Training loss: 1.3226 3.2393 sec/batch\n",
      "Epoch 12/20  Iteration 1997/3560 Training loss: 1.3215 3.1445 sec/batch\n",
      "Epoch 12/20  Iteration 1998/3560 Training loss: 1.3210 3.1573 sec/batch\n",
      "Epoch 12/20  Iteration 1999/3560 Training loss: 1.3203 3.1930 sec/batch\n",
      "Epoch 12/20  Iteration 2000/3560 Training loss: 1.3210 3.1602 sec/batch\n",
      "Epoch 12/20  Iteration 2001/3560 Training loss: 1.3207 3.2258 sec/batch\n",
      "Epoch 12/20  Iteration 2002/3560 Training loss: 1.3201 3.2128 sec/batch\n",
      "Epoch 12/20  Iteration 2003/3560 Training loss: 1.3199 3.1631 sec/batch\n",
      "Epoch 12/20  Iteration 2004/3560 Training loss: 1.3191 3.1852 sec/batch\n",
      "Epoch 12/20  Iteration 2005/3560 Training loss: 1.3188 3.1982 sec/batch\n",
      "Epoch 12/20  Iteration 2006/3560 Training loss: 1.3182 3.1202 sec/batch\n",
      "Epoch 12/20  Iteration 2007/3560 Training loss: 1.3180 3.4792 sec/batch\n",
      "Epoch 12/20  Iteration 2008/3560 Training loss: 1.3181 3.2301 sec/batch\n",
      "Epoch 12/20  Iteration 2009/3560 Training loss: 1.3175 3.0924 sec/batch\n",
      "Epoch 12/20  Iteration 2010/3560 Training loss: 1.3183 3.1885 sec/batch\n",
      "Epoch 12/20  Iteration 2011/3560 Training loss: 1.3181 3.2475 sec/batch\n",
      "Epoch 12/20  Iteration 2012/3560 Training loss: 1.3183 3.2567 sec/batch\n",
      "Epoch 12/20  Iteration 2013/3560 Training loss: 1.3182 3.1464 sec/batch\n",
      "Epoch 12/20  Iteration 2014/3560 Training loss: 1.3182 3.1770 sec/batch\n",
      "Epoch 12/20  Iteration 2015/3560 Training loss: 1.3185 3.2656 sec/batch\n",
      "Epoch 12/20  Iteration 2016/3560 Training loss: 1.3182 3.0505 sec/batch\n",
      "Epoch 12/20  Iteration 2017/3560 Training loss: 1.3176 3.1992 sec/batch\n",
      "Epoch 12/20  Iteration 2018/3560 Training loss: 1.3181 3.1880 sec/batch\n",
      "Epoch 12/20  Iteration 2019/3560 Training loss: 1.3181 3.1182 sec/batch\n",
      "Epoch 12/20  Iteration 2020/3560 Training loss: 1.3188 3.1898 sec/batch\n",
      "Epoch 12/20  Iteration 2021/3560 Training loss: 1.3191 3.2176 sec/batch\n",
      "Epoch 12/20  Iteration 2022/3560 Training loss: 1.3192 3.2014 sec/batch\n",
      "Epoch 12/20  Iteration 2023/3560 Training loss: 1.3190 3.1772 sec/batch\n",
      "Epoch 12/20  Iteration 2024/3560 Training loss: 1.3191 3.2671 sec/batch\n",
      "Epoch 12/20  Iteration 2025/3560 Training loss: 1.3192 3.1390 sec/batch\n",
      "Epoch 12/20  Iteration 2026/3560 Training loss: 1.3189 3.1630 sec/batch\n",
      "Epoch 12/20  Iteration 2027/3560 Training loss: 1.3190 3.2453 sec/batch\n",
      "Epoch 12/20  Iteration 2028/3560 Training loss: 1.3189 3.1725 sec/batch\n",
      "Epoch 12/20  Iteration 2029/3560 Training loss: 1.3193 3.1049 sec/batch\n",
      "Epoch 12/20  Iteration 2030/3560 Training loss: 1.3197 3.1809 sec/batch\n",
      "Epoch 12/20  Iteration 2031/3560 Training loss: 1.3202 3.1115 sec/batch\n",
      "Epoch 12/20  Iteration 2032/3560 Training loss: 1.3198 3.1751 sec/batch\n",
      "Epoch 12/20  Iteration 2033/3560 Training loss: 1.3197 3.1638 sec/batch\n",
      "Epoch 12/20  Iteration 2034/3560 Training loss: 1.3198 3.2015 sec/batch\n",
      "Epoch 12/20  Iteration 2035/3560 Training loss: 1.3196 3.2016 sec/batch\n",
      "Epoch 12/20  Iteration 2036/3560 Training loss: 1.3194 3.2051 sec/batch\n",
      "Epoch 12/20  Iteration 2037/3560 Training loss: 1.3187 3.1369 sec/batch\n",
      "Epoch 12/20  Iteration 2038/3560 Training loss: 1.3188 3.2254 sec/batch\n",
      "Epoch 12/20  Iteration 2039/3560 Training loss: 1.3182 3.1958 sec/batch\n",
      "Epoch 12/20  Iteration 2040/3560 Training loss: 1.3182 3.1558 sec/batch\n",
      "Epoch 12/20  Iteration 2041/3560 Training loss: 1.3177 3.1556 sec/batch\n",
      "Epoch 12/20  Iteration 2042/3560 Training loss: 1.3176 3.2255 sec/batch\n",
      "Epoch 12/20  Iteration 2043/3560 Training loss: 1.3174 3.2041 sec/batch\n",
      "Epoch 12/20  Iteration 2044/3560 Training loss: 1.3172 3.4312 sec/batch\n",
      "Epoch 12/20  Iteration 2045/3560 Training loss: 1.3171 3.1784 sec/batch\n",
      "Epoch 12/20  Iteration 2046/3560 Training loss: 1.3168 3.2182 sec/batch\n",
      "Epoch 12/20  Iteration 2047/3560 Training loss: 1.3166 3.1380 sec/batch\n",
      "Epoch 12/20  Iteration 2048/3560 Training loss: 1.3165 3.1574 sec/batch\n",
      "Epoch 12/20  Iteration 2049/3560 Training loss: 1.3162 3.1661 sec/batch\n",
      "Epoch 12/20  Iteration 2050/3560 Training loss: 1.3161 3.1532 sec/batch\n",
      "Epoch 12/20  Iteration 2051/3560 Training loss: 1.3156 3.1035 sec/batch\n",
      "Epoch 12/20  Iteration 2052/3560 Training loss: 1.3152 3.1966 sec/batch\n",
      "Epoch 12/20  Iteration 2053/3560 Training loss: 1.3150 3.1896 sec/batch\n",
      "Epoch 12/20  Iteration 2054/3560 Training loss: 1.3150 3.0799 sec/batch\n",
      "Epoch 12/20  Iteration 2055/3560 Training loss: 1.3150 3.2483 sec/batch\n",
      "Epoch 12/20  Iteration 2056/3560 Training loss: 1.3146 3.2235 sec/batch\n",
      "Epoch 12/20  Iteration 2057/3560 Training loss: 1.3141 2.9968 sec/batch\n",
      "Epoch 12/20  Iteration 2058/3560 Training loss: 1.3137 3.1697 sec/batch\n",
      "Epoch 12/20  Iteration 2059/3560 Training loss: 1.3137 3.2242 sec/batch\n",
      "Epoch 12/20  Iteration 2060/3560 Training loss: 1.3135 3.1486 sec/batch\n",
      "Epoch 12/20  Iteration 2061/3560 Training loss: 1.3133 3.1815 sec/batch\n",
      "Epoch 12/20  Iteration 2062/3560 Training loss: 1.3131 3.1087 sec/batch\n",
      "Epoch 12/20  Iteration 2063/3560 Training loss: 1.3130 3.4799 sec/batch\n",
      "Epoch 12/20  Iteration 2064/3560 Training loss: 1.3129 3.2430 sec/batch\n",
      "Epoch 12/20  Iteration 2065/3560 Training loss: 1.3128 3.1691 sec/batch\n",
      "Epoch 12/20  Iteration 2066/3560 Training loss: 1.3128 3.2440 sec/batch\n",
      "Epoch 12/20  Iteration 2067/3560 Training loss: 1.3126 3.2771 sec/batch\n",
      "Epoch 12/20  Iteration 2068/3560 Training loss: 1.3126 3.1179 sec/batch\n",
      "Epoch 12/20  Iteration 2069/3560 Training loss: 1.3124 3.2471 sec/batch\n",
      "Epoch 12/20  Iteration 2070/3560 Training loss: 1.3123 3.2338 sec/batch\n",
      "Epoch 12/20  Iteration 2071/3560 Training loss: 1.3121 3.1340 sec/batch\n",
      "Epoch 12/20  Iteration 2072/3560 Training loss: 1.3120 3.2552 sec/batch\n",
      "Epoch 12/20  Iteration 2073/3560 Training loss: 1.3117 3.2151 sec/batch\n",
      "Epoch 12/20  Iteration 2074/3560 Training loss: 1.3113 3.1462 sec/batch\n",
      "Epoch 12/20  Iteration 2075/3560 Training loss: 1.3113 3.2487 sec/batch\n",
      "Epoch 12/20  Iteration 2076/3560 Training loss: 1.3113 3.1646 sec/batch\n",
      "Epoch 12/20  Iteration 2077/3560 Training loss: 1.3112 3.3023 sec/batch\n",
      "Epoch 12/20  Iteration 2078/3560 Training loss: 1.3110 3.1563 sec/batch\n",
      "Epoch 12/20  Iteration 2079/3560 Training loss: 1.3108 3.2346 sec/batch\n",
      "Epoch 12/20  Iteration 2080/3560 Training loss: 1.3104 3.1969 sec/batch\n",
      "Epoch 12/20  Iteration 2081/3560 Training loss: 1.3100 3.2610 sec/batch\n",
      "Epoch 12/20  Iteration 2082/3560 Training loss: 1.3099 3.0891 sec/batch\n",
      "Epoch 12/20  Iteration 2083/3560 Training loss: 1.3098 3.2521 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Iteration 2084/3560 Training loss: 1.3095 3.1092 sec/batch\n",
      "Epoch 12/20  Iteration 2085/3560 Training loss: 1.3095 3.1830 sec/batch\n",
      "Epoch 12/20  Iteration 2086/3560 Training loss: 1.3095 3.2678 sec/batch\n",
      "Epoch 12/20  Iteration 2087/3560 Training loss: 1.3092 3.2032 sec/batch\n",
      "Epoch 12/20  Iteration 2088/3560 Training loss: 1.3089 3.2740 sec/batch\n",
      "Epoch 12/20  Iteration 2089/3560 Training loss: 1.3085 3.2369 sec/batch\n",
      "Epoch 12/20  Iteration 2090/3560 Training loss: 1.3083 3.1226 sec/batch\n",
      "Epoch 12/20  Iteration 2091/3560 Training loss: 1.3085 3.1629 sec/batch\n",
      "Epoch 12/20  Iteration 2092/3560 Training loss: 1.3085 3.2746 sec/batch\n",
      "Epoch 12/20  Iteration 2093/3560 Training loss: 1.3085 3.1159 sec/batch\n",
      "Epoch 12/20  Iteration 2094/3560 Training loss: 1.3085 3.2105 sec/batch\n",
      "Epoch 12/20  Iteration 2095/3560 Training loss: 1.3087 3.1272 sec/batch\n",
      "Epoch 12/20  Iteration 2096/3560 Training loss: 1.3087 3.1359 sec/batch\n",
      "Epoch 12/20  Iteration 2097/3560 Training loss: 1.3087 3.2406 sec/batch\n",
      "Epoch 12/20  Iteration 2098/3560 Training loss: 1.3087 3.2384 sec/batch\n",
      "Epoch 12/20  Iteration 2099/3560 Training loss: 1.3091 3.0626 sec/batch\n",
      "Epoch 12/20  Iteration 2100/3560 Training loss: 1.3092 3.2189 sec/batch\n",
      "Epoch 12/20  Iteration 2101/3560 Training loss: 1.3091 3.2071 sec/batch\n",
      "Epoch 12/20  Iteration 2102/3560 Training loss: 1.3093 3.1415 sec/batch\n",
      "Epoch 12/20  Iteration 2103/3560 Training loss: 1.3091 3.1604 sec/batch\n",
      "Epoch 12/20  Iteration 2104/3560 Training loss: 1.3093 3.2919 sec/batch\n",
      "Epoch 12/20  Iteration 2105/3560 Training loss: 1.3092 3.1363 sec/batch\n",
      "Epoch 12/20  Iteration 2106/3560 Training loss: 1.3094 3.1429 sec/batch\n",
      "Epoch 12/20  Iteration 2107/3560 Training loss: 1.3095 3.2089 sec/batch\n",
      "Epoch 12/20  Iteration 2108/3560 Training loss: 1.3093 3.3278 sec/batch\n",
      "Epoch 12/20  Iteration 2109/3560 Training loss: 1.3090 3.1258 sec/batch\n",
      "Epoch 12/20  Iteration 2110/3560 Training loss: 1.3088 3.1571 sec/batch\n",
      "Epoch 12/20  Iteration 2111/3560 Training loss: 1.3089 3.1934 sec/batch\n",
      "Epoch 12/20  Iteration 2112/3560 Training loss: 1.3088 3.1416 sec/batch\n",
      "Epoch 12/20  Iteration 2113/3560 Training loss: 1.3088 3.1805 sec/batch\n",
      "Epoch 12/20  Iteration 2114/3560 Training loss: 1.3087 3.2385 sec/batch\n",
      "Epoch 12/20  Iteration 2115/3560 Training loss: 1.3087 3.1194 sec/batch\n",
      "Epoch 12/20  Iteration 2116/3560 Training loss: 1.3086 3.1159 sec/batch\n",
      "Epoch 12/20  Iteration 2117/3560 Training loss: 1.3084 3.2023 sec/batch\n",
      "Epoch 12/20  Iteration 2118/3560 Training loss: 1.3084 3.1381 sec/batch\n",
      "Epoch 12/20  Iteration 2119/3560 Training loss: 1.3086 3.0268 sec/batch\n",
      "Epoch 12/20  Iteration 2120/3560 Training loss: 1.3086 3.4831 sec/batch\n",
      "Epoch 12/20  Iteration 2121/3560 Training loss: 1.3085 3.1939 sec/batch\n",
      "Epoch 12/20  Iteration 2122/3560 Training loss: 1.3085 3.1637 sec/batch\n",
      "Epoch 12/20  Iteration 2123/3560 Training loss: 1.3085 3.4053 sec/batch\n",
      "Epoch 12/20  Iteration 2124/3560 Training loss: 1.3084 3.4478 sec/batch\n",
      "Epoch 12/20  Iteration 2125/3560 Training loss: 1.3085 5.8931 sec/batch\n",
      "Epoch 12/20  Iteration 2126/3560 Training loss: 1.3089 4.7925 sec/batch\n",
      "Epoch 12/20  Iteration 2127/3560 Training loss: 1.3088 3.9944 sec/batch\n",
      "Epoch 12/20  Iteration 2128/3560 Training loss: 1.3088 3.7654 sec/batch\n",
      "Epoch 12/20  Iteration 2129/3560 Training loss: 1.3087 3.4079 sec/batch\n",
      "Epoch 12/20  Iteration 2130/3560 Training loss: 1.3086 3.2881 sec/batch\n",
      "Epoch 12/20  Iteration 2131/3560 Training loss: 1.3087 3.1697 sec/batch\n",
      "Epoch 12/20  Iteration 2132/3560 Training loss: 1.3087 3.2618 sec/batch\n",
      "Epoch 12/20  Iteration 2133/3560 Training loss: 1.3087 3.2824 sec/batch\n",
      "Epoch 12/20  Iteration 2134/3560 Training loss: 1.3085 3.1532 sec/batch\n",
      "Epoch 12/20  Iteration 2135/3560 Training loss: 1.3083 3.2941 sec/batch\n",
      "Epoch 12/20  Iteration 2136/3560 Training loss: 1.3085 3.2349 sec/batch\n",
      "Epoch 13/20  Iteration 2137/3560 Training loss: 1.4163 3.2831 sec/batch\n",
      "Epoch 13/20  Iteration 2138/3560 Training loss: 1.3655 3.1494 sec/batch\n",
      "Epoch 13/20  Iteration 2139/3560 Training loss: 1.3466 3.1439 sec/batch\n",
      "Epoch 13/20  Iteration 2140/3560 Training loss: 1.3406 3.2885 sec/batch\n",
      "Epoch 13/20  Iteration 2141/3560 Training loss: 1.3304 3.0546 sec/batch\n",
      "Epoch 13/20  Iteration 2142/3560 Training loss: 1.3182 3.0405 sec/batch\n",
      "Epoch 13/20  Iteration 2143/3560 Training loss: 1.3165 3.2665 sec/batch\n",
      "Epoch 13/20  Iteration 2144/3560 Training loss: 1.3135 3.1420 sec/batch\n",
      "Epoch 13/20  Iteration 2145/3560 Training loss: 1.3127 3.1303 sec/batch\n",
      "Epoch 13/20  Iteration 2146/3560 Training loss: 1.3118 3.4236 sec/batch\n",
      "Epoch 13/20  Iteration 2147/3560 Training loss: 1.3078 3.2197 sec/batch\n",
      "Epoch 13/20  Iteration 2148/3560 Training loss: 1.3068 3.0994 sec/batch\n",
      "Epoch 13/20  Iteration 2149/3560 Training loss: 1.3064 3.2459 sec/batch\n",
      "Epoch 13/20  Iteration 2150/3560 Training loss: 1.3070 3.1889 sec/batch\n",
      "Epoch 13/20  Iteration 2151/3560 Training loss: 1.3057 3.1074 sec/batch\n",
      "Epoch 13/20  Iteration 2152/3560 Training loss: 1.3043 3.1990 sec/batch\n",
      "Epoch 13/20  Iteration 2153/3560 Training loss: 1.3041 3.1681 sec/batch\n",
      "Epoch 13/20  Iteration 2154/3560 Training loss: 1.3049 3.0430 sec/batch\n",
      "Epoch 13/20  Iteration 2155/3560 Training loss: 1.3045 3.1497 sec/batch\n",
      "Epoch 13/20  Iteration 2156/3560 Training loss: 1.3056 3.2286 sec/batch\n",
      "Epoch 13/20  Iteration 2157/3560 Training loss: 1.3048 3.1365 sec/batch\n",
      "Epoch 13/20  Iteration 2158/3560 Training loss: 1.3050 3.0957 sec/batch\n",
      "Epoch 13/20  Iteration 2159/3560 Training loss: 1.3037 3.1689 sec/batch\n",
      "Epoch 13/20  Iteration 2160/3560 Training loss: 1.3035 3.1606 sec/batch\n",
      "Epoch 13/20  Iteration 2161/3560 Training loss: 1.3030 3.1067 sec/batch\n",
      "Epoch 13/20  Iteration 2162/3560 Training loss: 1.3013 3.2042 sec/batch\n",
      "Epoch 13/20  Iteration 2163/3560 Training loss: 1.2997 3.1870 sec/batch\n",
      "Epoch 13/20  Iteration 2164/3560 Training loss: 1.3002 3.1976 sec/batch\n",
      "Epoch 13/20  Iteration 2165/3560 Training loss: 1.3000 3.0912 sec/batch\n",
      "Epoch 13/20  Iteration 2166/3560 Training loss: 1.3005 3.2228 sec/batch\n",
      "Epoch 13/20  Iteration 2167/3560 Training loss: 1.2998 3.0669 sec/batch\n",
      "Epoch 13/20  Iteration 2168/3560 Training loss: 1.2986 3.1336 sec/batch\n",
      "Epoch 13/20  Iteration 2169/3560 Training loss: 1.2987 3.1850 sec/batch\n",
      "Epoch 13/20  Iteration 2170/3560 Training loss: 1.2990 3.0693 sec/batch\n",
      "Epoch 13/20  Iteration 2171/3560 Training loss: 1.2991 3.2338 sec/batch\n",
      "Epoch 13/20  Iteration 2172/3560 Training loss: 1.2988 3.1768 sec/batch\n",
      "Epoch 13/20  Iteration 2173/3560 Training loss: 1.2983 3.0681 sec/batch\n",
      "Epoch 13/20  Iteration 2174/3560 Training loss: 1.2971 3.3385 sec/batch\n",
      "Epoch 13/20  Iteration 2175/3560 Training loss: 1.2960 3.2631 sec/batch\n",
      "Epoch 13/20  Iteration 2176/3560 Training loss: 1.2954 3.0689 sec/batch\n",
      "Epoch 13/20  Iteration 2177/3560 Training loss: 1.2949 3.2170 sec/batch\n",
      "Epoch 13/20  Iteration 2178/3560 Training loss: 1.2956 3.2410 sec/batch\n",
      "Epoch 13/20  Iteration 2179/3560 Training loss: 1.2955 3.0895 sec/batch\n",
      "Epoch 13/20  Iteration 2180/3560 Training loss: 1.2946 3.1291 sec/batch\n",
      "Epoch 13/20  Iteration 2181/3560 Training loss: 1.2948 3.3317 sec/batch\n",
      "Epoch 13/20  Iteration 2182/3560 Training loss: 1.2940 3.0637 sec/batch\n",
      "Epoch 13/20  Iteration 2183/3560 Training loss: 1.2935 3.0977 sec/batch\n",
      "Epoch 13/20  Iteration 2184/3560 Training loss: 1.2931 3.2741 sec/batch\n",
      "Epoch 13/20  Iteration 2185/3560 Training loss: 1.2931 3.1090 sec/batch\n",
      "Epoch 13/20  Iteration 2186/3560 Training loss: 1.2932 3.2071 sec/batch\n",
      "Epoch 13/20  Iteration 2187/3560 Training loss: 1.2928 3.1431 sec/batch\n",
      "Epoch 13/20  Iteration 2188/3560 Training loss: 1.2936 3.1965 sec/batch\n",
      "Epoch 13/20  Iteration 2189/3560 Training loss: 1.2935 3.1675 sec/batch\n",
      "Epoch 13/20  Iteration 2190/3560 Training loss: 1.2936 3.0581 sec/batch\n",
      "Epoch 13/20  Iteration 2191/3560 Training loss: 1.2934 3.2511 sec/batch\n",
      "Epoch 13/20  Iteration 2192/3560 Training loss: 1.2934 3.1286 sec/batch\n",
      "Epoch 13/20  Iteration 2193/3560 Training loss: 1.2935 3.0927 sec/batch\n",
      "Epoch 13/20  Iteration 2194/3560 Training loss: 1.2931 3.2039 sec/batch\n",
      "Epoch 13/20  Iteration 2195/3560 Training loss: 1.2927 3.1151 sec/batch\n",
      "Epoch 13/20  Iteration 2196/3560 Training loss: 1.2931 3.1351 sec/batch\n",
      "Epoch 13/20  Iteration 2197/3560 Training loss: 1.2931 3.1845 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2198/3560 Training loss: 1.2938 3.1714 sec/batch\n",
      "Epoch 13/20  Iteration 2199/3560 Training loss: 1.2940 3.0961 sec/batch\n",
      "Epoch 13/20  Iteration 2200/3560 Training loss: 1.2942 3.2838 sec/batch\n",
      "Epoch 13/20  Iteration 2201/3560 Training loss: 1.2940 3.0776 sec/batch\n",
      "Epoch 13/20  Iteration 2202/3560 Training loss: 1.2941 3.1042 sec/batch\n",
      "Epoch 13/20  Iteration 2203/3560 Training loss: 1.2943 3.1660 sec/batch\n",
      "Epoch 13/20  Iteration 2204/3560 Training loss: 1.2940 3.2033 sec/batch\n",
      "Epoch 13/20  Iteration 2205/3560 Training loss: 1.2941 3.0709 sec/batch\n",
      "Epoch 13/20  Iteration 2206/3560 Training loss: 1.2940 3.1049 sec/batch\n",
      "Epoch 13/20  Iteration 2207/3560 Training loss: 1.2947 3.2012 sec/batch\n",
      "Epoch 13/20  Iteration 2208/3560 Training loss: 1.2950 3.0564 sec/batch\n",
      "Epoch 13/20  Iteration 2209/3560 Training loss: 1.2954 3.2537 sec/batch\n",
      "Epoch 13/20  Iteration 2210/3560 Training loss: 1.2951 3.1848 sec/batch\n",
      "Epoch 13/20  Iteration 2211/3560 Training loss: 1.2950 3.0402 sec/batch\n",
      "Epoch 13/20  Iteration 2212/3560 Training loss: 1.2952 3.1302 sec/batch\n",
      "Epoch 13/20  Iteration 2213/3560 Training loss: 1.2950 3.1710 sec/batch\n",
      "Epoch 13/20  Iteration 2214/3560 Training loss: 1.2950 3.0968 sec/batch\n",
      "Epoch 13/20  Iteration 2215/3560 Training loss: 1.2944 3.1247 sec/batch\n",
      "Epoch 13/20  Iteration 2216/3560 Training loss: 1.2944 3.2322 sec/batch\n",
      "Epoch 13/20  Iteration 2217/3560 Training loss: 1.2939 3.1911 sec/batch\n",
      "Epoch 13/20  Iteration 2218/3560 Training loss: 1.2937 3.1513 sec/batch\n",
      "Epoch 13/20  Iteration 2219/3560 Training loss: 1.2932 3.2566 sec/batch\n",
      "Epoch 13/20  Iteration 2220/3560 Training loss: 1.2931 3.1649 sec/batch\n",
      "Epoch 13/20  Iteration 2221/3560 Training loss: 1.2929 3.1182 sec/batch\n",
      "Epoch 13/20  Iteration 2222/3560 Training loss: 1.2928 3.0664 sec/batch\n",
      "Epoch 13/20  Iteration 2223/3560 Training loss: 1.2926 3.2010 sec/batch\n",
      "Epoch 13/20  Iteration 2224/3560 Training loss: 1.2924 3.0731 sec/batch\n",
      "Epoch 13/20  Iteration 2225/3560 Training loss: 1.2920 3.0093 sec/batch\n",
      "Epoch 13/20  Iteration 2226/3560 Training loss: 1.2921 3.2272 sec/batch\n",
      "Epoch 13/20  Iteration 2227/3560 Training loss: 1.2918 3.1208 sec/batch\n",
      "Epoch 13/20  Iteration 2228/3560 Training loss: 1.2917 3.0470 sec/batch\n",
      "Epoch 13/20  Iteration 2229/3560 Training loss: 1.2915 3.2218 sec/batch\n",
      "Epoch 13/20  Iteration 2230/3560 Training loss: 1.2911 3.0655 sec/batch\n",
      "Epoch 13/20  Iteration 2231/3560 Training loss: 1.2908 3.2659 sec/batch\n",
      "Epoch 13/20  Iteration 2232/3560 Training loss: 1.2908 3.1668 sec/batch\n",
      "Epoch 13/20  Iteration 2233/3560 Training loss: 1.2908 3.2076 sec/batch\n",
      "Epoch 13/20  Iteration 2234/3560 Training loss: 1.2904 3.1067 sec/batch\n",
      "Epoch 13/20  Iteration 2235/3560 Training loss: 1.2900 3.1230 sec/batch\n",
      "Epoch 13/20  Iteration 2236/3560 Training loss: 1.2896 3.1618 sec/batch\n",
      "Epoch 13/20  Iteration 2237/3560 Training loss: 1.2896 3.1240 sec/batch\n",
      "Epoch 13/20  Iteration 2238/3560 Training loss: 1.2894 3.1835 sec/batch\n",
      "Epoch 13/20  Iteration 2239/3560 Training loss: 1.2893 3.6853 sec/batch\n",
      "Epoch 13/20  Iteration 2240/3560 Training loss: 1.2891 3.1825 sec/batch\n",
      "Epoch 13/20  Iteration 2241/3560 Training loss: 1.2890 3.1152 sec/batch\n",
      "Epoch 13/20  Iteration 2242/3560 Training loss: 1.2890 3.2281 sec/batch\n",
      "Epoch 13/20  Iteration 2243/3560 Training loss: 1.2890 3.1238 sec/batch\n",
      "Epoch 13/20  Iteration 2244/3560 Training loss: 1.2890 3.1868 sec/batch\n",
      "Epoch 13/20  Iteration 2245/3560 Training loss: 1.2888 3.2309 sec/batch\n",
      "Epoch 13/20  Iteration 2246/3560 Training loss: 1.2889 3.0527 sec/batch\n",
      "Epoch 13/20  Iteration 2247/3560 Training loss: 1.2887 3.1740 sec/batch\n",
      "Epoch 13/20  Iteration 2248/3560 Training loss: 1.2886 3.2536 sec/batch\n",
      "Epoch 13/20  Iteration 2249/3560 Training loss: 1.2885 3.1123 sec/batch\n",
      "Epoch 13/20  Iteration 2250/3560 Training loss: 1.2883 3.1190 sec/batch\n",
      "Epoch 13/20  Iteration 2251/3560 Training loss: 1.2880 3.1853 sec/batch\n",
      "Epoch 13/20  Iteration 2252/3560 Training loss: 1.2876 3.1683 sec/batch\n",
      "Epoch 13/20  Iteration 2253/3560 Training loss: 1.2876 3.0218 sec/batch\n",
      "Epoch 13/20  Iteration 2254/3560 Training loss: 1.2876 3.1891 sec/batch\n",
      "Epoch 13/20  Iteration 2255/3560 Training loss: 1.2875 3.2428 sec/batch\n",
      "Epoch 13/20  Iteration 2256/3560 Training loss: 1.2874 3.0626 sec/batch\n",
      "Epoch 13/20  Iteration 2257/3560 Training loss: 1.2872 3.1867 sec/batch\n",
      "Epoch 13/20  Iteration 2258/3560 Training loss: 1.2868 3.1839 sec/batch\n",
      "Epoch 13/20  Iteration 2259/3560 Training loss: 1.2864 3.0617 sec/batch\n",
      "Epoch 13/20  Iteration 2260/3560 Training loss: 1.2864 3.1647 sec/batch\n",
      "Epoch 13/20  Iteration 2261/3560 Training loss: 1.2862 3.2889 sec/batch\n",
      "Epoch 13/20  Iteration 2262/3560 Training loss: 1.2858 3.0637 sec/batch\n",
      "Epoch 13/20  Iteration 2263/3560 Training loss: 1.2858 3.1237 sec/batch\n",
      "Epoch 13/20  Iteration 2264/3560 Training loss: 1.2858 3.1858 sec/batch\n",
      "Epoch 13/20  Iteration 2265/3560 Training loss: 1.2856 3.1361 sec/batch\n",
      "Epoch 13/20  Iteration 2266/3560 Training loss: 1.2853 3.1725 sec/batch\n",
      "Epoch 13/20  Iteration 2267/3560 Training loss: 1.2849 3.1589 sec/batch\n",
      "Epoch 13/20  Iteration 2268/3560 Training loss: 1.2847 3.1944 sec/batch\n",
      "Epoch 13/20  Iteration 2269/3560 Training loss: 1.2848 3.1434 sec/batch\n",
      "Epoch 13/20  Iteration 2270/3560 Training loss: 1.2849 3.0610 sec/batch\n",
      "Epoch 13/20  Iteration 2271/3560 Training loss: 1.2848 3.2167 sec/batch\n",
      "Epoch 13/20  Iteration 2272/3560 Training loss: 1.2848 3.1281 sec/batch\n",
      "Epoch 13/20  Iteration 2273/3560 Training loss: 1.2850 3.1349 sec/batch\n",
      "Epoch 13/20  Iteration 2274/3560 Training loss: 1.2851 3.2312 sec/batch\n",
      "Epoch 13/20  Iteration 2275/3560 Training loss: 1.2852 3.1669 sec/batch\n",
      "Epoch 13/20  Iteration 2276/3560 Training loss: 1.2852 3.0828 sec/batch\n",
      "Epoch 13/20  Iteration 2277/3560 Training loss: 1.2855 3.2660 sec/batch\n",
      "Epoch 13/20  Iteration 2278/3560 Training loss: 1.2855 3.1095 sec/batch\n",
      "Epoch 13/20  Iteration 2279/3560 Training loss: 1.2855 3.1215 sec/batch\n",
      "Epoch 13/20  Iteration 2280/3560 Training loss: 1.2858 3.1690 sec/batch\n",
      "Epoch 13/20  Iteration 2281/3560 Training loss: 1.2857 3.0535 sec/batch\n",
      "Epoch 13/20  Iteration 2282/3560 Training loss: 1.2859 3.1765 sec/batch\n",
      "Epoch 13/20  Iteration 2283/3560 Training loss: 1.2859 3.2291 sec/batch\n",
      "Epoch 13/20  Iteration 2284/3560 Training loss: 1.2861 3.0312 sec/batch\n",
      "Epoch 13/20  Iteration 2285/3560 Training loss: 1.2862 3.1154 sec/batch\n",
      "Epoch 13/20  Iteration 2286/3560 Training loss: 1.2860 3.1866 sec/batch\n",
      "Epoch 13/20  Iteration 2287/3560 Training loss: 1.2857 3.1090 sec/batch\n",
      "Epoch 13/20  Iteration 2288/3560 Training loss: 1.2855 3.3458 sec/batch\n",
      "Epoch 13/20  Iteration 2289/3560 Training loss: 1.2856 3.2299 sec/batch\n",
      "Epoch 13/20  Iteration 2290/3560 Training loss: 1.2856 3.1462 sec/batch\n",
      "Epoch 13/20  Iteration 2291/3560 Training loss: 1.2856 3.1030 sec/batch\n",
      "Epoch 13/20  Iteration 2292/3560 Training loss: 1.2856 3.2180 sec/batch\n",
      "Epoch 13/20  Iteration 2293/3560 Training loss: 1.2856 3.1141 sec/batch\n",
      "Epoch 13/20  Iteration 2294/3560 Training loss: 1.2855 3.1833 sec/batch\n",
      "Epoch 13/20  Iteration 2295/3560 Training loss: 1.2853 3.0905 sec/batch\n",
      "Epoch 13/20  Iteration 2296/3560 Training loss: 1.2854 3.2015 sec/batch\n",
      "Epoch 13/20  Iteration 2297/3560 Training loss: 1.2856 3.1255 sec/batch\n",
      "Epoch 13/20  Iteration 2298/3560 Training loss: 1.2856 3.1596 sec/batch\n",
      "Epoch 13/20  Iteration 2299/3560 Training loss: 1.2856 3.1578 sec/batch\n",
      "Epoch 13/20  Iteration 2300/3560 Training loss: 1.2856 3.1657 sec/batch\n",
      "Epoch 13/20  Iteration 2301/3560 Training loss: 1.2856 3.1196 sec/batch\n",
      "Epoch 13/20  Iteration 2302/3560 Training loss: 1.2855 3.1411 sec/batch\n",
      "Epoch 13/20  Iteration 2303/3560 Training loss: 1.2856 3.1902 sec/batch\n",
      "Epoch 13/20  Iteration 2304/3560 Training loss: 1.2860 3.1031 sec/batch\n",
      "Epoch 13/20  Iteration 2305/3560 Training loss: 1.2860 3.2208 sec/batch\n",
      "Epoch 13/20  Iteration 2306/3560 Training loss: 1.2861 3.2272 sec/batch\n",
      "Epoch 13/20  Iteration 2307/3560 Training loss: 1.2860 3.0711 sec/batch\n",
      "Epoch 13/20  Iteration 2308/3560 Training loss: 1.2859 3.1498 sec/batch\n",
      "Epoch 13/20  Iteration 2309/3560 Training loss: 1.2861 3.1143 sec/batch\n",
      "Epoch 13/20  Iteration 2310/3560 Training loss: 1.2860 3.1457 sec/batch\n",
      "Epoch 13/20  Iteration 2311/3560 Training loss: 1.2861 3.1372 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Iteration 2312/3560 Training loss: 1.2859 3.1904 sec/batch\n",
      "Epoch 13/20  Iteration 2313/3560 Training loss: 1.2858 3.2111 sec/batch\n",
      "Epoch 13/20  Iteration 2314/3560 Training loss: 1.2859 3.1473 sec/batch\n",
      "Epoch 14/20  Iteration 2315/3560 Training loss: 1.3916 3.1824 sec/batch\n",
      "Epoch 14/20  Iteration 2316/3560 Training loss: 1.3433 3.0842 sec/batch\n",
      "Epoch 14/20  Iteration 2317/3560 Training loss: 1.3228 3.1829 sec/batch\n",
      "Epoch 14/20  Iteration 2318/3560 Training loss: 1.3157 3.2195 sec/batch\n",
      "Epoch 14/20  Iteration 2319/3560 Training loss: 1.3062 3.1040 sec/batch\n",
      "Epoch 14/20  Iteration 2320/3560 Training loss: 1.2943 3.1640 sec/batch\n",
      "Epoch 14/20  Iteration 2321/3560 Training loss: 1.2939 3.1524 sec/batch\n",
      "Epoch 14/20  Iteration 2322/3560 Training loss: 1.2915 3.0649 sec/batch\n",
      "Epoch 14/20  Iteration 2323/3560 Training loss: 1.2903 3.0920 sec/batch\n",
      "Epoch 14/20  Iteration 2324/3560 Training loss: 1.2892 3.1880 sec/batch\n",
      "Epoch 14/20  Iteration 2325/3560 Training loss: 1.2859 3.2185 sec/batch\n",
      "Epoch 14/20  Iteration 2326/3560 Training loss: 1.2851 3.1980 sec/batch\n",
      "Epoch 14/20  Iteration 2327/3560 Training loss: 1.2851 3.1274 sec/batch\n",
      "Epoch 14/20  Iteration 2328/3560 Training loss: 1.2851 3.1905 sec/batch\n",
      "Epoch 14/20  Iteration 2329/3560 Training loss: 1.2834 3.1876 sec/batch\n",
      "Epoch 14/20  Iteration 2330/3560 Training loss: 1.2816 3.0819 sec/batch\n",
      "Epoch 14/20  Iteration 2331/3560 Training loss: 1.2818 3.2075 sec/batch\n",
      "Epoch 14/20  Iteration 2332/3560 Training loss: 1.2829 3.2257 sec/batch\n",
      "Epoch 14/20  Iteration 2333/3560 Training loss: 1.2826 3.0304 sec/batch\n",
      "Epoch 14/20  Iteration 2334/3560 Training loss: 1.2837 3.2006 sec/batch\n",
      "Epoch 14/20  Iteration 2335/3560 Training loss: 1.2835 3.1012 sec/batch\n",
      "Epoch 14/20  Iteration 2336/3560 Training loss: 1.2837 3.0682 sec/batch\n",
      "Epoch 14/20  Iteration 2337/3560 Training loss: 1.2823 3.2552 sec/batch\n",
      "Epoch 14/20  Iteration 2338/3560 Training loss: 1.2824 3.1510 sec/batch\n",
      "Epoch 14/20  Iteration 2339/3560 Training loss: 1.2822 3.0931 sec/batch\n",
      "Epoch 14/20  Iteration 2340/3560 Training loss: 1.2804 3.1861 sec/batch\n",
      "Epoch 14/20  Iteration 2341/3560 Training loss: 1.2792 3.1193 sec/batch\n",
      "Epoch 14/20  Iteration 2342/3560 Training loss: 1.2796 3.1870 sec/batch\n",
      "Epoch 14/20  Iteration 2343/3560 Training loss: 1.2798 3.1932 sec/batch\n",
      "Epoch 14/20  Iteration 2344/3560 Training loss: 1.2800 3.0733 sec/batch\n",
      "Epoch 14/20  Iteration 2345/3560 Training loss: 1.2794 3.3555 sec/batch\n",
      "Epoch 14/20  Iteration 2346/3560 Training loss: 1.2782 3.2515 sec/batch\n",
      "Epoch 14/20  Iteration 2347/3560 Training loss: 1.2785 3.0929 sec/batch\n",
      "Epoch 14/20  Iteration 2348/3560 Training loss: 1.2788 3.1819 sec/batch\n",
      "Epoch 14/20  Iteration 2349/3560 Training loss: 1.2786 3.2005 sec/batch\n",
      "Epoch 14/20  Iteration 2350/3560 Training loss: 1.2785 3.1472 sec/batch\n",
      "Epoch 14/20  Iteration 2351/3560 Training loss: 1.2781 3.2888 sec/batch\n",
      "Epoch 14/20  Iteration 2352/3560 Training loss: 1.2769 3.2010 sec/batch\n",
      "Epoch 14/20  Iteration 2353/3560 Training loss: 1.2755 3.1605 sec/batch\n",
      "Epoch 14/20  Iteration 2354/3560 Training loss: 1.2751 3.2127 sec/batch\n",
      "Epoch 14/20  Iteration 2355/3560 Training loss: 1.2747 3.0955 sec/batch\n",
      "Epoch 14/20  Iteration 2356/3560 Training loss: 1.2754 3.1891 sec/batch\n",
      "Epoch 14/20  Iteration 2357/3560 Training loss: 1.2752 3.1110 sec/batch\n",
      "Epoch 14/20  Iteration 2358/3560 Training loss: 1.2747 3.2361 sec/batch\n",
      "Epoch 14/20  Iteration 2359/3560 Training loss: 1.2748 3.1631 sec/batch\n",
      "Epoch 14/20  Iteration 2360/3560 Training loss: 1.2742 3.0865 sec/batch\n",
      "Epoch 14/20  Iteration 2361/3560 Training loss: 1.2739 3.2001 sec/batch\n",
      "Epoch 14/20  Iteration 2362/3560 Training loss: 1.2738 3.1217 sec/batch\n",
      "Epoch 14/20  Iteration 2363/3560 Training loss: 1.2737 3.1468 sec/batch\n",
      "Epoch 14/20  Iteration 2364/3560 Training loss: 1.2742 3.1196 sec/batch\n",
      "Epoch 14/20  Iteration 2365/3560 Training loss: 1.2738 3.1473 sec/batch\n",
      "Epoch 14/20  Iteration 2366/3560 Training loss: 1.2745 3.1010 sec/batch\n",
      "Epoch 14/20  Iteration 2367/3560 Training loss: 1.2745 3.2129 sec/batch\n",
      "Epoch 14/20  Iteration 2368/3560 Training loss: 1.2747 3.0495 sec/batch\n",
      "Epoch 14/20  Iteration 2369/3560 Training loss: 1.2745 3.1841 sec/batch\n",
      "Epoch 14/20  Iteration 2370/3560 Training loss: 1.2745 3.1518 sec/batch\n",
      "Epoch 14/20  Iteration 2371/3560 Training loss: 1.2748 3.3615 sec/batch\n",
      "Epoch 14/20  Iteration 2372/3560 Training loss: 1.2745 3.1337 sec/batch\n",
      "Epoch 14/20  Iteration 2373/3560 Training loss: 1.2739 3.0881 sec/batch\n",
      "Epoch 14/20  Iteration 2374/3560 Training loss: 1.2745 3.1728 sec/batch\n",
      "Epoch 14/20  Iteration 2375/3560 Training loss: 1.2745 3.1373 sec/batch\n",
      "Epoch 14/20  Iteration 2376/3560 Training loss: 1.2753 3.0584 sec/batch\n",
      "Epoch 14/20  Iteration 2377/3560 Training loss: 1.2754 3.2203 sec/batch\n",
      "Epoch 14/20  Iteration 2378/3560 Training loss: 1.2755 3.1060 sec/batch\n",
      "Epoch 14/20  Iteration 2379/3560 Training loss: 1.2753 3.0599 sec/batch\n",
      "Epoch 14/20  Iteration 2380/3560 Training loss: 1.2755 3.2906 sec/batch\n",
      "Epoch 14/20  Iteration 2381/3560 Training loss: 1.2756 3.1991 sec/batch\n",
      "Epoch 14/20  Iteration 2382/3560 Training loss: 1.2754 3.0532 sec/batch\n",
      "Epoch 14/20  Iteration 2383/3560 Training loss: 1.2756 3.1762 sec/batch\n",
      "Epoch 14/20  Iteration 2384/3560 Training loss: 1.2755 3.1243 sec/batch\n",
      "Epoch 14/20  Iteration 2385/3560 Training loss: 1.2760 3.0762 sec/batch\n",
      "Epoch 14/20  Iteration 2386/3560 Training loss: 1.2763 3.1514 sec/batch\n",
      "Epoch 14/20  Iteration 2387/3560 Training loss: 1.2768 3.1736 sec/batch\n",
      "Epoch 14/20  Iteration 2388/3560 Training loss: 1.2765 3.1472 sec/batch\n",
      "Epoch 14/20  Iteration 2389/3560 Training loss: 1.2765 3.1421 sec/batch\n",
      "Epoch 14/20  Iteration 2390/3560 Training loss: 1.2765 3.2555 sec/batch\n",
      "Epoch 14/20  Iteration 2391/3560 Training loss: 1.2763 3.1914 sec/batch\n",
      "Epoch 14/20  Iteration 2392/3560 Training loss: 1.2762 3.1026 sec/batch\n",
      "Epoch 14/20  Iteration 2393/3560 Training loss: 1.2757 3.1367 sec/batch\n",
      "Epoch 14/20  Iteration 2394/3560 Training loss: 1.2758 3.1997 sec/batch\n",
      "Epoch 14/20  Iteration 2395/3560 Training loss: 1.2753 3.1464 sec/batch\n",
      "Epoch 14/20  Iteration 2396/3560 Training loss: 1.2753 3.1670 sec/batch\n",
      "Epoch 14/20  Iteration 2397/3560 Training loss: 1.2748 3.1752 sec/batch\n",
      "Epoch 14/20  Iteration 2398/3560 Training loss: 1.2746 3.0840 sec/batch\n",
      "Epoch 14/20  Iteration 2399/3560 Training loss: 1.2745 3.1353 sec/batch\n",
      "Epoch 14/20  Iteration 2400/3560 Training loss: 1.2743 3.1885 sec/batch\n",
      "Epoch 14/20  Iteration 2401/3560 Training loss: 1.2740 3.1250 sec/batch\n",
      "Epoch 14/20  Iteration 2402/3560 Training loss: 1.2737 3.3907 sec/batch\n",
      "Epoch 14/20  Iteration 2403/3560 Training loss: 1.2733 3.2415 sec/batch\n",
      "Epoch 14/20  Iteration 2404/3560 Training loss: 1.2735 3.0949 sec/batch\n",
      "Epoch 14/20  Iteration 2405/3560 Training loss: 1.2732 3.1261 sec/batch\n",
      "Epoch 14/20  Iteration 2406/3560 Training loss: 1.2731 3.1699 sec/batch\n",
      "Epoch 14/20  Iteration 2407/3560 Training loss: 1.2728 3.1893 sec/batch\n",
      "Epoch 14/20  Iteration 2408/3560 Training loss: 1.2725 3.0949 sec/batch\n",
      "Epoch 14/20  Iteration 2409/3560 Training loss: 1.2723 3.2915 sec/batch\n",
      "Epoch 14/20  Iteration 2410/3560 Training loss: 1.2723 3.1101 sec/batch\n",
      "Epoch 14/20  Iteration 2411/3560 Training loss: 1.2723 3.1907 sec/batch\n",
      "Epoch 14/20  Iteration 2412/3560 Training loss: 1.2720 3.2287 sec/batch\n",
      "Epoch 14/20  Iteration 2413/3560 Training loss: 1.2716 3.1162 sec/batch\n",
      "Epoch 14/20  Iteration 2414/3560 Training loss: 1.2712 3.1275 sec/batch\n",
      "Epoch 14/20  Iteration 2415/3560 Training loss: 1.2713 3.1864 sec/batch\n",
      "Epoch 14/20  Iteration 2416/3560 Training loss: 1.2711 3.1463 sec/batch\n",
      "Epoch 14/20  Iteration 2417/3560 Training loss: 1.2710 3.0468 sec/batch\n",
      "Epoch 14/20  Iteration 2418/3560 Training loss: 1.2709 3.1624 sec/batch\n",
      "Epoch 14/20  Iteration 2419/3560 Training loss: 1.2708 3.2591 sec/batch\n",
      "Epoch 14/20  Iteration 2420/3560 Training loss: 1.2708 3.1939 sec/batch\n",
      "Epoch 14/20  Iteration 2421/3560 Training loss: 1.2707 3.1876 sec/batch\n",
      "Epoch 14/20  Iteration 2422/3560 Training loss: 1.2707 3.1991 sec/batch\n",
      "Epoch 14/20  Iteration 2423/3560 Training loss: 1.2706 3.1022 sec/batch\n",
      "Epoch 14/20  Iteration 2424/3560 Training loss: 1.2706 3.0611 sec/batch\n",
      "Epoch 14/20  Iteration 2425/3560 Training loss: 1.2704 3.2708 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Iteration 2426/3560 Training loss: 1.2705 3.1431 sec/batch\n",
      "Epoch 14/20  Iteration 2427/3560 Training loss: 1.2704 3.0060 sec/batch\n",
      "Epoch 14/20  Iteration 2428/3560 Training loss: 1.2702 3.1925 sec/batch\n",
      "Epoch 14/20  Iteration 2429/3560 Training loss: 1.2700 3.2537 sec/batch\n",
      "Epoch 14/20  Iteration 2430/3560 Training loss: 1.2697 2.9936 sec/batch\n",
      "Epoch 14/20  Iteration 2431/3560 Training loss: 1.2697 3.1210 sec/batch\n",
      "Epoch 14/20  Iteration 2432/3560 Training loss: 1.2697 3.3377 sec/batch\n",
      "Epoch 14/20  Iteration 2433/3560 Training loss: 1.2696 3.0227 sec/batch\n",
      "Epoch 14/20  Iteration 2434/3560 Training loss: 1.2695 3.1636 sec/batch\n",
      "Epoch 14/20  Iteration 2435/3560 Training loss: 1.2693 3.1994 sec/batch\n",
      "Epoch 14/20  Iteration 2436/3560 Training loss: 1.2690 3.0982 sec/batch\n",
      "Epoch 14/20  Iteration 2437/3560 Training loss: 1.2686 3.1655 sec/batch\n",
      "Epoch 14/20  Iteration 2438/3560 Training loss: 1.2685 3.1447 sec/batch\n",
      "Epoch 14/20  Iteration 2439/3560 Training loss: 1.2685 3.1123 sec/batch\n",
      "Epoch 14/20  Iteration 2440/3560 Training loss: 1.2681 3.1430 sec/batch\n",
      "Epoch 14/20  Iteration 2441/3560 Training loss: 1.2681 3.1003 sec/batch\n",
      "Epoch 14/20  Iteration 2442/3560 Training loss: 1.2681 3.2260 sec/batch\n",
      "Epoch 14/20  Iteration 2443/3560 Training loss: 1.2678 3.1632 sec/batch\n",
      "Epoch 14/20  Iteration 2444/3560 Training loss: 1.2675 3.1418 sec/batch\n",
      "Epoch 14/20  Iteration 2445/3560 Training loss: 1.2670 3.1677 sec/batch\n",
      "Epoch 14/20  Iteration 2446/3560 Training loss: 1.2669 3.1734 sec/batch\n",
      "Epoch 14/20  Iteration 2447/3560 Training loss: 1.2670 3.1135 sec/batch\n",
      "Epoch 14/20  Iteration 2448/3560 Training loss: 1.2670 3.3002 sec/batch\n",
      "Epoch 14/20  Iteration 2449/3560 Training loss: 1.2670 3.0845 sec/batch\n",
      "Epoch 14/20  Iteration 2450/3560 Training loss: 1.2670 3.0877 sec/batch\n",
      "Epoch 14/20  Iteration 2451/3560 Training loss: 1.2672 3.2186 sec/batch\n",
      "Epoch 14/20  Iteration 2452/3560 Training loss: 1.2673 3.0315 sec/batch\n",
      "Epoch 14/20  Iteration 2453/3560 Training loss: 1.2673 3.1670 sec/batch\n",
      "Epoch 14/20  Iteration 2454/3560 Training loss: 1.2674 3.2563 sec/batch\n",
      "Epoch 14/20  Iteration 2455/3560 Training loss: 1.2676 3.1368 sec/batch\n",
      "Epoch 14/20  Iteration 2456/3560 Training loss: 1.2677 3.1037 sec/batch\n",
      "Epoch 14/20  Iteration 2457/3560 Training loss: 1.2676 3.1604 sec/batch\n",
      "Epoch 14/20  Iteration 2458/3560 Training loss: 1.2679 3.1185 sec/batch\n",
      "Epoch 14/20  Iteration 2459/3560 Training loss: 1.2678 3.3124 sec/batch\n",
      "Epoch 14/20  Iteration 2460/3560 Training loss: 1.2678 3.2144 sec/batch\n",
      "Epoch 14/20  Iteration 2461/3560 Training loss: 1.2678 3.1606 sec/batch\n",
      "Epoch 14/20  Iteration 2462/3560 Training loss: 1.2680 3.1225 sec/batch\n",
      "Epoch 14/20  Iteration 2463/3560 Training loss: 1.2682 3.2298 sec/batch\n",
      "Epoch 14/20  Iteration 2464/3560 Training loss: 1.2681 3.1663 sec/batch\n",
      "Epoch 14/20  Iteration 2465/3560 Training loss: 1.2678 3.0840 sec/batch\n",
      "Epoch 14/20  Iteration 2466/3560 Training loss: 1.2677 3.1623 sec/batch\n",
      "Epoch 14/20  Iteration 2467/3560 Training loss: 1.2678 3.2534 sec/batch\n",
      "Epoch 14/20  Iteration 2468/3560 Training loss: 1.2678 3.2467 sec/batch\n",
      "Epoch 14/20  Iteration 2469/3560 Training loss: 1.2677 3.0916 sec/batch\n",
      "Epoch 14/20  Iteration 2470/3560 Training loss: 1.2677 3.2910 sec/batch\n",
      "Epoch 14/20  Iteration 2471/3560 Training loss: 1.2677 3.0776 sec/batch\n",
      "Epoch 14/20  Iteration 2472/3560 Training loss: 1.2676 3.0916 sec/batch\n",
      "Epoch 14/20  Iteration 2473/3560 Training loss: 1.2674 3.0980 sec/batch\n",
      "Epoch 14/20  Iteration 2474/3560 Training loss: 1.2675 3.1907 sec/batch\n",
      "Epoch 14/20  Iteration 2475/3560 Training loss: 1.2676 3.1157 sec/batch\n",
      "Epoch 14/20  Iteration 2476/3560 Training loss: 1.2676 3.0751 sec/batch\n",
      "Epoch 14/20  Iteration 2477/3560 Training loss: 1.2675 3.2247 sec/batch\n",
      "Epoch 14/20  Iteration 2478/3560 Training loss: 1.2675 3.1614 sec/batch\n",
      "Epoch 14/20  Iteration 2479/3560 Training loss: 1.2675 3.1026 sec/batch\n",
      "Epoch 14/20  Iteration 2480/3560 Training loss: 1.2674 3.1079 sec/batch\n",
      "Epoch 14/20  Iteration 2481/3560 Training loss: 1.2675 3.1125 sec/batch\n",
      "Epoch 14/20  Iteration 2482/3560 Training loss: 1.2680 3.1278 sec/batch\n",
      "Epoch 14/20  Iteration 2483/3560 Training loss: 1.2680 3.1784 sec/batch\n",
      "Epoch 14/20  Iteration 2484/3560 Training loss: 1.2680 3.1751 sec/batch\n",
      "Epoch 14/20  Iteration 2485/3560 Training loss: 1.2680 3.1734 sec/batch\n",
      "Epoch 14/20  Iteration 2486/3560 Training loss: 1.2678 3.1864 sec/batch\n",
      "Epoch 14/20  Iteration 2487/3560 Training loss: 1.2680 3.1283 sec/batch\n",
      "Epoch 14/20  Iteration 2488/3560 Training loss: 1.2680 3.2057 sec/batch\n",
      "Epoch 14/20  Iteration 2489/3560 Training loss: 1.2681 3.1644 sec/batch\n",
      "Epoch 14/20  Iteration 2490/3560 Training loss: 1.2679 3.0788 sec/batch\n",
      "Epoch 14/20  Iteration 2491/3560 Training loss: 1.2678 3.1343 sec/batch\n",
      "Epoch 14/20  Iteration 2492/3560 Training loss: 1.2679 3.1186 sec/batch\n",
      "Epoch 15/20  Iteration 2493/3560 Training loss: 1.3828 3.1021 sec/batch\n",
      "Epoch 15/20  Iteration 2494/3560 Training loss: 1.3314 3.1799 sec/batch\n",
      "Epoch 15/20  Iteration 2495/3560 Training loss: 1.3105 3.1406 sec/batch\n",
      "Epoch 15/20  Iteration 2496/3560 Training loss: 1.3035 3.1919 sec/batch\n",
      "Epoch 15/20  Iteration 2497/3560 Training loss: 1.2921 3.1180 sec/batch\n",
      "Epoch 15/20  Iteration 2498/3560 Training loss: 1.2793 3.1781 sec/batch\n",
      "Epoch 15/20  Iteration 2499/3560 Training loss: 1.2795 3.1172 sec/batch\n",
      "Epoch 15/20  Iteration 2500/3560 Training loss: 1.2768 3.2058 sec/batch\n",
      "Epoch 15/20  Iteration 2501/3560 Training loss: 1.2751 3.0880 sec/batch\n",
      "Epoch 15/20  Iteration 2502/3560 Training loss: 1.2740 3.2202 sec/batch\n",
      "Epoch 15/20  Iteration 2503/3560 Training loss: 1.2707 3.1488 sec/batch\n",
      "Epoch 15/20  Iteration 2504/3560 Training loss: 1.2698 3.0826 sec/batch\n",
      "Epoch 15/20  Iteration 2505/3560 Training loss: 1.2696 3.1577 sec/batch\n",
      "Epoch 15/20  Iteration 2506/3560 Training loss: 1.2698 3.2656 sec/batch\n",
      "Epoch 15/20  Iteration 2507/3560 Training loss: 1.2683 3.1084 sec/batch\n",
      "Epoch 15/20  Iteration 2508/3560 Training loss: 1.2665 3.1470 sec/batch\n",
      "Epoch 15/20  Iteration 2509/3560 Training loss: 1.2669 3.2259 sec/batch\n",
      "Epoch 15/20  Iteration 2510/3560 Training loss: 1.2679 3.0292 sec/batch\n",
      "Epoch 15/20  Iteration 2511/3560 Training loss: 1.2672 3.2173 sec/batch\n",
      "Epoch 15/20  Iteration 2512/3560 Training loss: 1.2681 3.1579 sec/batch\n",
      "Epoch 15/20  Iteration 2513/3560 Training loss: 1.2671 3.1292 sec/batch\n",
      "Epoch 15/20  Iteration 2514/3560 Training loss: 1.2674 3.3828 sec/batch\n",
      "Epoch 15/20  Iteration 2515/3560 Training loss: 1.2664 3.1042 sec/batch\n",
      "Epoch 15/20  Iteration 2516/3560 Training loss: 1.2665 3.3471 sec/batch\n",
      "Epoch 15/20  Iteration 2517/3560 Training loss: 1.2665 3.1458 sec/batch\n",
      "Epoch 15/20  Iteration 2518/3560 Training loss: 1.2648 3.1306 sec/batch\n",
      "Epoch 15/20  Iteration 2519/3560 Training loss: 1.2637 3.1841 sec/batch\n",
      "Epoch 15/20  Iteration 2520/3560 Training loss: 1.2641 3.1601 sec/batch\n",
      "Epoch 15/20  Iteration 2521/3560 Training loss: 1.2641 3.1331 sec/batch\n",
      "Epoch 15/20  Iteration 2522/3560 Training loss: 1.2645 3.1168 sec/batch\n",
      "Epoch 15/20  Iteration 2523/3560 Training loss: 1.2638 3.2225 sec/batch\n",
      "Epoch 15/20  Iteration 2524/3560 Training loss: 1.2630 3.1961 sec/batch\n",
      "Epoch 15/20  Iteration 2525/3560 Training loss: 1.2630 3.2107 sec/batch\n",
      "Epoch 15/20  Iteration 2526/3560 Training loss: 1.2634 3.1715 sec/batch\n",
      "Epoch 15/20  Iteration 2527/3560 Training loss: 1.2631 3.1700 sec/batch\n",
      "Epoch 15/20  Iteration 2528/3560 Training loss: 1.2628 3.1540 sec/batch\n",
      "Epoch 15/20  Iteration 2529/3560 Training loss: 1.2621 3.1598 sec/batch\n",
      "Epoch 15/20  Iteration 2530/3560 Training loss: 1.2609 3.1558 sec/batch\n",
      "Epoch 15/20  Iteration 2531/3560 Training loss: 1.2599 3.0837 sec/batch\n",
      "Epoch 15/20  Iteration 2532/3560 Training loss: 1.2592 3.1433 sec/batch\n",
      "Epoch 15/20  Iteration 2533/3560 Training loss: 1.2585 3.1545 sec/batch\n",
      "Epoch 15/20  Iteration 2534/3560 Training loss: 1.2592 3.0971 sec/batch\n",
      "Epoch 15/20  Iteration 2535/3560 Training loss: 1.2588 3.1432 sec/batch\n",
      "Epoch 15/20  Iteration 2536/3560 Training loss: 1.2582 3.0863 sec/batch\n",
      "Epoch 15/20  Iteration 2537/3560 Training loss: 1.2583 3.1758 sec/batch\n",
      "Epoch 15/20  Iteration 2538/3560 Training loss: 1.2576 3.1536 sec/batch\n",
      "Epoch 15/20  Iteration 2539/3560 Training loss: 1.2570 3.1030 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2540/3560 Training loss: 1.2568 3.1475 sec/batch\n",
      "Epoch 15/20  Iteration 2541/3560 Training loss: 1.2566 3.1559 sec/batch\n",
      "Epoch 15/20  Iteration 2542/3560 Training loss: 1.2566 3.1763 sec/batch\n",
      "Epoch 15/20  Iteration 2543/3560 Training loss: 1.2560 3.2329 sec/batch\n",
      "Epoch 15/20  Iteration 2544/3560 Training loss: 1.2567 3.1541 sec/batch\n",
      "Epoch 15/20  Iteration 2545/3560 Training loss: 1.2566 3.1885 sec/batch\n",
      "Epoch 15/20  Iteration 2546/3560 Training loss: 1.2567 3.1377 sec/batch\n",
      "Epoch 15/20  Iteration 2547/3560 Training loss: 1.2565 3.1012 sec/batch\n",
      "Epoch 15/20  Iteration 2548/3560 Training loss: 1.2564 3.2045 sec/batch\n",
      "Epoch 15/20  Iteration 2549/3560 Training loss: 1.2566 3.1409 sec/batch\n",
      "Epoch 15/20  Iteration 2550/3560 Training loss: 1.2563 3.0638 sec/batch\n",
      "Epoch 15/20  Iteration 2551/3560 Training loss: 1.2558 3.2035 sec/batch\n",
      "Epoch 15/20  Iteration 2552/3560 Training loss: 1.2562 3.0877 sec/batch\n",
      "Epoch 15/20  Iteration 2553/3560 Training loss: 1.2562 3.0855 sec/batch\n",
      "Epoch 15/20  Iteration 2554/3560 Training loss: 1.2570 3.2416 sec/batch\n",
      "Epoch 15/20  Iteration 2555/3560 Training loss: 1.2573 3.0939 sec/batch\n",
      "Epoch 15/20  Iteration 2556/3560 Training loss: 1.2574 3.0850 sec/batch\n",
      "Epoch 15/20  Iteration 2557/3560 Training loss: 1.2573 3.1305 sec/batch\n",
      "Epoch 15/20  Iteration 2558/3560 Training loss: 1.2573 3.1593 sec/batch\n",
      "Epoch 15/20  Iteration 2559/3560 Training loss: 1.2576 3.1577 sec/batch\n",
      "Epoch 15/20  Iteration 2560/3560 Training loss: 1.2573 3.0696 sec/batch\n",
      "Epoch 15/20  Iteration 2561/3560 Training loss: 1.2574 3.1964 sec/batch\n",
      "Epoch 15/20  Iteration 2562/3560 Training loss: 1.2573 3.2521 sec/batch\n",
      "Epoch 15/20  Iteration 2563/3560 Training loss: 1.2580 3.1735 sec/batch\n",
      "Epoch 15/20  Iteration 2564/3560 Training loss: 1.2583 3.1973 sec/batch\n",
      "Epoch 15/20  Iteration 2565/3560 Training loss: 1.2588 3.2132 sec/batch\n",
      "Epoch 15/20  Iteration 2566/3560 Training loss: 1.2585 3.0770 sec/batch\n",
      "Epoch 15/20  Iteration 2567/3560 Training loss: 1.2584 3.1817 sec/batch\n",
      "Epoch 15/20  Iteration 2568/3560 Training loss: 1.2587 3.1687 sec/batch\n",
      "Epoch 15/20  Iteration 2569/3560 Training loss: 1.2585 3.1465 sec/batch\n",
      "Epoch 15/20  Iteration 2570/3560 Training loss: 1.2584 3.1198 sec/batch\n",
      "Epoch 15/20  Iteration 2571/3560 Training loss: 1.2578 3.2045 sec/batch\n",
      "Epoch 15/20  Iteration 2572/3560 Training loss: 1.2578 3.1157 sec/batch\n",
      "Epoch 15/20  Iteration 2573/3560 Training loss: 1.2573 3.2841 sec/batch\n",
      "Epoch 15/20  Iteration 2574/3560 Training loss: 1.2573 3.2595 sec/batch\n",
      "Epoch 15/20  Iteration 2575/3560 Training loss: 1.2568 3.1175 sec/batch\n",
      "Epoch 15/20  Iteration 2576/3560 Training loss: 1.2568 3.1277 sec/batch\n",
      "Epoch 15/20  Iteration 2577/3560 Training loss: 1.2565 3.1506 sec/batch\n",
      "Epoch 15/20  Iteration 2578/3560 Training loss: 1.2565 3.1939 sec/batch\n",
      "Epoch 15/20  Iteration 2579/3560 Training loss: 1.2562 3.1349 sec/batch\n",
      "Epoch 15/20  Iteration 2580/3560 Training loss: 1.2560 3.1603 sec/batch\n",
      "Epoch 15/20  Iteration 2581/3560 Training loss: 1.2557 3.1405 sec/batch\n",
      "Epoch 15/20  Iteration 2582/3560 Training loss: 1.2556 3.1828 sec/batch\n",
      "Epoch 15/20  Iteration 2583/3560 Training loss: 1.2553 3.2285 sec/batch\n",
      "Epoch 15/20  Iteration 2584/3560 Training loss: 1.2554 3.1729 sec/batch\n",
      "Epoch 15/20  Iteration 2585/3560 Training loss: 1.2550 3.1307 sec/batch\n",
      "Epoch 15/20  Iteration 2586/3560 Training loss: 1.2547 3.1416 sec/batch\n",
      "Epoch 15/20  Iteration 2587/3560 Training loss: 1.2544 3.1028 sec/batch\n",
      "Epoch 15/20  Iteration 2588/3560 Training loss: 1.2545 3.1313 sec/batch\n",
      "Epoch 15/20  Iteration 2589/3560 Training loss: 1.2544 3.2257 sec/batch\n",
      "Epoch 15/20  Iteration 2590/3560 Training loss: 1.2540 3.1464 sec/batch\n",
      "Epoch 15/20  Iteration 2591/3560 Training loss: 1.2535 3.1711 sec/batch\n",
      "Epoch 15/20  Iteration 2592/3560 Training loss: 1.2532 3.1576 sec/batch\n",
      "Epoch 15/20  Iteration 2593/3560 Training loss: 1.2532 3.1757 sec/batch\n",
      "Epoch 15/20  Iteration 2594/3560 Training loss: 1.2531 3.1243 sec/batch\n",
      "Epoch 15/20  Iteration 2595/3560 Training loss: 1.2529 3.1204 sec/batch\n",
      "Epoch 15/20  Iteration 2596/3560 Training loss: 1.2527 3.2614 sec/batch\n",
      "Epoch 15/20  Iteration 2597/3560 Training loss: 1.2526 3.1103 sec/batch\n",
      "Epoch 15/20  Iteration 2598/3560 Training loss: 1.2525 3.0662 sec/batch\n",
      "Epoch 15/20  Iteration 2599/3560 Training loss: 1.2525 3.2428 sec/batch\n",
      "Epoch 15/20  Iteration 2600/3560 Training loss: 1.2525 3.2077 sec/batch\n",
      "Epoch 15/20  Iteration 2601/3560 Training loss: 1.2523 3.0858 sec/batch\n",
      "Epoch 15/20  Iteration 2602/3560 Training loss: 1.2523 3.1279 sec/batch\n",
      "Epoch 15/20  Iteration 2603/3560 Training loss: 1.2522 3.2895 sec/batch\n",
      "Epoch 15/20  Iteration 2604/3560 Training loss: 1.2521 3.0480 sec/batch\n",
      "Epoch 15/20  Iteration 2605/3560 Training loss: 1.2521 3.1285 sec/batch\n",
      "Epoch 15/20  Iteration 2606/3560 Training loss: 1.2520 3.2399 sec/batch\n",
      "Epoch 15/20  Iteration 2607/3560 Training loss: 1.2517 3.1485 sec/batch\n",
      "Epoch 15/20  Iteration 2608/3560 Training loss: 1.2514 3.1484 sec/batch\n",
      "Epoch 15/20  Iteration 2609/3560 Training loss: 1.2514 3.2004 sec/batch\n",
      "Epoch 15/20  Iteration 2610/3560 Training loss: 1.2513 3.1602 sec/batch\n",
      "Epoch 15/20  Iteration 2611/3560 Training loss: 1.2512 3.1420 sec/batch\n",
      "Epoch 15/20  Iteration 2612/3560 Training loss: 1.2511 3.2861 sec/batch\n",
      "Epoch 15/20  Iteration 2613/3560 Training loss: 1.2509 3.1380 sec/batch\n",
      "Epoch 15/20  Iteration 2614/3560 Training loss: 1.2506 3.1008 sec/batch\n",
      "Epoch 15/20  Iteration 2615/3560 Training loss: 1.2501 3.1621 sec/batch\n",
      "Epoch 15/20  Iteration 2616/3560 Training loss: 1.2501 3.2170 sec/batch\n",
      "Epoch 15/20  Iteration 2617/3560 Training loss: 1.2500 3.0766 sec/batch\n",
      "Epoch 15/20  Iteration 2618/3560 Training loss: 1.2496 3.1534 sec/batch\n",
      "Epoch 15/20  Iteration 2619/3560 Training loss: 1.2496 3.1518 sec/batch\n",
      "Epoch 15/20  Iteration 2620/3560 Training loss: 1.2496 3.1249 sec/batch\n",
      "Epoch 15/20  Iteration 2621/3560 Training loss: 1.2494 3.1182 sec/batch\n",
      "Epoch 15/20  Iteration 2622/3560 Training loss: 1.2491 3.2938 sec/batch\n",
      "Epoch 15/20  Iteration 2623/3560 Training loss: 1.2487 3.1157 sec/batch\n",
      "Epoch 15/20  Iteration 2624/3560 Training loss: 1.2485 3.1071 sec/batch\n",
      "Epoch 15/20  Iteration 2625/3560 Training loss: 1.2487 3.2567 sec/batch\n",
      "Epoch 15/20  Iteration 2626/3560 Training loss: 1.2487 3.0577 sec/batch\n",
      "Epoch 15/20  Iteration 2627/3560 Training loss: 1.2487 3.1428 sec/batch\n",
      "Epoch 15/20  Iteration 2628/3560 Training loss: 1.2487 3.1821 sec/batch\n",
      "Epoch 15/20  Iteration 2629/3560 Training loss: 1.2488 3.1323 sec/batch\n",
      "Epoch 15/20  Iteration 2630/3560 Training loss: 1.2489 3.3328 sec/batch\n",
      "Epoch 15/20  Iteration 2631/3560 Training loss: 1.2489 3.2346 sec/batch\n",
      "Epoch 15/20  Iteration 2632/3560 Training loss: 1.2489 3.1908 sec/batch\n",
      "Epoch 15/20  Iteration 2633/3560 Training loss: 1.2493 3.0901 sec/batch\n",
      "Epoch 15/20  Iteration 2634/3560 Training loss: 1.2493 3.1980 sec/batch\n",
      "Epoch 15/20  Iteration 2635/3560 Training loss: 1.2492 3.2364 sec/batch\n",
      "Epoch 15/20  Iteration 2636/3560 Training loss: 1.2495 3.1236 sec/batch\n",
      "Epoch 15/20  Iteration 2637/3560 Training loss: 1.2494 3.2058 sec/batch\n",
      "Epoch 15/20  Iteration 2638/3560 Training loss: 1.2495 3.4190 sec/batch\n",
      "Epoch 15/20  Iteration 2639/3560 Training loss: 1.2495 3.1923 sec/batch\n",
      "Epoch 15/20  Iteration 2640/3560 Training loss: 1.2498 3.2286 sec/batch\n",
      "Epoch 15/20  Iteration 2641/3560 Training loss: 1.2500 3.3333 sec/batch\n",
      "Epoch 15/20  Iteration 2642/3560 Training loss: 1.2498 3.1593 sec/batch\n",
      "Epoch 15/20  Iteration 2643/3560 Training loss: 1.2496 3.1216 sec/batch\n",
      "Epoch 15/20  Iteration 2644/3560 Training loss: 1.2494 3.3149 sec/batch\n",
      "Epoch 15/20  Iteration 2645/3560 Training loss: 1.2494 3.1574 sec/batch\n",
      "Epoch 15/20  Iteration 2646/3560 Training loss: 1.2494 3.1081 sec/batch\n",
      "Epoch 15/20  Iteration 2647/3560 Training loss: 1.2494 3.1774 sec/batch\n",
      "Epoch 15/20  Iteration 2648/3560 Training loss: 1.2494 3.1296 sec/batch\n",
      "Epoch 15/20  Iteration 2649/3560 Training loss: 1.2495 3.1357 sec/batch\n",
      "Epoch 15/20  Iteration 2650/3560 Training loss: 1.2493 3.1715 sec/batch\n",
      "Epoch 15/20  Iteration 2651/3560 Training loss: 1.2491 3.1714 sec/batch\n",
      "Epoch 15/20  Iteration 2652/3560 Training loss: 1.2492 3.0903 sec/batch\n",
      "Epoch 15/20  Iteration 2653/3560 Training loss: 1.2494 3.1266 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Iteration 2654/3560 Training loss: 1.2494 3.1776 sec/batch\n",
      "Epoch 15/20  Iteration 2655/3560 Training loss: 1.2494 3.3144 sec/batch\n",
      "Epoch 15/20  Iteration 2656/3560 Training loss: 1.2494 3.1521 sec/batch\n",
      "Epoch 15/20  Iteration 2657/3560 Training loss: 1.2494 3.2186 sec/batch\n",
      "Epoch 15/20  Iteration 2658/3560 Training loss: 1.2493 3.0884 sec/batch\n",
      "Epoch 15/20  Iteration 2659/3560 Training loss: 1.2495 3.1517 sec/batch\n",
      "Epoch 15/20  Iteration 2660/3560 Training loss: 1.2499 3.2828 sec/batch\n",
      "Epoch 15/20  Iteration 2661/3560 Training loss: 1.2499 3.1084 sec/batch\n",
      "Epoch 15/20  Iteration 2662/3560 Training loss: 1.2500 3.1793 sec/batch\n",
      "Epoch 15/20  Iteration 2663/3560 Training loss: 1.2499 3.1948 sec/batch\n",
      "Epoch 15/20  Iteration 2664/3560 Training loss: 1.2498 3.1008 sec/batch\n",
      "Epoch 15/20  Iteration 2665/3560 Training loss: 1.2499 3.1987 sec/batch\n",
      "Epoch 15/20  Iteration 2666/3560 Training loss: 1.2499 3.1967 sec/batch\n",
      "Epoch 15/20  Iteration 2667/3560 Training loss: 1.2499 3.2566 sec/batch\n",
      "Epoch 15/20  Iteration 2668/3560 Training loss: 1.2498 3.8839 sec/batch\n",
      "Epoch 15/20  Iteration 2669/3560 Training loss: 1.2497 3.8064 sec/batch\n",
      "Epoch 15/20  Iteration 2670/3560 Training loss: 1.2498 3.6415 sec/batch\n",
      "Epoch 16/20  Iteration 2671/3560 Training loss: 1.3751 3.1648 sec/batch\n",
      "Epoch 16/20  Iteration 2672/3560 Training loss: 1.3179 3.2778 sec/batch\n",
      "Epoch 16/20  Iteration 2673/3560 Training loss: 1.2927 3.4289 sec/batch\n",
      "Epoch 16/20  Iteration 2674/3560 Training loss: 1.2874 3.3536 sec/batch\n",
      "Epoch 16/20  Iteration 2675/3560 Training loss: 1.2765 3.3182 sec/batch\n",
      "Epoch 16/20  Iteration 2676/3560 Training loss: 1.2633 3.1901 sec/batch\n",
      "Epoch 16/20  Iteration 2677/3560 Training loss: 1.2619 3.0405 sec/batch\n",
      "Epoch 16/20  Iteration 2678/3560 Training loss: 1.2590 3.2852 sec/batch\n",
      "Epoch 16/20  Iteration 2679/3560 Training loss: 1.2577 3.1937 sec/batch\n",
      "Epoch 16/20  Iteration 2680/3560 Training loss: 1.2566 3.0547 sec/batch\n",
      "Epoch 16/20  Iteration 2681/3560 Training loss: 1.2523 3.1901 sec/batch\n",
      "Epoch 16/20  Iteration 2682/3560 Training loss: 1.2512 3.1740 sec/batch\n",
      "Epoch 16/20  Iteration 2683/3560 Training loss: 1.2507 3.1596 sec/batch\n",
      "Epoch 16/20  Iteration 2684/3560 Training loss: 1.2512 3.1636 sec/batch\n",
      "Epoch 16/20  Iteration 2685/3560 Training loss: 1.2497 3.1927 sec/batch\n",
      "Epoch 16/20  Iteration 2686/3560 Training loss: 1.2483 3.3332 sec/batch\n",
      "Epoch 16/20  Iteration 2687/3560 Training loss: 1.2482 3.1178 sec/batch\n",
      "Epoch 16/20  Iteration 2688/3560 Training loss: 1.2485 3.1681 sec/batch\n",
      "Epoch 16/20  Iteration 2689/3560 Training loss: 1.2481 3.1616 sec/batch\n",
      "Epoch 16/20  Iteration 2690/3560 Training loss: 1.2488 3.1681 sec/batch\n",
      "Epoch 16/20  Iteration 2691/3560 Training loss: 1.2481 3.0802 sec/batch\n",
      "Epoch 16/20  Iteration 2692/3560 Training loss: 1.2489 3.1934 sec/batch\n",
      "Epoch 16/20  Iteration 2693/3560 Training loss: 1.2483 3.1479 sec/batch\n",
      "Epoch 16/20  Iteration 2694/3560 Training loss: 1.2487 3.1902 sec/batch\n",
      "Epoch 16/20  Iteration 2695/3560 Training loss: 1.2487 3.1466 sec/batch\n",
      "Epoch 16/20  Iteration 2696/3560 Training loss: 1.2471 3.1391 sec/batch\n",
      "Epoch 16/20  Iteration 2697/3560 Training loss: 1.2459 3.1189 sec/batch\n",
      "Epoch 16/20  Iteration 2698/3560 Training loss: 1.2464 3.1496 sec/batch\n",
      "Epoch 16/20  Iteration 2699/3560 Training loss: 1.2464 3.2048 sec/batch\n",
      "Epoch 16/20  Iteration 2700/3560 Training loss: 1.2466 3.1998 sec/batch\n",
      "Epoch 16/20  Iteration 2701/3560 Training loss: 1.2460 3.1308 sec/batch\n",
      "Epoch 16/20  Iteration 2702/3560 Training loss: 1.2451 3.1164 sec/batch\n",
      "Epoch 16/20  Iteration 2703/3560 Training loss: 1.2452 3.1593 sec/batch\n",
      "Epoch 16/20  Iteration 2704/3560 Training loss: 1.2456 3.2631 sec/batch\n",
      "Epoch 16/20  Iteration 2705/3560 Training loss: 1.2454 3.1029 sec/batch\n",
      "Epoch 16/20  Iteration 2706/3560 Training loss: 1.2455 3.2736 sec/batch\n",
      "Epoch 16/20  Iteration 2707/3560 Training loss: 1.2450 3.3439 sec/batch\n",
      "Epoch 16/20  Iteration 2708/3560 Training loss: 1.2441 3.1661 sec/batch\n",
      "Epoch 16/20  Iteration 2709/3560 Training loss: 1.2427 3.2503 sec/batch\n",
      "Epoch 16/20  Iteration 2710/3560 Training loss: 1.2422 3.1800 sec/batch\n",
      "Epoch 16/20  Iteration 2711/3560 Training loss: 1.2418 3.1979 sec/batch\n",
      "Epoch 16/20  Iteration 2712/3560 Training loss: 1.2426 3.1817 sec/batch\n",
      "Epoch 16/20  Iteration 2713/3560 Training loss: 1.2423 3.1420 sec/batch\n",
      "Epoch 16/20  Iteration 2714/3560 Training loss: 1.2417 3.1165 sec/batch\n",
      "Epoch 16/20  Iteration 2715/3560 Training loss: 1.2417 3.2487 sec/batch\n",
      "Epoch 16/20  Iteration 2716/3560 Training loss: 1.2410 3.2248 sec/batch\n",
      "Epoch 16/20  Iteration 2717/3560 Training loss: 1.2408 3.1785 sec/batch\n",
      "Epoch 16/20  Iteration 2718/3560 Training loss: 1.2405 3.1868 sec/batch\n",
      "Epoch 16/20  Iteration 2719/3560 Training loss: 1.2404 3.1688 sec/batch\n",
      "Epoch 16/20  Iteration 2720/3560 Training loss: 1.2407 3.3052 sec/batch\n",
      "Epoch 16/20  Iteration 2721/3560 Training loss: 1.2402 3.0906 sec/batch\n",
      "Epoch 16/20  Iteration 2722/3560 Training loss: 1.2409 3.1693 sec/batch\n",
      "Epoch 16/20  Iteration 2723/3560 Training loss: 1.2408 3.1563 sec/batch\n",
      "Epoch 16/20  Iteration 2724/3560 Training loss: 1.2410 3.2216 sec/batch\n",
      "Epoch 16/20  Iteration 2725/3560 Training loss: 1.2409 3.1647 sec/batch\n",
      "Epoch 16/20  Iteration 2726/3560 Training loss: 1.2409 3.1692 sec/batch\n",
      "Epoch 16/20  Iteration 2727/3560 Training loss: 1.2413 3.2522 sec/batch\n",
      "Epoch 16/20  Iteration 2728/3560 Training loss: 1.2412 3.1113 sec/batch\n",
      "Epoch 16/20  Iteration 2729/3560 Training loss: 1.2405 3.0802 sec/batch\n",
      "Epoch 16/20  Iteration 2730/3560 Training loss: 1.2409 3.1721 sec/batch\n",
      "Epoch 16/20  Iteration 2731/3560 Training loss: 1.2409 3.1283 sec/batch\n",
      "Epoch 16/20  Iteration 2732/3560 Training loss: 1.2417 3.2568 sec/batch\n",
      "Epoch 16/20  Iteration 2733/3560 Training loss: 1.2420 3.1554 sec/batch\n",
      "Epoch 16/20  Iteration 2734/3560 Training loss: 1.2422 3.1366 sec/batch\n",
      "Epoch 16/20  Iteration 2735/3560 Training loss: 1.2421 3.2075 sec/batch\n",
      "Epoch 16/20  Iteration 2736/3560 Training loss: 1.2421 3.1182 sec/batch\n",
      "Epoch 16/20  Iteration 2737/3560 Training loss: 1.2424 3.1557 sec/batch\n",
      "Epoch 16/20  Iteration 2738/3560 Training loss: 1.2423 3.2491 sec/batch\n",
      "Epoch 16/20  Iteration 2739/3560 Training loss: 1.2423 3.1534 sec/batch\n",
      "Epoch 16/20  Iteration 2740/3560 Training loss: 1.2422 3.0728 sec/batch\n",
      "Epoch 16/20  Iteration 2741/3560 Training loss: 1.2427 3.1848 sec/batch\n",
      "Epoch 16/20  Iteration 2742/3560 Training loss: 1.2430 3.1673 sec/batch\n",
      "Epoch 16/20  Iteration 2743/3560 Training loss: 1.2434 3.4188 sec/batch\n",
      "Epoch 16/20  Iteration 2744/3560 Training loss: 1.2431 3.2085 sec/batch\n",
      "Epoch 16/20  Iteration 2745/3560 Training loss: 1.2430 3.1665 sec/batch\n",
      "Epoch 16/20  Iteration 2746/3560 Training loss: 1.2431 3.2245 sec/batch\n",
      "Epoch 16/20  Iteration 2747/3560 Training loss: 1.2430 3.1616 sec/batch\n",
      "Epoch 16/20  Iteration 2748/3560 Training loss: 1.2428 3.0706 sec/batch\n",
      "Epoch 16/20  Iteration 2749/3560 Training loss: 1.2423 3.2264 sec/batch\n",
      "Epoch 16/20  Iteration 2750/3560 Training loss: 1.2424 3.1124 sec/batch\n",
      "Epoch 16/20  Iteration 2751/3560 Training loss: 1.2420 3.1431 sec/batch\n",
      "Epoch 16/20  Iteration 2752/3560 Training loss: 1.2420 3.2067 sec/batch\n",
      "Epoch 16/20  Iteration 2753/3560 Training loss: 1.2415 3.1289 sec/batch\n",
      "Epoch 16/20  Iteration 2754/3560 Training loss: 1.2415 3.1672 sec/batch\n",
      "Epoch 16/20  Iteration 2755/3560 Training loss: 1.2412 3.4624 sec/batch\n",
      "Epoch 16/20  Iteration 2756/3560 Training loss: 1.2411 3.1476 sec/batch\n",
      "Epoch 16/20  Iteration 2757/3560 Training loss: 1.2408 3.1242 sec/batch\n",
      "Epoch 16/20  Iteration 2758/3560 Training loss: 1.2404 3.2766 sec/batch\n",
      "Epoch 16/20  Iteration 2759/3560 Training loss: 1.2401 3.0768 sec/batch\n",
      "Epoch 16/20  Iteration 2760/3560 Training loss: 1.2401 3.1502 sec/batch\n",
      "Epoch 16/20  Iteration 2761/3560 Training loss: 1.2398 3.2494 sec/batch\n",
      "Epoch 16/20  Iteration 2762/3560 Training loss: 1.2398 3.0705 sec/batch\n",
      "Epoch 16/20  Iteration 2763/3560 Training loss: 1.2394 3.1047 sec/batch\n",
      "Epoch 16/20  Iteration 2764/3560 Training loss: 1.2391 3.0448 sec/batch\n",
      "Epoch 16/20  Iteration 2765/3560 Training loss: 1.2388 3.1936 sec/batch\n",
      "Epoch 16/20  Iteration 2766/3560 Training loss: 1.2389 3.2261 sec/batch\n",
      "Epoch 16/20  Iteration 2767/3560 Training loss: 1.2388 3.1349 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Iteration 2768/3560 Training loss: 1.2386 3.1802 sec/batch\n",
      "Epoch 16/20  Iteration 2769/3560 Training loss: 1.2383 3.1663 sec/batch\n",
      "Epoch 16/20  Iteration 2770/3560 Training loss: 1.2380 3.1025 sec/batch\n",
      "Epoch 16/20  Iteration 2771/3560 Training loss: 1.2379 3.1887 sec/batch\n",
      "Epoch 16/20  Iteration 2772/3560 Training loss: 1.2377 3.1520 sec/batch\n",
      "Epoch 16/20  Iteration 2773/3560 Training loss: 1.2377 3.1662 sec/batch\n",
      "Epoch 16/20  Iteration 2774/3560 Training loss: 1.2376 3.0745 sec/batch\n",
      "Epoch 16/20  Iteration 2775/3560 Training loss: 1.2375 3.2380 sec/batch\n",
      "Epoch 16/20  Iteration 2776/3560 Training loss: 1.2375 3.1152 sec/batch\n",
      "Epoch 16/20  Iteration 2777/3560 Training loss: 1.2374 3.0928 sec/batch\n",
      "Epoch 16/20  Iteration 2778/3560 Training loss: 1.2374 3.1547 sec/batch\n",
      "Epoch 16/20  Iteration 2779/3560 Training loss: 1.2372 3.1383 sec/batch\n",
      "Epoch 16/20  Iteration 2780/3560 Training loss: 1.2372 3.1020 sec/batch\n",
      "Epoch 16/20  Iteration 2781/3560 Training loss: 1.2370 3.2165 sec/batch\n",
      "Epoch 16/20  Iteration 2782/3560 Training loss: 1.2369 3.1320 sec/batch\n",
      "Epoch 16/20  Iteration 2783/3560 Training loss: 1.2367 3.1160 sec/batch\n",
      "Epoch 16/20  Iteration 2784/3560 Training loss: 1.2366 3.2162 sec/batch\n",
      "Epoch 16/20  Iteration 2785/3560 Training loss: 1.2363 3.0562 sec/batch\n",
      "Epoch 16/20  Iteration 2786/3560 Training loss: 1.2360 3.1071 sec/batch\n",
      "Epoch 16/20  Iteration 2787/3560 Training loss: 1.2360 3.2335 sec/batch\n",
      "Epoch 16/20  Iteration 2788/3560 Training loss: 1.2360 3.0984 sec/batch\n",
      "Epoch 16/20  Iteration 2789/3560 Training loss: 1.2359 3.1463 sec/batch\n",
      "Epoch 16/20  Iteration 2790/3560 Training loss: 1.2358 3.1985 sec/batch\n",
      "Epoch 16/20  Iteration 2791/3560 Training loss: 1.2356 3.1657 sec/batch\n",
      "Epoch 16/20  Iteration 2792/3560 Training loss: 1.2352 3.1743 sec/batch\n",
      "Epoch 16/20  Iteration 2793/3560 Training loss: 1.2349 3.1454 sec/batch\n",
      "Epoch 16/20  Iteration 2794/3560 Training loss: 1.2348 3.1698 sec/batch\n",
      "Epoch 16/20  Iteration 2795/3560 Training loss: 1.2346 3.2035 sec/batch\n",
      "Epoch 16/20  Iteration 2796/3560 Training loss: 1.2343 3.1129 sec/batch\n",
      "Epoch 16/20  Iteration 2797/3560 Training loss: 1.2343 3.1829 sec/batch\n",
      "Epoch 16/20  Iteration 2798/3560 Training loss: 1.2343 3.1378 sec/batch\n",
      "Epoch 16/20  Iteration 2799/3560 Training loss: 1.2341 3.3631 sec/batch\n",
      "Epoch 16/20  Iteration 2800/3560 Training loss: 1.2337 3.2646 sec/batch\n",
      "Epoch 16/20  Iteration 2801/3560 Training loss: 1.2333 3.2096 sec/batch\n",
      "Epoch 16/20  Iteration 2802/3560 Training loss: 1.2332 3.0455 sec/batch\n",
      "Epoch 16/20  Iteration 2803/3560 Training loss: 1.2333 3.1787 sec/batch\n",
      "Epoch 16/20  Iteration 2804/3560 Training loss: 1.2334 3.1388 sec/batch\n",
      "Epoch 16/20  Iteration 2805/3560 Training loss: 1.2334 3.0118 sec/batch\n",
      "Epoch 16/20  Iteration 2806/3560 Training loss: 1.2334 3.2498 sec/batch\n",
      "Epoch 16/20  Iteration 2807/3560 Training loss: 1.2335 3.2157 sec/batch\n",
      "Epoch 16/20  Iteration 2808/3560 Training loss: 1.2337 3.1112 sec/batch\n",
      "Epoch 16/20  Iteration 2809/3560 Training loss: 1.2338 3.1602 sec/batch\n",
      "Epoch 16/20  Iteration 2810/3560 Training loss: 1.2338 3.1549 sec/batch\n",
      "Epoch 16/20  Iteration 2811/3560 Training loss: 1.2342 3.1459 sec/batch\n",
      "Epoch 16/20  Iteration 2812/3560 Training loss: 1.2343 3.1550 sec/batch\n",
      "Epoch 16/20  Iteration 2813/3560 Training loss: 1.2343 3.2524 sec/batch\n",
      "Epoch 16/20  Iteration 2814/3560 Training loss: 1.2345 3.1380 sec/batch\n",
      "Epoch 16/20  Iteration 2815/3560 Training loss: 1.2344 3.1400 sec/batch\n",
      "Epoch 16/20  Iteration 2816/3560 Training loss: 1.2346 3.1758 sec/batch\n",
      "Epoch 16/20  Iteration 2817/3560 Training loss: 1.2346 3.1455 sec/batch\n",
      "Epoch 16/20  Iteration 2818/3560 Training loss: 1.2348 3.1506 sec/batch\n",
      "Epoch 16/20  Iteration 2819/3560 Training loss: 1.2349 3.1352 sec/batch\n",
      "Epoch 16/20  Iteration 2820/3560 Training loss: 1.2348 3.1470 sec/batch\n",
      "Epoch 16/20  Iteration 2821/3560 Training loss: 1.2345 3.1793 sec/batch\n",
      "Epoch 16/20  Iteration 2822/3560 Training loss: 1.2343 3.1189 sec/batch\n",
      "Epoch 16/20  Iteration 2823/3560 Training loss: 1.2344 3.2460 sec/batch\n",
      "Epoch 16/20  Iteration 2824/3560 Training loss: 1.2343 3.1715 sec/batch\n",
      "Epoch 16/20  Iteration 2825/3560 Training loss: 1.2342 3.1246 sec/batch\n",
      "Epoch 16/20  Iteration 2826/3560 Training loss: 1.2342 3.2210 sec/batch\n",
      "Epoch 16/20  Iteration 2827/3560 Training loss: 1.2343 3.1130 sec/batch\n",
      "Epoch 16/20  Iteration 2828/3560 Training loss: 1.2342 3.2083 sec/batch\n",
      "Epoch 16/20  Iteration 2829/3560 Training loss: 1.2340 3.2138 sec/batch\n",
      "Epoch 16/20  Iteration 2830/3560 Training loss: 1.2341 3.1250 sec/batch\n",
      "Epoch 16/20  Iteration 2831/3560 Training loss: 1.2343 3.1582 sec/batch\n",
      "Epoch 16/20  Iteration 2832/3560 Training loss: 1.2343 3.2506 sec/batch\n",
      "Epoch 16/20  Iteration 2833/3560 Training loss: 1.2343 3.0996 sec/batch\n",
      "Epoch 16/20  Iteration 2834/3560 Training loss: 1.2343 3.1439 sec/batch\n",
      "Epoch 16/20  Iteration 2835/3560 Training loss: 1.2342 3.2214 sec/batch\n",
      "Epoch 16/20  Iteration 2836/3560 Training loss: 1.2341 3.1741 sec/batch\n",
      "Epoch 16/20  Iteration 2837/3560 Training loss: 1.2343 3.0416 sec/batch\n",
      "Epoch 16/20  Iteration 2838/3560 Training loss: 1.2347 3.1573 sec/batch\n",
      "Epoch 16/20  Iteration 2839/3560 Training loss: 1.2348 3.1649 sec/batch\n",
      "Epoch 16/20  Iteration 2840/3560 Training loss: 1.2348 3.0785 sec/batch\n",
      "Epoch 16/20  Iteration 2841/3560 Training loss: 1.2348 3.1323 sec/batch\n",
      "Epoch 16/20  Iteration 2842/3560 Training loss: 1.2347 3.2591 sec/batch\n",
      "Epoch 16/20  Iteration 2843/3560 Training loss: 1.2348 3.0965 sec/batch\n",
      "Epoch 16/20  Iteration 2844/3560 Training loss: 1.2347 3.1117 sec/batch\n",
      "Epoch 16/20  Iteration 2845/3560 Training loss: 1.2348 3.1860 sec/batch\n",
      "Epoch 16/20  Iteration 2846/3560 Training loss: 1.2347 3.1451 sec/batch\n",
      "Epoch 16/20  Iteration 2847/3560 Training loss: 1.2345 3.1130 sec/batch\n",
      "Epoch 16/20  Iteration 2848/3560 Training loss: 1.2347 3.1975 sec/batch\n",
      "Epoch 17/20  Iteration 2849/3560 Training loss: 1.3335 3.1390 sec/batch\n",
      "Epoch 17/20  Iteration 2850/3560 Training loss: 1.2939 3.1892 sec/batch\n",
      "Epoch 17/20  Iteration 2851/3560 Training loss: 1.2731 3.1151 sec/batch\n",
      "Epoch 17/20  Iteration 2852/3560 Training loss: 1.2686 3.2586 sec/batch\n",
      "Epoch 17/20  Iteration 2853/3560 Training loss: 1.2571 3.1369 sec/batch\n",
      "Epoch 17/20  Iteration 2854/3560 Training loss: 1.2462 3.3164 sec/batch\n",
      "Epoch 17/20  Iteration 2855/3560 Training loss: 1.2448 3.3194 sec/batch\n",
      "Epoch 17/20  Iteration 2856/3560 Training loss: 1.2417 3.4794 sec/batch\n",
      "Epoch 17/20  Iteration 2857/3560 Training loss: 1.2415 3.2393 sec/batch\n",
      "Epoch 17/20  Iteration 2858/3560 Training loss: 1.2403 3.2177 sec/batch\n",
      "Epoch 17/20  Iteration 2859/3560 Training loss: 1.2368 3.0785 sec/batch\n",
      "Epoch 17/20  Iteration 2860/3560 Training loss: 1.2371 3.1164 sec/batch\n",
      "Epoch 17/20  Iteration 2861/3560 Training loss: 1.2378 3.2044 sec/batch\n",
      "Epoch 17/20  Iteration 2862/3560 Training loss: 1.2376 3.1103 sec/batch\n",
      "Epoch 17/20  Iteration 2863/3560 Training loss: 1.2361 3.2359 sec/batch\n",
      "Epoch 17/20  Iteration 2864/3560 Training loss: 1.2349 3.2454 sec/batch\n",
      "Epoch 17/20  Iteration 2865/3560 Training loss: 1.2349 3.1699 sec/batch\n",
      "Epoch 17/20  Iteration 2866/3560 Training loss: 1.2356 3.1632 sec/batch\n",
      "Epoch 17/20  Iteration 2867/3560 Training loss: 1.2352 3.2388 sec/batch\n",
      "Epoch 17/20  Iteration 2868/3560 Training loss: 1.2364 3.1110 sec/batch\n",
      "Epoch 17/20  Iteration 2869/3560 Training loss: 1.2361 3.1438 sec/batch\n",
      "Epoch 17/20  Iteration 2870/3560 Training loss: 1.2366 3.2337 sec/batch\n",
      "Epoch 17/20  Iteration 2871/3560 Training loss: 1.2360 3.2348 sec/batch\n",
      "Epoch 17/20  Iteration 2872/3560 Training loss: 1.2363 3.0841 sec/batch\n",
      "Epoch 17/20  Iteration 2873/3560 Training loss: 1.2359 3.1363 sec/batch\n",
      "Epoch 17/20  Iteration 2874/3560 Training loss: 1.2340 3.3148 sec/batch\n",
      "Epoch 17/20  Iteration 2875/3560 Training loss: 1.2324 3.1827 sec/batch\n",
      "Epoch 17/20  Iteration 2876/3560 Training loss: 1.2329 3.1347 sec/batch\n",
      "Epoch 17/20  Iteration 2877/3560 Training loss: 1.2330 3.2222 sec/batch\n",
      "Epoch 17/20  Iteration 2878/3560 Training loss: 1.2331 3.0523 sec/batch\n",
      "Epoch 17/20  Iteration 2879/3560 Training loss: 1.2322 3.1873 sec/batch\n",
      "Epoch 17/20  Iteration 2880/3560 Training loss: 1.2312 3.2598 sec/batch\n",
      "Epoch 17/20  Iteration 2881/3560 Training loss: 1.2313 3.0677 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 2882/3560 Training loss: 1.2314 3.1459 sec/batch\n",
      "Epoch 17/20  Iteration 2883/3560 Training loss: 1.2311 3.2066 sec/batch\n",
      "Epoch 17/20  Iteration 2884/3560 Training loss: 1.2311 3.1337 sec/batch\n",
      "Epoch 17/20  Iteration 2885/3560 Training loss: 1.2307 3.0968 sec/batch\n",
      "Epoch 17/20  Iteration 2886/3560 Training loss: 1.2295 3.2565 sec/batch\n",
      "Epoch 17/20  Iteration 2887/3560 Training loss: 1.2286 3.1831 sec/batch\n",
      "Epoch 17/20  Iteration 2888/3560 Training loss: 1.2280 3.1104 sec/batch\n",
      "Epoch 17/20  Iteration 2889/3560 Training loss: 1.2275 3.2119 sec/batch\n",
      "Epoch 17/20  Iteration 2890/3560 Training loss: 1.2281 3.2048 sec/batch\n",
      "Epoch 17/20  Iteration 2891/3560 Training loss: 1.2276 3.1118 sec/batch\n",
      "Epoch 17/20  Iteration 2892/3560 Training loss: 1.2269 3.2067 sec/batch\n",
      "Epoch 17/20  Iteration 2893/3560 Training loss: 1.2271 3.1387 sec/batch\n",
      "Epoch 17/20  Iteration 2894/3560 Training loss: 1.2265 3.1911 sec/batch\n",
      "Epoch 17/20  Iteration 2895/3560 Training loss: 1.2262 3.1060 sec/batch\n",
      "Epoch 17/20  Iteration 2896/3560 Training loss: 1.2257 3.1538 sec/batch\n",
      "Epoch 17/20  Iteration 2897/3560 Training loss: 1.2258 3.2145 sec/batch\n",
      "Epoch 17/20  Iteration 2898/3560 Training loss: 1.2260 3.1728 sec/batch\n",
      "Epoch 17/20  Iteration 2899/3560 Training loss: 1.2255 3.2319 sec/batch\n",
      "Epoch 17/20  Iteration 2900/3560 Training loss: 1.2262 3.1294 sec/batch\n",
      "Epoch 17/20  Iteration 2901/3560 Training loss: 1.2262 3.1546 sec/batch\n",
      "Epoch 17/20  Iteration 2902/3560 Training loss: 1.2265 3.2292 sec/batch\n",
      "Epoch 17/20  Iteration 2903/3560 Training loss: 1.2264 3.1329 sec/batch\n",
      "Epoch 17/20  Iteration 2904/3560 Training loss: 1.2264 3.2013 sec/batch\n",
      "Epoch 17/20  Iteration 2905/3560 Training loss: 1.2266 3.2419 sec/batch\n",
      "Epoch 17/20  Iteration 2906/3560 Training loss: 1.2264 3.0875 sec/batch\n",
      "Epoch 17/20  Iteration 2907/3560 Training loss: 1.2257 3.1706 sec/batch\n",
      "Epoch 17/20  Iteration 2908/3560 Training loss: 1.2262 3.1518 sec/batch\n",
      "Epoch 17/20  Iteration 2909/3560 Training loss: 1.2261 3.1534 sec/batch\n",
      "Epoch 17/20  Iteration 2910/3560 Training loss: 1.2271 3.1051 sec/batch\n",
      "Epoch 17/20  Iteration 2911/3560 Training loss: 1.2273 3.1607 sec/batch\n",
      "Epoch 17/20  Iteration 2912/3560 Training loss: 1.2274 3.2242 sec/batch\n",
      "Epoch 17/20  Iteration 2913/3560 Training loss: 1.2274 3.3461 sec/batch\n",
      "Epoch 17/20  Iteration 2914/3560 Training loss: 1.2276 3.2230 sec/batch\n",
      "Epoch 17/20  Iteration 2915/3560 Training loss: 1.2278 3.2582 sec/batch\n",
      "Epoch 17/20  Iteration 2916/3560 Training loss: 1.2276 3.0832 sec/batch\n",
      "Epoch 17/20  Iteration 2917/3560 Training loss: 1.2277 3.1491 sec/batch\n",
      "Epoch 17/20  Iteration 2918/3560 Training loss: 1.2275 3.2585 sec/batch\n",
      "Epoch 17/20  Iteration 2919/3560 Training loss: 1.2281 3.1324 sec/batch\n",
      "Epoch 17/20  Iteration 2920/3560 Training loss: 1.2283 3.1257 sec/batch\n",
      "Epoch 17/20  Iteration 2921/3560 Training loss: 1.2288 3.1584 sec/batch\n",
      "Epoch 17/20  Iteration 2922/3560 Training loss: 1.2286 3.2762 sec/batch\n",
      "Epoch 17/20  Iteration 2923/3560 Training loss: 1.2286 3.1952 sec/batch\n",
      "Epoch 17/20  Iteration 2924/3560 Training loss: 1.2287 3.1342 sec/batch\n",
      "Epoch 17/20  Iteration 2925/3560 Training loss: 1.2286 3.1277 sec/batch\n",
      "Epoch 17/20  Iteration 2926/3560 Training loss: 1.2284 3.0772 sec/batch\n",
      "Epoch 17/20  Iteration 2927/3560 Training loss: 1.2277 3.1487 sec/batch\n",
      "Epoch 17/20  Iteration 2928/3560 Training loss: 1.2277 3.3024 sec/batch\n",
      "Epoch 17/20  Iteration 2929/3560 Training loss: 1.2272 3.1299 sec/batch\n",
      "Epoch 17/20  Iteration 2930/3560 Training loss: 1.2271 3.1486 sec/batch\n",
      "Epoch 17/20  Iteration 2931/3560 Training loss: 1.2267 3.2223 sec/batch\n",
      "Epoch 17/20  Iteration 2932/3560 Training loss: 1.2268 3.1297 sec/batch\n",
      "Epoch 17/20  Iteration 2933/3560 Training loss: 1.2266 3.1278 sec/batch\n",
      "Epoch 17/20  Iteration 2934/3560 Training loss: 1.2266 3.1580 sec/batch\n",
      "Epoch 17/20  Iteration 2935/3560 Training loss: 1.2265 3.1478 sec/batch\n",
      "Epoch 17/20  Iteration 2936/3560 Training loss: 1.2262 3.0677 sec/batch\n",
      "Epoch 17/20  Iteration 2937/3560 Training loss: 1.2260 3.2282 sec/batch\n",
      "Epoch 17/20  Iteration 2938/3560 Training loss: 1.2261 3.1006 sec/batch\n",
      "Epoch 17/20  Iteration 2939/3560 Training loss: 1.2259 3.2124 sec/batch\n",
      "Epoch 17/20  Iteration 2940/3560 Training loss: 1.2258 3.1888 sec/batch\n",
      "Epoch 17/20  Iteration 2941/3560 Training loss: 1.2255 3.1358 sec/batch\n",
      "Epoch 17/20  Iteration 2942/3560 Training loss: 1.2251 3.1600 sec/batch\n",
      "Epoch 17/20  Iteration 2943/3560 Training loss: 1.2249 3.2642 sec/batch\n",
      "Epoch 17/20  Iteration 2944/3560 Training loss: 1.2250 3.1569 sec/batch\n",
      "Epoch 17/20  Iteration 2945/3560 Training loss: 1.2250 3.3442 sec/batch\n",
      "Epoch 17/20  Iteration 2946/3560 Training loss: 1.2246 3.2754 sec/batch\n",
      "Epoch 17/20  Iteration 2947/3560 Training loss: 1.2242 3.2037 sec/batch\n",
      "Epoch 17/20  Iteration 2948/3560 Training loss: 1.2240 3.1148 sec/batch\n",
      "Epoch 17/20  Iteration 2949/3560 Training loss: 1.2240 3.1624 sec/batch\n",
      "Epoch 17/20  Iteration 2950/3560 Training loss: 1.2238 3.1304 sec/batch\n",
      "Epoch 17/20  Iteration 2951/3560 Training loss: 1.2236 3.1448 sec/batch\n",
      "Epoch 17/20  Iteration 2952/3560 Training loss: 1.2235 3.1363 sec/batch\n",
      "Epoch 17/20  Iteration 2953/3560 Training loss: 1.2233 3.1746 sec/batch\n",
      "Epoch 17/20  Iteration 2954/3560 Training loss: 1.2232 3.1393 sec/batch\n",
      "Epoch 17/20  Iteration 2955/3560 Training loss: 1.2232 3.0449 sec/batch\n",
      "Epoch 17/20  Iteration 2956/3560 Training loss: 1.2233 3.2324 sec/batch\n",
      "Epoch 17/20  Iteration 2957/3560 Training loss: 1.2231 3.1852 sec/batch\n",
      "Epoch 17/20  Iteration 2958/3560 Training loss: 1.2231 2.9745 sec/batch\n",
      "Epoch 17/20  Iteration 2959/3560 Training loss: 1.2230 3.1790 sec/batch\n",
      "Epoch 17/20  Iteration 2960/3560 Training loss: 1.2229 3.1902 sec/batch\n",
      "Epoch 17/20  Iteration 2961/3560 Training loss: 1.2228 3.0520 sec/batch\n",
      "Epoch 17/20  Iteration 2962/3560 Training loss: 1.2228 3.1935 sec/batch\n",
      "Epoch 17/20  Iteration 2963/3560 Training loss: 1.2225 3.1982 sec/batch\n",
      "Epoch 17/20  Iteration 2964/3560 Training loss: 1.2222 3.0459 sec/batch\n",
      "Epoch 17/20  Iteration 2965/3560 Training loss: 1.2222 3.1487 sec/batch\n",
      "Epoch 17/20  Iteration 2966/3560 Training loss: 1.2222 3.2609 sec/batch\n",
      "Epoch 17/20  Iteration 2967/3560 Training loss: 1.2221 3.1336 sec/batch\n",
      "Epoch 17/20  Iteration 2968/3560 Training loss: 1.2220 3.1647 sec/batch\n",
      "Epoch 17/20  Iteration 2969/3560 Training loss: 1.2219 3.1283 sec/batch\n",
      "Epoch 17/20  Iteration 2970/3560 Training loss: 1.2215 3.3685 sec/batch\n",
      "Epoch 17/20  Iteration 2971/3560 Training loss: 1.2212 3.2220 sec/batch\n",
      "Epoch 17/20  Iteration 2972/3560 Training loss: 1.2211 3.1631 sec/batch\n",
      "Epoch 17/20  Iteration 2973/3560 Training loss: 1.2209 3.1944 sec/batch\n",
      "Epoch 17/20  Iteration 2974/3560 Training loss: 1.2206 3.0690 sec/batch\n",
      "Epoch 17/20  Iteration 2975/3560 Training loss: 1.2206 3.1135 sec/batch\n",
      "Epoch 17/20  Iteration 2976/3560 Training loss: 1.2207 3.1586 sec/batch\n",
      "Epoch 17/20  Iteration 2977/3560 Training loss: 1.2204 3.1108 sec/batch\n",
      "Epoch 17/20  Iteration 2978/3560 Training loss: 1.2202 3.1538 sec/batch\n",
      "Epoch 17/20  Iteration 2979/3560 Training loss: 1.2198 3.2122 sec/batch\n",
      "Epoch 17/20  Iteration 2980/3560 Training loss: 1.2197 3.1764 sec/batch\n",
      "Epoch 17/20  Iteration 2981/3560 Training loss: 1.2198 3.1648 sec/batch\n",
      "Epoch 17/20  Iteration 2982/3560 Training loss: 1.2198 3.1736 sec/batch\n",
      "Epoch 17/20  Iteration 2983/3560 Training loss: 1.2199 3.1489 sec/batch\n",
      "Epoch 17/20  Iteration 2984/3560 Training loss: 1.2199 3.1659 sec/batch\n",
      "Epoch 17/20  Iteration 2985/3560 Training loss: 1.2201 3.2206 sec/batch\n",
      "Epoch 17/20  Iteration 2986/3560 Training loss: 1.2201 3.0683 sec/batch\n",
      "Epoch 17/20  Iteration 2987/3560 Training loss: 1.2201 3.1502 sec/batch\n",
      "Epoch 17/20  Iteration 2988/3560 Training loss: 1.2201 3.1429 sec/batch\n",
      "Epoch 17/20  Iteration 2989/3560 Training loss: 1.2204 3.2179 sec/batch\n",
      "Epoch 17/20  Iteration 2990/3560 Training loss: 1.2204 3.1031 sec/batch\n",
      "Epoch 17/20  Iteration 2991/3560 Training loss: 1.2204 3.1862 sec/batch\n",
      "Epoch 17/20  Iteration 2992/3560 Training loss: 1.2207 3.1566 sec/batch\n",
      "Epoch 17/20  Iteration 2993/3560 Training loss: 1.2205 3.0820 sec/batch\n",
      "Epoch 17/20  Iteration 2994/3560 Training loss: 1.2207 3.2213 sec/batch\n",
      "Epoch 17/20  Iteration 2995/3560 Training loss: 1.2207 3.1325 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Iteration 2996/3560 Training loss: 1.2209 3.0795 sec/batch\n",
      "Epoch 17/20  Iteration 2997/3560 Training loss: 1.2210 3.1367 sec/batch\n",
      "Epoch 17/20  Iteration 2998/3560 Training loss: 1.2209 3.2466 sec/batch\n",
      "Epoch 17/20  Iteration 2999/3560 Training loss: 1.2206 3.1209 sec/batch\n",
      "Epoch 17/20  Iteration 3000/3560 Training loss: 1.2204 3.1522 sec/batch\n",
      "Epoch 17/20  Iteration 3001/3560 Training loss: 1.2204 3.1895 sec/batch\n",
      "Epoch 17/20  Iteration 3002/3560 Training loss: 1.2204 3.1498 sec/batch\n",
      "Epoch 17/20  Iteration 3003/3560 Training loss: 1.2204 3.1365 sec/batch\n",
      "Epoch 17/20  Iteration 3004/3560 Training loss: 1.2203 3.1356 sec/batch\n",
      "Epoch 17/20  Iteration 3005/3560 Training loss: 1.2203 3.3085 sec/batch\n",
      "Epoch 17/20  Iteration 3006/3560 Training loss: 1.2203 3.1523 sec/batch\n",
      "Epoch 17/20  Iteration 3007/3560 Training loss: 1.2200 3.0899 sec/batch\n",
      "Epoch 17/20  Iteration 3008/3560 Training loss: 1.2201 3.2539 sec/batch\n",
      "Epoch 17/20  Iteration 3009/3560 Training loss: 1.2203 3.1476 sec/batch\n",
      "Epoch 17/20  Iteration 3010/3560 Training loss: 1.2202 3.1857 sec/batch\n",
      "Epoch 17/20  Iteration 3011/3560 Training loss: 1.2202 3.1742 sec/batch\n",
      "Epoch 17/20  Iteration 3012/3560 Training loss: 1.2201 3.2471 sec/batch\n",
      "Epoch 17/20  Iteration 3013/3560 Training loss: 1.2201 3.1928 sec/batch\n",
      "Epoch 17/20  Iteration 3014/3560 Training loss: 1.2201 3.1837 sec/batch\n",
      "Epoch 17/20  Iteration 3015/3560 Training loss: 1.2203 3.2062 sec/batch\n",
      "Epoch 17/20  Iteration 3016/3560 Training loss: 1.2207 3.1796 sec/batch\n",
      "Epoch 17/20  Iteration 3017/3560 Training loss: 1.2208 3.1176 sec/batch\n",
      "Epoch 17/20  Iteration 3018/3560 Training loss: 1.2208 3.1742 sec/batch\n",
      "Epoch 17/20  Iteration 3019/3560 Training loss: 1.2208 3.2285 sec/batch\n",
      "Epoch 17/20  Iteration 3020/3560 Training loss: 1.2207 3.1543 sec/batch\n",
      "Epoch 17/20  Iteration 3021/3560 Training loss: 1.2209 3.1360 sec/batch\n",
      "Epoch 17/20  Iteration 3022/3560 Training loss: 1.2209 3.2192 sec/batch\n",
      "Epoch 17/20  Iteration 3023/3560 Training loss: 1.2210 3.0792 sec/batch\n",
      "Epoch 17/20  Iteration 3024/3560 Training loss: 1.2208 3.1598 sec/batch\n",
      "Epoch 17/20  Iteration 3025/3560 Training loss: 1.2207 3.2196 sec/batch\n",
      "Epoch 17/20  Iteration 3026/3560 Training loss: 1.2209 3.4199 sec/batch\n",
      "Epoch 18/20  Iteration 3027/3560 Training loss: 1.3284 3.2014 sec/batch\n",
      "Epoch 18/20  Iteration 3028/3560 Training loss: 1.2817 3.2004 sec/batch\n",
      "Epoch 18/20  Iteration 3029/3560 Training loss: 1.2626 3.1115 sec/batch\n",
      "Epoch 18/20  Iteration 3030/3560 Training loss: 1.2574 3.2186 sec/batch\n",
      "Epoch 18/20  Iteration 3031/3560 Training loss: 1.2462 3.0933 sec/batch\n",
      "Epoch 18/20  Iteration 3032/3560 Training loss: 1.2352 3.1102 sec/batch\n",
      "Epoch 18/20  Iteration 3033/3560 Training loss: 1.2343 3.1418 sec/batch\n",
      "Epoch 18/20  Iteration 3034/3560 Training loss: 1.2315 3.0993 sec/batch\n",
      "Epoch 18/20  Iteration 3035/3560 Training loss: 1.2305 3.0845 sec/batch\n",
      "Epoch 18/20  Iteration 3036/3560 Training loss: 1.2286 3.2123 sec/batch\n",
      "Epoch 18/20  Iteration 3037/3560 Training loss: 1.2251 3.1685 sec/batch\n",
      "Epoch 18/20  Iteration 3038/3560 Training loss: 1.2244 3.1233 sec/batch\n",
      "Epoch 18/20  Iteration 3039/3560 Training loss: 1.2239 3.1879 sec/batch\n",
      "Epoch 18/20  Iteration 3040/3560 Training loss: 1.2241 3.2478 sec/batch\n",
      "Epoch 18/20  Iteration 3041/3560 Training loss: 1.2227 3.1911 sec/batch\n",
      "Epoch 18/20  Iteration 3042/3560 Training loss: 1.2213 3.1161 sec/batch\n",
      "Epoch 18/20  Iteration 3043/3560 Training loss: 1.2219 3.2754 sec/batch\n",
      "Epoch 18/20  Iteration 3044/3560 Training loss: 1.2228 3.2080 sec/batch\n",
      "Epoch 18/20  Iteration 3045/3560 Training loss: 1.2222 3.2412 sec/batch\n",
      "Epoch 18/20  Iteration 3046/3560 Training loss: 1.2230 3.6536 sec/batch\n",
      "Epoch 18/20  Iteration 3047/3560 Training loss: 1.2223 3.6137 sec/batch\n",
      "Epoch 18/20  Iteration 3048/3560 Training loss: 1.2228 3.5282 sec/batch\n",
      "Epoch 18/20  Iteration 3049/3560 Training loss: 1.2221 3.3195 sec/batch\n",
      "Epoch 18/20  Iteration 3050/3560 Training loss: 1.2223 3.2571 sec/batch\n",
      "Epoch 18/20  Iteration 3051/3560 Training loss: 1.2224 3.1021 sec/batch\n",
      "Epoch 18/20  Iteration 3052/3560 Training loss: 1.2210 3.2301 sec/batch\n",
      "Epoch 18/20  Iteration 3053/3560 Training loss: 1.2198 3.1319 sec/batch\n",
      "Epoch 18/20  Iteration 3054/3560 Training loss: 1.2203 3.1435 sec/batch\n",
      "Epoch 18/20  Iteration 3055/3560 Training loss: 1.2202 3.0763 sec/batch\n",
      "Epoch 18/20  Iteration 3056/3560 Training loss: 1.2205 3.2442 sec/batch\n",
      "Epoch 18/20  Iteration 3057/3560 Training loss: 1.2199 3.1298 sec/batch\n",
      "Epoch 18/20  Iteration 3058/3560 Training loss: 1.2187 3.1505 sec/batch\n",
      "Epoch 18/20  Iteration 3059/3560 Training loss: 1.2188 3.1620 sec/batch\n",
      "Epoch 18/20  Iteration 3060/3560 Training loss: 1.2187 3.1286 sec/batch\n",
      "Epoch 18/20  Iteration 3061/3560 Training loss: 1.2182 3.0879 sec/batch\n",
      "Epoch 18/20  Iteration 3062/3560 Training loss: 1.2180 3.1122 sec/batch\n",
      "Epoch 18/20  Iteration 3063/3560 Training loss: 1.2171 3.2089 sec/batch\n",
      "Epoch 18/20  Iteration 3064/3560 Training loss: 1.2161 3.1645 sec/batch\n",
      "Epoch 18/20  Iteration 3065/3560 Training loss: 1.2151 3.2030 sec/batch\n",
      "Epoch 18/20  Iteration 3066/3560 Training loss: 1.2148 3.0809 sec/batch\n",
      "Epoch 18/20  Iteration 3067/3560 Training loss: 1.2141 3.1797 sec/batch\n",
      "Epoch 18/20  Iteration 3068/3560 Training loss: 1.2146 3.1247 sec/batch\n",
      "Epoch 18/20  Iteration 3069/3560 Training loss: 1.2142 3.1303 sec/batch\n",
      "Epoch 18/20  Iteration 3070/3560 Training loss: 1.2136 3.1906 sec/batch\n",
      "Epoch 18/20  Iteration 3071/3560 Training loss: 1.2136 3.1176 sec/batch\n",
      "Epoch 18/20  Iteration 3072/3560 Training loss: 1.2133 3.1345 sec/batch\n",
      "Epoch 18/20  Iteration 3073/3560 Training loss: 1.2130 3.1081 sec/batch\n",
      "Epoch 18/20  Iteration 3074/3560 Training loss: 1.2126 3.2131 sec/batch\n",
      "Epoch 18/20  Iteration 3075/3560 Training loss: 1.2127 3.1798 sec/batch\n",
      "Epoch 18/20  Iteration 3076/3560 Training loss: 1.2129 3.0777 sec/batch\n",
      "Epoch 18/20  Iteration 3077/3560 Training loss: 1.2125 3.0661 sec/batch\n",
      "Epoch 18/20  Iteration 3078/3560 Training loss: 1.2133 3.2347 sec/batch\n",
      "Epoch 18/20  Iteration 3079/3560 Training loss: 1.2133 3.0963 sec/batch\n",
      "Epoch 18/20  Iteration 3080/3560 Training loss: 1.2136 3.1073 sec/batch\n",
      "Epoch 18/20  Iteration 3081/3560 Training loss: 1.2134 3.2708 sec/batch\n",
      "Epoch 18/20  Iteration 3082/3560 Training loss: 1.2133 3.1694 sec/batch\n",
      "Epoch 18/20  Iteration 3083/3560 Training loss: 1.2134 3.2759 sec/batch\n",
      "Epoch 18/20  Iteration 3084/3560 Training loss: 1.2131 3.3237 sec/batch\n",
      "Epoch 18/20  Iteration 3085/3560 Training loss: 1.2126 3.1379 sec/batch\n",
      "Epoch 18/20  Iteration 3086/3560 Training loss: 1.2132 3.0590 sec/batch\n",
      "Epoch 18/20  Iteration 3087/3560 Training loss: 1.2132 3.2295 sec/batch\n",
      "Epoch 18/20  Iteration 3088/3560 Training loss: 1.2140 3.2048 sec/batch\n",
      "Epoch 18/20  Iteration 3089/3560 Training loss: 1.2143 3.0275 sec/batch\n",
      "Epoch 18/20  Iteration 3090/3560 Training loss: 1.2143 3.1099 sec/batch\n",
      "Epoch 18/20  Iteration 3091/3560 Training loss: 1.2142 3.1816 sec/batch\n",
      "Epoch 18/20  Iteration 3092/3560 Training loss: 1.2143 3.1560 sec/batch\n",
      "Epoch 18/20  Iteration 3093/3560 Training loss: 1.2145 3.1403 sec/batch\n",
      "Epoch 18/20  Iteration 3094/3560 Training loss: 1.2143 3.1953 sec/batch\n",
      "Epoch 18/20  Iteration 3095/3560 Training loss: 1.2144 3.1532 sec/batch\n",
      "Epoch 18/20  Iteration 3096/3560 Training loss: 1.2142 3.1768 sec/batch\n",
      "Epoch 18/20  Iteration 3097/3560 Training loss: 1.2148 3.1617 sec/batch\n",
      "Epoch 18/20  Iteration 3098/3560 Training loss: 1.2151 3.1511 sec/batch\n",
      "Epoch 18/20  Iteration 3099/3560 Training loss: 1.2157 3.1689 sec/batch\n",
      "Epoch 18/20  Iteration 3100/3560 Training loss: 1.2155 3.2321 sec/batch\n",
      "Epoch 18/20  Iteration 3101/3560 Training loss: 1.2154 3.1744 sec/batch\n",
      "Epoch 18/20  Iteration 3102/3560 Training loss: 1.2155 3.1241 sec/batch\n",
      "Epoch 18/20  Iteration 3103/3560 Training loss: 1.2153 3.2567 sec/batch\n",
      "Epoch 18/20  Iteration 3104/3560 Training loss: 1.2153 3.1570 sec/batch\n",
      "Epoch 18/20  Iteration 3105/3560 Training loss: 1.2146 3.1430 sec/batch\n",
      "Epoch 18/20  Iteration 3106/3560 Training loss: 1.2146 3.1767 sec/batch\n",
      "Epoch 18/20  Iteration 3107/3560 Training loss: 1.2142 3.2596 sec/batch\n",
      "Epoch 18/20  Iteration 3108/3560 Training loss: 1.2142 3.1413 sec/batch\n",
      "Epoch 18/20  Iteration 3109/3560 Training loss: 1.2137 3.1550 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Iteration 3110/3560 Training loss: 1.2137 3.2014 sec/batch\n",
      "Epoch 18/20  Iteration 3111/3560 Training loss: 1.2135 3.1233 sec/batch\n",
      "Epoch 18/20  Iteration 3112/3560 Training loss: 1.2134 3.1158 sec/batch\n",
      "Epoch 18/20  Iteration 3113/3560 Training loss: 1.2133 3.2969 sec/batch\n",
      "Epoch 18/20  Iteration 3114/3560 Training loss: 1.2129 3.0899 sec/batch\n",
      "Epoch 18/20  Iteration 3115/3560 Training loss: 1.2125 3.0523 sec/batch\n",
      "Epoch 18/20  Iteration 3116/3560 Training loss: 1.2126 3.3284 sec/batch\n",
      "Epoch 18/20  Iteration 3117/3560 Training loss: 1.2123 3.0551 sec/batch\n",
      "Epoch 18/20  Iteration 3118/3560 Training loss: 1.2123 3.1491 sec/batch\n",
      "Epoch 18/20  Iteration 3119/3560 Training loss: 1.2120 3.2284 sec/batch\n",
      "Epoch 18/20  Iteration 3120/3560 Training loss: 1.2117 3.1585 sec/batch\n",
      "Epoch 18/20  Iteration 3121/3560 Training loss: 1.2114 3.1171 sec/batch\n",
      "Epoch 18/20  Iteration 3122/3560 Training loss: 1.2114 3.1726 sec/batch\n",
      "Epoch 18/20  Iteration 3123/3560 Training loss: 1.2114 3.2660 sec/batch\n",
      "Epoch 18/20  Iteration 3124/3560 Training loss: 1.2111 3.1011 sec/batch\n",
      "Epoch 18/20  Iteration 3125/3560 Training loss: 1.2107 3.0420 sec/batch\n",
      "Epoch 18/20  Iteration 3126/3560 Training loss: 1.2104 3.2423 sec/batch\n",
      "Epoch 18/20  Iteration 3127/3560 Training loss: 1.2103 3.1157 sec/batch\n",
      "Epoch 18/20  Iteration 3128/3560 Training loss: 1.2102 3.1545 sec/batch\n",
      "Epoch 18/20  Iteration 3129/3560 Training loss: 1.2102 3.1698 sec/batch\n",
      "Epoch 18/20  Iteration 3130/3560 Training loss: 1.2101 3.1431 sec/batch\n",
      "Epoch 18/20  Iteration 3131/3560 Training loss: 1.2100 3.1035 sec/batch\n",
      "Epoch 18/20  Iteration 3132/3560 Training loss: 1.2099 3.2374 sec/batch\n",
      "Epoch 18/20  Iteration 3133/3560 Training loss: 1.2098 3.1486 sec/batch\n",
      "Epoch 18/20  Iteration 3134/3560 Training loss: 1.2098 3.0861 sec/batch\n",
      "Epoch 18/20  Iteration 3135/3560 Training loss: 1.2097 3.2168 sec/batch\n",
      "Epoch 18/20  Iteration 3136/3560 Training loss: 1.2097 3.1888 sec/batch\n",
      "Epoch 18/20  Iteration 3137/3560 Training loss: 1.2095 3.0911 sec/batch\n",
      "Epoch 18/20  Iteration 3138/3560 Training loss: 1.2095 3.2260 sec/batch\n",
      "Epoch 18/20  Iteration 3139/3560 Training loss: 1.2095 3.2225 sec/batch\n",
      "Epoch 18/20  Iteration 3140/3560 Training loss: 1.2094 3.3419 sec/batch\n",
      "Epoch 18/20  Iteration 3141/3560 Training loss: 1.2092 3.2186 sec/batch\n",
      "Epoch 18/20  Iteration 3142/3560 Training loss: 1.2089 3.1787 sec/batch\n",
      "Epoch 18/20  Iteration 3143/3560 Training loss: 1.2089 3.3624 sec/batch\n",
      "Epoch 18/20  Iteration 3144/3560 Training loss: 1.2090 3.7252 sec/batch\n",
      "Epoch 18/20  Iteration 3145/3560 Training loss: 1.2089 3.8084 sec/batch\n",
      "Epoch 18/20  Iteration 3146/3560 Training loss: 1.2089 3.6619 sec/batch\n",
      "Epoch 18/20  Iteration 3147/3560 Training loss: 1.2087 3.3483 sec/batch\n",
      "Epoch 18/20  Iteration 3148/3560 Training loss: 1.2084 3.7153 sec/batch\n",
      "Epoch 18/20  Iteration 3149/3560 Training loss: 1.2081 3.6162 sec/batch\n",
      "Epoch 18/20  Iteration 3150/3560 Training loss: 1.2081 3.3719 sec/batch\n",
      "Epoch 18/20  Iteration 3151/3560 Training loss: 1.2080 3.1182 sec/batch\n",
      "Epoch 18/20  Iteration 3152/3560 Training loss: 1.2077 3.1236 sec/batch\n",
      "Epoch 18/20  Iteration 3153/3560 Training loss: 1.2077 3.1706 sec/batch\n",
      "Epoch 18/20  Iteration 3154/3560 Training loss: 1.2077 3.1656 sec/batch\n",
      "Epoch 18/20  Iteration 3155/3560 Training loss: 1.2076 3.0523 sec/batch\n",
      "Epoch 18/20  Iteration 3156/3560 Training loss: 1.2072 3.1639 sec/batch\n",
      "Epoch 18/20  Iteration 3157/3560 Training loss: 1.2068 3.2462 sec/batch\n",
      "Epoch 18/20  Iteration 3158/3560 Training loss: 1.2067 3.2509 sec/batch\n",
      "Epoch 18/20  Iteration 3159/3560 Training loss: 1.2068 3.6530 sec/batch\n",
      "Epoch 18/20  Iteration 3160/3560 Training loss: 1.2068 3.2129 sec/batch\n",
      "Epoch 18/20  Iteration 3161/3560 Training loss: 1.2068 3.0536 sec/batch\n",
      "Epoch 18/20  Iteration 3162/3560 Training loss: 1.2068 3.1434 sec/batch\n",
      "Epoch 18/20  Iteration 3163/3560 Training loss: 1.2070 3.2307 sec/batch\n",
      "Epoch 18/20  Iteration 3164/3560 Training loss: 1.2072 3.0888 sec/batch\n",
      "Epoch 18/20  Iteration 3165/3560 Training loss: 1.2072 3.1659 sec/batch\n",
      "Epoch 18/20  Iteration 3166/3560 Training loss: 1.2072 3.2286 sec/batch\n",
      "Epoch 18/20  Iteration 3167/3560 Training loss: 1.2076 3.0932 sec/batch\n",
      "Epoch 18/20  Iteration 3168/3560 Training loss: 1.2077 3.1844 sec/batch\n",
      "Epoch 18/20  Iteration 3169/3560 Training loss: 1.2075 3.2314 sec/batch\n",
      "Epoch 18/20  Iteration 3170/3560 Training loss: 1.2078 3.1263 sec/batch\n",
      "Epoch 18/20  Iteration 3171/3560 Training loss: 1.2077 3.2349 sec/batch\n",
      "Epoch 18/20  Iteration 3172/3560 Training loss: 1.2079 3.3472 sec/batch\n",
      "Epoch 18/20  Iteration 3173/3560 Training loss: 1.2079 3.2191 sec/batch\n",
      "Epoch 18/20  Iteration 3174/3560 Training loss: 1.2082 3.1946 sec/batch\n",
      "Epoch 18/20  Iteration 3175/3560 Training loss: 1.2083 3.1220 sec/batch\n",
      "Epoch 18/20  Iteration 3176/3560 Training loss: 1.2082 3.2033 sec/batch\n",
      "Epoch 18/20  Iteration 3177/3560 Training loss: 1.2079 3.1815 sec/batch\n",
      "Epoch 18/20  Iteration 3178/3560 Training loss: 1.2077 3.0856 sec/batch\n",
      "Epoch 18/20  Iteration 3179/3560 Training loss: 1.2078 3.1859 sec/batch\n",
      "Epoch 18/20  Iteration 3180/3560 Training loss: 1.2078 3.1089 sec/batch\n",
      "Epoch 18/20  Iteration 3181/3560 Training loss: 1.2078 3.1087 sec/batch\n",
      "Epoch 18/20  Iteration 3182/3560 Training loss: 1.2078 3.1556 sec/batch\n",
      "Epoch 18/20  Iteration 3183/3560 Training loss: 1.2078 3.0683 sec/batch\n",
      "Epoch 18/20  Iteration 3184/3560 Training loss: 1.2078 3.3789 sec/batch\n",
      "Epoch 18/20  Iteration 3185/3560 Training loss: 1.2076 3.2858 sec/batch\n",
      "Epoch 18/20  Iteration 3186/3560 Training loss: 1.2077 3.0687 sec/batch\n",
      "Epoch 18/20  Iteration 3187/3560 Training loss: 1.2078 3.1240 sec/batch\n",
      "Epoch 18/20  Iteration 3188/3560 Training loss: 1.2079 3.2400 sec/batch\n",
      "Epoch 18/20  Iteration 3189/3560 Training loss: 1.2079 3.0448 sec/batch\n",
      "Epoch 18/20  Iteration 3190/3560 Training loss: 1.2078 3.0702 sec/batch\n",
      "Epoch 18/20  Iteration 3191/3560 Training loss: 1.2078 3.1831 sec/batch\n",
      "Epoch 18/20  Iteration 3192/3560 Training loss: 1.2077 3.1895 sec/batch\n",
      "Epoch 18/20  Iteration 3193/3560 Training loss: 1.2079 3.0663 sec/batch\n",
      "Epoch 18/20  Iteration 3194/3560 Training loss: 1.2082 3.1748 sec/batch\n",
      "Epoch 18/20  Iteration 3195/3560 Training loss: 1.2083 3.4435 sec/batch\n",
      "Epoch 18/20  Iteration 3196/3560 Training loss: 1.2084 3.3419 sec/batch\n",
      "Epoch 18/20  Iteration 3197/3560 Training loss: 1.2083 3.1686 sec/batch\n",
      "Epoch 18/20  Iteration 3198/3560 Training loss: 1.2082 3.1429 sec/batch\n",
      "Epoch 18/20  Iteration 3199/3560 Training loss: 1.2084 3.1500 sec/batch\n",
      "Epoch 18/20  Iteration 3200/3560 Training loss: 1.2084 3.1601 sec/batch\n",
      "Epoch 18/20  Iteration 3201/3560 Training loss: 1.2084 3.2915 sec/batch\n",
      "Epoch 18/20  Iteration 3202/3560 Training loss: 1.2082 3.1921 sec/batch\n",
      "Epoch 18/20  Iteration 3203/3560 Training loss: 1.2081 3.1848 sec/batch\n",
      "Epoch 18/20  Iteration 3204/3560 Training loss: 1.2083 3.0775 sec/batch\n",
      "Epoch 19/20  Iteration 3205/3560 Training loss: 1.3062 3.2186 sec/batch\n",
      "Epoch 19/20  Iteration 3206/3560 Training loss: 1.2660 3.1444 sec/batch\n",
      "Epoch 19/20  Iteration 3207/3560 Training loss: 1.2451 3.0999 sec/batch\n",
      "Epoch 19/20  Iteration 3208/3560 Training loss: 1.2391 3.2093 sec/batch\n",
      "Epoch 19/20  Iteration 3209/3560 Training loss: 1.2275 3.3507 sec/batch\n",
      "Epoch 19/20  Iteration 3210/3560 Training loss: 1.2170 3.1100 sec/batch\n",
      "Epoch 19/20  Iteration 3211/3560 Training loss: 1.2164 3.2138 sec/batch\n",
      "Epoch 19/20  Iteration 3212/3560 Training loss: 1.2136 3.1748 sec/batch\n",
      "Epoch 19/20  Iteration 3213/3560 Training loss: 1.2132 3.1290 sec/batch\n",
      "Epoch 19/20  Iteration 3214/3560 Training loss: 1.2119 3.2042 sec/batch\n",
      "Epoch 19/20  Iteration 3215/3560 Training loss: 1.2090 3.2035 sec/batch\n",
      "Epoch 19/20  Iteration 3216/3560 Training loss: 1.2086 3.1496 sec/batch\n",
      "Epoch 19/20  Iteration 3217/3560 Training loss: 1.2090 3.1951 sec/batch\n",
      "Epoch 19/20  Iteration 3218/3560 Training loss: 1.2086 3.1257 sec/batch\n",
      "Epoch 19/20  Iteration 3219/3560 Training loss: 1.2073 3.1818 sec/batch\n",
      "Epoch 19/20  Iteration 3220/3560 Training loss: 1.2060 3.2990 sec/batch\n",
      "Epoch 19/20  Iteration 3221/3560 Training loss: 1.2053 3.2801 sec/batch\n",
      "Epoch 19/20  Iteration 3222/3560 Training loss: 1.2072 3.1784 sec/batch\n",
      "Epoch 19/20  Iteration 3223/3560 Training loss: 1.2070 3.1049 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3224/3560 Training loss: 1.2083 3.0804 sec/batch\n",
      "Epoch 19/20  Iteration 3225/3560 Training loss: 1.2079 3.1739 sec/batch\n",
      "Epoch 19/20  Iteration 3226/3560 Training loss: 1.2083 3.1716 sec/batch\n",
      "Epoch 19/20  Iteration 3227/3560 Training loss: 1.2075 3.1246 sec/batch\n",
      "Epoch 19/20  Iteration 3228/3560 Training loss: 1.2077 3.1199 sec/batch\n",
      "Epoch 19/20  Iteration 3229/3560 Training loss: 1.2077 3.1827 sec/batch\n",
      "Epoch 19/20  Iteration 3230/3560 Training loss: 1.2063 3.2983 sec/batch\n",
      "Epoch 19/20  Iteration 3231/3560 Training loss: 1.2054 3.1961 sec/batch\n",
      "Epoch 19/20  Iteration 3232/3560 Training loss: 1.2059 3.1742 sec/batch\n",
      "Epoch 19/20  Iteration 3233/3560 Training loss: 1.2059 3.1531 sec/batch\n",
      "Epoch 19/20  Iteration 3234/3560 Training loss: 1.2060 3.0858 sec/batch\n",
      "Epoch 19/20  Iteration 3235/3560 Training loss: 1.2052 3.1129 sec/batch\n",
      "Epoch 19/20  Iteration 3236/3560 Training loss: 1.2043 3.1961 sec/batch\n",
      "Epoch 19/20  Iteration 3237/3560 Training loss: 1.2041 3.1553 sec/batch\n",
      "Epoch 19/20  Iteration 3238/3560 Training loss: 1.2042 3.0626 sec/batch\n",
      "Epoch 19/20  Iteration 3239/3560 Training loss: 1.2037 3.1192 sec/batch\n",
      "Epoch 19/20  Iteration 3240/3560 Training loss: 1.2035 3.2044 sec/batch\n",
      "Epoch 19/20  Iteration 3241/3560 Training loss: 1.2028 3.1044 sec/batch\n",
      "Epoch 19/20  Iteration 3242/3560 Training loss: 1.2016 3.1382 sec/batch\n",
      "Epoch 19/20  Iteration 3243/3560 Training loss: 1.2005 3.4495 sec/batch\n",
      "Epoch 19/20  Iteration 3244/3560 Training loss: 1.2003 3.1525 sec/batch\n",
      "Epoch 19/20  Iteration 3245/3560 Training loss: 1.1998 3.1530 sec/batch\n",
      "Epoch 19/20  Iteration 3246/3560 Training loss: 1.2007 3.1516 sec/batch\n",
      "Epoch 19/20  Iteration 3247/3560 Training loss: 1.2006 3.1063 sec/batch\n",
      "Epoch 19/20  Iteration 3248/3560 Training loss: 1.2001 3.1136 sec/batch\n",
      "Epoch 19/20  Iteration 3249/3560 Training loss: 1.2002 3.1837 sec/batch\n",
      "Epoch 19/20  Iteration 3250/3560 Training loss: 1.1996 3.1153 sec/batch\n",
      "Epoch 19/20  Iteration 3251/3560 Training loss: 1.1991 3.1638 sec/batch\n",
      "Epoch 19/20  Iteration 3252/3560 Training loss: 1.1989 3.2577 sec/batch\n",
      "Epoch 19/20  Iteration 3253/3560 Training loss: 1.1989 3.2935 sec/batch\n",
      "Epoch 19/20  Iteration 3254/3560 Training loss: 1.1991 3.1565 sec/batch\n",
      "Epoch 19/20  Iteration 3255/3560 Training loss: 1.1987 3.0837 sec/batch\n",
      "Epoch 19/20  Iteration 3256/3560 Training loss: 1.1995 3.2457 sec/batch\n",
      "Epoch 19/20  Iteration 3257/3560 Training loss: 1.1995 3.3189 sec/batch\n",
      "Epoch 19/20  Iteration 3258/3560 Training loss: 1.1997 3.1180 sec/batch\n",
      "Epoch 19/20  Iteration 3259/3560 Training loss: 1.1996 3.2162 sec/batch\n",
      "Epoch 19/20  Iteration 3260/3560 Training loss: 1.1995 3.1892 sec/batch\n",
      "Epoch 19/20  Iteration 3261/3560 Training loss: 1.1997 3.1845 sec/batch\n",
      "Epoch 19/20  Iteration 3262/3560 Training loss: 1.1994 3.1746 sec/batch\n",
      "Epoch 19/20  Iteration 3263/3560 Training loss: 1.1989 3.1686 sec/batch\n",
      "Epoch 19/20  Iteration 3264/3560 Training loss: 1.1997 3.1250 sec/batch\n",
      "Epoch 19/20  Iteration 3265/3560 Training loss: 1.1996 3.1763 sec/batch\n",
      "Epoch 19/20  Iteration 3266/3560 Training loss: 1.2005 3.1192 sec/batch\n",
      "Epoch 19/20  Iteration 3267/3560 Training loss: 1.2008 3.1574 sec/batch\n",
      "Epoch 19/20  Iteration 3268/3560 Training loss: 1.2010 3.1797 sec/batch\n",
      "Epoch 19/20  Iteration 3269/3560 Training loss: 1.2009 3.2784 sec/batch\n",
      "Epoch 19/20  Iteration 3270/3560 Training loss: 1.2011 3.2993 sec/batch\n",
      "Epoch 19/20  Iteration 3271/3560 Training loss: 1.2013 3.1513 sec/batch\n",
      "Epoch 19/20  Iteration 3272/3560 Training loss: 1.2012 3.1561 sec/batch\n",
      "Epoch 19/20  Iteration 3273/3560 Training loss: 1.2012 3.1643 sec/batch\n",
      "Epoch 19/20  Iteration 3274/3560 Training loss: 1.2010 3.1787 sec/batch\n",
      "Epoch 19/20  Iteration 3275/3560 Training loss: 1.2017 3.1228 sec/batch\n",
      "Epoch 19/20  Iteration 3276/3560 Training loss: 1.2021 3.1967 sec/batch\n",
      "Epoch 19/20  Iteration 3277/3560 Training loss: 1.2026 3.0965 sec/batch\n",
      "Epoch 19/20  Iteration 3278/3560 Training loss: 1.2024 3.1512 sec/batch\n",
      "Epoch 19/20  Iteration 3279/3560 Training loss: 1.2024 3.1849 sec/batch\n",
      "Epoch 19/20  Iteration 3280/3560 Training loss: 1.2025 3.0669 sec/batch\n",
      "Epoch 19/20  Iteration 3281/3560 Training loss: 1.2024 3.2142 sec/batch\n",
      "Epoch 19/20  Iteration 3282/3560 Training loss: 1.2024 3.3218 sec/batch\n",
      "Epoch 19/20  Iteration 3283/3560 Training loss: 1.2018 3.1368 sec/batch\n",
      "Epoch 19/20  Iteration 3284/3560 Training loss: 1.2018 3.0823 sec/batch\n",
      "Epoch 19/20  Iteration 3285/3560 Training loss: 1.2014 3.2147 sec/batch\n",
      "Epoch 19/20  Iteration 3286/3560 Training loss: 1.2014 3.1100 sec/batch\n",
      "Epoch 19/20  Iteration 3287/3560 Training loss: 1.2010 3.0862 sec/batch\n",
      "Epoch 19/20  Iteration 3288/3560 Training loss: 1.2010 3.2007 sec/batch\n",
      "Epoch 19/20  Iteration 3289/3560 Training loss: 1.2008 3.2310 sec/batch\n",
      "Epoch 19/20  Iteration 3290/3560 Training loss: 1.2007 3.0814 sec/batch\n",
      "Epoch 19/20  Iteration 3291/3560 Training loss: 1.2006 3.2676 sec/batch\n",
      "Epoch 19/20  Iteration 3292/3560 Training loss: 1.2004 3.1941 sec/batch\n",
      "Epoch 19/20  Iteration 3293/3560 Training loss: 1.2000 3.1067 sec/batch\n",
      "Epoch 19/20  Iteration 3294/3560 Training loss: 1.2001 3.5209 sec/batch\n",
      "Epoch 19/20  Iteration 3295/3560 Training loss: 1.1999 3.1781 sec/batch\n",
      "Epoch 19/20  Iteration 3296/3560 Training loss: 1.1999 3.0647 sec/batch\n",
      "Epoch 19/20  Iteration 3297/3560 Training loss: 1.1996 3.1892 sec/batch\n",
      "Epoch 19/20  Iteration 3298/3560 Training loss: 1.1993 3.1790 sec/batch\n",
      "Epoch 19/20  Iteration 3299/3560 Training loss: 1.1991 3.1304 sec/batch\n",
      "Epoch 19/20  Iteration 3300/3560 Training loss: 1.1992 3.1303 sec/batch\n",
      "Epoch 19/20  Iteration 3301/3560 Training loss: 1.1992 3.1667 sec/batch\n",
      "Epoch 19/20  Iteration 3302/3560 Training loss: 1.1990 3.0455 sec/batch\n",
      "Epoch 19/20  Iteration 3303/3560 Training loss: 1.1986 3.1348 sec/batch\n",
      "Epoch 19/20  Iteration 3304/3560 Training loss: 1.1983 3.1718 sec/batch\n",
      "Epoch 19/20  Iteration 3305/3560 Training loss: 1.1983 3.1655 sec/batch\n",
      "Epoch 19/20  Iteration 3306/3560 Training loss: 1.1982 3.1057 sec/batch\n",
      "Epoch 19/20  Iteration 3307/3560 Training loss: 1.1982 3.4096 sec/batch\n",
      "Epoch 19/20  Iteration 3308/3560 Training loss: 1.1980 3.1578 sec/batch\n",
      "Epoch 19/20  Iteration 3309/3560 Training loss: 1.1979 3.3671 sec/batch\n",
      "Epoch 19/20  Iteration 3310/3560 Training loss: 1.1978 3.2965 sec/batch\n",
      "Epoch 19/20  Iteration 3311/3560 Training loss: 1.1978 3.1553 sec/batch\n",
      "Epoch 19/20  Iteration 3312/3560 Training loss: 1.1979 3.0916 sec/batch\n",
      "Epoch 19/20  Iteration 3313/3560 Training loss: 1.1977 3.2501 sec/batch\n",
      "Epoch 19/20  Iteration 3314/3560 Training loss: 1.1978 3.1128 sec/batch\n",
      "Epoch 19/20  Iteration 3315/3560 Training loss: 1.1976 3.1247 sec/batch\n",
      "Epoch 19/20  Iteration 3316/3560 Training loss: 1.1976 3.1579 sec/batch\n",
      "Epoch 19/20  Iteration 3317/3560 Training loss: 1.1975 3.2053 sec/batch\n",
      "Epoch 19/20  Iteration 3318/3560 Training loss: 1.1974 3.0730 sec/batch\n",
      "Epoch 19/20  Iteration 3319/3560 Training loss: 1.1971 3.3115 sec/batch\n",
      "Epoch 19/20  Iteration 3320/3560 Training loss: 1.1968 3.1978 sec/batch\n",
      "Epoch 19/20  Iteration 3321/3560 Training loss: 1.1968 3.1191 sec/batch\n",
      "Epoch 19/20  Iteration 3322/3560 Training loss: 1.1968 3.1838 sec/batch\n",
      "Epoch 19/20  Iteration 3323/3560 Training loss: 1.1967 3.2227 sec/batch\n",
      "Epoch 19/20  Iteration 3324/3560 Training loss: 1.1967 3.1652 sec/batch\n",
      "Epoch 19/20  Iteration 3325/3560 Training loss: 1.1966 3.1477 sec/batch\n",
      "Epoch 19/20  Iteration 3326/3560 Training loss: 1.1963 3.1604 sec/batch\n",
      "Epoch 19/20  Iteration 3327/3560 Training loss: 1.1960 3.0936 sec/batch\n",
      "Epoch 19/20  Iteration 3328/3560 Training loss: 1.1959 3.0515 sec/batch\n",
      "Epoch 19/20  Iteration 3329/3560 Training loss: 1.1958 3.2962 sec/batch\n",
      "Epoch 19/20  Iteration 3330/3560 Training loss: 1.1955 3.1556 sec/batch\n",
      "Epoch 19/20  Iteration 3331/3560 Training loss: 1.1954 3.1337 sec/batch\n",
      "Epoch 19/20  Iteration 3332/3560 Training loss: 1.1954 3.3802 sec/batch\n",
      "Epoch 19/20  Iteration 3333/3560 Training loss: 1.1953 3.2036 sec/batch\n",
      "Epoch 19/20  Iteration 3334/3560 Training loss: 1.1949 3.1026 sec/batch\n",
      "Epoch 19/20  Iteration 3335/3560 Training loss: 1.1945 3.1248 sec/batch\n",
      "Epoch 19/20  Iteration 3336/3560 Training loss: 1.1944 3.2524 sec/batch\n",
      "Epoch 19/20  Iteration 3337/3560 Training loss: 1.1945 3.0732 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Iteration 3338/3560 Training loss: 1.1946 3.1753 sec/batch\n",
      "Epoch 19/20  Iteration 3339/3560 Training loss: 1.1946 3.2118 sec/batch\n",
      "Epoch 19/20  Iteration 3340/3560 Training loss: 1.1947 3.1074 sec/batch\n",
      "Epoch 19/20  Iteration 3341/3560 Training loss: 1.1950 3.1322 sec/batch\n",
      "Epoch 19/20  Iteration 3342/3560 Training loss: 1.1950 3.2094 sec/batch\n",
      "Epoch 19/20  Iteration 3343/3560 Training loss: 1.1951 3.0958 sec/batch\n",
      "Epoch 19/20  Iteration 3344/3560 Training loss: 1.1952 3.2965 sec/batch\n",
      "Epoch 19/20  Iteration 3345/3560 Training loss: 1.1955 3.1102 sec/batch\n",
      "Epoch 19/20  Iteration 3346/3560 Training loss: 1.1957 3.1730 sec/batch\n",
      "Epoch 19/20  Iteration 3347/3560 Training loss: 1.1956 3.2003 sec/batch\n",
      "Epoch 19/20  Iteration 3348/3560 Training loss: 1.1959 3.1721 sec/batch\n",
      "Epoch 19/20  Iteration 3349/3560 Training loss: 1.1958 3.2610 sec/batch\n",
      "Epoch 19/20  Iteration 3350/3560 Training loss: 1.1960 3.1343 sec/batch\n",
      "Epoch 19/20  Iteration 3351/3560 Training loss: 1.1960 3.1739 sec/batch\n",
      "Epoch 19/20  Iteration 3352/3560 Training loss: 1.1962 3.1912 sec/batch\n",
      "Epoch 19/20  Iteration 3353/3560 Training loss: 1.1964 3.0887 sec/batch\n",
      "Epoch 19/20  Iteration 3354/3560 Training loss: 1.1963 3.3692 sec/batch\n",
      "Epoch 19/20  Iteration 3355/3560 Training loss: 1.1960 3.2372 sec/batch\n",
      "Epoch 19/20  Iteration 3356/3560 Training loss: 1.1958 3.0847 sec/batch\n",
      "Epoch 19/20  Iteration 3357/3560 Training loss: 1.1959 3.1094 sec/batch\n",
      "Epoch 19/20  Iteration 3358/3560 Training loss: 1.1958 3.2478 sec/batch\n",
      "Epoch 19/20  Iteration 3359/3560 Training loss: 1.1959 3.0841 sec/batch\n",
      "Epoch 19/20  Iteration 3360/3560 Training loss: 1.1958 3.1523 sec/batch\n",
      "Epoch 19/20  Iteration 3361/3560 Training loss: 1.1958 3.2999 sec/batch\n",
      "Epoch 19/20  Iteration 3362/3560 Training loss: 1.1957 3.0580 sec/batch\n",
      "Epoch 19/20  Iteration 3363/3560 Training loss: 1.1955 3.0749 sec/batch\n",
      "Epoch 19/20  Iteration 3364/3560 Training loss: 1.1956 3.2075 sec/batch\n",
      "Epoch 19/20  Iteration 3365/3560 Training loss: 1.1958 3.3125 sec/batch\n",
      "Epoch 19/20  Iteration 3366/3560 Training loss: 1.1958 3.4009 sec/batch\n",
      "Epoch 19/20  Iteration 3367/3560 Training loss: 1.1957 3.1721 sec/batch\n",
      "Epoch 19/20  Iteration 3368/3560 Training loss: 1.1957 3.1646 sec/batch\n",
      "Epoch 19/20  Iteration 3369/3560 Training loss: 1.1957 3.0825 sec/batch\n",
      "Epoch 19/20  Iteration 3370/3560 Training loss: 1.1957 3.0931 sec/batch\n",
      "Epoch 19/20  Iteration 3371/3560 Training loss: 1.1959 3.1990 sec/batch\n",
      "Epoch 19/20  Iteration 3372/3560 Training loss: 1.1963 3.1705 sec/batch\n",
      "Epoch 19/20  Iteration 3373/3560 Training loss: 1.1963 3.1205 sec/batch\n",
      "Epoch 19/20  Iteration 3374/3560 Training loss: 1.1964 3.1811 sec/batch\n",
      "Epoch 19/20  Iteration 3375/3560 Training loss: 1.1964 3.1843 sec/batch\n",
      "Epoch 19/20  Iteration 3376/3560 Training loss: 1.1963 3.1228 sec/batch\n",
      "Epoch 19/20  Iteration 3377/3560 Training loss: 1.1965 3.4580 sec/batch\n",
      "Epoch 19/20  Iteration 3378/3560 Training loss: 1.1965 3.1564 sec/batch\n",
      "Epoch 19/20  Iteration 3379/3560 Training loss: 1.1965 3.0832 sec/batch\n",
      "Epoch 19/20  Iteration 3380/3560 Training loss: 1.1963 3.0874 sec/batch\n",
      "Epoch 19/20  Iteration 3381/3560 Training loss: 1.1962 3.2077 sec/batch\n",
      "Epoch 19/20  Iteration 3382/3560 Training loss: 1.1964 3.1631 sec/batch\n",
      "Epoch 20/20  Iteration 3383/3560 Training loss: 1.2998 3.1498 sec/batch\n",
      "Epoch 20/20  Iteration 3384/3560 Training loss: 1.2587 3.1417 sec/batch\n",
      "Epoch 20/20  Iteration 3385/3560 Training loss: 1.2394 3.1401 sec/batch\n",
      "Epoch 20/20  Iteration 3386/3560 Training loss: 1.2324 3.1401 sec/batch\n",
      "Epoch 20/20  Iteration 3387/3560 Training loss: 1.2234 3.2321 sec/batch\n",
      "Epoch 20/20  Iteration 3388/3560 Training loss: 1.2106 3.0644 sec/batch\n",
      "Epoch 20/20  Iteration 3389/3560 Training loss: 1.2109 3.1982 sec/batch\n",
      "Epoch 20/20  Iteration 3390/3560 Training loss: 1.2080 3.3382 sec/batch\n",
      "Epoch 20/20  Iteration 3391/3560 Training loss: 1.2073 3.0554 sec/batch\n",
      "Epoch 20/20  Iteration 3392/3560 Training loss: 1.2059 3.1759 sec/batch\n",
      "Epoch 20/20  Iteration 3393/3560 Training loss: 1.2022 3.1941 sec/batch\n",
      "Epoch 20/20  Iteration 3394/3560 Training loss: 1.2016 3.0201 sec/batch\n",
      "Epoch 20/20  Iteration 3395/3560 Training loss: 1.2014 3.1554 sec/batch\n",
      "Epoch 20/20  Iteration 3396/3560 Training loss: 1.2010 3.2648 sec/batch\n",
      "Epoch 20/20  Iteration 3397/3560 Training loss: 1.1998 3.0928 sec/batch\n",
      "Epoch 20/20  Iteration 3398/3560 Training loss: 1.1984 3.1198 sec/batch\n",
      "Epoch 20/20  Iteration 3399/3560 Training loss: 1.1987 3.1528 sec/batch\n",
      "Epoch 20/20  Iteration 3400/3560 Training loss: 1.2001 3.2272 sec/batch\n",
      "Epoch 20/20  Iteration 3401/3560 Training loss: 1.2001 3.1724 sec/batch\n",
      "Epoch 20/20  Iteration 3402/3560 Training loss: 1.2011 3.1883 sec/batch\n",
      "Epoch 20/20  Iteration 3403/3560 Training loss: 1.2008 3.1660 sec/batch\n",
      "Epoch 20/20  Iteration 3404/3560 Training loss: 1.2012 3.1233 sec/batch\n",
      "Epoch 20/20  Iteration 3405/3560 Training loss: 1.2005 3.0680 sec/batch\n",
      "Epoch 20/20  Iteration 3406/3560 Training loss: 1.2007 3.3075 sec/batch\n",
      "Epoch 20/20  Iteration 3407/3560 Training loss: 1.2010 3.0891 sec/batch\n",
      "Epoch 20/20  Iteration 3408/3560 Training loss: 1.1997 3.0913 sec/batch\n",
      "Epoch 20/20  Iteration 3409/3560 Training loss: 1.1983 3.0831 sec/batch\n",
      "Epoch 20/20  Iteration 3410/3560 Training loss: 1.1989 3.1980 sec/batch\n",
      "Epoch 20/20  Iteration 3411/3560 Training loss: 1.1989 3.2345 sec/batch\n",
      "Epoch 20/20  Iteration 3412/3560 Training loss: 1.1990 3.2542 sec/batch\n",
      "Epoch 20/20  Iteration 3413/3560 Training loss: 1.1983 3.2697 sec/batch\n",
      "Epoch 20/20  Iteration 3414/3560 Training loss: 1.1973 3.1283 sec/batch\n",
      "Epoch 20/20  Iteration 3415/3560 Training loss: 1.1972 3.1902 sec/batch\n",
      "Epoch 20/20  Iteration 3416/3560 Training loss: 1.1973 3.1455 sec/batch\n",
      "Epoch 20/20  Iteration 3417/3560 Training loss: 1.1969 3.1213 sec/batch\n",
      "Epoch 20/20  Iteration 3418/3560 Training loss: 1.1967 3.2032 sec/batch\n",
      "Epoch 20/20  Iteration 3419/3560 Training loss: 1.1962 3.1568 sec/batch\n",
      "Epoch 20/20  Iteration 3420/3560 Training loss: 1.1949 3.1424 sec/batch\n",
      "Epoch 20/20  Iteration 3421/3560 Training loss: 1.1939 3.1469 sec/batch\n",
      "Epoch 20/20  Iteration 3422/3560 Training loss: 1.1936 3.6600 sec/batch\n",
      "Epoch 20/20  Iteration 3423/3560 Training loss: 1.1928 3.1850 sec/batch\n",
      "Epoch 20/20  Iteration 3424/3560 Training loss: 1.1934 3.1363 sec/batch\n",
      "Epoch 20/20  Iteration 3425/3560 Training loss: 1.1933 3.1518 sec/batch\n",
      "Epoch 20/20  Iteration 3426/3560 Training loss: 1.1928 3.1769 sec/batch\n",
      "Epoch 20/20  Iteration 3427/3560 Training loss: 1.1931 3.1299 sec/batch\n",
      "Epoch 20/20  Iteration 3428/3560 Training loss: 1.1926 3.1032 sec/batch\n",
      "Epoch 20/20  Iteration 3429/3560 Training loss: 1.1923 3.1335 sec/batch\n",
      "Epoch 20/20  Iteration 3430/3560 Training loss: 1.1918 3.1546 sec/batch\n",
      "Epoch 20/20  Iteration 3431/3560 Training loss: 1.1918 3.0992 sec/batch\n",
      "Epoch 20/20  Iteration 3432/3560 Training loss: 1.1917 3.1573 sec/batch\n",
      "Epoch 20/20  Iteration 3433/3560 Training loss: 1.1911 3.1401 sec/batch\n",
      "Epoch 20/20  Iteration 3434/3560 Training loss: 1.1917 3.3057 sec/batch\n",
      "Epoch 20/20  Iteration 3435/3560 Training loss: 1.1918 3.2930 sec/batch\n",
      "Epoch 20/20  Iteration 3436/3560 Training loss: 1.1919 3.1218 sec/batch\n",
      "Epoch 20/20  Iteration 3437/3560 Training loss: 1.1916 3.1745 sec/batch\n",
      "Epoch 20/20  Iteration 3438/3560 Training loss: 1.1916 3.2158 sec/batch\n",
      "Epoch 20/20  Iteration 3439/3560 Training loss: 1.1918 3.0789 sec/batch\n",
      "Epoch 20/20  Iteration 3440/3560 Training loss: 1.1917 3.1479 sec/batch\n",
      "Epoch 20/20  Iteration 3441/3560 Training loss: 1.1911 3.1556 sec/batch\n",
      "Epoch 20/20  Iteration 3442/3560 Training loss: 1.1915 3.1332 sec/batch\n",
      "Epoch 20/20  Iteration 3443/3560 Training loss: 1.1914 3.0879 sec/batch\n",
      "Epoch 20/20  Iteration 3444/3560 Training loss: 1.1922 3.1205 sec/batch\n",
      "Epoch 20/20  Iteration 3445/3560 Training loss: 1.1924 3.2308 sec/batch\n",
      "Epoch 20/20  Iteration 3446/3560 Training loss: 1.1925 3.1307 sec/batch\n",
      "Epoch 20/20  Iteration 3447/3560 Training loss: 1.1923 3.4880 sec/batch\n",
      "Epoch 20/20  Iteration 3448/3560 Training loss: 1.1926 3.1351 sec/batch\n",
      "Epoch 20/20  Iteration 3449/3560 Training loss: 1.1927 3.1450 sec/batch\n",
      "Epoch 20/20  Iteration 3450/3560 Training loss: 1.1926 3.1481 sec/batch\n",
      "Epoch 20/20  Iteration 3451/3560 Training loss: 1.1928 3.1920 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Iteration 3452/3560 Training loss: 1.1927 3.1643 sec/batch\n",
      "Epoch 20/20  Iteration 3453/3560 Training loss: 1.1934 3.1615 sec/batch\n",
      "Epoch 20/20  Iteration 3454/3560 Training loss: 1.1935 3.1730 sec/batch\n",
      "Epoch 20/20  Iteration 3455/3560 Training loss: 1.1940 3.1669 sec/batch\n",
      "Epoch 20/20  Iteration 3456/3560 Training loss: 1.1937 3.1455 sec/batch\n",
      "Epoch 20/20  Iteration 3457/3560 Training loss: 1.1937 3.0180 sec/batch\n",
      "Epoch 20/20  Iteration 3458/3560 Training loss: 1.1939 3.2509 sec/batch\n",
      "Epoch 20/20  Iteration 3459/3560 Training loss: 1.1938 3.3928 sec/batch\n",
      "Epoch 20/20  Iteration 3460/3560 Training loss: 1.1937 3.0358 sec/batch\n",
      "Epoch 20/20  Iteration 3461/3560 Training loss: 1.1930 3.1918 sec/batch\n",
      "Epoch 20/20  Iteration 3462/3560 Training loss: 1.1929 3.1829 sec/batch\n",
      "Epoch 20/20  Iteration 3463/3560 Training loss: 1.1924 3.1761 sec/batch\n",
      "Epoch 20/20  Iteration 3464/3560 Training loss: 1.1923 3.2138 sec/batch\n",
      "Epoch 20/20  Iteration 3465/3560 Training loss: 1.1917 3.1320 sec/batch\n",
      "Epoch 20/20  Iteration 3466/3560 Training loss: 1.1917 3.1284 sec/batch\n",
      "Epoch 20/20  Iteration 3467/3560 Training loss: 1.1914 3.1417 sec/batch\n",
      "Epoch 20/20  Iteration 3468/3560 Training loss: 1.1914 3.1277 sec/batch\n",
      "Epoch 20/20  Iteration 3469/3560 Training loss: 1.1912 3.1144 sec/batch\n",
      "Epoch 20/20  Iteration 3470/3560 Training loss: 1.1910 3.2521 sec/batch\n",
      "Epoch 20/20  Iteration 3471/3560 Training loss: 1.1907 3.1999 sec/batch\n",
      "Epoch 20/20  Iteration 3472/3560 Training loss: 1.1908 3.1744 sec/batch\n",
      "Epoch 20/20  Iteration 3473/3560 Training loss: 1.1905 3.0943 sec/batch\n",
      "Epoch 20/20  Iteration 3474/3560 Training loss: 1.1904 3.0981 sec/batch\n",
      "Epoch 20/20  Iteration 3475/3560 Training loss: 1.1900 3.1450 sec/batch\n",
      "Epoch 20/20  Iteration 3476/3560 Training loss: 1.1897 3.1351 sec/batch\n",
      "Epoch 20/20  Iteration 3477/3560 Training loss: 1.1896 3.1715 sec/batch\n",
      "Epoch 20/20  Iteration 3478/3560 Training loss: 1.1897 3.2021 sec/batch\n",
      "Epoch 20/20  Iteration 3479/3560 Training loss: 1.1897 3.3712 sec/batch\n",
      "Epoch 20/20  Iteration 3480/3560 Training loss: 1.1894 3.4143 sec/batch\n",
      "Epoch 20/20  Iteration 3481/3560 Training loss: 1.1890 3.1739 sec/batch\n",
      "Epoch 20/20  Iteration 3482/3560 Training loss: 1.1887 3.0601 sec/batch\n",
      "Epoch 20/20  Iteration 3483/3560 Training loss: 1.1887 3.3395 sec/batch\n",
      "Epoch 20/20  Iteration 3484/3560 Training loss: 1.1886 3.0922 sec/batch\n",
      "Epoch 20/20  Iteration 3485/3560 Training loss: 1.1885 3.1019 sec/batch\n",
      "Epoch 20/20  Iteration 3486/3560 Training loss: 1.1884 3.2407 sec/batch\n",
      "Epoch 20/20  Iteration 3487/3560 Training loss: 1.1884 3.1408 sec/batch\n",
      "Epoch 20/20  Iteration 3488/3560 Training loss: 1.1883 3.1071 sec/batch\n",
      "Epoch 20/20  Iteration 3489/3560 Training loss: 1.1882 3.2817 sec/batch\n",
      "Epoch 20/20  Iteration 3490/3560 Training loss: 1.1882 3.1588 sec/batch\n",
      "Epoch 20/20  Iteration 3491/3560 Training loss: 1.1880 3.1079 sec/batch\n",
      "Epoch 20/20  Iteration 3492/3560 Training loss: 1.1880 3.4019 sec/batch\n",
      "Epoch 20/20  Iteration 3493/3560 Training loss: 1.1878 3.1922 sec/batch\n",
      "Epoch 20/20  Iteration 3494/3560 Training loss: 1.1877 3.0820 sec/batch\n",
      "Epoch 20/20  Iteration 3495/3560 Training loss: 1.1876 3.1398 sec/batch\n",
      "Epoch 20/20  Iteration 3496/3560 Training loss: 1.1875 3.1706 sec/batch\n",
      "Epoch 20/20  Iteration 3497/3560 Training loss: 1.1873 3.1840 sec/batch\n",
      "Epoch 20/20  Iteration 3498/3560 Training loss: 1.1869 3.1092 sec/batch\n",
      "Epoch 20/20  Iteration 3499/3560 Training loss: 1.1869 3.1894 sec/batch\n",
      "Epoch 20/20  Iteration 3500/3560 Training loss: 1.1869 3.2242 sec/batch\n",
      "Epoch 20/20  Iteration 3501/3560 Training loss: 1.1868 3.0666 sec/batch\n",
      "Epoch 20/20  Iteration 3502/3560 Training loss: 1.1867 3.0848 sec/batch\n",
      "Epoch 20/20  Iteration 3503/3560 Training loss: 1.1867 3.2572 sec/batch\n",
      "Epoch 20/20  Iteration 3504/3560 Training loss: 1.1864 3.3822 sec/batch\n",
      "Epoch 20/20  Iteration 3505/3560 Training loss: 1.1860 3.0699 sec/batch\n",
      "Epoch 20/20  Iteration 3506/3560 Training loss: 1.1859 3.2541 sec/batch\n",
      "Epoch 20/20  Iteration 3507/3560 Training loss: 1.1858 3.1148 sec/batch\n",
      "Epoch 20/20  Iteration 3508/3560 Training loss: 1.1855 3.0941 sec/batch\n",
      "Epoch 20/20  Iteration 3509/3560 Training loss: 1.1855 3.2143 sec/batch\n",
      "Epoch 20/20  Iteration 3510/3560 Training loss: 1.1854 3.1291 sec/batch\n",
      "Epoch 20/20  Iteration 3511/3560 Training loss: 1.1852 3.1228 sec/batch\n",
      "Epoch 20/20  Iteration 3512/3560 Training loss: 1.1849 3.6160 sec/batch\n",
      "Epoch 20/20  Iteration 3513/3560 Training loss: 1.1845 3.3313 sec/batch\n",
      "Epoch 20/20  Iteration 3514/3560 Training loss: 1.1844 3.1394 sec/batch\n",
      "Epoch 20/20  Iteration 3515/3560 Training loss: 1.1846 3.1398 sec/batch\n",
      "Epoch 20/20  Iteration 3516/3560 Training loss: 1.1847 3.1501 sec/batch\n",
      "Epoch 20/20  Iteration 3517/3560 Training loss: 1.1847 3.4164 sec/batch\n",
      "Epoch 20/20  Iteration 3518/3560 Training loss: 1.1848 3.2469 sec/batch\n",
      "Epoch 20/20  Iteration 3519/3560 Training loss: 1.1850 3.0976 sec/batch\n",
      "Epoch 20/20  Iteration 3520/3560 Training loss: 1.1852 3.1324 sec/batch\n",
      "Epoch 20/20  Iteration 3521/3560 Training loss: 1.1852 3.2418 sec/batch\n",
      "Epoch 20/20  Iteration 3522/3560 Training loss: 1.1852 3.1200 sec/batch\n",
      "Epoch 20/20  Iteration 3523/3560 Training loss: 1.1856 3.1861 sec/batch\n",
      "Epoch 20/20  Iteration 3524/3560 Training loss: 1.1857 3.1311 sec/batch\n",
      "Epoch 20/20  Iteration 3525/3560 Training loss: 1.1857 3.1909 sec/batch\n",
      "Epoch 20/20  Iteration 3526/3560 Training loss: 1.1860 3.0621 sec/batch\n",
      "Epoch 20/20  Iteration 3527/3560 Training loss: 1.1859 3.0861 sec/batch\n",
      "Epoch 20/20  Iteration 3528/3560 Training loss: 1.1861 3.2394 sec/batch\n",
      "Epoch 20/20  Iteration 3529/3560 Training loss: 1.1862 3.2359 sec/batch\n",
      "Epoch 20/20  Iteration 3530/3560 Training loss: 1.1864 3.1681 sec/batch\n",
      "Epoch 20/20  Iteration 3531/3560 Training loss: 1.1865 3.2979 sec/batch\n",
      "Epoch 20/20  Iteration 3532/3560 Training loss: 1.1864 3.0859 sec/batch\n",
      "Epoch 20/20  Iteration 3533/3560 Training loss: 1.1861 3.0470 sec/batch\n",
      "Epoch 20/20  Iteration 3534/3560 Training loss: 1.1860 3.2406 sec/batch\n",
      "Epoch 20/20  Iteration 3535/3560 Training loss: 1.1861 3.3903 sec/batch\n",
      "Epoch 20/20  Iteration 3536/3560 Training loss: 1.1860 3.0865 sec/batch\n",
      "Epoch 20/20  Iteration 3537/3560 Training loss: 1.1860 3.1657 sec/batch\n",
      "Epoch 20/20  Iteration 3538/3560 Training loss: 1.1860 3.1513 sec/batch\n",
      "Epoch 20/20  Iteration 3539/3560 Training loss: 1.1860 3.0687 sec/batch\n",
      "Epoch 20/20  Iteration 3540/3560 Training loss: 1.1859 3.4943 sec/batch\n",
      "Epoch 20/20  Iteration 3541/3560 Training loss: 1.1857 3.2179 sec/batch\n",
      "Epoch 20/20  Iteration 3542/3560 Training loss: 1.1858 3.0941 sec/batch\n",
      "Epoch 20/20  Iteration 3543/3560 Training loss: 1.1859 3.0981 sec/batch\n",
      "Epoch 20/20  Iteration 3544/3560 Training loss: 1.1860 3.2532 sec/batch\n",
      "Epoch 20/20  Iteration 3545/3560 Training loss: 1.1859 3.1152 sec/batch\n",
      "Epoch 20/20  Iteration 3546/3560 Training loss: 1.1859 3.1633 sec/batch\n",
      "Epoch 20/20  Iteration 3547/3560 Training loss: 1.1859 3.1817 sec/batch\n",
      "Epoch 20/20  Iteration 3548/3560 Training loss: 1.1858 3.1276 sec/batch\n",
      "Epoch 20/20  Iteration 3549/3560 Training loss: 1.1860 3.1095 sec/batch\n",
      "Epoch 20/20  Iteration 3550/3560 Training loss: 1.1863 3.1336 sec/batch\n",
      "Epoch 20/20  Iteration 3551/3560 Training loss: 1.1863 3.1309 sec/batch\n",
      "Epoch 20/20  Iteration 3552/3560 Training loss: 1.1864 3.3868 sec/batch\n",
      "Epoch 20/20  Iteration 3553/3560 Training loss: 1.1863 3.1681 sec/batch\n",
      "Epoch 20/20  Iteration 3554/3560 Training loss: 1.1862 3.1776 sec/batch\n",
      "Epoch 20/20  Iteration 3555/3560 Training loss: 1.1864 3.1329 sec/batch\n",
      "Epoch 20/20  Iteration 3556/3560 Training loss: 1.1864 3.1538 sec/batch\n",
      "Epoch 20/20  Iteration 3557/3560 Training loss: 1.1865 3.2082 sec/batch\n",
      "Epoch 20/20  Iteration 3558/3560 Training loss: 1.1864 3.1906 sec/batch\n",
      "Epoch 20/20  Iteration 3559/3560 Training loss: 1.1862 3.1485 sec/batch\n",
      "Epoch 20/20  Iteration 3560/3560 Training loss: 1.1864 3.1780 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 100\n",
    "num_steps = 100\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "for lstm_size in [128,256,512]:\n",
    "    for num_layers in [1, 2]:\n",
    "        for learning_rate in [0.002, 0.001]:\n",
    "            log_string = 'logs/4/lr={},rl={},ru={}'.format(learning_rate, num_layers, lstm_size)\n",
    "            writer = tf.summary.FileWriter(log_string)\n",
    "            model = build_rnn(len(vocab), \n",
    "                    batch_size=batch_size,\n",
    "                    num_steps=num_steps,\n",
    "                    learning_rate=learning_rate,\n",
    "                    lstm_size=lstm_size,\n",
    "                    num_layers=num_layers)\n",
    "            \n",
    "            train(model, epochs, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/anna/i1780_l512_1.256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i200_l512_2.436.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i400_l512_1.992.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i600_l512_1.758.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i800_l512_1.604.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1000_l512_1.485.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1200_l512_1.407.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1400_l512_1.348.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1600_l512_1.306.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1780_l512_1.256.ckpt\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/anna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    prime = \"Far\"\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/anna/i3560_l512_1.122.ckpt\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-21-d00abe777cd1>\", line 2, in <module>\n    samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n  File \"<ipython-input-19-2da60cc9b044>\", line 5, in sample\n    saver = tf.train.Saver()\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1284, in __init__\n    self.build()\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1296, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1333, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 781, in _build_internal\n    restore_sequentially, reshape)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 400, in _AddRestoreOps\n    restore_sequentially)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 832, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1463, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-d00abe777cd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"checkpoints/anna/i3560_l512_1.122.ckpt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Far\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-2da60cc9b044>\u001b[0m in \u001b[0;36msample\u001b[0;34m(checkpoint, n_samples, lstm_size, vocab_size, prime)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprime\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1766\u001b[0m         \u001b[0mshould_reraise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mshould_reraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception_traceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1769\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mexception_traceback\u001b[0m  \u001b[0;31m# avoid reference cycles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1750\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1752\u001b[0;31m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1753\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1754\u001b[0m       \u001b[0mexception_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-21-d00abe777cd1>\", line 2, in <module>\n    samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n  File \"<ipython-input-19-2da60cc9b044>\", line 5, in sample\n    saver = tf.train.Saver()\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1284, in __init__\n    self.build()\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1296, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1333, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 781, in _build_internal\n    restore_sequentially, reshape)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 400, in _AddRestoreOps\n    restore_sequentially)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 832, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1463, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/Users/xlnwel/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i3560_l512_1.122.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i200_l512_2.432.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i600_l512_1.750.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i1000_l512_1.484.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
